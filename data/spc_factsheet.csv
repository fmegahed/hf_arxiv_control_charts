is_spc_paper,chart_family,chart_statistic,phase,application_domain,assumes_normality,handles_autocorrelation,handles_missing_data,evaluation_type,performance_metrics,sample_size_requirements,code_used,software_platform,code_availability_source,software_urls,summary,key_equations,key_results,limitations_stated,limitations_unstated,future_work_stated,future_work_unstated,id,pdf_path,llm_provider,llm_model,repeat_id,extracted_at
TRUE,Multivariate|Bayesian,EWMA|Other,Both,Finance/economics|Manufacturing (general),TRUE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Other,Uses a two-phase scheme with a historical Phase I sample of size N* (examples: N*=150 for LME differenced series; N*=180 for production data). No general minimum-sample-size rule is given beyond requiring enough Phase I data for stable parameter estimation/model identification.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a new multivariate SPC chart for vector autocorrelated/serially correlated processes using a Bayesian multivariate local level (state space) model fitted via discount weighted regression (DWR). It compares the predictive one-step-ahead error density from the Bayesian model to a specified in-control (target) error distribution using sequential Bayes factors, and charts the log Bayes factor (LBF) with a univariate modified EWMA designed for autocorrelated data. The approach is intended to detect departures in the mean vector, covariance/dispersion matrix, or more generally the entire target distribution (via mismatch between predictive and target densities). Phase I fits/tunes the DWR model and identifies an AR/ARMA dependence model for LBF to set modified-EWMA limits to achieve a desired in-control ARL; Phase II monitors new observations. Two real-data examples (London Metal Exchange prices and a 5-dimensional plastic mould temperature series) illustrate detection of volatility/dispersion changes and sustained out-of-control behavior.","State-space/local level model: $y_t=\mu_t+\varepsilon_t$, $\mu_t=\mu_{t-1}+\omega_t$, with $\varepsilon_t\sim N_p(0,\Sigma)$ and $\omega_t\sim N_p(0,\Omega_t\Sigma)$ (discount-factor-driven). Sequential Bayes factor compares predictive error density to target error density: $BF(t)=f_e(e_t\mid\Sigma=S_{t-1},\mathbf y_{t-1})/f_\epsilon(\epsilon_t)$, yielding a closed form (Eq. 6) and $LBF(t)=\log BF(t)$ (Eq. 7). Charting uses modified EWMA on $x_t=LBF(t)$: $z_t=\lambda x_t+(1-\lambda)z_{t-1}$ with limits $\mu_z\pm c\sigma_z$; for AR(1) $x_t=\phi x_{t-1}+\nu_t$, $\sigma_z^2=\frac{\sigma^2\lambda\{1+\phi(1-\lambda)\}}{(1-\phi^2)(2-\lambda)\{1-\phi(1-\lambda)\}}$.","A theorem establishes convergence properties for the DWR estimators: $S_t\xrightarrow{P}\Sigma$ and $P_t\to P=(\sqrt{\delta^2+4}-\delta)/2$, supporting stable long-run forecasting variance and simplifying design. In the LME example, Phase I uses $t=1$–150 and Phase II $t=151$–210; an AR(1) dependence with $\phi\approx0.1$ is used for LBF, and with target in-control ARL 370.4 (for $\lambda=0.05,\phi=0.1$) they use $c=2.469$; charts with $\lambda=0.2$ and $0.5$ signal out-of-control at $t=172$ while $\lambda=0.05$ and $0.1$ do not. In the production example (Phase I $t=1$–180, Phase II $t=181$–276), they fit AR(1) $LBF(t)=-4.624+0.062LBF(t-1)+\nu_t$ (intercept removed for in-control centering) and the modified EWMA indicates the process is largely out of control in Phase II, consistent with prior residual-chart findings.","The paper notes that comparisons to standard multivariate charts (Hotelling’s $T^2$, M-EWMA) are difficult because the proposed method is model-based and results can be confounded by time-series model misspecification; a fair comparison would require removing model-uncertainty effects and is left for a separate study. It also acknowledges that the LBF process is both autocorrelated and non-normal, making chart design more challenging. It cautions that large EWMA smoothing (e.g., $\lambda=0.5$) may not ensure correct limits unless normality is strongly supported, recommending in practice $\lambda\le 0.2$.","The method’s Bayes-factor expression and target comparison are derived under (multivariate) normal predictive/target densities; robustness beyond roughly symmetric in-control LBF is not fully characterized, and heavy tails or skewness could affect false-alarm properties. Phase I requires fitting both a multivariate state-space model and an AR/ARMA model for LBF, adding modeling burden and potential sensitivity to specification/estimation choices (e.g., discount factor $\delta$, AR order). Reported performance focuses on illustrative cases and selected ARL table choices rather than a broad simulation comparison against established multivariate time-series SPC methods across varied shift sizes/types and dependence strengths.","The authors suggest extending beyond modified EWMA to other univariate charts once LBF is obtained, including modified CUSUM and nonparametric control charts. They also note that, while they used normal predictive/target densities for simplicity, the framework can be applied with other analytic-form densities, implying extension to non-Gaussian targets/predictives. They highlight that designing charts for an autocorrelated and non-normal Bayes-factor process is a key challenge that can motivate further research.","Develop systematic design/calibration methods for modified-EWMA limits under estimated ARMA dependence (and non-normal LBF), e.g., via bootstrap or Bayesian posterior predictive calibration to better control in-control ARL. Provide comprehensive benchmarking versus residual-based state-space charts, ARMA+Hotelling/MCUSUM/MEWMA approaches, and modern change-point detectors across multiple dependence structures and shift scenarios (mean, covariance, tail behavior). Release reference software (R/Python) implementing DWR fitting, LBF computation, ARMA identification, and chart design to improve reproducibility and adoption, and extend to missing/irregularly sampled multivariate series common in practice.",0802.0218v1,local_papers/arxiv/0802.0218v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:41:37Z
TRUE,Univariate|Other,Shewhart|Other,Both,Food/agriculture,TRUE,TRUE,FALSE,Simulation study|Other,Other,Not discussed.,TRUE,Other,Not provided,NA,"The paper proposes a methodology to integrate Automated Process Control (APC/CAP) with Statistical Process Control (SPC/MSP) for a dynamic continuous process, illustrated by gluten addition in flour to maintain protein content near a target. The incoming raw-material protein content is modeled as an AR(1) process with measurement error on the analyzed output, and the authors advocate building SPC charts on model residuals rather than on the autocorrelated raw series. They propose using an individuals–moving range Shewhart chart (X, Rm) on AR(1) residuals to verify assumptions (normality, lack of autocorrelation) and to detect assignable causes affecting the input stream. On the APC side, they derive a new feedback control rule inspired by a first-order system (discretized transfer function), generalizing the Fearn & Marris (1990) adjustment rule via tunable parameters (λ1, λ2). Simulation comparisons (30 replications) show the proposed rule improves centering and reduces variability of the output protein measurement relative to the literature rule, supporting the benefit of integrating feedback adjustment with SPC monitoring/diagnostics.","Process/output model: $Y_i = X_i + u_{i-1}$ (after approximation), and measurement model $Z_i = Y_i + \varepsilon_i$ with $\varepsilon_i\sim N(0,\sigma_\varepsilon^2)$. Input dynamics: AR(1) $X_i-\mu = \phi(X_{i-1}-\mu)+\upsilon_i$ with $\upsilon_i\sim N(0,\sigma_\nu^2)$. Control rules: Fearn & Marris rule $u_i = u_{i-1}-(Z_i-\tau)$; proposed generalized first-order discrete rule $u_i-u_0=\lambda_1(u_{i-1}-u_0)-\lambda_2(Z_i-\tau)$ (recovering Fearn & Marris when $\lambda_1=\lambda_2=1$). Residuals for SPC charting: $\hat x_i = X_i-\hat\phi X_{i-1}-\hat C$ plotted on an individuals/moving-range chart.","Simulations used parameters $X_0=\mu=10$, $u_0=6$, $\phi=0.7$, $\tau=16$, $\sigma_\nu^2=0.5$, $\sigma_\varepsilon^2=0.5$, with 30 replications per rule and results averaged. Under the Fearn & Marris (1990) rule, the output had $E(Z_i)=16.23$ and $V(Z_i)=1.11$ (less centered, more variable). Under the proposed rule with $\lambda_1=\lambda_2=0.5$, the output improved to $E(Z_i)=16.01$ and $V(Z_i)=0.87$ (closer to target and reduced variability). For the input AR(1) modeling step, EViews estimation reported $\hat\phi\approx 0.669$ and $\hat C\approx 8.856$, and residual normality was checked via Jarque–Bera; an individuals/moving-range chart of residuals showed at least one outlying point indicating an assignable cause.",The authors note that this work is limited to the control-rule aspect of their broader integration architecture (“nous nous limitons à la règle de contrôle”). They also remark that it is necessary to verify that the coefficients of the raw-material arrival model remain constant over time when applying the residual-based charting approach.,"The SPC component is described at a high level (residual I–MR charting) without specifying control-limit calculations, in-control ARL design targets, or how estimation uncertainty in $\hat\phi$ and $\hat C$ impacts false-alarm rates. Performance evaluation focuses on mean/variance of $Z_i$ rather than standard SPC detection metrics (e.g., ARL/ATS), making it hard to assess monitoring effectiveness. The simulation study appears tied to one parameter setting and a single AR(1) structure; robustness to non-normal innovations, model misspecification, or stronger autocorrelation is not demonstrated. Implementation details (e.g., sampling interval choice, actuator constraints/costs, stability margins) and reproducible code are not provided.","They propose optimizing the configuration of the new control rule by selecting optimal values of the coefficients $\lambda_1$ and $\lambda_2$. They also plan to validate the integration structure for more general input models beyond AR(1), such as ARMA and ARIMA families, and mention further study of stability behavior using the transfer function viewpoint.","Derive and report SPC performance measures (in-control/out-of-control ARL or ATS) for the residual chart under parameter estimation and model uncertainty, and provide guidance for setting chart limits. Extend the integrated framework to multivariate quality characteristics (e.g., multiple composition variables) and to constraints/penalties on control actions (economic design of adjustment + chart). Provide open, reproducible simulation/implementation code and a real industrial case study to validate benefits beyond synthetic data. Investigate robustness to non-normal, heavy-tailed, and time-varying dynamics (e.g., regime shifts) and develop adaptive or self-starting versions that update AR parameters online.",0804.4325v1,local_papers/arxiv/0804.4325v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:42:17Z
TRUE,Univariate|Nonparametric,Shewhart|Other,Both,Manufacturing (general),FALSE,FALSE,NA,Approximation methods|Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|Detection probability|False alarm rate|Steady-state ARL|Conditional expected delay|Other,Not discussed (tables provide example designs for various reference-sample sizes m and test-sample sizes n; recommendations include using LR-approximation for m ≥ 50 and n ≥ 5; and faster approximations FR/FA for very large reference samples such as m ≥ 1000).,TRUE,Other,Personal website,http://www.win.tue.nl/~markvdw,"The paper proposes a Phase II, two-sided Shewhart-type nonparametric control chart for detecting location shifts using the Mann–Whitney (Wilcoxon rank-sum) statistic computed between a fixed Phase I reference sample of size m and each Phase II test sample of size n. Because successive statistics reuse the same reference sample, the authors derive in-control and out-of-control performance via conditioning on the reference sample, yielding distribution-free in-control properties for continuous distributions and enabling computation of unconditional ARL as an expectation of the conditional geometric run length. To implement the chart, they develop fast methods to compute control limits targeting a specified ARL0 (e.g., 370, 400, 500), using exact probability-generating-function calculations when feasible and Lugannani–Rice saddlepoint (plus Edgeworth/normal) approximations and Monte Carlo integration otherwise. Simulation comparisons show the proposed chart has much better in-control performance than the classical Shewhart X̄ chart with estimated limits (notably higher 5th percentiles of conditional ARL0), while being nearly as effective under normality and more powerful under heavy-tailed (Laplace) and skewed (Gamma) distributions. A piston-ring manufacturing dataset illustrates implementation and signaling behavior, and interactive software is provided online for computing limits and plotting the chart.","The charting statistic for each Phase II sample is the Mann–Whitney count $M_{XY}=\sum_{i=1}^m\sum_{j=1}^n I(X_i<Y_j)$, which lies in $[0,mn]$; the two-sided chart signals if $M_{XY}<L_{mn}$ or $M_{XY}>U_{mn}$ with $L_{mn}=mn-U_{mn}$ under in-control symmetry. Using conditioning on the Phase I reference sample $X=x$, the (unconditional) ARL is $\mathrm{ARL}=E\{1/p_G(x)\}$ where $p_G(x)=P(M_{xY}<mn-U_{mn})+P(M_{xY}>U_{mn})$ and, given $x$, the run length is geometric. Tail probabilities needed for $p_G(x)$ are computed via the pgf $H_2(z)=(\sum_{l=0}^m a_l z^l)^n$ (exact) or approximated using the Lugannani–Rice saddlepoint formula for $P(M_{xY}>U_{mn})$.","For ARL0 targeting and computation, Table 1 shows the Lugannani–Rice (LR) approximation yields ARL0 estimates close to 500 across many (m,n) settings with much lower computing time than exact enumeration (e.g., m=100,n=10: LR ARL0≈506 vs exact≈505; exact time ~1920s vs LR ~47s for K=1000). In-control comparisons (Table 4) at n=5 and ARL0=500 show the MW chart has substantially larger 5th percentiles of conditional ARL0(X) than the Shewhart X̄ chart for small-to-moderate m (e.g., m=50: MW 5th perc=97 vs Shewhart=49; m=150: 251 vs 154), indicating fewer unexpectedly short in-control runs when parameters are estimated. Out-of-control simulations indicate MW is almost as effective as X̄ under Normal shifts but clearly better under Laplace and Gamma(2,2) shift alternatives (lower unconditional ARLδ and lower 95th percentile of ARLδ(X) across shifts). A worked design example for m=375,n=7,target ARL0=400 yields UCL=2139 and LCL=486 with attained ARL0≈394.5; the 5th percentile of conditional ARL0(X) at this design is ≈252.8.",None stated.,"The distribution-free guarantee is stated for continuous distributions; performance and validity under ties/discrete measurements (common in practice) are not developed. The method assumes independent test samples over time; no explicit treatment of autocorrelation, batch-to-batch dependence, or other process dynamics is provided, which can substantially affect run-length properties. Implementation relies on Monte Carlo integration and saddlepoint/Edgeworth approximations whose accuracy may degrade in extreme tails or for certain (m,n) combinations, and the paper does not provide a fully general bound/guarantee on approximation error for control-limit calibration.","The authors propose further work on (i) a detailed treatment of nonparametric individuals charts (n=1) using fast exact computations, and (ii) developing nonparametric monitoring for dispersion/scale (and potentially combined location–scale monitoring) using a nonparametric test for scale. They also discuss alternative chart design criteria based on lower percentiles of the conditional ARL distribution or on $P_0(N\le T)$ to avoid very short in-control runs, and note these criteria are supported in their software.","Extending the MW chart to explicitly handle autocorrelated observations (e.g., via residual-based monitoring or block/bootstrap calibration) would improve applicability in many industrial time-series settings. Developing robust tie-handling and discrete-data variants (midranks/permutation-based calibration) would broaden use in modern digital measurement systems. Providing open-source implementations (e.g., R/Python packages) and large-scale benchmarking against modern robust parametric and nonparametric EWMA/CUSUM variants (including self-starting versions) would strengthen practical adoption and comparative evidence.",0805.2292v1,local_papers/arxiv/0805.2292v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:42:55Z
TRUE,Multivariate|High-dimensional|Other,Shewhart|CUSUM|EWMA|Hotelling T-squared|MEWMA|MCUSUM|Other,Both,Manufacturing (general)|Semiconductor/electronics|Finance/economics|Other,TRUE,NA,NA,Case study (real dataset)|Other,False alarm rate|Other,Not discussed.,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is a short overview of Multivariate Statistical Process Control (MSPC) with emphasis on the practical “interpretation problem,” i.e., diagnosing which variable(s) drive an out-of-control signal from a multivariate chart. It reviews major families of MSPC charts for monitoring the multivariate mean and variability, including Hotelling’s $T^2$ (Mahalanobis-distance) Shewhart-type charts, MCUSUM and MEWMA schemes, and notes dimensionality-reduction approaches (PCA/PLS) for high-dimensional, collinear process data. For interpretation after a signal, it surveys methods such as Bonferroni/simultaneous intervals, (bi)variate elliptical regions, decomposition/partitioning of $T^2$, PCA score diagnostics and contribution plots, and graphical tools (e.g., MP charts, biplots, radial plots, Andrews curves). The paper includes an industrial chemical-process example with three correlated variables: Phase I checks multivariate normality and correlation, estimates parameters, and Phase II monitoring with a $T^2$ chart flags an out-of-control point, followed by PCA-based/graphical diagnostics to identify the responsible variable. The contribution is primarily integrative and application-oriented, synthesizing MSPC monitoring and interpretation techniques and highlighting open research areas (robust, nonparametric, and attribute-based multivariate charts).","For multivariate Shewhart monitoring of the mean, the core statistic is Hotelling’s $T^2$ / Mahalanobis distance. With known parameters, $T_i^2 = n(\bar{\mathbf X}_i-\boldsymbol\mu)'\boldsymbol\Sigma^{-1}(\bar{\mathbf X}_i-\boldsymbol\mu)$ for subgroup $i$, which under multivariate normality follows a $\chi^2_p$ distribution. With parameters estimated from Phase I data, $T_i^2 = n(\bar{\mathbf X}_i-\bar{\mathbf X})'\mathbf S^{-1}(\bar{\mathbf X}_i-\bar{\mathbf X})$, with control limits based on an $F$ distribution (scaled) under standard assumptions; variability monitoring is described via generalized variance $|\mathbf S_i|$ or trace $\mathrm{tr}(\mathbf S_i)$.","In the chemical-process example (three variables), a Phase I $T^2$ chart indicates the process is in control up to time point 100 (with reported UCL about 14.16). In Phase II, the $T^2$ chart signals an out-of-control condition at the 101st observation. Using two PCA/graphical interpretation procedures cited from Maravelakis et al. (2002) and Maravelakis & Bersimis, the paper concludes that the third variable is responsible for the signal at that time point. The article does not report ARL/ATS comparisons or numerical detection-delay benchmarks; results are presented mainly as diagnostic identification in a case example.","The authors note that interpreting an out-of-control signal in multivariate charts remains an open and challenging area needing further investigation. They also point out practical limitations of traditional multivariate Shewhart/CUSUM/EWMA schemes in high-dimensional systems with collinearity, motivating the use of projection methods such as PCA/PLS.","As an overview paper, it does not provide a systematic performance evaluation (e.g., ARL/ATS/SDRL) across competing methods, so practical superiority claims are not quantitatively established. The worked example relies on multivariate normality and (implicitly) independence assumptions; robustness to non-normality, autocorrelation, and parameter-estimation uncertainty in Phase II is not analyzed. Guidance on Phase I sample size, tuning choices (e.g., MCUSUM/MEWMA parameters, PCA component selection), and how sensitive interpretation methods are to those choices is not developed. Although software tools are mentioned (e.g., Minitab macro in cited work), the paper itself provides no implementation details to support reproducibility.","The paper suggests further research on robust design of multivariate control charts, development of nonparametric multivariate control charts, and research on multivariate attributes control charts. It also explicitly highlights the interpretation/diagnosis of out-of-control multivariate signals as an open research area requiring more work.","Develop self-starting and adaptive MSPC procedures that integrate Phase I parameter uncertainty and update estimates online while controlling false-alarm risk. Extend interpretation tools to autocorrelated and dynamic processes (e.g., using state-space or dynamic PCA/PLS) and quantify diagnostic accuracy (probability of correct variable identification) under common shift scenarios. Provide standardized benchmarks and comprehensive simulation studies comparing interpretation methods (e.g., $T^2$ decomposition vs. contribution plots vs. graphical displays) under non-normality, missing data, and high-dimensional settings. Release open-source implementations (R/Python) for key interpretation techniques to improve adoption and reproducibility.",0901.2880v1,local_papers/arxiv/0901.2880v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:43:19Z
TRUE,Univariate|Nonparametric,CUSUM,Phase II,Manufacturing (general)|Other,FALSE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length),"Requires Phase I in-control data to estimate the in-control distribution via (smoothed) bootstrap; simulations use m = 1000 Phase I observations (and note results are satisfactory for m ≈ 100, improving with larger m). In the aluminum case study, the first 50 observations are used as Phase I data.",TRUE,R,Not provided,http://www.world-aluminium.org/About+Aluminium/Production/Smelting/index.html,"The paper proposes a distribution-free Phase II univariate CUSUM procedure for detecting upward location shifts when both the in-control distribution F and out-of-control distribution G are unknown. The key idea is to use time-varying (sprint-length–dependent) control limits h_j based on the conditional distribution of the CUSUM statistic C_n given the sprint length T_n (time since last reset to zero), rather than a single fixed limit. These conditional distributions, and hence the sequence of limits up to a chosen j_max (plus a final limit h* for T_n>j_max), are estimated using a smoothed bootstrap from Phase I in-control data, followed by an iterative calibration step to match a nominal in-control ARL0. Simulation studies show the classical normal-theory CUSUM can have severely miscalibrated in-control ARL under skewed/heavy-tailed F, while the bootstrap method keeps the actual in-control ARL close to ARL0 with competitive out-of-control performance. A real-data application to aluminum smeltering residuals (after pre-whitening) demonstrates practical use and shows detected shifts in impurity-related variables.","The baseline one-sided CUSUM is defined by $C_0=0$ and $C_n=\max(C_{n-1}+X_n-k,0)$, signaling when $C_n$ exceeds a control limit. The sprint length is $T_n=0$ if $C_n=0$, otherwise $T_n=j$ if $C_n,\ldots,C_{n-j+1}\neq 0$ and $C_{n-j}=0$; control limits depend on $\Pr(C_n\mid T_n=j)$. Limits $h_j$ (for $j\le j_{\max}$) and $h^*$ (for $T_n>j_{\max}$) are estimated by (smoothed) bootstrap sampling from $\hat F$ and then calibrated iteratively so the estimated in-control ARL matches the nominal $\text{ARL}_0=E_F\inf\{n>0:C_n>h\}$ (generalized to varying limits).","When F is known and skewed, the classical normal CUSUM’s actual in-control ARL can be far from nominal (ARL0=200): in Case II (right-skew), ARL≈669.67; in Case III (left-skew), ARL≈119.84, whereas the proposed method stays near nominal (e.g., 207.11 and 194.79, respectively). In the unknown-F setting with Phase I size m=1000 and ARL0=200, the bootstrap CUSUM variants typically achieve in-control ARLs within about one standard error of 200 across normal and skewed cases, while classical/nonparametric comparators can be substantially miscalibrated. Example: Case I (normal), δ=0.5, jmax=50 gives ARL1 around 17–19 for bootstrap variants versus 19.08 for classical; for δ=1 and moderate/large jmax, bootstrap variants can significantly outperform the classical CUSUM in some fixed-n1 scenarios (paired t-test p<0.001 reported for one comparison). In the aluminum smeltering case study (after pre-whitening), bootstrap CUSUM B2 signals shifts at time points 61 (SiO2), 83 (Fe2O3), and 62 (MgO) using the first 50 points as Phase I.","The authors note (i) because the method does not use a specific out-of-control model G, it may be less sensitive than the optimal classical CUSUM in the fully Normal, known-parameter setting; (ii) performance depends on how well $\hat F$ (estimated via kernel density smoothing) approximates F, so a moderate Phase I sample is required; and (iii) setup requires substantial computation (they report ~20 seconds in a typical example on an 800 MHz dual-processor Pentium III).","The method assumes independent observations in Phase II (autocorrelation is handled only via pre-whitening in the case study), so performance may degrade if dependence remains or pre-whitening is misspecified. The bootstrap calibration is simulation-heavy and depends on tuning choices (j_max, target $E[T_n]$, smoothing/bandwidth, B, N1, ε, \~ε), but the paper provides limited guidance on principled selection beyond empirical ranges. The procedure is developed primarily for one-sided upward location shifts; robustness to variance/shape changes is not fully characterized, and post-signal diagnosis (what changed) is not built into the chart.","They call for research on guidelines for choosing j_max and k (or equivalently linking to $E[T_n]$) since the dependence of (C_n,T_n) on these parameters is poorly understood. They also suggest studying detection of shifts in variance or other distributional features and developing combined procedures to diagnose whether the detected change is in mean, variance, etc. Extension of the proposed approach to multidimensional (multivariate) monitoring is explicitly mentioned as future work.","Develop self-starting or online re-estimation variants that update $\hat F$ and limits adaptively under concept drift while controlling false alarms. Provide theoretical guarantees (e.g., asymptotic validity/consistency of the smoothed-bootstrap limits and calibration) and robustness analyses under model misspecification and residual autocorrelation. Release reference software (e.g., an R package) and benchmarking suites to standardize comparisons across nonparametric CUSUM approaches and to support practitioner adoption.",0906.1421v1,local_papers/arxiv/0906.1421v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:44:02Z
TRUE,Univariate|Nonparametric|Other,Other|Change-point,Phase II,Finance/economics|Theoretical/simulation only,NA,TRUE,FALSE,Simulation study,ARL (Average Run Length)|Expected detection delay|Other,Not discussed (monitoring starts at k = ⌊Tκ⌋; simulations use T = 250 with starts k = 50 or 75 and bandwidth h = 25).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes sequential monitoring procedures—termed weighted Dickey–Fuller (DF) control charts—to detect a change from a unit-root (random walk) behavior to stationarity in a univariate time series as quickly as possible. The key idea is a kernel-weighted sequential DF process that down-weights older observations, yielding a stopping time that signals when the statistic crosses a lower control limit. Under weak dependence/heteroskedasticity assumptions (functional CLTs for innovations and squared innovations), the author derives functional limit theorems for the weighted DF process under the unit-root null and under local-to-unity alternatives, and shows the corresponding asymptotic distribution of the stopping time. Because serial correlation induces a nuisance parameter in the limit law, two practical chart designs are developed: (i) estimate the nuisance parameter online (Newey–West) and use the corresponding estimated control limit, or (ii) transform the DF process to obtain an asymptotically nuisance-parameter-free limit distribution. A finite-sample simulation study (ARMA(1,1)) compares the estimated-limit and transformed approaches, reporting rejection rates and delays; the estimated-control-limit DF chart generally performs better than the transformed approach, while t-type variants show mixed performance depending on the MA parameter sign.","The main charting statistic is the kernel-weighted sequential DF process $D_T(s)=\frac{\lfloor Ts\rfloor^{-1}\sum_{t=1}^{\lfloor Ts\rfloor} Y_{t-1}\,\Delta Y_t\,K((\lfloor Ts\rfloor-t)/h)}{\lfloor Ts\rfloor^{-2}\sum_{t=1}^{\lfloor Ts\rfloor} Y_{t-1}^2}$ for $s\in[0,1]$, with stopping time $S_T=\inf\{k\le t\le T: D_T(t/T)<c\}$. Under $\rho=1$ the limit process is (Eq. 15) $\mathcal D_\vartheta(s)=\frac{\frac{s}{2}\{K(0)B(s)^2+\zeta\int_0^s B(r)^2K'(\zeta(s-r))dr-\vartheta^{-2}\int_0^s K(\zeta(s-r))dr\}}{\int_0^s B(r)^2dr}$, where $\vartheta=\eta/\sigma$ is a nuisance parameter. The nuisance parameter can be estimated online via a Newey–West estimator $\hat\vartheta_t^2=\hat\eta_t^2/\hat\sigma_t^2$ with Bartlett weights, or removed via a transformed process $E_T(s)$ (Eq. 12) designed so that $E_T\Rightarrow \mathcal D_1$.","Asymptotically, $D_T(\cdot)\Rightarrow \mathcal D_\vartheta(\cdot)$ under the unit-root null, and the normalized stopping time satisfies $S_T/T\Rightarrow \inf\{s\in[\kappa,1]:\mathcal D_\vartheta(s)<c\}$. With online Newey–West estimation (lag truncation $m=o(T^{1/2})$), the estimated-limit chart $\hat S_T$ is shown to be asymptotically size-$\alpha$ (consistent calibration), and the transformed chart $Z_T$ is asymptotically nuisance-parameter invariant. Under local-to-unity alternatives $\rho_T=1+a/T$, $D_T(\cdot)$ converges to a functional of an Ornstein–Uhlenbeck process $Z_a$, providing local asymptotic power. In simulations with $T=250$, Gaussian kernel with $h=25$, and nominal $\alpha=5\%$, the estimated-control-limit weighted DF chart typically shows better finite-sample behavior than the transformed approach; tables report rejection probabilities and delays/CARL across $(\rho,\beta)$ in an ARMA(1,1) model (e.g., higher rejection as $\rho$ decreases from 1 to 0.9, with strong dependence on $\beta$).","The authors note that the DF test’s statistical properties “strongly depend on a correct specification of the correlation structure of the innovation sequence,” motivating nuisance-parameter handling. In the simulation discussion they also state that a single summary such as CARL may not adequately describe behavior, since mixtures of very early and very late signals can yield counterintuitive CARL values.","The work is primarily asymptotic and may require careful finite-sample calibration (control limits are obtained by simulating limit laws), which can be computationally burdensome and sensitive to kernel/bandwidth choices; guidance for selecting $K$, $h$, and $\kappa$ in practice is limited. Although dependence and heteroskedasticity are allowed through invariance principles, the procedures still rely on conditions (e.g., strong mixing, moment assumptions, $m=o(T^{1/2})$) that may be hard to verify for real economic series. Real-data case studies are absent, so practical robustness to model misspecification (structural breaks, nonlinearities, heavy tails) and operational interpretability in applied monitoring settings is not demonstrated. The paper frames monitoring with a fixed horizon $T$; performance and calibration for open-ended (infinite-horizon) monitoring or steady-state ARL are not treated explicitly.",None stated.,"Provide practitioner-focused design guidance for choosing the kernel, bandwidth $h$, and monitoring start $\kappa$ (e.g., data-driven/adaptive bandwidth, sensitivity analyses) and study robustness under heavy-tailed or nonlinear innovation structures beyond the stated mixing/finite-moment conditions. Extend the framework to open-ended monitoring (no fixed $T$) with steady-state/long-run false alarm control and compare against classic sequential change-point procedures (CUSUM/GLR) on common benchmarks. Add real-world econometric case studies (e.g., online stationarity detection for spreads/cointegration residuals) and release reproducible code for simulating control limits and implementing the charts.",1001.1833v1,local_papers/arxiv/1001.1833v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:44:34Z
TRUE,Univariate|Nonparametric|Image-based monitoring|Other,Shewhart|Change-point|Other,Phase II,Manufacturing (general)|Finance/economics|Theoretical/simulation only|Other,FALSE,TRUE,NA,Exact distribution theory|Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),"Assumes a pre-run (historical) in-control sample to initially fill the moving buffer of length M (e.g., Z_{-1},…,Z_{-M}). Simulations tune (M,k) to target in-control ARL levels (~435 or ~840); example pairs include (M,k)=(150,1.8) for ARL0≈435 and (M,k)=(111,2.19) for ARL0≈840.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a modified binary (attribute) control chart for detecting small mean shifts by thresholding observations into signs and counting the number of positives in a moving window (finite buffer) of length M, rather than using the classical binomial Np chart with non-overlapping samples. The method targets detection of unknown shift sizes under an unknown symmetric error distribution, and is positioned as robust to non-normality (including heavy-tailed errors) because it uses only thresholded/binary data. The authors provide asymptotic theory under a change-point model with local alternatives (p1=p0+Δ/√N) and establish a functional central limit theorem for the centered moving-window count process, yielding a Brownian-motion limit with drift after the change. They extend validity beyond independence to dependent data satisfying a martingale-difference-array condition, motivated by time-series and locally dependent image data. Extensive Monte Carlo simulations (30,000 runs per scenario) show substantially smaller out-of-control ARL (and smaller run-length dispersion) than several competitors for very small shifts (≈0.1–0.25σ), while being slower for larger shifts (>0.5σ).","Observations follow a change-point model $Y_n=Y+m\,\mathbf{1}(n-q)+\varepsilon_n$ with symmetric in-control error distribution $F$. Binary indicators are $Z_n=\mathrm{sign}(Y_n)\in\{0,1\}$ and the moving-window count is $J_n=\sum_{i=n-M}^{n-1} Z_i$. The modified p-chart signals when $J_n$ exceeds control limits $\mathrm{UCL}=Mp_0+k\sqrt{Mp_0(1-p_0)}$ (and optionally $\mathrm{LCL}=Mp_0-k\sqrt{Mp_0(1-p_0)}$), typically with $p_0=1/2$. Under local alternatives $p_1=p_0+\Delta/\sqrt{N}$, a functional CLT yields $J_N(t)\Rightarrow \eta_0[B(t)-B(t-M(t))]+\text{drift}$ and a corresponding stopping-time limit.","With in-control ARL tuned to about 435 and Gaussian errors, the chart with (M,k)=(150,1.8) achieves out-of-control ARL ≈243.54 for a 0.1σ shift and ≈97.58 for a 0.25σ shift, with run-length SDs ≈172.58 and ≈58.68 respectively (Table 3). The paper reports competitor ARLs from Han & Tsung (2004) in the same settings of roughly 295–324 (for 0.1σ) and 105–110 (for 0.25σ), indicating faster detection for very small shifts. For larger shifts (e.g., ≥0.5σ), the proposed chart is slower than CUSUM/EWMA-type competitors, motivating running multiple charts in parallel. Under non-normal errors, performance remains strong for small shifts; e.g., Laplace errors with (M,k)=(40,2.22) give ARL ≈191.35 at 0.1 shift and ≈59.51 at 0.25 (Table 5), and Cauchy errors with (M,k)=(28,2.28) give ARL ≈334.82 at 0.1 and ≈167.28 at 0.25.","The authors note that the modified binary chart is much slower than CUSUM/EWMA/GLR-type charts when the jump is larger than about 0.5σ, and recommend handling this by applying several charts simultaneously and alarming when one signals. They also point out a practical design constraint: because $J_n$ is integer-valued, one generally cannot tune the in-control ARL to equal a target value exactly, only approximately.","The chart relies on choosing a threshold (implicitly at 0/target), so performance may degrade under target mis-specification, drift, or asymmetric in-control distributions; the symmetry assumption is central to $p_0=1/2$. The moving-window overlap induces dependence in $J_n$, so exact ARL calibration for finite samples may be nontrivial beyond simulation-based tuning, especially under general dependence structures not well captured by the martingale-difference condition. The paper does not provide implementation details/software, which may hinder reproducibility and adoption. Comparisons appear focused on normally distributed errors and a specific competitor set; broader benchmarking (e.g., against modern nonparametric CUSUM/EWMA or adaptive procedures) is not shown.",None stated.,"Develop practical, data-driven rules for selecting the threshold and buffer length M under unknown/asymmetric in-control distributions and in the presence of slow drift. Provide finite-sample calibration methods (e.g., exact/Markov-chain approximations or bootstrap) for in-control ARL under dependence beyond martingale differences. Extend the approach to multivariate/high-dimensional binary indicators (multiple thresholds/features) and provide open-source software for design and deployment in streaming applications such as real-time image inspection.",1001.1841v1,local_papers/arxiv/1001.1841v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:45:16Z
TRUE,Univariate|Other,Change-point|Other,Phase II,Finance/economics|Manufacturing (general)|Theoretical/simulation only|Other,NA,TRUE,FALSE,Exact distribution theory|Simulation study|Other,ARL (Average Run Length)|MRL (Median Run Length)|False alarm rate|Detection probability|Other,"Monitoring starts at time k = ⌊Tκ⌋ for some κ ∈ (0,1) to avoid basing inference on too few observations. Residual computation requires at least p+1 observations (polynomial degree p).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a sequential monitoring (control chart) procedure to detect when regression errors in a polynomial regression model change from a random walk (unit-root/integrated) behavior to stationarity. It computes sequentially updated least-squares residuals at each time t using all data up to t, and applies a kernel-weighted variance-ratio statistic related to KPSS/Breitung-type unit root testing to form a stopping time (signal when the statistic falls below a control limit). The main theoretical contribution is functional central limit theorems for the sequentially updated residual process and for the induced weighted variance-ratio process, yielding an asymptotic (large-T) distribution for the stopping time under both the in-control random-walk model and a change-point model. A key practical implication is that the asymptotic null distribution of the monitoring statistic/stopping time is nuisance-parameter free (does not depend on the long-run variance scale η), enabling control-limit selection to target an asymptotic type I error (false alarm) probability. Finite-sample behavior is studied via Monte Carlo simulations (T=500) under correlated innovations and various change-point locations, reporting signal probabilities and conditional average run length, and showing the procedure is comparatively robust to correlation parameters but struggles with very late changes under a fixed horizon.","Observations follow $Y_t=\beta_0+\beta_1 t+\cdots+\beta_p t^p+\varepsilon_t$, with null $H_0:\rho_t=1$ (random-walk errors) and change-point alternative where $\rho_t$ switches to $|\rho|<1$. At each time $t$, compute sequentially updated LS residuals $\hat\varepsilon_j(t)$ (recomputed using $Y_1,\ldots,Y_t$), and form the kernel-weighted variance-ratio statistic $$U_t=\frac{t^{-4}\sum_{i=1}^t\left(\sum_{j=1}^i \hat\varepsilon_j(t)\right)^2 K\big((i-t)/h\big)}{t^{-2}\sum_{j=1}^t \hat\varepsilon_j(t)^2},\quad t\ge p+1.$$ The control chart (stopping time) is $R_T=\inf\{k\le t\le T: U_t\le c_R\}$ (with $k=\lfloor T\kappa\rfloor$), and asymptotic results are given via weak convergence of the induced process $V_T(s)$ to a Brownian-motion functional.","Under the in-control model (random-walk regression errors with weakly dependent increments), the paper derives FCLTs for (i) the sequentially updated residual process and (ii) the kernel-weighted variance-ratio process $V_T(s)$, and shows $R_T/T \Rightarrow R=\inf\{s\in[\kappa,1]:V(s)\le c_R\}$. The limiting null law is distribution-free in the sense that it depends on the kernel $K$ and $\zeta=T/h$ but not on the long-run variance scale parameter $\eta$, enabling simulation-based control-limit calibration for a target asymptotic type I error $\alpha$ (via $c_R=F^{-1}(1-\alpha)$ for $F$ of $\inf_{s\in[\kappa,1]}V(s)$). Simulations with horizon $T=500$ (Gaussian kernel, $h\in\{25,50\}$) report empirical signal probabilities and conditional average run length (CARL) as functions of the control limit, and power (rejection rates) for change-points (e.g., early change-points show much higher rejection rates than late ones). Table 1 shows substantial power for early changes (e.g., change-point 25 yields rejection rates around 0.44–0.97 depending on correlation settings) and near-nominal false alarm rates under no-change (about 0.03–0.10 depending on settings).","The paper notes that monitoring is conducted with a finite time horizon $T$ and that average run length behavior is affected by this horizon (e.g., CARL conditional on signaling before $T$), commenting that without a horizon the ARLs would be substantially higher. It also remarks that very late changes are hard to detect in the simulated setting with $N=500$, which is inherently challenging for the problem. No explicit broader limitations (e.g., model misspecification) are emphasized beyond these practical/horizon-related considerations.","The procedure is tailored to detecting a change from integrated (random-walk) errors to stationarity in polynomial regression and may be sensitive to other departures (e.g., structural breaks in regression coefficients, heteroskedasticity patterns not covered by assumptions, or long-memory/near-unit-root behavior). The theory assumes conditions ensuring an FCLT for innovations and does not explicitly address strong autocorrelation in residuals beyond that framework, nor does it provide guidance for robust finite-sample calibration when $T$ is small or when polynomial degree p is misspecified. Control-limit selection relies on simulating Brownian-functionals (asymptotic calibration); finite-sample size distortions and practical implementation details (numerical integration, discretization error) could matter but are not deeply analyzed. The method is univariate and does not cover multivariate/profile monitoring extensions commonly needed in modern SPC.","The paper briefly notes that extending the finite-horizon monitoring framework to infinite monitoring is straightforward (by moving from $D[0,1]$ to $D[0,\infty)$), but does not lay out a detailed agenda of additional future research directions beyond these remarks.","Develop finite-sample calibrated control limits (e.g., bootstrap or simulation under fitted models) to improve performance for moderate T and reduce reliance on asymptotics. Extend the approach to handle serial dependence structures more explicitly (e.g., ARMA/long-memory errors) and to be robust against conditional heteroskedasticity and heavy tails beyond the stated assumptions. Generalize to multivariate regressions or multiple related time series (high-dimensional monitoring) and to profile monitoring contexts where the regression relationship itself may change. Provide open-source software implementations to facilitate adoption and to standardize kernel/bandwidth choices and diagnostics.",1001.1845v1,local_papers/arxiv/1001.1845v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:45:49Z
TRUE,Univariate|Other,EWMA|Other,Phase II,Network/cybersecurity|Other,TRUE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate|Detection probability|Other,Uses rolling/windowed estimation from past observations: aggregate traffic is over 10-second intervals; local means/covariances estimated from a window of size m with recommendation that prediction is robust for m ≥ 10 (and often works for m < 10). Flow-level means are computed over windows (example: 2000 seconds) for PCA factor construction.,TRUE,None / Not applicable,Not provided,http://www.internet2.edu/about|http://www.isi.edu/nsnam/ns/,"The paper develops a network-wide Gaussian modeling framework linking unobserved link-level traffic to observed link measurements through the routing matrix and flow-to-link relationship $Y(t)=AX(t)$. It proposes a network-specific mean–variance model learned offline from expensive NetFlow-derived flow estimates: flow means are modeled via a low-dimensional PCA factor structure $\mu_X(t)=F\beta(t)$, and flow variances follow a power-law mean–variance relationship, yielding $\Sigma_X=\sigma^2\,\mathrm{diag}(|F\beta|^{2\gamma})$ (with $\gamma\approx 3/4$). Parameters are estimated online from observed links using an iterated generalized least squares (iGLS) procedure, and the resulting plug-in kriging predictor substantially improves prediction accuracy over ordinary kriging in many scenarios on Internet2 traffic data. For anomaly detection when all links are observed, the paper constructs control charts on residuals $Y_\ell(t)-\hat Y_\ell(t)$ using an EWMA statistic with control limits adjusted for long-range dependence (modeled by fractional Gaussian noise), to reduce false alarms. The work connects network tomography/prediction (“network kriging”) with SPC-style monitoring by providing statistically calibrated residual-based charts for detecting and isolating traffic shifts.","Core network model: routing equation $Y(t)=AX(t)$ with $X(t)\sim N(\mu_X(t),\Sigma_X(t))$ implying $Y(t)\sim N(A\mu_X(t),A\Sigma_X(t)A^\top)$. Network-specific mean–variance model: $\mu_X(t)=F\beta(t)$ and $\Sigma_X=\sigma^2\,\mathrm{diag}(|F\beta|^{2\gamma})$, leading to $Y(t)=AF\beta+\sigma A\,\mathrm{diag}(|F\beta|^{\gamma})Z(t)$ with $Z(t)\sim N(0,I)$. Online estimation uses iGLS: $\hat\beta_{k+1}=[(A_oF)^\top G(\hat\beta_k)A_oF]^{-1}(A_oF)^\top G(\hat\beta_k)\bar Y_o$ with $G(\beta)=[A_o\mathrm{diag}(|F\beta|^{2\gamma})A_o^\top]^{-1}$; prediction uses kriging with plug-in covariances. For monitoring, EWMA $\tilde Z_t=(1-\phi)\sum_{j\ge0}\phi^j Z_{t-j}$ is used, and its variance is adjusted for LRD via an integral expression (Prop. 4) under fGn.","On Internet2 backbone data, the network-specific model (with e.g. p=2 PCA factors and $\gamma\approx 3/4$) achieves markedly lower relative MSE than ordinary kriging across scenarios where observed and unobserved links share enough flows; the paper reports average ReMSE improvement of about 0.1887 (18.87 percentage points) over scenarios 1–9, with improvements ranging up to 78%. The learned PCA loading matrix $F$ is empirically robust over time: a model trained from Feb 19, 2009 NetFlow-derived flows can be used to predict multiple other days (including weeks later) with similar ReMSE behavior in many scenarios. Calibration studies show prediction is relatively insensitive to $\gamma\in[0.5,2]$, to p as long as p does not exceed the number of observed links (identifiability), and to window size m for mean estimation provided m\ge 10. For anomaly detection, standard EWMA limits (i.i.d. assumption) produce excessive false positives on long-range dependent traffic, while the LRD-adjusted EWMA control limits better isolate the onset of injected mean-shift anomalies in residuals.","The authors note that flow-level (NetFlow) data needed to learn the matrix $F$ are expensive and computationally intensive to process, making repeated or fully online use impractical; $F$ must be updated if structural changes in the network occur. They also state the model assumes flows are (approximately) uncorrelated and that strong correlations can arise under congestion/heavy traffic, which is outside the scope of the paper. Additionally, identifiability requires that the number of mean factors p not exceed the number of observed links (rank condition on $A_oF$); otherwise parameters cannot be identified and performance may degrade.","The monitoring component is developed as residual charting after building a prediction model; it does not provide ARL/false-alarm guarantees for the full procedure (model-estimation + residual EWMA), so in-control properties under estimation error may differ from nominal. The Gaussian assumption for flow/link vectors and diagonal $\Sigma_X$ may be fragile for heavy-tailed traffic or bursty regimes; robustness to non-Gaussianity is not systematically studied. Practical deployment would require automated selection of observed-link sets, thresholds, and handling of missing/irregular measurements; these operational issues are not addressed. The anomaly detection examples rely on injected mean shifts and do not evaluate a broader set of attack/fault types (variance changes, route changes, intermittent anomalies) or compare against alternative SPC/change-point detectors.","The paper suggests (implicitly through discussion) that studying heavy-traffic/congested scenarios where flow correlations are non-negligible is an interesting direction but is outside the present scope. It also notes the learned model must be updated when structural network changes occur, implying future work on updating/adapting the model over time and under changing routing/traffic structure.","Extend the control-charting framework to formally control in-control false alarm rates under parameter estimation (Phase I/II separation), e.g., via bootstrap/Markov-chain approximations or steady-state ARL calculations for LRD-adjusted EWMA on estimated residuals. Develop robust/nonparametric or heavy-tailed variants of the model (and residual charts) to handle stable/alpha-stable traffic regimes and outliers. Incorporate explicit autocorrelation and cross-correlation structure across flows/links (beyond common $\rho(i)$) and evaluate performance under routing changes or partial observation with missing data. Provide open-source implementation and standardized benchmarks for network monitoring comparisons against CUSUM/GLR/change-point and modern ML-based detectors.",1005.4641v1,local_papers/arxiv/1005.4641v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:46:39Z
TRUE,Univariate,Shewhart|Change-point|Other,Phase II,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,NA,Exact distribution theory|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|Detection probability|False alarm rate|Other,Not discussed,NA,None / Not applicable,Not applicable (No code used),NA,"The paper proposes a modified runs-rules scheme for Shewhart-type $\bar{X}$ charts, termed the modified $r/m$ (denoted $M\!-
 r/m$) control chart, to improve sensitivity to small and moderate mean shifts while maintaining a specified in-control ARL. The rule signals when $r$ points fall beyond the UCL (or below the LCL) with at most $(m-r)$ intervening points that are restricted to lie between the center line and the corresponding control limit (rather than anywhere between LCL and UCL as in the standard $r/m$ rule). Assuming independent normal observations with known $\sigma=1$, the authors tabulate exact in-control and out-of-control run length properties for many $(r,m)$ combinations, with control limits set so that $ARL_0=370.4$ (matching the 3-sigma Shewhart chart). Across shifts from 0 to 2.6 standard deviations, the modified $M\!-
 r/5$ schemes (particularly $r=2,3,4$) provide the best overall ARL performance among the considered schemes, while for larger shifts the standard 1/1 Shewhart chart can be slightly faster. They also compare $M\!-
 r/5$ charts to the Western Electric rules (C1234) at matched $ARL_0=94.75$, finding comparable ARL behavior with a simpler rule structure.","Standard $r/m$ rule signals if within the last $m$ observations at least $r$ fall above UCL (or at least $r$ fall below LCL). The proposed modified rule $M\!-
 r/m$ signals if there exist $r$ points above UCL (below LCL) such that any separating points (at most $m-r$ of them) lie between the center line (CL) and UCL (between CL and LCL), respectively. Control limits (UCL/LCL) are chosen to achieve a target in-control average run length, e.g., $ARL_0=370.4$ (and separately $ARL_0=94.75$ for comparison to Western Electric rules), with symmetry about the CL.","With limits calibrated to $ARL_0=370.4$, Table 1 shows that each $M\!-
 r/m$ dominates its corresponding standard $r/m$ in ARL for every listed mean shift (0 to 4$\sigma$). For shifts between 0 and 2.6$\sigma$, the best overall performance is achieved by modified $M\!-
 r/5$ schemes ($r=2,3,4$); e.g., at shift 1.0, $ARL$ is 18.26 (M-2/5), 15.46 (M-3/5), and 16.18 (M-4/5) versus 43.90 for the standard 1/1 chart. Percentiles for run length are provided for M-2/5, M-3/5, and M-4/5 at $ARL_0=370.4$; for M-2/5 in control, the 5th/50th/95th percentiles are 21/257/1105. At matched $ARL_0=94.75$ (Table 5), the modified M-$r$/5 charts yield ARLs close to Western Electric C1234; for example at shift 1.0, C1234 has ARL 9.22 vs 9.59 (M-2/5), 9.28 (M-3/5), and 10.22 (M-4/5), with semi-interquantile ranges also reported.",None stated,"The development assumes independent, normally distributed observations with known and constant $\sigma=1$, so robustness to non-normality, parameter estimation error, and autocorrelation is not established. The paper focuses on signaling performance (ARL/percentiles) but does not address diagnostic guidance (identifying assignable causes) or economic design/optimization for choosing $(r,m)$ and limits in practice beyond matching $ARL_0$. Real industrial case studies are not presented, so practical performance under messy data conditions (e.g., drifts, mixtures, rounding) is unclear. Computational implementation details are omitted (no algorithm/pseudocode or software), which may hinder adoption for practitioners needing automated rule evaluation.",None stated,"Extend the modified $r/m$ rules to settings with unknown parameters (Phase I estimation) and provide guidance on how estimation affects in-control false alarm properties. Study robustness to autocorrelation and non-normal distributions (e.g., via robust or nonparametric variants) and consider adaptations for individual observations vs subgroup means. Develop optimal selection procedures for $(r,m)$ and control limits under explicit objectives (e.g., minimizing expected delay subject to false alarm constraints or economic cost). Provide open-source software implementations and validate on real manufacturing datasets to benchmark against CUSUM/EWMA and modern adaptive schemes.",1007.3225v1,local_papers/arxiv/1007.3225v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:47:13Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Other,FALSE,FALSE,FALSE,Case study (real dataset),Other,Not discussed,NA,None / Not applicable,Not applicable (No code used),NA,"The paper proposes using Statistical Process Control (SPC) control charts to monitor software reliability using inter-failure time data modeled by a finite-failure NHPP with an exponential form (Goel–Okumoto mean value function). It develops a Modified Maximum Likelihood Estimation (MMLE) approach that yields analytical (non-iterative) parameter estimates for the NHPP parameters, as an alternative to iterative MLE for the rate parameter. Using the fitted mean value function, the authors construct a univariate “mean value chart” (an individuals-style chart) by plotting successive differences of the estimated cumulative mean value function versus failure number. Control limits are derived by mapping selected CDF/probability levels (0.00135, 0.5, 0.99865) through the model to obtain LCL/CL/UCL on the chart scale. A case study (Xie et al., 2002 inter-failure times) shows two out-of-control signals (at the 10th and 25th failures), interpreted as early detection of software process issues while the remaining points are within limits, indicating stability.","The NHPP mean value function is exponential/Goel–Okumoto: $m(t)=a(1-e^{-bt})$ with $a>0,b>0$, and intensity $\lambda(t)=\frac{dm(t)}{dt}=b(a-m(t))=abe^{-bt}$. Likelihood for failure times $s_1,\dots,s_n$ is $L=e^{-m(s_n)}\prod_{k=1}^n\lambda(s_k)$, with standard MLE equations replaced by MMLE approximations that linearize terms to obtain an explicit estimator for $b$ and then $a$. Control limits are obtained by equating the model CDF/probability levels (0.00135, 0.5, 0.99865) and transforming them to limits on the mean value chart scale (reported as approximately LCL $\approx0.0459$, CL $\approx16.698$, UCL $\approx33.351$).","Using the inter-failure-time dataset (30 failures) from Xie et al. (2002), the MMLE parameter estimates reported are $\hat a=33.396342$ and $\hat b=0.003962$. The mean value control chart (based on successive differences of $m(t)$) signals two out-of-control points: at the 10th and 25th failures, where the plotted statistic falls below the LCL. The authors interpret these as early detection of assignable causes affecting the software failure process, while the remaining observations lie within control limits and are described as stable. No ARL/ATS or false-alarm/detection-delay metrics are reported.",None stated.,"The charting approach and limits are not evaluated with standard SPC performance metrics (e.g., in-control ARL, out-of-control ARL/ATS), so false-alarm and detection performance are unclear. The method appears to assume independent inter-failure times governed by the specified NHPP model; robustness to model misspecification, non-exponential behavior, or temporal dependence is not assessed. Only a single illustrative dataset is analyzed, with limited comparison against alternative software reliability monitoring schemes (e.g., CUSUM/EWMA or other SRGMs).",None stated.,"Evaluate the proposed chart using ARL/ATS (including steady-state vs. zero-state) and compare against CUSUM/EWMA and likelihood-based change-point charts for small shifts in failure intensity. Study robustness to NHPP model misspecification and extend the approach to handle autocorrelation/operational profile changes and missing/irregular failure reporting. Provide an implementation (e.g., R/Python) and guidance for Phase I initialization and parameter-estimation uncertainty propagation into control limits.",1111.1826v1,local_papers/arxiv/1111.1826v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:47:42Z
TRUE,Univariate|Profile monitoring|Nonparametric|Bayesian|Other,Shewhart|CUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Other,Both,Healthcare/medical|Finance/economics|Manufacturing (general)|Theoretical/simulation only|Other,NA,NA,NA,Approximation methods|Simulation study|Markov chain|Other,ARL (Average Run Length)|Detection probability|False alarm rate|Other,"Uses Phase I reference sample of size n past in-control observations X_{-n},…,X_{-1} to estimate the in-control state; simulations consider n=50, 100, 500, 1000, 10000 (and examples discuss n=100, 1000). No explicit minimum n recommendation is given beyond illustrating strong effects for small/moderate n.",TRUE,R,Package registry (CRAN/PyPI),NA,"The paper studies how estimating the in-control state (Phase I) degrades the conditional in-control performance of control charts (Phase II), making metrics like false-alarm probability and conditional ARL highly variable and often worse than nominal. It proposes a general bootstrap-based adjustment that constructs one-sided confidence intervals for chart performance quantities q(P;\hat\xi) conditional on the estimated in-control parameters, and then tunes chart thresholds to guarantee (with probability 1-\alpha) a desired conditional in-control performance. The approach is presented for Shewhart and CUSUM charts and extends to risk-adjusted charts based on regression models (linear/logistic) and survival (Cox) models; nonparametric bootstrap variants provide robustness to model misspecification. Large-sample validity is established via an extended functional delta method (Hadamard differentiability with converging nuisance parameter) and bootstrap weak convergence. Simulations (implemented in R) show that adjusted thresholds substantially reduce the probability of too-short in-control ARLs while incurring only modest loss in out-of-control performance, and that nonparametric bootstrap helps under distributional misspecification.","The core construction forms a one-sided bootstrap confidence interval for an in-control performance quantity q(P;\hat\xi) via \(( -\infty,\; q(\hat P;\hat\xi) - p^*_\alpha)\), where \(p^*_\alpha\) is the \(1-\alpha\) empirical quantile of \(q(\hat P^*_b;\hat\xi^*_b) - q(\hat P;\hat\xi^*_b)\) over bootstrap replicates. Thresholds are then adjusted to \(q(\hat P;\hat\xi) - p^*_\alpha\) when q is a threshold-calibration functional (e.g., cARL or chit). Example CUSUM (mean-shift normal) uses \(S_t=\max(0,S_{t-1}+X_t-\hat\mu-\Delta/2)\) (unscaled) or \(S_t=\max(0,S_{t-1}+(X_t-\mu-\Delta/2)/\sigma)\) (scaled) with signal time \(\tau=\inf\{t>0:S_t\ge c\}\). For likelihood-based CUSUMs, increments are \(\log\{f_1(X_t;\theta)/f_0(X_t;\theta)\}\).","In simulations calibrating to in-control ARL 100, unadjusted (and unconditional-bias-adjusted) thresholds can yield a substantial probability of conditional ARL far below 100; the proposed bootstrap adjustment is designed to ensure (e.g.) a 90% guarantee that conditional ARL \(\ge 100\). Nominal 90% one-sided interval coverage for CUSUM examples (Table 1) is close to target for transformed metrics (e.g., log(ARL), logit(hit))—around 0.89–0.92 for n=500 and somewhat worse for n=50 in untransformed ARL. Using adjusted thresholds, figures show that the lower tail of the conditional in-control ARL distribution is greatly reduced (meeting the 90% guarantee), while out-of-control ARL increases only slightly. Under model misspecification (e.g., Exponential or scaled \(\chi^2\) with variance 1), the nonparametric bootstrap adjustment maintains the intended in-control guarantees better than the parametric normal-bootstrap adjustment.","Shewhart charts depend strongly on tail behavior, making nonparametric methods (or simple nonparametric bootstrap) problematic for small samples; the authors therefore primarily recommend a parametric bootstrap for Shewhart charts. They note that verifying continuity of the limiting distribution required for some theoretical conditions (e.g., condition d in Theorem 1 for certain nonparametric CUSUM setups) is outside the scope of the paper. For differing fitting and deployment periods in the survival setting, they state that more complicated resampling (e.g., separately resampling arrivals and survival/censoring components) would be needed.","The method requires repeatedly computing q(P;\xi) (e.g., ARL or threshold calibrations) inside the bootstrap loop, which can be computationally heavy for complex charts/models and may limit routine industrial use without efficient implementations. Finite-sample calibration quality depends on bootstrap size B and on the accuracy of the inner evaluation method (e.g., Markov-chain approximation or Monte Carlo), introducing nested simulation error that is not fully quantified. The approach focuses on guaranteeing in-control conditional performance; it does not provide optimal choices of \(\alpha\) or explicit trade-offs for out-of-control detection delay beyond illustrative simulations. The paper largely assumes independent observations; applicability under serial correlation is conjectured but not demonstrated with explicit methods for autocorrelation.","The authors suggest the approach should extend to many other charts beyond Shewhart and CUSUM and conjecture applicability in broader settings such as other regression models, autocorrelated data, multivariate data, and other control-chart extensions. They also note the bootstrap procedure could be adapted for bias adjustments of unconditional performance, though this is not emphasized as the main goal.","Develop computationally efficient implementations (e.g., analytic gradients/delta-method approximations, variance reduction, parallelized bootstrap, or reusable Markov-chain solvers) to make guaranteed-performance tuning feasible for high-throughput monitoring. Extend the framework with explicit treatments for autocorrelated and nonstationary data (e.g., ARMA residual charts, block bootstrap, dependent wild bootstrap) and study guarantee validity under dependence. Provide practitioner guidance on choosing \(\alpha\), bootstrap type (parametric vs nonparametric), and transformations (log/logit) via systematic robustness and sensitivity analyses. Release and validate the promised software as a maintained R package with reproducible examples and benchmarks across common SPC scenarios (including multivariate/high-dimensional charts).",1111.4180v1,local_papers/arxiv/1111.4180v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:48:32Z
TRUE,Univariate,Shewhart,Both,Theoretical/simulation only,FALSE,FALSE,NA,Exact distribution theory,NA,Not discussed,FALSE,R,Not applicable (No code used),http://www.r-project.org,"This note reviews and consolidates exact (fiducial) confidence limits for common discrete distributions used in attribute control charting, with emphasis on avoiding historical graph/table lookups. It explains that when distribution parameters are known, exact control limits can be computed directly from distribution quantiles using modern software; when parameters are unknown, limits can be obtained by inverting tail probabilities, yielding fiducial limits. The paper provides explicit two-sided fiducial limit formulas for the binomial parameter p (relevant to p charts), Poisson mean \(\lambda\) (relevant to c and u charts), and geometric/Bernoulli parameter p via a negative-binomial relationship (relevant to g charts). The main contribution is presenting these limits in closed form using F and gamma quantiles, enabling exact computation in software such as R (e.g., qf, qgamma) rather than interpolation from tables. No performance comparison (e.g., ARL) is conducted; the focus is on exact limit construction for attribute charts under discrete models with unknown parameters.","For binomial \(X\sim\text{Bin}(n,p)\), fiducial limits invert equal tails and yield \(p_L=\left(1+\frac{n-x+1}{x\,F^{(\alpha/2)}_{2x,2(n-x+1)}}\right)^{-1}\) and \(p_U=\left(1+\frac{n-x}{(x+1)\,F^{(1-\alpha/2)}_{2(x+1),2(n-x)}}\right)^{-1}\), with boundary cases \(p_L=0\) at \(x=0\) and \(p_U=1\) at \(x=n\). For Poisson with sample sum \(Y=\sum_{j=1}^n X_j\sim\text{Pois}(n\lambda)\), \(\lambda_L=\frac{1}{2n}\Gamma^{(\alpha/2)}_{y,2}\) and \(\lambda_U=\frac{1}{2n}\Gamma^{(1-\alpha/2)}_{y+1,2}\) (gamma quantiles), with \(\lambda_L=0\) at \(y=0\). For geometric data with \(Y\) negative-binomial, \(p_L=\left(1+\frac{y+1}{n}F^{(1-\alpha/2)}_{2(y+1),2n}\right)^{-1}\) and \(p_U=\left(\frac{n F^{(1-\alpha/2)}_{2n,2y}}{y+n F^{(1-\alpha/2)}_{2n,2y}}\right)^{-1}\) (with \(p_U=1\) if \(y=0\)).",NA,"The note does not present a detailed empirical study; it mainly reviews/provides formulas for fiducial limits and emphasizes that older table/graph approaches restrict achievable confidence levels and require interpolation errors. It also notes that some earlier exact-limit tables assume the true distribution parameter is known, which is often unrealistic in practice.","The paper does not translate the fiducial confidence limits into specific control-chart design guidance (e.g., how to choose \(\alpha\) to achieve a desired in-control ARL) or evaluate resulting false-alarm/detection performance. It does not discuss effects of overdispersion, model misspecification, or serial dependence, which are common with attribute/count data and can invalidate discrete-model-based limits. No worked case studies are provided to demonstrate practical implementation details (e.g., subgrouping choices for u/p charts or handling varying n).",None stated.,"A natural extension is to study how fiducial-limit-based control limits translate into in-control/out-of-control run-length properties (ARL/SDRL) and to provide guidance for selecting \(\alpha\) under unknown parameters. Robust or overdispersion-aware variants (e.g., beta-binomial/negative-binomial models) and methods accommodating autocorrelation would improve practical applicability. Providing software implementations (e.g., an R function/package) and real-data case studies for p/u/c/g charts would facilitate adoption and reproducibility.",1203.3882v1,local_papers/arxiv/1203.3882v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:48:51Z
TRUE,Profile monitoring|Nonparametric|Functional data analysis,Other,Both,Manufacturing (general)|Theoretical/simulation only,FALSE,TRUE,TRUE,Approximation methods|Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"Not discussed (but they note that with insufficient Phase I profiles, corrections may be needed; and sparse/irregularly spaced profiles may yield unstable threshold estimation).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes Phase I/Phase II profile control charts for functional responses using a nonparametric location–scale model estimated via robust L-1 (least absolute deviation) regression. Phase I estimates a reference profile (conditional median function) and a reference deviation function (conditional MAD) using kernel smoothing with jackknife bias correction, allowing a general stationary dependence structure within each densely sampled profile. Phase II monitoring uses three deviation metrics: a standardized median (location) deviation for vertical shifts, plus a maximal standardized residual and a cumulative standardized residual for local and overall shape deviations. Control limits are obtained empirically (or by bootstrap) from the Phase I profiles, with an overall significance level controlled by a rule that accounts for signaling on any of the three metrics. The method is demonstrated on vertical density profiles of engineered wood boards and via simulations mimicking those profiles, showing good detection power for sinusoidal distortions and local spikes while maintaining near-nominal false alarm rates.","Phase I model: $Y_{i,j}=\delta_i+\mu(x_{i,j})+s(x_{i,j})e_{i,j}$ with $\delta_i=\mathrm{median}(Y_i)$, $\mu(x)$ the conditional median of the centered profile, and $s(x)$ the conditional MAD. Estimate $\mu(x)$ by local LAD kernel smoothing $\hat\mu_{b_n}(x)=\arg\min_\theta \sum_{i=1}^n\sum_{j=1}^{m_i}|Y_{i,j}-\delta_i-\theta|K_{b_n}(x_{i,j}-x)$ and bias-correct via jackknife $\tilde\mu_{b_n}(x)=2\hat\mu_{b_n}(x)-\hat\mu_{\sqrt{2}b_n}(x)$; estimate $s(x)$ analogously on absolute residuals with $\tilde s_{h_n}(x)$. Phase II statistics: $D=|\delta^*-\hat\mu_\delta|/\hat s_\delta$, standardized residuals $\hat e_l=(Y_l-\delta^*-\tilde\mu_{b_n}(x_l))/\tilde s_{h_n}(x_l)$, and $T^{(1)}=\max_l|\hat e_l|$, $T^{(2)}=\sum_l|\hat e_l|$ with control limits from empirical/bootstrapped $(1-\alpha)$ quantiles of the corresponding Phase I statistics.","On the real VDP dataset (24 Phase I profiles, each with 314 measurements at spacing 0.002 inch), cross-validation selected bandwidths $b_n=0.015$ and $h_n=0.01$. Using per-metric significance level $\alpha=0.03$ (overall $\alpha_0\approx0.12\approx3/24$), the reported control limits were 2.99 (for $D$), 7.94 (for $T^{(1)}$), and 663.6 (for $T^{(2)}$), identifying profiles A6 (largest location shift), A3 (largest maximal deviation), and B6 (largest cumulative deviation) as most extreme. In simulations with overall significance level 0.05, type I error was about 5% (Gaussian errors) and 8% (scaled $t_3$ errors). Detection rates increased with distortion magnitude: for sinusoidal shape distortion (Model a), Gaussian detection was 45%, 80%, 98% for $A=0.75,1.0,1.25$; for spiky local distortion (Model b), Gaussian detection was 36%, 82%, 100% for $B=0.02,0.03,0.04$ (lower under $t_3$ errors: e.g., 21%, 62%, 100% for $A$ and 12%, 44%, 95% for $B$).","They note that control limits estimated from data are subject to estimation error, and with insufficient Phase I profiles, corrections may be needed to ensure good properties. They also state that using Phase I profiles to form reference distributions implicitly requires enough Phase I profiles measured at similar locations; with sparse, irregularly spaced profiles there may be insufficient Phase I information to stably estimate thresholds, requiring further research.","The approach is framed as a screening procedure rather than a full sequential control-chart design; it does not report standard SPC run-length metrics (e.g., in-control/out-of-control ARL) for sustained Phase II monitoring, so practitioners may lack guidance on long-run false alarm behavior. Empirical/bootstrapped control limits depend strongly on representativeness and cleanliness of Phase I; while L-1 is robust, influential Phase I anomalies could still distort estimated reference functions and thresholds. The method uses three correlated metrics with an ad hoc calibration of $\alpha$ to achieve an overall level, but does not quantify power/false alarm tradeoffs under dependence between metrics beyond the chosen rule.","They suggest studying alternative deviation scores (e.g., using a residual percentile instead of the maximum) and their properties. They discuss targeting one metric when a specific deviation type is of primary concern, incorporating special structure (e.g., symmetry constraints) to improve efficiency, and extending theory to more general error sequences (e.g., linear combinations of stationary sequences). They also mention the need for corrections when Phase I is small and for methods that handle sparse, irregularly spaced profiles where Phase I data may be insufficient to estimate stable screening thresholds.","Develop a true Phase II online monitoring scheme with designed signaling properties (e.g., steady-state ARL/ATS) and guidance for setting limits under repeated sampling. Extend the method to multivariate profiles (multiple responses per profile) and high-dimensional settings, and provide diagnostic tools to localize where/why a profile signaled (beyond max/sum residuals). Provide open-source software with recommended default bandwidth/bootstrapping choices and computational strategies for large numbers of profiles and dense grids.",1203.4661v1,local_papers/arxiv/1203.4661v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:49:36Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Service industry|Other,FALSE,FALSE,FALSE,Case study (real dataset)|Other,Other,"Suggests grouping inter-failure times into non-overlapping subgroups of size 4 or 5 (e.g., 100 inter-failure times grouped into 20 subgroups of size 5); more generally choose r<n (preferably a factor of n) and set k=n/r subgroups.",TRUE,C/C++,Not provided,NA,"The paper proposes an SPC-based monitoring mechanism for software reliability using an NHPP framework and order statistics applied to inter-failure time data. Inter-failure times are grouped into non-overlapping subgroups of size r (illustrated with r=4 and r=5), and a mean value function based on an exponential NHPP model (Goel–Okumoto form) is used to compute expected cumulative failures. A mean value control chart is constructed from successive differences of the mean value function evaluated at cumulative times, with control limits derived by mapping selected CDF/PDF tail probabilities (0.00135, 0.5, 0.99865) into time thresholds and then into m(t) values. Model parameters (a,b) are estimated by maximum likelihood and solved numerically via the Newton–Raphson method using a C program. The approach is illustrated on Musa (1975) failure data, and the resulting charts show out-of-control signals (points below LCL), argued to support early detection of process deterioration in software reliability.","The NHPP mean value function is modeled in Goel–Okumoto/exponential form as $m(t)=a(1-e^{-bt})$ with failure intensity $\lambda(t)=m'(t)=abe^{-bt}$. For grouped data (order-statistics approach) with subgroup size $r$, the paper writes a grouped mean value function as $m(t_s)=[a(1-e^{-bt_s})]^r$ (eq. 2.1.1) and forms an MLE log-likelihood over grouped observations; $a$ and $b$ are obtained by solving score equations (eqs. 2.2.8–2.2.9) via Newton–Raphson. Control limits are obtained by equating the distribution to probabilities 0.00135, 0.5, 0.99865 to get $T_L,T_C,T_U$ and then converting to $m(T_L),m(T_C),m(T_U)$; the chart plots successive differences of $m(t)$ across failure indices.","Using Musa (1975) inter-failure data grouped by order, the paper reports parameter estimates: for 4th-order grouping, $\hat a=2.415117$ and $\hat b=0.000099$; for 5th-order grouping, $\hat a=1.933309$ and $\hat b=0.000114$. It uses probability levels 0.00135, 0.5, and 0.99865 to set control limits; for the 4th-order example it reports $m(t_U)=33.3512569382986$, $m(t_C)=16.6981710073481$, and $m(t_L)=0.04508506100108$. The mean value charts (for both 4th- and 5th-order grouped data) show out-of-control signals below the LCL, interpreted as early warnings of assignable-cause deterioration in software reliability. No ARL/ATS/false-alarm numerical comparisons are reported.",None stated.,"The chart design is not calibrated using standard SPC run-length properties (e.g., in-control ARL), and the chosen probability levels (0.00135/0.99865) are not tied to a demonstrated false-alarm rate for the constructed statistic (successive differences of $m(t)$). The method assumes an exponential NHPP/Goel–Okumoto-type form and uses grouped/order-statistics transformations; robustness to model mis-specification, non-NHPP behavior, or dependency in failure times is not assessed. Evaluation is limited to a single classic dataset without broader simulation or benchmark comparisons against alternative software reliability control charts.",None stated.,"Calibrate and compare the proposed chart using run-length metrics (ARL/ATS, SDRL) under in-control and various out-of-control scenarios, including different shift types (e.g., change in failure intensity/decay rate). Extend the approach to handle autocorrelation or nonstationarity in failure data, and assess robustness under alternative SRGMs (e.g., Weibull/log-logistic NHPP) or nonparametric limits. Provide reproducible software (e.g., an R/Python package) and apply the method to multiple real software projects to validate practical utility and tuning guidance (choice of subgroup size r, control-limit probabilities).",1205.6440v1,local_papers/arxiv/1205.6440v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:50:06Z
TRUE,Univariate|Other,CUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Other,Phase II,Finance/economics|Manufacturing (general)|Theoretical/simulation only,TRUE,TRUE,NA,Simulation study|Markov chain,ARL (Average Run Length)|ATS (Average Time to Signal)|Expected detection delay|Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper develops several sequential control charts to detect an increase in variance (scale change) in a Gaussian time series, using likelihood ratio (LR), generalized likelihood ratio (GLR), sequential probability ratio test (SPRT), Shiryaev–Roberts (SR), and generalized versions of SPRT and SR. The change model keeps the mean fixed and scales deviations from the mean by a factor \(\Delta>1\) after an unknown change-point \(\tau\). For Gaussian processes the authors express likelihoods via innovations/prediction-error decompositions and provide recursive implementations for key cases (notably AR(1), also AR(2)), enabling efficient online updating. An extensive Monte Carlo study (calibrated to in-control ARL 500) compares out-of-control ARL and average delay across methods for AR(1) targets over a range of correlations and shift sizes. Results indicate LR and SPRT charts (with a reference value \(\Delta^*\)) are best when \(\Delta^*\) matches the true shift, while the generalized SR (GSR) chart is particularly strong for small variance increases and for late-occurring changes.","The variance-change model is \(X_t=Y_t\) for \(t<\tau\) and \(X_t=\mu+\Delta(Y_t-\mu)\) for \(t\ge\tau\) with \(\Delta>1\). Using the innovations form of a Gaussian process, the in-control log-likelihood is proportional to \(\sum_{j=1}^n (X_j-\hat X_j)^2/v_{j-1}\). The iid-type variance CUSUM recursion is \(S_n^+(\Delta^*)=\max\{0, S_{n-1}^+(\Delta^*)+(X_n-\mu)^2/\gamma_0-K(\Delta^*)\}\) with \(K(\Delta)=\log(\Delta^2)/(1-1/\Delta^2)\). For Gaussian time series, LR/GLR/SR statistics are built from likelihood ratios over change-point \(\tau\) and (for GLR/GSR) maximizing over \(\Delta\), with AR(1) yielding explicit recursive updates (e.g., (12) for LR and (13) for SPRT-based residual CUSUM).","All charts are calibrated to in-control ARL 500, and performance is evaluated via simulated out-of-control ARL and average delay for AR(1) targets across \(\phi_1\in\{-0.9,\ldots,0.9\}\) and \(\Delta\) values (with \(\Delta^*\) searched over \{1.10,1.20,1.30,1.40,1.50,1.75,2.00,2.25,2.50,2.75,3.00\} for reference-value charts). The LR and SPRT charts achieve their minimum out-of-control ARL when \(\Delta^*=\Delta\); among reference-value methods, LR and SPRT perform best and similarly. For small changes (reported as \(\Delta\le 1.3\)), the generalized Shiryaev–Roberts (GSR) chart attains the smallest ARL, sometimes outperforming even optimally tuned reference-value charts. For larger changes (roughly \(\Delta\ge 1.4\)), LR and SPRT dominate provided \(\Delta^*\) is not substantially below the true \(\Delta\); the generalized SPRT becomes competitive for larger shifts (noted as best generalized scheme for \(\Delta\ge 1.75\)).","The authors note that for time-dependent processes explicit formulas for ARL and related criteria are generally not available, so control limits and performance measures must be obtained via simulation. They also point out that some schemes (e.g., GLR) lack recursive presentations, making computation more time-consuming (hence fewer simulation replications for GLR).","The framework assumes a Gaussian model and a pure scale (variance) increase with mean unchanged; robustness to non-Gaussian innovations, heavy tails, conditional heteroskedasticity (e.g., GARCH), or simultaneous mean/variance shifts is not addressed. The study focuses primarily on AR(1) in simulations, so empirical guidance for broader ARMA structures and parameter estimation uncertainty (Phase I/estimated \(\phi\), \(\sigma^2\)) is limited. Practical implementation details such as estimating \(\mu\), model identification, and the impact of model misspecification on false-alarm behavior are not systematically evaluated.",None stated.,"Extend the charts and comparisons to non-Gaussian and heavy-tailed time series (including robust/Rank-based variants) and to conditional-variance models (e.g., GARCH) where variance dynamics are intrinsic. Study performance under parameter estimation and model misspecification (Phase I-to-Phase II transition), including self-starting or adaptive versions that update \(\phi\) and \(\sigma^2\) online. Provide software implementations and computational shortcuts for non-recursive GLR-type statistics and evaluate on real financial/industrial datasets with autocorrelation and structural breaks.",1209.4678v1,local_papers/arxiv/1209.4678v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:50:41Z
TRUE,Univariate|Multivariate|Nonparametric,Shewhart|EWMA|Hotelling T-squared|MEWMA|Other,Both,Theoretical/simulation only,TRUE,FALSE,NA,Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),"Univariate Phase I uses 80 in-control rational subgroups; Phase II generates 10,000 subgroups to estimate ARL. Univariate subgroup sizes studied: 10, 11, 12, 14, 15, and 20 (EWMA uses subgroup size 20). Multivariate examples use subgroup size 20 with 100 in-control subgroups for bootstrap reference and 1000 bootstrap resamples.",TRUE,R|MATLAB,Not provided,http://sas.uwaterloo.ca/stats_navigation/techreports/07WorkingPapers/2007-07.pdf,"The paper proposes robust alternatives to classical Shewhart $\bar{X}$–$S$ charts by replacing the subgroup mean with a Tukey trimmed mean and the subgroup standard deviation/dispersion with winsorized variability, aiming to reduce sensitivity to outliers while maintaining efficiency. It extends the idea to univariate EWMA charts by using trimmed subgroup means and winsorized scale in the standard EWMA control-limit formula, and to multivariate monitoring by defining trimming via data-depth functions (spatial, Tukey, Liu/simplicial, and Oja depth). For multivariate Shewhart-type monitoring it modifies Hotelling’s $T^2$ using the trimmed mean vector and winsorized dispersion matrix; for multivariate EWMA it defines a depth-trimmed MEWMA statistic based on trimmed mean vectors and winsorized covariance. Because the finite-sample distributions of the proposed multivariate statistics depend on depth choice, trimming cutvalue, and subgroup size, the paper uses bootstrap resampling to set empirical control limits. Simulation comparisons report that the proposed charts substantially outperform classical mean/variance-based charts in the presence of outliers while being comparable when no outliers are present, with depth-based multivariate versions also comparing favorably to rank-based and MCD-based robust alternatives in their tested scenarios.","Univariate $100\alpha\%$ trimmed mean: $\bar X_t=\frac{\sum_{i=t+1}^{n-t}x_{(i)}}{n(1-2\alpha)}$ with $t=\lfloor n\alpha+0.4\rfloor$; its standard error uses winsorized SD $s_w$: $se_t=s_w/\sqrt{n(1-2\alpha)}$. Univariate EWMA replacement: $Z_i=\lambda\bar X_{t,i}+(1-\lambda)Z_{i-1}$ and plug-in robust limits using $\bar{\bar X}_t$ and $s_t$ (winsorized-based). Robust Hotelling-type statistic: $\tau^2=(\bar X_t-\mu)^T S_w^{-1}(\bar X_t-\mu)$ where trimming is defined by a depth function and $S_w$ is the winsorized dispersion matrix from depth-trimmed/winsorized vectors. Robust MEWMA: $Z_{it}=\lambda\bar X_{it}+(1-\lambda)Z_{i-1,t}$ and charting statistic $\psi^2=(Z_{it}-\hat\mu_z)^T S_{zw}^{-1}(Z_{it}-\hat\mu_z)$ with $S_{zw}$ a winsorized covariance estimate for the $Z$ sequence; multivariate limits are obtained from bootstrap empirical quantiles (e.g., 90th percentile UCL, LCL = 0).","Across multiple simulation tables, the proposed trimmed/winsorized charts show much larger in-control ARL than classical charts when outliers are injected, indicating fewer false alarms under contamination (e.g., univariate Shewhart: with 1 outlier at subgroup size 20, mean-based ARL ≈ 25.1 vs 10% trimmed ARL ≈ 180). For univariate EWMA with an outlier, mean-based ARLs drop to about 1.19–2.69 while 10% trimmed EWMA maintains ARLs around 116.7–152 depending on $\lambda$. In multivariate $T^2$ under a single outlier with mean (5,5), the classical mean/covariance approach signals quickly (ARL ≈ 17.26) while depth-trimmed versions maintain substantially higher ARL (e.g., Liu depth ≈ 107, Tukey depth ≈ 111; spatial depth ≈ 147). In a comparison against Liu’s rank method and an MCD-based robust chart, the proposed method balances shift-detection and robustness: for a 0.25 shift ARL ≈ 24.9 (better than MCD ≈ 60.0, worse than Liu ≈ 13.4), while for 1 outlier from N(3,3) ARL ≈ 104.8 (far more robust than Liu ≈ 16.7 and comparable to MCD ≈ 94.3).","The authors note that the distributions of the proposed multivariate statistics $\tau^2$ and $\psi^2$ are hard to determine and standard distribution fits (e.g., gamma) are inadequate in the upper tail, so bootstrap is required for reliable cutoffs, which is computationally difficult in practice. They also note that performance and reliability depend on subgroup size and the trimming cutvalue; poor cutvalue choices can yield unstable UCL/LCL. Additionally, some depth functions have applicability limitations: Tukey depth can be unreliable for small subgroup sizes and assumes only a limited range of values for certain sample sizes, and Liu depth is limited to bivariate data and does not extend to higher dimensions.","The proposed procedures rely heavily on empirical/bootstrapped control limits tuned to specific subgroup size, trimming level, depth function, and in-control dataset, so performance may degrade under process autocorrelation, nonstationarity, or changes in sampling structure; no treatment of serial dependence is provided. Phase I parameter estimation under contamination is addressed via trimming/winsorization, but the impact of estimating depth cutvalues and covariance under high-dimensional settings (p large relative to subgroup size) is not studied, and matrix inversion stability for $S_w^{-1}$ may be problematic. Comparisons are mostly simulation-based with limited real-data validation, and the choices of quantile-based UCLs (e.g., 90th/95th) make direct fairness vs standard 3-sigma ARL targets somewhat unclear without explicit ARL0 calibration across methods.",None stated.,"Develop calibrated design procedures that set control limits to achieve a common in-control ARL (ARL0) across classical and robust charts, especially for the bootstrap-based multivariate statistics. Extend the methods to autocorrelated/streaming contexts (e.g., robust residual-based charts for ARMA processes) and to higher-dimensional monitoring with regularized/winsorized covariance estimation. Provide software implementations (e.g., an R package) with practical guidance on selecting trimming percentages, depth functions, and cutvalue estimation, and validate on real industrial/healthcare datasets.",1211.4262v1,local_papers/arxiv/1211.4262v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:51:32Z
TRUE,Univariate|Nonparametric,CUSUM|Change-point,Phase II,Theoretical/simulation only,FALSE,FALSE,NA,Simulation study,ARL (Average Run Length),Requires an available in-control reference sample of size m (stated as assumed larger than 10 in simulations; examples include m=10 and m=50) and sequential future observations n (tables reported for n up to 490).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a new univariate distribution-free (nonparametric) upper-sided CUSUM control chart for detecting small shifts in the process location, built from the standardized Mann–Whitney (Wilcoxon rank-sum) statistic under a change-point framing. The chart updates a one-sided CUSUM recursion using sequential standardized Mann–Whitney statistics computed from an in-control reference sample of size m and incoming future observations. Because the conditional signaling probability and decision values are analytically intractable, the authors estimate control limits h_{m,n}(\alpha) via Monte Carlo simulation to match specified in-control ARLs. Simulation results (for N(0,1) data) show the proposed chart is faster than a change-point chart (Hawkins et al., 2003) for small mean shifts, while the change-point chart is more sensitive for larger shifts. The work extends prior Mann–Whitney-based nonparametric monitoring (e.g., Zhou et al., 2009) by providing a CUSUM-type procedure rather than an EWMA/max-type chart.","Mann–Whitney statistic for split t of l: $MW_{t,l}=\sum_{i=1}^{t}\sum_{j=t+1}^{l} I(x_j<x_i)$, with in-control mean $E_0(MW_{t,l})=\tfrac{t(l-t)}{2}$ and variance $\operatorname{Var}_0(MW_{t,l})=\tfrac{t(l-t)(l+1)}{12}$. Standardized statistic: $SMW_{t,l}=(MW_{t,l}-E_0)/\sqrt{\operatorname{Var}_0}$. Proposed upper one-sided CUSUM: $S_j(m,n)=\max\{0,\,S_{j-1}(m,n)+SMW_{j,(m+n)}-k\}$ with $S_{m-m_0-1}=0$, $k=\Delta/2$ (set to $1/2$ in tables), and signaling via $S_{\max}(m,n)=\max_{m-m_0\le j\le m+n-1} S_j(m,n) > h_{m,n}(\alpha)$.","Decision values $h_{m,n}(\alpha)$ are tabulated (estimated by simulation using one million sequences of length 500 from N(0,1)) for target in-control ARLs 100, 200, 370, and 500, with m=10 or 50, $m_0=4$, and n from 1 to 490; limits increase initially and then stabilize. In ARL comparisons for N(0,1) with m=10 and $\alpha=0.005$ (ARL0=200), the proposed CUSUM outperforms the Hawkins et al. change-point chart for small shifts; e.g., at $\delta=0.025$ and $\tau=100$, ARL is 96.4 (CUSUM) vs 130.7 (C-PC), and at $\delta=0.050$ and $\tau=100$, 26.8 vs 41.1. For larger shifts the change-point chart is better; e.g., at $\delta=2.75$ and $\tau=100$, ARL is 3.1 (CUSUM) vs 2.0 (C-PC). The authors also argue qualitatively (via information/rank-use comparisons) that their CUSUM is more sensitive than several earlier nonparametric CUSUMs (Bakir–Reynolds, McDonald, Yang–Cheng) for small mean shifts.",The authors note that computing the proposed CUSUM statistic and especially obtaining the decision values/control limits requires computer programming/simulation because the needed conditional probabilities are too intricate for analytic solution. They also state that nonparametric control charts in general are largely theoretical and that development of practical applications is highly desired.,"Control limits are estimated under N(0,1) simulation even though the method is positioned as distribution-free; robustness of the estimated $h_{m,n}(\alpha)$ across non-normal in-control distributions is not demonstrated. The procedure assumes independent observations (no treatment of autocorrelation), which can strongly affect rank-based sequential monitoring in practice. The chart is framed with an in-control reference sample and fixed choices (e.g., $m_0=4$, $k=1/2$ in tables), but guidance on selecting $m_0$ and $k$ (or sensitivity of results to these choices) is limited.","They state that practical application development/real-world use of nonparametric control charts is highly desired, beyond the theoretical research focus.","Provide distribution-free (or robust) calibration methods for $h_{m,n}(\alpha)$ that do not rely on normal simulations, and study in-control performance under a range of continuous distributions. Extend the approach to autocorrelated data (e.g., by prewhitening or block/rank adjustments) and to two-sided or variance-shift monitoring. Release reproducible software (e.g., an R/Python implementation) and include real case studies to validate computational feasibility and operational performance.",1305.4318v1,local_papers/arxiv/1305.4318v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:52:12Z
TRUE,Univariate|Bayesian|Other,Shewhart|Other,Both,Manufacturing (general)|Food/agriculture|Other,FALSE,FALSE,NA,Case study (real dataset)|Simulation study,Detection probability|Other,"Very small subgroup sizes are explicitly targeted (e.g., n=2 or n=4 destructive tests per sampling period). Phase I examples use the first 10 samples (wood example, n=4) and about 18–22 samples (concrete example, n=2), noting typical Phase I guidance of ~40–50 data but allowing much fewer.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Bayesian Shewhart-type control chart for continuously comparing two processes by monitoring the ratio of their Weibull percentiles (e.g., 5th percentile strength), rather than means/variances. Both in-control and out-of-control parameters are treated as unknown; the chart avoids normalizing transformations and updates using accumulated information from all samples. A re-parameterization of the Weibull in terms of a chosen percentile $x_R$ and shape $\beta$ is combined with a Uniform prior for $\beta$ and an Inverse-Weibull prior for $x_R$, yielding recursive posterior updates. Control limits are obtained by transforming the conditional posterior of $x_R$ to a standard Gamma variable and the posterior of the percentile-ratio to an Inverted Beta variable, giving dynamically updated limits that can be computed from the first sample and then fixed after Phase I. The method is illustrated on two real applications (wood modulus of rupture; concrete compressive strength) and a simulated mean-shift scenario, showing the ratio chart can signal non-homogeneity and can work with very small sample sizes typical of destructive testing.","Weibull is re-parameterized by a percentile $x_R$: $F(x;x_R,\beta)=1-\exp\{-\ln(1/R)(x/x_R)^\beta\}$. The conditional posterior of $x_R$ given $\beta_k$ is transformed via $z=x_R^{-\beta_k}A(k)$ to obtain $z\sim\mathrm{Gamma}(k n+1,1)$, so percentile limits for $x_R$ follow from $x_R=z^{-1/\beta_k}A(k)^{1/\beta_k}$. For the ratio $u=x_R/y_R$, after transforming $v=u^{\beta_k}C(k)$, $v$ follows an Inverted-Beta form, and $u$ control limits are obtained by back-transforming Inverted-Beta percentiles.","In the wood example (Douglas fir MOR), Phase I uses the first 10 samples of size n=4 for each process; with $\alpha=0.27\%$ the fitted ratio chart shows no out-of-control signals, supporting homogeneity of the two productions. A simulated out-of-control state created by multiplying the last 15 observations of the second process by 1.15 (≈15% mean increase from ~4.5 to ~5.2) is signaled by the ratio chart at the 12th sample after the shift. In the concrete example, with n=2 per sample and 44 samples per line, the paper treats the first 22 samples as a tuning/Phase I period and notes Phase I can be reduced to ~18–22 samples (vs typical 40–50) while still achieving stable monitoring; the ratio chart shows no signal, supporting homogeneity.","The author notes that a complete understanding of the chart’s performance “needs further investigation,” specifically suggesting that a large Monte Carlo study will be carried out in a future dedicated paper.","The approach assumes the two percentiles are independent and share a common Weibull shape parameter $\beta$; violations (dependence between processes or unequal/unstable shapes) could degrade performance and are not thoroughly analyzed. Control-limit calibration relies on the chosen priors and the empirical/semi-empirical updating of hyperparameters (a compromise between Bayes and empirical Bayes), but sensitivity to misspecified priors and robustness across a wide range of shift types is not systematically evaluated. Performance is not summarized with standard SPC metrics (e.g., ARL/ATS/SDRL) across scenarios, limiting comparability with established CUSUM/EWMA or other percentile/ratio charts.",The paper explicitly suggests further investigation of performance via a “large Monte Carlo study” to be presented in a subsequent paper.,"Provide comprehensive ARL/ATS/SDRL comparisons versus alternative approaches (e.g., likelihood-based, CUSUM/EWMA for Weibull percentiles/ratios) under various shifts in scale, shape, and both. Study robustness to unequal or time-varying $\beta$, dependence between processes, and model misspecification (non-Weibull tails). Release implementation code (e.g., an R/Python package) and add guidance for prior elicitation and Phase I sizing/calibration in practical destructive-testing regimes.",1305.5962v3,local_papers/arxiv/1305.5962v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:52:45Z
TRUE,Univariate|Bayesian|Other,Shewhart|Other,Both,Manufacturing (general)|Semiconductor/electronics|Healthcare/medical|Finance/economics|Environmental monitoring|Network/cybersecurity|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),"Phase I is discussed as typically requiring a “sufficiently great number (say 50)” in-control training data; they recommend using about 50 training observations as a compromise (narrow limits vs. responsiveness). Example uses Phase I with 10 samples of size n=5, and other studies use m=25, m=30, and m×n=50 settings.",TRUE,Other,Not provided,NA,"The paper develops a semi-empirical Bayesian, Shewhart-type control chart for monitoring a specified Weibull percentile x_R (and optionally the Weibull shape parameter β) when both in-control and out-of-control parameters are unknown. Unlike many Weibull charts that transform data to approximate normality, it monitors Weibull percentiles directly using a Bayesian updating scheme that combines an initial prior with all accumulated samples (“learning by sampling”), yielding a memory-type chart. Control limits are probability-based and adapt during Phase I as more data accumulate, and then are fixed for Phase II monitoring. Performance is demonstrated with a carbon-fiber breaking-strength example and by Monte Carlo studies reporting ARL/SDRL across multiple out-of-control scenarios (mean/variance/shape changes), showing improved detection versus prior Weibull percentile charts (e.g., Padgett & Spurrier 1990) and a prior Bayesian Shewhart-type Weibull percentile chart (Erto & Pallotta 2007). The method is positioned for small-sample, high-reliability/rare-event monitoring and short-run/low-volume settings.","The Weibull percentile is parameterized via x_R = δ[\ln(1/R)]^{1/β}, and the chart targets x_R directly. With a Uniform prior for β on (β1,β2) and an Inverse-Weibull prior for x_R, the posterior after k samples of size n combines all accumulated data (likelihood over i=1,…,kn). Point estimates are the posterior mean \hat{β}_k from the marginal posterior of β and the conditional posterior mean \hat{x}_{R,k}=E(x_R\mid x^k,\bar{β}_k), where \bar{β}_k is an average of past \hat{β}’s. Control limits for x_R are derived by transforming the conditional posterior to a standard Gamma variable z with shape γ=kn+1 and mapping z-quantiles back to x_R via x_R = z^{-1/\bar{β}_k}\{a^{-\bar{β}_k}+\ln(1/R)\sum x_i^{\bar{β}_k}\}^{1/\bar{β}_k}.","In the Padgett & Spurrier (1990) comparison scenarios (n=5, m=25), the proposed chart (EP&M) shows lower out-of-control ARLs than P&S and than Erto & Pallotta (2007); e.g., for R=0.90 with β shifting 3→2 (δ=1), P&S reports ARL 84.82 while EP&M reports ARL 54.9 with SDRL 25.0. For a carbon-fiber case (R=0.99, n=5), a simulated percentile deterioration to x_0.99=0.26 is signaled after 7 samples using 10 Phase I samples; with a longer Phase I (additional resampling) and narrower limits, the signal occurs by the 3rd sample. Monte Carlo studies use N=1000 replications and report ARL/SDRL across scenarios affecting (μ,σ,β); detection remains good even with n=1, and the total number of observations to signal is noted to be roughly similar across subgroup sizes when m×n is held constant.",None stated.,"The approach depends on specifying a bounded Uniform prior interval for β; if the true β lies outside (β1,β2), posterior support cannot recover, which can compromise detection/estimation. The method’s performance is demonstrated largely under correct model specification (Weibull) and does not explore robustness to Weibull misfit, autocorrelation, or censoring (common in reliability data). Implementation requires numerical integration/root finding for β-posteriors/limits (and Gamma quantiles), but no reproducible software is provided, which may hinder adoption and benchmarking.",The authors note that the Bayesian framework enables additional opportunities such as computing the posterior predictive density function for process capability analysis.,"Assess robustness to departures from Weibull assumptions (e.g., mixtures, lognormal alternatives), censored/truncated lifetimes, and serial dependence; and develop diagnostic tools for model checking. Extend to multivariate or profile/functional reliability settings (multiple percentiles or multiple components) and to adaptive/optimal design choices for (R, α, n, m) under economic or Bayesian decision criteria. Provide open-source software (e.g., R/Python) and guidance for eliciting β-interval and x_R priors in practice, including sensitivity and calibration workflows.",1308.0691v1,local_papers/arxiv/1308.0691v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:53:21Z
TRUE,Univariate|Other,GLR (Generalized Likelihood Ratio)|Change-point|Other,Phase I,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,NA,Simulation study|Other,Detection probability|False alarm rate|Other,Uses individual observations in Phase I with a batch size of m=200 in simulations; cites guidance that a trial control chart typically uses about 20–25 samples of size 3–5 (Montgomery).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a likelihood-ratio-test (LRT) based retrospective (Phase I) model to detect and estimate multiple change points (multiple step shifts) in the mean of i.i.d. univariate Gaussian observations with common variance. The method evaluates candidate split points, uses the split maximizing the LRT statistic as the estimated change location, and extends to multiple changes via binary segmentation. To avoid bias in the LRT statistic across split locations, it normalizes the LRT by its in-control expected value, with thresholds calibrated by simulation to achieve specified false-alarm probabilities. Monte Carlo experiments (m=200, various shift sizes δ and up to seven detectable change points) assess estimator performance using accuracy (bias/standard error of estimated change locations) and precision (coverage of confidence intervals around estimated locations). The authors report that estimates are approximately unbiased for a single change and remain reasonably accurate/precise with multiple changes, improving as shift magnitude increases, while noting reliance on correct distributional assumptions.","For a candidate split at $m_1$ (with $m_2=m-m_1$), the maximized log-likelihoods under the two-segment model are combined as $l_a=l_1+l_2$, while the in-control (single-segment) maximized log-likelihood is $l_0$. The LRT statistic is $\text{lrt}(m_1,m_2)=-2(l_0-l_a)$, which is asymptotically $\chi^2$ with 2 d.f. The paper further uses a normalized statistic $N\text{lrt}(m_1,m_2)=\text{lrt}(m_1,m_2)/\mathbb{E}[\text{lrt}(m_1,m_2)]$ where the expectation is estimated under IC via simulation, and signals if any value exceeds an upper threshold; multiple changes are found by recursive (binary) segmentation.","Simulation uses m=200, σ=1, step-shift sizes δ∈{0.5,1,2,3,4,5}, uniformly spaced multiple shifts (e.g., for R=3 at 50/100/150; for R=4 at 40/80/120/160), with 1000 replications for estimating change-point locations. Example reported estimates: for R=3 and δ=2, true (50,100,150) estimated as (51.3, 98.7, 151.6); for R=4 and δ=3, true (40,80,120,160) estimated as (39.3, 79.2, 122, 160.3). Thresholds are calibrated via 5000 IC simulations: false-alarm probabilities (0.03, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01) correspond to thresholds (7.0089, 7.5745, 7.3684, 8.3876, 8.1206, 8.0292, 7.9153). Precision results include that with δ=0.5, 31% of $\hat\tau_1$ equal the true change point and about 50% of estimates are within 2 units; across shift sizes, about 90% of estimates fall within 15 units of the true location.","The authors state the method is restricted by distributional assumptions: forming the LRT requires knowing the exact distribution of the Phase I data, which is rarely available in real problems. They also note situations where heterogeneous segments may not follow identical distributional forms across groups, reducing practicability. They suggest using nonparametric approaches (e.g., clustering) or more generalized likelihood-based tests (GLRT) to mitigate these issues.","The approach assumes i.i.d. observations; common Phase I issues like autocorrelation or other dependence structures are not modeled and could inflate false alarms or bias change-point estimates. Control thresholds and the normalization by $\mathbb{E}[\text{lrt}(m_1,m_2)]$ are obtained via simulation for a fixed m=200, so operating characteristics may not transfer directly to other batch sizes or to unknown/estimated variance without additional calibration. The binary segmentation strategy can miss closely spaced changes or produce suboptimal multiple-change configurations compared with global optimization methods (e.g., dynamic programming) when changes are numerous.","They plan to generalize the approach to more general distributions (e.g., exponential family/normal family) and to detect other shift types such as linear trends or sporadic changes. They also propose extending the framework to simultaneously detect changes in more than one moment of the distribution (e.g., mean and variance). They additionally mention developing a more flexible generalized form (e.g., for Johnson family or exponential family distributions) when different segments may follow different distributional forms.","Developing a Phase I version that is robust to autocorrelation (e.g., via time-series residual charts or likelihoods for ARMA errors) would improve applicability to real processes. Providing an explicit procedure for unknown variance (including robust variance estimation under multiple shifts/outliers) and studying its impact on false-alarm control would strengthen practical guidance. Releasing an implementation (e.g., R/Python) and benchmarking against modern multiple-change-point algorithms (PELT, wild binary segmentation, fused lasso) on both synthetic and real datasets would clarify competitiveness and usability.",1403.0668v1,local_papers/arxiv/1403.0668v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:53:58Z
FALSE,Univariate,Shewhart|Other,Phase II,Other,NA,FALSE,NA,Other,Other,Not discussed,TRUE,Other,Not provided,www.dibru.ac.in|http://www.internalauditoronline.org/|http://www.jmp.com/software/pro/,"The paper applies educational data mining to four years (2010–2014) of Dibrugarh University BA/BSc examination data, analyzing pass percentages by subject, gender, and caste/category. It uses SAS JMP to visualize results through plots including run charts and control charts (Individuals and Moving Range charts) as exploratory/diagnostic graphics rather than developing new SPC methodology. The study reports descriptive findings: female candidates perform better than male candidates, general-category students outperform other categories, and certain majors show consistently high pass rates. Methods mentioned include regression and Bayesian classifiers, but the paper’s main contribution is comparative visualization and descriptive analysis of examination outcomes. No run-length/ARL theory, chart design, or SPC performance evaluation is presented, so SPC is not the primary focus.",Not applicable,"From the authors’ conclusions: female candidates’ pass percentages are higher than males (illustrated via bubble plot). General-category students have better results than other caste/category groups (illustrated via one-way analysis and 3D scatterplot). For BA majors, Bengali Major and Statistics Major are reported at 100% pass percentage, while some majors show fluctuating trends. For BSc majors, Geology, Electronics, Home Science, and Statistics majors are reported as consistently above average.",None stated,"The control charts are used as visualization for cross-sectional comparisons of pass percentages by subject/category rather than for monitoring a time-ordered process; thus the SPC interpretation (in-control/out-of-control) is unclear. The paper does not specify rational subgrouping, independence/autocorrelation considerations, or how control limits were estimated/validated, and it provides no ARL/false-alarm evaluation. Data preprocessing, missing data handling, and model validation details (e.g., for regression/Bayesian classifiers) are not described, limiting reproducibility and inferential strength.",The authors state they plan to develop a predictive model for BA and BSc students based on various variables.,"If SPC-style monitoring is intended, future work could define a true time-ordered monitoring framework (e.g., yearly/semester cohorts or within-exam sessions), establish rational subgrouping, and evaluate control-chart performance (false alarm rate/ARL) under realistic data-generating assumptions. Robust/nonparametric alternatives could be considered for bounded pass-percentage data, along with formal handling of changing cohort sizes and covariate adjustment (e.g., logistic/binomial modeling with control limits on residuals). Publishing cleaned datasets and JMP scripts (or code in R/Python) would improve reproducibility.",1411.2081v1,local_papers/arxiv/1411.2081v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:54:19Z
TRUE,Univariate|Other,CUSUM|Change-point|Other,Phase II,Transportation/logistics|Other,NA,TRUE,FALSE,Simulation study|Markov chain|Approximation methods|Other,False alarm rate|Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a modified CUSUM-based procedure to automatically detect steady-state intervals in pedestrian bottleneck experiments using density and speed time series (as surrogates for flow stability). The method replaces the classic two-sided CUSUM with a bounded, step-function update on z-scored observations and then defines steady state as periods where the statistic stays below a calibrated threshold. The threshold is calibrated via a fitted AR(1) model for the reference (manually selected) steady segment, enabling control of the false-transition risk level and explicitly accounting for autocorrelation. An analytical approximation of the stationary distribution of the AR(1)+bounded-statistic Markov chain is developed via numerical discretization and solved efficiently with a block Thomas algorithm; simulations are also discussed as an alternative. The approach is applied to three bottleneck experiment datasets and shows more reproducible steady-state detection than manual selection, and the flow difference between “all states” and “steady state” is mainly governed by the participant-to-width ratio N/b with a critical value around 115 persons/m.","Flow definitions: $J = N/T = 1/\Delta t_i$ and the smoother proxy $J = \rho\, v\, W$. Classic CUSUM uses $s_i^+=\max\{0,s_{i-1}^+ + x_i - Q(\alpha)\}$ and $s_i^-=\max\{0,s_{i-1}^- + Q(1-\alpha)-x_i\}$. The modified bounded statistic is $s_i = \min\{\max\{0, s_{i-1}+F(\tilde x_i)\}, s_{\max}\}$ with $F(\tilde x_i)=1$ if $|\tilde x_i|>q(\alpha)$ else $-1$, and $\tilde x_i=(x_i-\mu)/\sigma$; steady state is where $s_i<\theta$ (with reaction-time correction $t_{\text{reaching}}=(s_{\max}-\theta)/f$, $t_{\text{leaving}}=\theta/f$). Threshold calibration models the reference z-scores with AR(1): $y_i=c y_{i-1}+\sqrt{1-c^2}\,\varepsilon_i$, and derives the stationary distribution of $(y_i,s_i)$ via a Markov-chain balance equation and numerical discretization to obtain $\theta(\gamma)$.","The authors fix key tuning parameters at $\alpha=0.99$, $\gamma=0.99$, and $s_{\max}=100$, and recommend choosing $s_{\max}\approx 2\theta$. Robustness checks using three non-overlapping manually chosen reference segments on the same series yield nearly identical detected steady-state intervals, indicating reproducibility with respect to reasonable reference choices. In application to three bottleneck experiment groups (EG, AO, UO), the flow–width relationship remains approximately linear and consistent with constant flow per unit width. The absolute difference $Z=|J_{\text{all}}-J_{\text{steady}}|$ decreases as the ratio $N/b$ increases, with a reported critical value of approximately $N/b\approx 115$ persons/m above which the all-state flow is almost identical to steady-state flow.","The authors note that the detected transitions are delayed relative to the true transitions due to an inherent reaction time in the detection procedure, and they correct this by subtracting computed reaction times. They also state that the Markov-chain balance equations for the AR(1)-based analytical calibration do not have an explicit closed-form solution and require numerical approximation with an error that is difficult to estimate.","The method depends on having a manually selected reference steady segment to estimate $\mu$, $\sigma$, and the autocorrelation $c$, so it is not self-starting and may still inherit subjectivity from the initial reference choice (even if reduced). Although AR(1) is used to capture autocorrelation, higher-order dependence, nonstationarity, or non-Gaussian dynamics in pedestrian streams could invalidate the calibration and alter false-alarm properties. Performance is not summarized in standard SPC terms (e.g., ARL/ATS vs. shift size) across controlled scenarios, making it harder to compare with established CUSUM design literature. Implementation details (software, parameter sensitivity studies across broader settings) are limited, which may hinder practical adoption and reproducibility.","The authors suggest that future studies should use steady-state flow (rather than all-state flow) when combining results across different bottleneck experiments. For experiments where steady state is difficult or impossible to detect, they propose using the ratio $N/b$ to estimate the likely difference between all-state and steady-state flow.","Develop a self-starting or fully automatic reference-selection strategy to remove remaining manual input and quantify uncertainty in steady-state boundaries. Extend calibration beyond AR(1) to ARMA/long-memory or non-Gaussian models, and evaluate robustness under model misspecification with controlled false-alarm guarantees. Report standard SPC detection metrics (e.g., in-control/out-of-control ARL or expected detection delay) under synthetic transient/steady scenarios to enable comparison with alternative change-point and control-chart methods. Provide open-source implementation and guidelines for selecting $\alpha$, $\gamma$, and $s_{\max}$ across different sampling rates and noise levels.",1506.02433v1,local_papers/arxiv/1506.02433v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:54:54Z
TRUE,Univariate|Bayesian|Other,Other,Both,Manufacturing (general)|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Other,"Phase I uses m training samples of size n (examples: m=10, 20, 30 with n=4; Monte Carlo uses m=20 with n=5). Authors recommend exploiting about 80–100 training data (individual observations) as a compromise for setting control limits and the last joint posterior used to start Phase II.",TRUE,None / Not applicable,Not provided,NA,"The paper investigates the performance of a Bayesian control chart designed to continuously compare two processes by monitoring the ratio of their Weibull percentiles, u = x_R/y_R. It assumes both processes follow Weibull distributions with a common (stable) shape parameter β, while the percentiles are treated as independent random variables; priors are specified via a plausible value and an interval (β1, β2). A transformation maps the posterior of the ratio to an Inverted Beta distribution, enabling control limits to be set from its percentiles for a chosen false-alarm risk α. Performance is studied via (i) a real-data example on modulus of rupture (MOR) for two specimen sizes, and (ii) Monte Carlo experiments reporting ARL/SDRL under various percentile-shift scenarios. The paper highlights a tradeoff: more Phase I training data narrows the control interval but can reduce Phase II responsiveness because the resulting posterior used as a prior becomes too strong.","Weibull CDF: F(x;\delta,\beta)=1-\exp\{-(x/\delta)^\beta\}. The monitored parameter is the percentile ratio u=x_R/y_R; its posterior density (given data and β) is provided in Eq. (2) with A(k), B(k) in Eq. (3). Using the one-to-one transform v = u^{\beta_k} C(k) (Eq. 9), v has an Inverted Beta density (Eq. 10), so LCL/UCL for u are obtained by back-transforming the Inverted Beta percentiles v_{\alpha/2} and v_{1-\alpha/2}.","In the MOR case study with false-alarm risk \alpha=0.27% and an induced Phase II shift (multiplying the last 15 samples of process y by 1.15), the chart signaled after 12 Phase II samples when Phase I used m=10 (with UCL−LCL≈0.15). Extending Phase I to m=20 or m=30 narrowed intervals (≈0.12 and ≈0.10) but reduced responsiveness (no signal within the last 15 samples) unless only the most recent 10 IC samples were used to form the Phase II prior, yielding RL=10 (m=20) and RL=5 (m=30). Robustness to poor priors (±50% shifts in prior x_R, y_R, β) showed signal counts ranging roughly 11–21 runs with similar control-limit widths. Monte Carlo (N=1000) with n=5, m=20, R=0.95 reported in-control ARL≈370 and out-of-control ARLs such as 29.9 (SDRL 6.4) for (x_R^out,y_R^out)=(0.5,0.8) and 4.3 (1.1) for (0.5,1.5), with worst cases when only one process shifts moderately (e.g., 0.5/0.8, 0.8/1.2, 0.5/1.0 and inverses).",None stated.,"The chart relies on key assumptions that may be restrictive in practice: a common and stable Weibull shape parameter β across both processes and independence of the percentile parameters x_R and y_R. Control limits depend on the Bayesian updating scheme and the choice of how much Phase I data to include in the Phase II prior, which introduces a tuning/operational decision that could be subjective and application-dependent. The paper does not provide software or implementation details, which may hinder reproducibility and adoption.",None stated.,"Extend the method to allow different and/or time-varying shape parameters across the two processes (and provide a companion chart for monitoring β changes). Develop guidance or data-driven rules for choosing how many Phase I observations to include in the Phase II prior (to balance interval tightness vs. detection delay). Provide an implementation package (e.g., R/Python) and evaluate performance under dependence/autocorrelation and model misspecification (non-Weibull or contaminated data).",1507.01044v1,local_papers/arxiv/1507.01044v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:55:26Z
TRUE,Multivariate|Nonparametric,Other,Phase II,Manufacturing (general)|Theoretical/simulation only|Other,NA,FALSE,FALSE,Simulation study,ARL (Average Run Length)|False alarm rate,Not discussed (uses n=500 historical/reference observations in simulations; 5000 future observations used to estimate achieved false alarm rates/ARL).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a refined empirical half-space (Tukey) depth estimator, $R_n$, that replaces the empirical depth’s tail behavior with an extreme-value-theory (EVT) extrapolation so depth remains positive and informative outside the convex hull of the data. In one dimension it uses EVT tail probability estimators (based on an intermediate order statistic threshold $k$ and estimators of the extreme value index), and in higher dimensions it refines depth along all projection directions using multivariate regular variation and a Hill-type tail index estimator. The authors prove uniform ratio consistency of $R_n(x)/D(x)$ over regions extending far beyond the data cloud, where the classical empirical depth $D_n$ fails (often becoming zero). Simulation studies across multiple univariate and multivariate distributions show $R_n$ substantially improves tail depth estimation relative to $D_n$, especially for very small depth levels. The refinement is demonstrated in applications including nonparametric multivariate statistical process control (SPC), where using $R_n$ corrects severely inflated false alarm rates from $D_n$ and yields near-nominal false alarm performance while maintaining competitive detection (ARL).","Half-space depth: $D(x)=\inf_{\|u\|=1}P(u^T X\ge u^T x)$ and empirical depth $D_n(x)=\inf_{\|u\|=1}\frac{1}{n}\#\{i:u^T X_i\ge u^T x\}$. Univariate EVT tail estimator for $P(X>x)$: $p^r_n(x)=\frac{k}{n}\left(\max\{0,1+\hat\gamma\frac{x-\hat b}{\hat a}\}\right)^{-1/\hat\gamma}$ with $\hat b=X_{n-k:n}$; $R_n(x)$ equals $D_n(x)$ in the central region and uses EVT tail estimates beyond thresholds. Multivariate refinement uses projected data $W_i=u^T X_i$ and (under regular variation) tail estimate $p_{n,u}(w)=\frac{k}{n}(w/W_{n-k:n})^{-\hat\alpha}$; then $R_n(x)=\inf_{\|u\|=1}\{1-\hat F_u(u^T x^-)\}$ with empirical vs EVT splicing depending on whether the estimated tail probability is above/below $k/n$.","In SPC simulations with $n=500$ bivariate in-control reference data and nominal false alarm rate $\alpha=0.0027$, the $D_n$-based depth control procedure produces markedly inflated achieved false alarm rates because many future points fall outside the convex hull and get depth 0. Replacing $D_n$ by $R_n$ brings achieved false alarm rates close to the nominal level under both bivariate normal and heavy-tailed bivariate elliptical in-control distributions (shown via boxplots over 100 replications). For detection, ARL boxplots under mean shift, variance inflation, and combined shifts show the $R_n$-based procedure performs comparably to the (infeasible) procedure based on the true depth $D$, and close to a parametric Hotelling-$T^2$ approach when normality holds. Across tail-depth estimation simulations at very small depth levels (e.g., $D(x)$ as low as $1/2000$ with $n=500$), $R_n(x)/D(x)$ is well-centered near 1 while $D_n(x)$ is frequently 0.","The authors note that performance depends on the threshold parameter $k$ and that choosing an optimal $k$ is a known difficult problem in extreme value statistics; they use a heuristic stability-plot approach and acknowledge it may be suboptimal. They also remark that for light-tailed cases like the bivariate normal (not satisfying their multivariate regular variation assumptions), they modify the tail estimator (use the univariate-form EVT estimator and approximate the infimum over directions with random directions), implying the main theory does not directly apply there.","The SPC study focuses on achieving a pointwise false alarm rate for independently sampled future observations; it does not analyze run-length distributions or steady-state/conditional ARL under sequential monitoring, which are central in SPC practice. The depth-based signaling rule is essentially a one-step hypothesis test per observation; extensions to time-ordered dependence (autocorrelation) and batch/subgroup structures are not treated. Practical deployment would require efficient computation of $R_n$ in higher dimensions and guidance for selecting $k$ and number of directions (when approximating the infimum), but no software or computational complexity analysis is provided.","They suggest it would be worthwhile to develop a procedure for selecting an optimal $k$ specifically for $R_n$. They also propose investigating whether the EVT-based approach can be modified to refine other depth functions that suffer from the same empirical-count tail problem (e.g., simplicial depth), noting such modifications may be nontrivial because those depths lack the same projection structure.","For SPC specifically, studying the sequential run-length distribution (in-control and out-of-control ARL/SDRL/steady-state ARL) of an $R_n$-based control chart, including control-limit calibration under estimated in-control models, would strengthen the contribution’s process-monitoring relevance. Developing robust/adaptive versions that handle autocorrelated streams or concept drift, and providing scalable algorithms (and open-source implementations) for high-dimensional settings would improve practical adoption. Extending the method to profile monitoring or functional data depth variants could broaden applicability to modern manufacturing/healthcare sensing contexts.",1510.08694v1,local_papers/arxiv/1510.08694v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:56:10Z
FALSE,Other,CUSUM|Other,Both,Network/cybersecurity|Other,TRUE,FALSE,FALSE,Exact distribution theory|Simulation study|Other,ARL (Average Run Length)|Expected detection delay|False alarm rate|Other,Not discussed.,TRUE,None / Not applicable,Not provided,http://www.gs1.org/gsmp/kc/epcglobal/uhfc1g2/uhfc1g2_1_0_9-standard-20050126.pdf|http://www.rfidjournal.com/article/articleview/1080/1/1|http://www.rfidjournal.com/article/articleview/1385|http://www.rfidjournal.com/article/articleview/3211/1/1,"The paper proposes a unified framework for estimating RFID tag population in both static and dynamic settings using an Extended Kalman Filter (EKF), with a CUSUM change-detection layer to identify population shifts and adjust filter aggressiveness. The state is the (possibly time-varying) tag count, and the measurement is the idle-slot frequency from framed-slotted ALOHA; a normal approximation is used for large-scale systems. For the static case, the authors derive Lyapunov-drift-based sufficient conditions on tuning parameters (e.g., bounds on EKF pseudo-covariance update parameters) that guarantee mean-square exponential boundedness of estimation error and convergence of relative error toward zero. For the dynamic case, they model tag arrivals/departures as process noise and provide sufficient moment bounds under which the estimator remains stable, again using Lyapunov drift analysis. Extensive simulations (for populations around 10^4 and 10^5) show rapid convergence even with large initial misestimation and good tracking under dynamic changes, with CUSUM enabling faster reaction to detected shifts. Relative to SPC literature, CUSUM is used as an auxiliary change detector inside an RFID estimation/control framework rather than as a primary contribution to control chart methodology.","The measurement model uses idle-slot frequency: $y_k=N_k/L_k$, approximated as $y_k=p(z_k)+u_k$ with $p(z_k)\approx e^{-z_k/L_k}$ and Gaussian $u_k$ (large-scale normal approximation). State dynamics are $z_{k+1}=z_k$ (static) or $z_{k+1}=z_k+w_k$ (dynamic). The EKF update uses innovation $v_k=y_k-p(\hat z_{k|k-1})$ and Kalman gain $K_k=\frac{P_{k|k-1}C_k}{P_{k|k-1}C_k^2+R_k}$ with $R_k=\phi_k P_{k|k-1}C_k^2$; the CUSUM statistic is driven by a normalized innovation $\Phi_k$ and updates $g_k^+=\max\{0,g_{k-1}^+ + \Phi_k-\Upsilon\}$, $g_k^-=\min\{0,g_{k-1}^- + \Phi_k+\Upsilon\}$ with alarm if $g_k^+>\theta$ or $g_k^-<-\theta$.","The authors provide sufficient analytical conditions (via Lyapunov drift) under which the EKF-based estimator yields estimation error that is exponentially bounded in mean square and bounded with probability one for both static and dynamic tag populations. For the dynamic model $z_{k+1}=z_k+w_k$, stability is guaranteed under bounds on the first two moments of $w_k$ (they denote time-varying bounds such as $\lambda_k$ and $\sigma_k$). In simulations, they use parameters such as $q=0.1$, $P_{0|0}=1$, $J=3$, $\theta=4$, $\Upsilon=0.5$, and switch $\phi_k$ between 0.25 and 10; the estimator converges rapidly for $z_0=10^4$ and $10^5$ even when the initial relative error is as large as 0.9. For dynamic scenarios with substantial step-like changes (up to about 0.4–0.5 of the current estimate in their setup), the CUSUM-triggered parameter change enables quick re-convergence and maintains small relative error thereafter.","The authors note that their stability conditions (e.g., bounds on initialization error and parameter constraints) are sufficient and may be “too stringent,” and they remark that simulations indicate stability even when these conditions are not strictly satisfied. They also rely on large-scale approximations (e.g., normal approximation for idle-slot counts / differentiability approximations) to derive tractable models and bounds.","The work is not an SPC/control-chart paper; CUSUM is used as a component, but there is no systematic SPC-style design (e.g., control limit selection to meet a target in-control ARL under specified shift sizes) or comparison against mainstream SPC charts for similar detection problems. The method assumes iid-style measurement noise and uses a normal approximation that may be inaccurate for small frame sizes or low tag counts; robustness to model mismatch (e.g., capture effects, reader errors, nonuniform slot selection, correlated tag dynamics) is not fully explored. No implementation details (code, platform) are provided, which limits reproducibility and practical adoption.","They propose extending the framework to tag estimation with multiple RFID readers with overlapping coverage areas, leveraging the developed theoretical framework for that more complex setting.","A natural extension would be explicit statistical design of the CUSUM parameters ($\theta,\Upsilon$) for desired false alarm/detection-delay tradeoffs under realistic RFID noise models, including non-Gaussian and small-sample regimes. Further work could add robustness to protocol imperfections (missed reads, capture, interference) and temporal dependence, and provide open-source implementations to support adoption and benchmarking against alternative sequential detection/estimation methods (e.g., particle filters, GLR-based change-point detectors).",1511.08355v1,local_papers/arxiv/1511.08355v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:57:02Z
TRUE,Multivariate|Nonparametric|Other,Machine learning-based|Other,Both,Energy/utilities|Other,FALSE,NA,FALSE,Simulation study|Case study (real dataset),Expected detection delay|Other,"For the sampling-based SVDD training used within each window, the paper recommends choosing the SVDD sample size as v+1, where v is the number of variables (e.g., v=41 so sample size=42 in the TE study). Phase I/II monitoring also uses sliding windows with user-chosen window length n and overlap m (example: n=10,000 and overlap=3,000).",TRUE,MATLAB|SAS,Not provided,http://depts.washington.edu/control/LARRY/TE/download.html|http://blogs.sas.com/content/subconsciousmusings/2015/10/09/multistage-modeling-delivers-the-roi-forinternet-of-things/--is-epub/,"The paper proposes a new SVDD-based multivariate control chart for high-frequency data streams, called the K_T chart, to address interpretability and operational overload issues of the original SVDD K-chart when data arrive at very high rates. The method uses sliding windows: for each window it trains an SVDD model and uses the SVDD center a as a window-level measure of central tendency and the SVDD threshold R^2 as a window-level measure of dispersion; these are monitored on two charts (an a-chart and an R^2-chart). Because per-window SVDD training is computationally expensive at high frequency, the authors use a sampling-based SVDD training method that iteratively updates a “master” set of support vectors to approximate full training quickly and robustly. Control limits are set as UCL=R_a^2 (and LCL=0) for the a-chart using an SVDD trained on the Phase I sequence of window centers, and as R^2 ± 3σ_{R^2} for the R^2-chart using Phase I window thresholds. The approach is illustrated on Tennessee Eastman process data (simulated via existing MATLAB code) with very large Phase I/II datasets, demonstrating detection of most injected faults via either the a or R^2 chart.","SVDD with Gaussian kernel K(x_i,x_j)=exp(-||x_i-x_j||^2/(2s^2)) is used to compute a window’s center a and threshold R^2; scoring uses the kernel distance dist^2(z)=K(z,z)-2\sum_i K(x_i,z)+\sum_{i,j}\alpha_i\alpha_j K(x_i,x_j), and flags outliers when dist^2(z)>R^2. For K_T charts, each window W_i produces (a_i,R_i^2); the a-chart plots dist^2(a_i) where a_i is scored against an SVDD model trained on the Phase I set of centers A={a_i}. Control limits are UCL_a=R_a^2, CL_a=R_a^2/2, LCL_a=0; and for the R^2 chart UCL_{R^2}=\bar{R^2}+3\sigma_{R^2}, CL_{R^2}=\bar{R^2}, LCL_{R^2}=\bar{R^2}-3\sigma_{R^2} (optional warning limits at ±2σ).","In the Tennessee Eastman study, Phase I used 125,352 in-control observations and Phase II used 4,598,620 observations with faults interleaved; windows were n=10,000 with overlap m=3,000, Gaussian bandwidth s=25.5 (Peak criterion), outlier fraction f=0.001, and sampling-method sample size 42 (v+1 with v=41 variables). The K_T charts detected the presence of faults using either the R^2 or a chart for all faults except fault 14, 15, and 19. Reported detection delays were summarized in windows: delay 1 window for faults 4,5,7; delay 2 for fault 1; delay 3 for fault 6; delay 4 for fault 16; delay 5 for faults 2,3,10,11,12,13,18; and delay 6 for faults 8,9,14,15,19,20. The paper also claims substantial speed gains from the sampling-based SVDD training versus full-window SVDD, enabling feasible high-frequency/edge monitoring, though exact runtimes are not specified in the provided text.","The authors note that the original SVDD K-chart lacks guidance for interpreting the kernel-distance plot to infer shifts in mean or changes in variation, limiting usefulness for high-frequency/big-data monitoring where analysts are inundated with results. They also highlight that standard SVDD training is computationally expensive for large/high-frequency windows, motivating their sampling-based training approach. No additional explicit limitations of the proposed K_T chart are stated beyond fault cases it failed to detect in the TE study (faults 14, 15, and 19).","The method relies on windowing choices (n and overlap m) and kernel/SVDD hyperparameters (bandwidth s and outlier fraction f); performance and false-alarm behavior may be sensitive to these choices, yet systematic robustness/ARL calibration results are not provided. The a-chart’s center line is set to R_a^2/2 (a heuristic) and the a-chart limits come from SVDD on centers rather than an explicit run-length design, so in-control false-alarm properties (e.g., ARL0) are unclear. Training an SVDD per window (even with sampling) can still be computationally heavy for very high-dimensional streams or strict real-time constraints, and the approach does not provide built-in diagnostics to attribute signals to specific variables (only proposed as future work). Autocorrelation/irregular sampling common in high-frequency sensor streams is not explicitly modeled; interpolation is used in the example, which could affect signal characteristics and detection performance.",The authors plan to develop methodology to identify which variables are responsible for changes in process center and spread when the K_T charts signal. They also plan to research a new process capability index based on K_T chart statistics such as the threshold R^2 and the center a.,"Derive or calibrate in-control false-alarm performance (e.g., ARL0/ATS) for the a and R^2 charts under realistic dependence (autocorrelation) and under parameter estimation, and provide principled guidance for choosing control limits beyond heuristic rules. Extend the approach to handle concept drift and adaptive updating of the Phase I SVDD model as processes evolve, including self-starting or online re-estimation strategies. Develop robust variants that explicitly address missing/irregularly sampled multivariate streams without relying on interpolation, and benchmark against alternative nonparametric/high-dimensional multivariate SPC methods. Provide open-source implementations and computational benchmarks (runtime/memory vs. full SVDD and competing charts) to support practitioner adoption.",1607.07423v3,local_papers/arxiv/1607.07423v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:57:54Z
TRUE,Univariate|Multivariate|Bayesian,EWMA|CUSUM,Both,Theoretical/simulation only,TRUE,FALSE,FALSE,Simulation study|Economic design|Other,ARL (Average Run Length)|Other,Not discussed,TRUE,MATLAB,Personal website,https://www.dropbox.com/s/vva7yd3d8y0qqy2/SimulationCodeCostMEWMA.m?dl=0|https://www.dropbox.com/s/xdqz4z4mw4m7qta/AARL1MEWMA.m?dl=0|https://www.dropbox.com/s/kpr3nmert92xybv/ANFAMEWMA.m?dl=0,"The paper studies the economic design of memory-type control charts and shows that the widely used Lorenzen–Vance (1986) long-run expected average cost formula is incorrect when charting statistics are dependent, as in EWMA/MEWMA, CUSUM, and Bayesian charts. It demonstrates via extensive numerical experiments that even with very accurately estimated in-control and out-of-control ARLs, plugging ARLs into the Lorenzen–Vance formula can yield average-cost estimates that deviate materially (reported differences up to about 20%), leading to economically suboptimal chart parameters. The authors propose a direct simulation estimator of the long-run expected average cost based on simulating renewal cycles and taking the ratio of average cycle cost to average cycle time. They derive a corrected (modified) version of the Lorenzen–Vance formula for memory-type charts that replaces a single out-of-control ARL with an average over conditional out-of-control ARLs across possible change times and introduces the average number of false alarms per cycle. They conclude that simulation-based optimization is a practical alternative, while the corrected analytical formula is computationally burdensome because it requires estimating many conditional quantities.","The economic objective is the long-run expected average cost $F=\lim_{T\to\infty} \mathbb{E}\left(\frac{\int_0^T C(t)\,dt}{T}\right)=\frac{\mathbb{E}[\text{cycle cost}]}{\mathbb{E}[\text{cycle time}]}$. The paper critiques the Lorenzen–Vance ARL-plug-in cost formula for dependent (memory-type) statistics and proposes estimating $F$ by simulation as $\hat F=\frac{\sum_{i=1}^N C_i}{\sum_{i=1}^N T_i}$ over simulated renewal cycles. For MEWMA, the chart statistic is $Z_m=R\bar X_m+(I-R)Z_{m-1}$ (equal-weight special case) and signals when $Y_m=Z_m'\Sigma_{Z_m}^{-1}Z_m>UL$; the modified cost formula replaces the single $ARL_1$ term with an average $AARL_1=\sum_{m\ge1} P(A_m)\,ARL_1^m$ and uses ANFA (average number of false alarms per cycle).","Across 36 benchmark instances for EWMA (univariate) and MEWMA (multivariate) with equal weights, the paper reports that costs computed by the Lorenzen–Vance ARL-based formula can differ from cycle-simulation estimates by percentages that “increase up to 20%,” with particularly large errors when the smoothing parameter $r$ is small. When $r=1$ (reducing to Shewhart/Hotelling $T^2$ where statistics are independent), the Lorenzen–Vance approach aligns closely with simulation, with maximum deviation reported around 0.35%, validating that the issue is dependence/memory. Using Lorenzen–Vance for optimization can produce substantially inferior economic designs: the additional-cost percentages from using the improper optimal $r$ range from about 0.20% to 45.64% (EWMA) and 0.26% to 44.55% (MEWMA). The modified formula (using $AARL_1$ and ANFA) produces objective values much closer to simulation for the worst-case instances, but is slow because it requires estimating many conditional ARLs.","The authors note that although they derive a modified Lorenzen–Vance formula for memory-type charts, it “cannot be a basis for an efficient computational method” because it requires computing/approximating many conditional out-of-control ARLs for sufficiently large truncation levels, which is time consuming. They also state that similar analyses could be presented for other memory-type charts (e.g., CUSUM and Bayesian) but omit them “for the sake of brevity.”","The empirical demonstrations focus primarily on EWMA/MEWMA mean-shift monitoring under (multivariate) normality and an exponential in-control time-to-failure model; conclusions may not transfer quantitatively to other distributions, failure-time models, or shift types (e.g., variance/shape changes) without additional study. The paper recommends simulation-based optimization, but does not provide a systematic comparison of optimization strategies (e.g., variance reduction, stochastic approximation, surrogate modeling) or computational cost scaling across parameter dimensions, which is important for practical economic design. It also does not address robustness to parameter estimation uncertainty from Phase I (e.g., estimating $\mu_0,\Sigma$) which can materially affect both ARLs and economic design in applications.","The paper states that prior economic-design results over the past 30 years that rely on the Lorenzen–Vance formula for memory-type charts “require reappraisal,” suggesting future investigations using the proposed simulation method. It also identifies “another interesting open research area” as developing a more efficient computational method than the cycle-based simulation approach for evaluating/optimizing the corrected economic objective.","Develop computationally efficient estimators for the corrected objective (e.g., regenerative simulation with variance reduction, control variates, or quasi-Monte Carlo) and pair them with modern stochastic optimization methods to reduce design time. Extend the correction and validation beyond EWMA/MEWMA to CUSUM and Bayesian charts explicitly, including multivariate/high-dimensional settings and other change types (variance, covariance, autocorrelation). Study Phase I parameter estimation effects and propose economically optimal, self-starting or robust versions of the corrected economic design that account for estimation error and model misspecification.",1708.06160v1,local_papers/arxiv/1708.06160v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:58:36Z
TRUE,Univariate|Other,Shewhart,Both,Manufacturing (general)|Theoretical/simulation only|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length),"For estimating Phase I control limits, the authors simulate m samples and report negligible relative bias (≤ 0.001) for the variance estimator when m ≥ 20 (tested with m between 5 and 25). In the real-data illustration they use 25 Phase I samples to estimate limits and then 75 additional samples for Phase II monitoring (k = 3 or 5).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes Shewhart-type X-bar control charts for monitoring a normally distributed process mean when samples are collected via neoteric ranked set sampling (NRSS), a ranked-set design that orders a single set of k^2 units and then measures k selected units. Control limits are centered at the in-control mean and use the standard deviation of the NRSS sample mean, with variance computed from order-statistic theory under perfect ranking and estimated via a large preliminary Monte Carlo study under imperfect ranking (modeled via a bivariate normal with correlation rho between the auxiliary and study variables). Performance is assessed by Monte Carlo ARL comparisons across k = 3,4,5 and a range of mean shifts, showing NRSS charts generally yield smaller out-of-control ARLs than SRS, RSS, ERSS, and MRSS charts, and retain advantages except in a few low-shift/low-correlation cases. The authors also provide Phase I estimators for unknown parameters (mean and variance of the NRSS mean) and show via simulation that the variance estimators bias is negligible for m ","The NRSS sample mean is \(\bar{Y}_{\text{NRSS}} = \frac{1}{nk}\sum_{j=1}^n\sum_{i=1}^k Y_{[(i-1)k+\ell]j}\), with variance \(\mathrm{Var}(\bar{Y}_{\text{NRSS}})=\frac{1}{nk^2}\sum_i \mathrm{Var}(Y_{[(i-1)k+\ell]})+\frac{2}{nk^2}\sum_{i<i'}\mathrm{Cov}(Y_{[(i-1)k+\ell]},Y_{[(i'-1)k+\ell]})\). The proposed Shewhart limits are \(\text{LCL}=\mu_0-A\sqrt{\mathrm{Var}(\bar{Y}_{\text{NRSS}})}\), \(\text{CL}=\mu_0\), \(\text{UCL}=\mu_0+A\sqrt{\mathrm{Var}(\bar{Y}_{\text{NRSS}})}\) (or substituting Phase I estimates \(\bar{\bar{Y}}_{\text{NRSS}}\) and \(\widehat{\mathrm{Var}}(\bar{Y}_{\text{NRSS}})\) when parameters are unknown). Mean shifts are parameterized by \(\delta = |\mu_Y-\mu_0|\sqrt{k}/\sigma_0\).","In the perfect-ranking setting (\(\rho=1\)) and with \(\text{ARL}_0\approx 370.5\), NRSS produces uniformly smaller out-of-control ARLs than SRS, RSS, ERSS, and MRSS across the simulated shifts for k = 3, 4, and 5 (e.g., k=5, \(\delta=0.8\): NRSS ARL 9.55 vs SRS 71.55; k=5, \(\delta=1.6\): NRSS 1.46 vs SRS 12.38). Average relative efficiency summarized via geometric means indicates SRS ARLs were about 2.39","The authors note that NRSS performance deteriorates with imperfect ranking as the correlation between the auxiliary and study variables decreases, which is common to RSS-based designs. They also acknowledge an operational drawback: ranking \(k^2\) units in a single set (as required by NRSS) may be more difficult than ranking k sets of size k when ranking is based on visual judgment, though less problematic with an auxiliary variable.","The proposed chart is essentially a Shewhart-type mean chart; it is not optimized for very small shifts compared with memory-type charts (EWMA/CUSUM), and the paper does not benchmark against those alternatives under equal false-alarm constraints. The analysis largely assumes independence between successive samples/points and does not address autocorrelation or nonstationarity, which can materially affect ARL in practice. Implementation requires either order-statistic variance/covariance computations (perfect ranking) or substantial preliminary simulation to calibrate \(\mathrm{Var}(\bar{Y}_{\text{NRSS}})\) under imperfect ranking, which may limit ease of deployment without provided software.","They suggest, as future work, developing NRSS designs based on two or more ordering cycles (motivated by NRSSs superiority over MRSS and by the fact that some double-ranked-set schemes can achieve lower ARLs, albeit with higher operational cost).","Provide software (e.g., R/Python) to compute NRSS control limits (including imperfect-ranking calibration) and to aid practitioners in selecting k and A for target \(\text{ARL}_0\) under specific ranking-quality levels. Extend the approach to autocorrelated processes (e.g., with model-based residual charts) and to robust/nonparametric versions when normality is questionable. Compare NRSS Shewhart charts against NRSS-adapted EWMA/CUSUM/GLR charts for small and moderate shifts under matched false-alarm rates, and study the effect of estimation error more fully (finite-m Phase I) on Phase II performance.",1709.04979v1,local_papers/arxiv/1709.04979v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T02:59:27Z
TRUE,Other,EWMA,Phase II,Network/cybersecurity|Other,NA,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Other,Not discussed.,TRUE,MATLAB,Not provided,NA,"The paper proposes a change-detection framework for dynamic streams of attributed networks by combining (i) a generalized linear model (GLM) for static attributed network modeling with (ii) a state-space evolution model for the GLM coefficients. Parameters are updated online via an Extended Kalman Filter (EKF) (and an approximate alternative based on a linearized Kalman filter on MLEs), enabling explicit handling of temporal dependence (network autocorrelation). Abrupt changes in the edge-formation mechanism are detected by monitoring Pearson prediction residuals (averaged over edges) using an EWMA control chart with limits tuned via Monte Carlo simulation to achieve a target in-control ARL. Performance is evaluated on simulated binary and weighted (Poisson) network streams under global and local shifts, showing substantially smaller detection delays than static GLM and sliding-window baselines for small shifts. A case study on the Enron email network demonstrates signals aligning with known time periods around the Enron scandal, including changes in CEO–CEO communications.","Observation (GLM) and state evolution: $w_{ij,t}\sim f(\theta_{ij,t})$, with $\theta_t=g(X_t\beta_t)$, and state transition $\beta_t=F\beta_{t-1}+\xi+\epsilon_t$. EKF one-step prediction uses $\beta_{t|t-1}=F\beta_{t-1|t-1}+\xi$, $w_{t|t-1}=g(X_t\beta_{t|t-1})$, and update $\beta_{t|t}=\beta_{t|t-1}+K_t(w_t-w_{t|t-1})$. Pearson residuals are $r_{i,t+1}=(w_{i,t+1}-w_{i,t+1|t})/\sqrt{\mathrm{var}(w_{i,t+1|t})}$ and the monitored statistic is $\bar r_{t+1}=\frac{1}{m}\sum_{i=1}^m r_{i,t+1}$; EWMA: $z_{t+1}=\lambda\bar r_{t+1}+(1-\lambda)z_t$ with time-varying limits $\pm l\,s\sqrt{\frac{\lambda}{1-\lambda}(1-(1-\lambda)^{2t})}$.","Control limits were calibrated by Monte Carlo to target $\mathrm{ARL}_0=200$; the resulting $(l,\lambda)$ include Dynamic: Binary $(2.44,0.1)$ and Weighted $(2.53,0.1)$ (Table 1). In simulations, the dynamic (EKF+EWMA) method yields much smaller out-of-control run lengths than static GLM and sliding-window baselines for small shifts; e.g., Binary Scenario 1 with shift magnitude $\delta=1.5$ had $\mathrm{ARL}_1=8.57$ (SE 0.144) for the dynamic method versus 113.27 (SE 3.869) static and 59.32 (SE 2.770) sliding-window (reported in text). For larger shifts (roughly $\delta\ge 2.5$), all methods become comparable, but the dynamic method remains slightly better in several settings. In the Enron case study, the EWMA chart signaled between weeks 79 and 89, aligning with the period when the Enron scandal was revealed; additional analysis indicated sharp changes in estimated CEO–CEO communication probabilities around weeks ~76 and ~89.","The authors note that detecting local changes in binary networks can be difficult because the excessive communication signal may not be completely captured due to limited data, reducing detection power (observed as larger ARL1 for Scenario 2 in binary networks). They also mention that when networks are large, the nonlinear EKF observation equation may be computationally burdensome, motivating an approximate linear-KF alternative.","The monitoring statistic averages Pearson residuals across all edges ($\bar r_t$), which can dilute highly localized anomalies and may reduce sensitivity compared with scan-statistic or subgraph-focused monitoring unless tailored subnetwork charts are constructed. The approach assumes conditional independence of edges given attributes and the state, and relies on correct specification of the GLM link/distribution and the linear state transition model; misspecification could inflate false alarms or degrade detection. Parameter choices for $F,\xi,Q,R$ and the Monte Carlo tuning procedure may be nontrivial in practice, and the paper does not provide a fully automated estimation/tuning workflow for real deployments.",The authors suggest combining community detection techniques with the proposed methodology to detect changes in community structures over time.,"Extend the charting/monitoring layer to better target localized or multi-community changes (e.g., multiple EWMA charts over adaptive subnetworks, scan-EWMA, or multivariate statistics over residual vectors). Develop robust/nonparametric variants to reduce sensitivity to GLM/state-model misspecification and overdispersion (especially for count-weighted networks). Provide practical guidance and estimation procedures for state-space parameters ($F,\xi,Q,R$) and for handling evolving node sets, missing snapshots, or irregular sampling common in real network streams.",1711.04441v1,local_papers/arxiv/1711.04441v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:00:11Z
TRUE,Univariate|Multivariate|Bayesian,Other,Both,Manufacturing (general)|Theoretical/simulation only,NA,NA,NA,Economic design|Other,Other,Not discussed,FALSE,None / Not applicable,Not applicable (No code used),NA,"This note critiques broad claims in the SPC literature that Bayesian control charts (specifically, posterior-probability threshold/control-limit policies) are generally economically optimal. It traces such claims in papers like Calabrese (1995) and Makis (2008, 2009) back to Taylor (1965, 1967) and argues that these citations do not establish general optimality. The authors emphasize that Taylor (1965) provides an analytical counterexample showing non-optimality of a Girshick–Rubin-type Bayesian rule in a non-100% inspection setting, undermining the idea that simple posterior-threshold Bayesian charts are optimal in general. The paper positions the key open problem as characterizing truly optimal quality-control policies for the common Bayesian process-control/POMDP formulations used in the literature. It is primarily a conceptual/commentary contribution rather than a new chart design or performance study.",Not applicable,Not applicable,None stated,"Because this is a short commentary, it does not provide new optimality theorems, constructive alternative (non-threshold) optimal policies, or numerical demonstrations for typical SPC models beyond citing Taylor’s counterexample. It also does not map out precise sufficient conditions under which posterior-threshold Bayesian charts are optimal, leaving practitioners without actionable guidance on when the criticized optimality claims might still hold. No empirical/simulation comparisons are included to quantify practical impact on ARL/cost for representative processes.","The authors state that an open area is to characterize the class of optimal quality-control charts for the discrete-time/continuous-time production systems considered in Girshick and Rubin (1952), Calabrese (1995), Makis (2008), and related work, and that characterizing optimal controls for the general class of POMDPs covering these quality-control problems remains a future research direction.","Develop explicit counterexamples and benchmarks in common Bayesian SPC models (attributes and variables; univariate/multivariate) to quantify how far posterior-threshold rules can be from optimal under realistic sampling/inspection costs. Derive verifiable sufficient conditions (e.g., monotone likelihood ratio/TP2, convexity, cost structure) under which threshold policies are optimal, and characterize when more complex policies (e.g., non-monotone, multi-threshold, or belief-dependent sampling) are required. Provide implementable algorithms and open-source software for computing optimal or near-optimal POMDP policies and comparing them to standard Bayesian control charts under both finite- and infinite-horizon objectives.",1712.02860v2,local_papers/arxiv/1712.02860v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:00:26Z
TRUE,Univariate|Self-starting|Nonparametric,CUSUM,Both,Manufacturing (general)|Network/cybersecurity|Healthcare/medical|Finance/economics|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length),"Self-starting scheme requires reference size at least m ≥ 2d − 1 for Theorem 1 to hold exactly; simulations suggest m=20 works well for d ∈ {10,20,30,40}. Monitoring in simulations and comparisons begins after a warm-up of 20 observations.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a distribution-free, nonparametric adaptive CUSUM control chart designed to detect arbitrary distributional changes (not limited to location or scale). The method converts each observation into categorical indicators via quantile-based binning, then constructs two likelihood-ratio CUSUMs that preserve ordering information: one using left-to-right cumulative unions (effective for location shifts) and one using center-outward unions (effective for scale changes), and signals using their maximum. To avoid specifying the unknown out-of-control distribution, it adaptively estimates multinomial cell probabilities using data since the most recent reset-to-zero time (a Lorden–Pollak style approach) with Dirichlet-prior smoothing, yielding a computationally simple recursion with no practitioner tuning parameter. A self-starting scheme updates the needed in-control quantiles sequentially so the chart can run with limited Phase I data while maintaining the nominal in-control ARL. Simulations compare favorably against nonparametric change-point-detection (CPD) charts (CvM- and Lepage-based), and a real manufacturing dataset (aluminium electrolytic capacitor capacitance) illustrates detection and built-in post-signal diagnostics (indicating a negative location shift).","Data are categorized into d bins using in-control quantiles to form multinomial indicators Y_t, then cumulative sums Z_{t,j}=\sum_{\ell\le j} Y_{t,\ell} are used to preserve ordering. The ordered-binning CUSUM recursion is S^{(i)}_t=\max\{0, S^{(i)}_{t-1}+\sum_{j=1}^{d-1}\frac{d^2}{j(d-j)}[Z^{(i)}_{t,j}\log(\frac{\sum_{\ell\le j}p^{(i)}_\ell}{j/d})+(1-Z^{(i)}_{t,j})\log(\frac{1-\sum_{\ell\le j}p^{(i)}_\ell}{1-j/d})]\}, for i=1 (left-to-right) and i=2 (center-outward). In the adaptive version, p^{(i)}_\ell is replaced by \hat p^{(i)}_{t,\ell}=(\alpha_\ell+N^{(i)}_{t,\ell})/(\sum_{r=1}^d \alpha_r+N^{(i)}_t), where counts N^{(i)}_{t,\ell} reset to 0 when the corresponding CUSUM returns to 0; the overall charting statistic is \hat S_t=\max(\hat S^{(1+)},\hat S^{(1-)},\hat S^{(2+)},\hat S^{(2-)}).","Control limits h for target ARL0 are obtained by simulation (10,000 replications) and bisection; e.g., for ARL0=500: h≈235.241 (d=20), and for ARL0=370: h≈218.886 (d=20). Self-starting ARL0 simulations (10,000 runs) show that with m=20 reference points, achieved ARL0 is close to nominal across N(0,1), t(2.5), and LN(1,0.5) for d up to 40 (e.g., ARL0=500 and d=20 gives ≈496–504 depending on distribution). Across extensive ARL1 comparisons versus CvM-CPD and Lepage-CPD, the proposed chart is typically near-best overall across location, scale (especially scale decreases), and several non-location/scale distribution changes; the authors recommend d=20 as a good compromise. In the real AEC manufacturing dataset (200 observations), with ARL0 set to 500 and d=20 (monitor after 20-point warm-up), the chart signals at observation 188 and diagnostics indicate a negative location shift.",None stated.,"Although claimed distribution-free under continuous in-control distributions, practical performance depends on accurate sequential quantile estimation; for small m relative to d (m < 2d−1) the early in-control behavior deviates from the nominal multinomial model. The choice of Dirichlet prior parameters \alpha is anchored to a specific “smallest meaningful” normal shift (±0.25), which introduces an implicit design choice that may not be optimal across applications. The paper does not address serial dependence; autocorrelation could inflate false alarms or delay detection when applied to time-series processes.","The authors plan to further evaluate performance of several derived variants that target specific change types, e.g., charts based only on \hat S^{(1+)} (positive location), \hat S^{(1-)} (negative location), \hat S^{(2+)} (scale increase), \hat S^{(2-)} (scale decrease), or combinations such as \max(\hat S^{(1+)},\hat S^{(1-)}).","Extend the method to explicitly handle autocorrelated observations (e.g., via residual charting, block bootstrap calibration of h, or model-based prewhitening) while preserving the distribution-free goal. Provide principled guidance or data-driven selection for d and for the prior \alpha beyond the fixed normal-shift heuristic, potentially via minimax or Bayesian predictive criteria. Develop and release reference implementations (e.g., an R/Python package) and evaluate robustness under ties/discrete data and under missing or irregularly sampled observations.",1712.05072v1,local_papers/arxiv/1712.05072v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:01:08Z
TRUE,Univariate|Other,Shewhart|EWMA|Machine learning-based|Other,Both,Manufacturing (general),TRUE,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Other,Not discussed (data context: 147 products with per-product lot counts ranging from 2–3 up to 173; Phase I uses 3/4 of one year’s data and Phase II uses 1/4 in simulations).,TRUE,Other,Not provided,NA,"The paper presents an SPC case study for monitoring a single manufacturing process that produces many different products with inherently different measurement levels, making separate per-product charts ineffective for detecting shared root causes. The method first standardizes measurements within each product (center/scale), then applies joint control charting across products using Individuals-Range (IR) charts and EWMA charts to detect deviations. A simulation study compares different standardization choices (mean vs. median; SD vs. robust SD vs. IQR vs. MAD) using in-control ARL (ARL0) and out-of-control ARL (ARL1), including a small outlier component to mimic measurement anomalies. Results suggest using the median for centering and a robust SD (M-estimator) for scaling, yielding ARL0 near the nominal 370 and good sensitivity under shifts. After a signal, a partition-tree model is used on process covariates (materials/tooling/line/operators/date) to propose potential root causes, illustrated in an application example.","Standardization per product is defined as $y^{(s)}_{ij}=(y_{ij}-\bar y_i)/s_i$ and a robust alternative $y^{(r)}_{ij}=(y_{ij}-\tilde y_i)/s^{(r)}_i$ (median center with robust scale such as M-estimated robust SD). The EWMA statistic is $z_i=\lambda y_i+(1-\lambda)z_{i-1}$ with $z_0:=\bar y$ and $\lambda=0.2$ used. Simulated outliers are generated via $y_{ij}=\mu_i+e_{ij}+B\cdot 25\cdot e_{ij,ol}$ with $B\sim\text{Bernoulli}(0.01)$ and increased outlier variance.","In-control ARL0 for the IR chart varies widely by standardization choice; excluding outliers, Mean+RStdDev gives ARL0=463.6 and Median+RStdDev gives ARL0=391.8, while including outliers reduces ARL0 to 157.7 (Mean+RStdDev) and 151.2 (Median+RStdDev). MAD- and IQR-based scaling can yield much lower ARL0 (more false alarms), especially when products have very few lots (MAD can be near zero). Under simulated shifts affecting subsets of products (root causes A/B/C), robust SD scaling generally produces lower ARL1 (faster detection) than plain SD at comparable ARL0; e.g., for root cause B at 4σ, ARL1 is 95.2 (Median+RStdDev) vs 110.1 (Median+StdDev). The paper recommends median centering with robust SD scaling as a compromise between realistic ARL0 and sensitivity (ARL1).","The authors note that standardization induces correlation among transformed observations, and this induced correlation structure is not explicitly accounted for, especially problematic when there are few lots per product. They state it remains open to justify or modify the approach for small per-product sample sizes, potentially using more advanced short-run SPC methods.","Control-limit performance is evaluated primarily via simulations tailored to one company’s product-mix and production schedule, so generalizability to other multi-product settings is uncertain. The monitoring approach treats the standardized series as suitable for conventional IR/EWMA limits despite cross-product heterogeneity and potential time dependence in production conditions, which may affect false-alarm rates beyond what ARL summaries capture. The root-cause step relies on observational covariates and a single interpretable learner (partition tree), so confounding and model instability could lead to misleading factor attributions without confirmatory analysis.","The authors suggest further research on incorporating other (potentially more advanced) control charts after standardization and on justifying or modifying the approach to account for the induced correlation structure, particularly for products with small numbers of production lots. They also point to potentially incorporating more advanced short-run SPC methods (as in cited work) if correlation/false-alarm behavior becomes problematic.","Develop a formal Phase I/Phase II framework that explicitly models the correlation induced by per-product standardization (e.g., hierarchical/empirical Bayes shrinkage or mixed models) and derives adjusted control limits with guaranteed ARL0. Extend the method to handle autocorrelation/irregular sampling (e.g., state-space EWMA/CUSUM) and to multivariate monitoring when multiple lab properties are tracked jointly. Provide a reproducible implementation (e.g., R/Python package) and benchmark against alternative pooling strategies (Q-charts, risk-adjusted charts, GLR/change-point methods) on multiple real multi-product datasets with documented root-cause outcomes.",1801.01660v1,local_papers/arxiv/1801.01660v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:01:43Z
TRUE,Univariate|Nonparametric|Other,Shewhart|Change-point|Other,Both,Manufacturing (general)|Theoretical/simulation only,FALSE,FALSE,NA,Simulation study|Markov chain|Case study (real dataset)|Other,ARL (Average Run Length)|Other,"Uses m0 historical (reference) in-control data; simulations set m0 = 20 and also examine m0 = 30, 50, 100. In the piston rings example, 25 in-control samples (subgroups of size 5) are used to set up Phase II monitoring.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes exact distribution-free runs- and patterns-type control charts for monitoring an unknown target value/location parameter for both continuous and discrete data. Observations are transformed to a Bernoulli sequence via thresholding, and monitoring is performed using runs/pattern statistics—studied in detail for the longest run and scan statistic—together with data-dependent (conditional) control limits. In-control run length is shown to be geometric with parameter α, implying exact ARL0 = 1/α for any underlying i.i.d. distribution, achieved by conditioning on the total number of ones and using random permutation arguments. The required conditional distributions for scan and longest-run statistics are computed exactly using a (modified) finite Markov chain imbedding (FMCI) technique, enabling exact conditional probabilities for sequential control limits. Performance is evaluated by simulation under normal, t(4), and gamma(1,1) distributions and by a piston rings case study, demonstrating robustness to non-normality and illustrating how choices of threshold c and window size r affect detection of mean shifts.","Data are dichotomized as $X_n=\mathbf{1}\{Y_n\ge c\}$ to form Bernoulli outcomes, then (i) scan statistic $S_n(r)=\max_{1\le t\le n-r+1}\sum_{i=t}^{t+r-1}X_i$ and (ii) longest run $L_n$ are monitored. Conditional in-control probabilities such as $P(S_n(r)\ge c_n(\alpha)\mid S_{n-1}(r)<c_{n-1}(\alpha),N_n)$ and $P(L_n\ge k_n(\alpha)\mid L_{n-1}<k_{n-1}(\alpha),N_n)$ are constrained to be ≤ α, yielding geometric in-control RL with ARL0 = 1/α. Exact conditional distributions like $P(S_n(r)<s\mid N_n=m)=\xi_0\prod_{t=1}^n N_t(m)\,\mathbf{1}$ are computed via FMCI transition matrices for patterns/runs in an $[n-m,m]$-specified random permutation.","By construction, the in-control run length distribution is geometric with parameter α for any i.i.d. underlying distribution, so ARL0 is exactly controlled at $1/\alpha$ (e.g., α = 0.005 gives ARL0 = 200; α = 0.0025 gives ARL0 = 400 in the case study). In simulation for the longest-run (R-2) chart under N(0,1), ARL1 depends strongly on threshold c; for mean shifts µ = 1,2,3 the best-performing c values are approximately 0,1,2 respectively, and a guideline of $c\approx \mu-1$ is suggested. Example ARL1s for R-2: under N(0,1), (µ=1,2,3) with varying c include 25.22 (c=0, µ=1), 7.39 (c=1, µ=2), and 3.70 (c=2, µ=3); under t(4) and gamma(1,1) with c=µ−1, ARL1s are (23.27, 5.16, 3.11) and (6.25, 3.84, 2.81) for µ=1,2,3, respectively. For scan-rule (R-1) charts with c=2, ARL0 is near 200 and ARL1 suggests larger r helps smaller shifts (e.g., µ=2: ARL1 14.61 for r=6 vs 10.25 for r=8), while smaller r helps larger shifts (µ=3: 3.88 for r=6 vs 4.39 for r=8). In the piston rings application (ARL0=400), the chart signals around t≈9–14, with average signal time ≈11.86 over 100 repeats of the procedure.","The authors note a drawback: if the chart fails to detect a mean shift early, performance can deteriorate because the sequential conditioning makes the procedure adapt to the shifted process, requiring progressively stronger evidence to signal later. They suggest mitigation strategies such as restarting monitoring, using a large reference sample, or using a lagged conditioning scheme so statistics at time t depend on t−k rather than t−1.","The approach hinges on the i.i.d. assumption; with autocorrelation, the permutation/conditioning arguments and exact in-control guarantees may fail without modification. The method requires repeated computation of conditional probabilities/control limits via FMCI, which may become computationally heavy for large n, large window sizes r, or more complex pattern sets, potentially limiting real-time deployment. The performance focus is primarily on location (mean) shifts via thresholding; sensitivity to variance/shape changes is only suggested (via alternative patterns) rather than thoroughly demonstrated with comparative studies against established distribution-free EWMA/CUSUM competitors.","The authors propose developing a lagged version of the conditioning argument (using dependence on t−k instead of t−1) to prevent the chart from adapting to a shifted process when early detection fails, and state they will pursue this idea in a subsequent paper.","Extend the framework to explicitly handle autocorrelated or Markov-dependent data while retaining (approximate) distribution-free in-control guarantees. Provide scalable algorithms/software (e.g., R/Python) for computing data-dependent control limits and randomized tests, including guidance on choosing r and c and computational complexity. Develop and benchmark designs for variance/scale shifts and joint location-scale monitoring using alternative pattern sets, and compare systematically to leading nonparametric EWMA/CUSUM and rank-based charts under common scenarios and contaminated distributions.",1801.06532v2,local_papers/arxiv/1801.06532v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:02:23Z
TRUE,Univariate|Self-starting|Other,CUSUM|Change-point,Phase II,Network/cybersecurity|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length),Not discussed,TRUE,R,Not provided,https://dev.twitter.com/docs/streaming-apis|https://www.R-project.org/|https://cran.r-project.org/package=rtweet|https://CRAN.R-project.org/package=stringr|https://CRAN.R-project.org/package=glue|https://CRAN.R-project.org/package=tidyverse|http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html|https://CRAN.R-project.org/package=changepoint|http://dx.doi.org/10.1561/1500000011|http://dl.acm.org/citation.cfm?id=2390470.2390490|http://apiwiki.twitter.com/Streaming-API-Documentation|http://proceedings.mlr.press/v17/bifet11a.html|https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.2213|http://www.sciencedirect.com/science/article/pii/S0925231212007606,"The paper proposes a lightweight, real-time framework to detect changes in Twitter hashtag sentiment streams using a lexicon-based sentiment score per tweet and online change detection. Change detection is performed via a two-sided, self-updating CUSUM control chart that does not require a historical Phase I reference sample; after each signal, the CUSUM is reset and the in-control mean parameter is re-initialized from recent observations. The CUSUM is presented in likelihood-ratio form and specialized to the Gaussian mean-shift case, with detection controlled by a threshold related to average run length (ARL) targets for false alarms and detection delay. The approach is demonstrated on a real Twitter stream for the hashtag ""theresamay"" (15,491 English tweets), showing detected positive/negative sentiment change points that visually align with moving-average shifts. The detected online change points are also compared against an offline multiple change-point method (via the changepoint package), with broadly consistent results aside from minor delays.","The online CUSUM is defined using the cumulative log-likelihood ratio $S_k=\sum_{i=1}^k s_i$ with $s_i=\ln\frac{p_{\theta_1}(y_i)}{p_{\theta_0}(y_i)}$. For Gaussian data with constant variance $\sigma^2$ and a mean shift from $\theta_0$ to $\theta_1$, $s_i=\frac{\theta_1-\theta_0}{\sigma^2}\left(y_i-\frac{\theta_0+\theta_1}{2}\right)$. The decision statistic is $g_k=S_k-m_k$ where $m_k=\min_{1\le j\le k} S_j$, and a signal occurs when $g_k>h$ (equivalently $S_k>m_k+h$); the stopping time is $t_a=\min\{k:g_k>h\}$.","A real-time case study streams tweets from 2018-03-15 to 2018-03-24 for hashtag ""theresamay"" yielding 15,491 English posts and a bounded per-tweet sentiment score time series (using the ""bing"" lexicon). For the two-sided CUSUM initialization they set $\theta_0=-0.5$ and change magnitude 0.5, implying $\theta^{pos}_1=0$ and $\theta^{neg}_1=-1$, and choose threshold $h=20$ to yield only a small number of changes per day. The detected online change points (positive in yellow, negative in blue) align with shifts in a moving average (window size 200). Offline multiple change-point detection (penalty $2\log(n)$, with $Q$ set to the number of CUSUM detections) produces similar change locations with only minor delays relative to the CUSUM signals.","The authors note that assuming known pre- and post-change parameters (e.g., $\theta_0$ and $\theta_1$ in the likelihood-ratio CUSUM) is unrealistic in practice and that usually these parameters would be estimated from test data, which they assume is not available. They also state that selecting the threshold $h$ is application/user-dependent.","The method applies a Gaussian mean-shift CUSUM to lexicon-derived sentiment scores that are discrete, bounded, and typically non-Gaussian; performance under heavy tails/zero inflation is not assessed. It assumes independence of observations, but tweet streams can be temporally clustered and autocorrelated, which can inflate false alarms and distort ARL. The paper does not provide a principled procedure for setting $\sigma$ (or estimating it online) or for calibrating $h$ to achieve a target in-control ARL under realistic streaming conditions.",They plan to examine approaches that enhance the robustness of the change detection algorithm and to further test methodologies for sentiment characterization.,"Extend the monitoring to distribution-free or bounded-data CUSUM variants (e.g., rank/sign-based or Bernoulli/Poisson models) and explicitly account for serial dependence using residual charts or time-series modeling before SPC. Provide an online parameter-learning/calibration strategy (self-starting estimation of location/scale and data-driven choice of $h$ to meet a target ARL). Validate across multiple hashtags and event types with ground-truth change annotations, and release reusable software for end-to-end streaming, monitoring, and diagnostics.",1804.00482v1,local_papers/arxiv/1804.00482v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:03:05Z
TRUE,Univariate|Other,Shewhart|Other,Both,Manufacturing (general)|Environmental monitoring|Other,TRUE,FALSE,TRUE,Simulation study|Case study (real dataset),False alarm rate|Other,"For initial implementation (Phase I), the paper suggests using 100 observations or more to ensure reasonably accurate initial parameter estimates; the worked example uses K=100 subgroups of size n=5.",TRUE,None / Not applicable,Not provided,NA,"The paper develops control-charting procedures for processes where measurements are highly left-censored due to a fixed detection limit, making traditional Shewhart charts inappropriate (especially when censoring exceeds ~70%). Assuming an in-control normal distribution, it replaces each censored observation (t \le C) with a conditional expected value (CEV) based on in-control parameters, then applies Shewhart-style subgroup statistics to monitor both the mean (CEV-X chart) and dispersion (CEV-S chart). The in-control parameters (\mu, \sigma) are estimated via an iterative maximum-likelihood algorithm that repeatedly imputes censored values with their CEV weights and re-estimates parameters until convergence. Because analytical control limits are not provided, standardized upper control limits for various subgroup sizes and censoring proportions are obtained via simulation using a Type I error (false alarm risk) of \alpha=0.0027. A geotextile drainage test example illustrates Phase I estimation and chart construction, emphasizing one-sided (upper) limits to detect increases in mean and/or variability under left censoring.","Censored observations are replaced by weights $w_i$: $w_i=t$ if $t>C$, and $w_i=W_c$ if $t\le C$, where the conditional expected value for left censoring is $W_c=E(T\mid T\le C)=\mu-\sigma\,\frac{\phi(Z_c)}{\Phi(Z_c)}$ with $Z_c=(C-\mu)/\sigma$. Parameters are estimated iteratively by MLE using $\hat\mu=\frac{1}{n}\sum_{i=1}^n w_i$ and a corresponding iterative update for $\hat\sigma$ involving $r$ (uncensored count) and a function $\lambda(Z_c)$ of $\phi/\Phi$. Final (one-sided) control limits are computed from standardized simulated coefficients: $\mathrm{UCL}_X=\mu+\sigma\,\mathrm{UCL}_{\bar X}$ and $\mathrm{UCL}_S=\sigma\,\mathrm{UCL}_S$.","Control limits are derived from simulation (more than 1000 simulated estimates per censoring level) and tabulated/curved for subgroup sizes $n=3,5,10,20$ and censoring proportions, using a false-alarm risk of $\alpha=0.0027$. In the worked geotextile example with detection limit $C=50$ ml/h and Phase I data of $K=100$ subgroups of size $n=5$, the initial (naive) estimates from censored data are reported as $\mu_0=50.0846$ and $\hat\sigma=0.2720$, while the proposed MLE/CEV approach yields $\hat\mu=49.0279$ and $\hat\sigma=0.9915$. The computed CEV weight is $W_c\approx 48.7330$ and the theoretical censoring proportion is reported as $P_c\approx 0.843$. Using standardized coefficients for $n=5$ at the relevant censoring level, the example uses $\mathrm{UCL}_{\bar X}=1.42$ and $\mathrm{UCL}_S=2.09$, giving $\mathrm{UCL}_X\approx 50.4358$ and $\mathrm{UCL}_S\approx 2.0524$, with no out-of-control points in the Phase I charts.","The authors note that when censoring is high the estimation algorithm can be imprecise and lead to biased parameter estimates, and that subgroups with all censored observations provide little information for detecting changes. They also state that maximum likelihood estimates work well for large samples and that the iterative MLE procedure can require substantial computational effort when the censoring level is large. They emphasize that detecting decreases in the mean (and, with >50% censoring, decreases in dispersion) is difficult under left censoring, motivating one-sided limits focused on increases.","The approach assumes a correctly specified normal in-control model and a fixed, known detection limit; robustness to non-normality, heteroscedasticity, or varying/uncertain limits is not analyzed. Control limits are obtained via simulation but details of the simulation design (e.g., number of replications beyond “>1000”, seed control, Monte Carlo error) and sensitivity to Phase I estimation uncertainty are limited, which can affect achieved in-control false-alarm performance. The method is essentially an imputation-based Shewhart framework and may be less sensitive to small sustained shifts than memory charts (e.g., EWMA/CUSUM) adapted for censoring; such comparisons are not provided. No implementation guidance (software/code) is given, which may hinder reproducibility for practitioners.","The paper states that many other practical censoring schemes should be investigated, implying extensions beyond fixed-level left censoring.","Extend the CEV idea to EWMA/CUSUM/GLR charts for improved sensitivity to small shifts under heavy censoring, and study steady-state and conditional signaling properties when parameters are estimated. Develop robust/nonparametric variants that reduce reliance on normality and evaluate performance under model misspecification and autocorrelation. Provide reproducible software and a clearer simulation protocol, and broaden validation with additional real industrial/environmental datasets and varying detection limits (e.g., time-varying or measurement-system drift).",1804.00760v2,local_papers/arxiv/1804.00760v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:03:46Z
TRUE,Univariate|Profile monitoring|Other,Shewhart|Other,Phase II,Manufacturing (general)|Environmental monitoring|Other,FALSE,FALSE,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length),"Not discussed (simulation uses n = 200, 300, 500, 1000; real examples use n = 18 and n = 729; ARL0 fixed at 200).",TRUE,R,Not provided,https://www.R-project.org/,"The paper proposes the Beta Regression Control Chart (BRCC) for monitoring fraction/rate/proportion data in (0,1) when the response depends on control variables. It models both the mean and dispersion of a beta-distributed quality characteristic via regression structures (beta regression with varying dispersion), producing observation-specific control limits based on beta quantiles. The chart targets detection of shifts in the process mean and/or dispersion while avoiding normality-based regression control chart issues (e.g., limits outside (0,1), inflated false alarms under asymmetry). Performance is evaluated primarily through extensive Monte Carlo simulation using ARL0 and ARL1 comparisons versus a constant-dispersion BRCC variant (BRCCC) and the standard regression control chart (RCC), showing faster detection (lower ARL1) especially when the mean is near 0 or 1 and for dispersion increases. Two real-data applications (tire manufacturing proportion loss and Brasília relative humidity proportions) illustrate practical use and show BRCC identifying out-of-control observations missed by BCC/RCC.","The BRCC fits a beta regression with varying dispersion: mean and dispersion follow link-based regressions $g(\mu_t)=\sum_{i=1}^k x_{ti}\beta_i$ and $h(\sigma_t)=\sum_{i=1}^s z_{ti}\gamma_i$ with $\mu_t\in(0,1)$ and $\sigma_t\in(0,1)$. For a chosen in-control ARL0, set $\alpha=1/\text{ARL0}$ and define time-varying control limits using the beta quantile function: $\text{UCL}_t=Q(\alpha/2;\mu_t,\sigma_t)$ and $\text{LCL}_t=Q(1-\alpha/2;\mu_t,\sigma_t)$ (implemented with MLEs $\hat\mu_t,\hat\sigma_t$ from maximizing the log-likelihood). Constant-dispersion can be tested via an LR test on dispersion-submodel coefficients: $LR=2\{\ell(\hat\beta,\hat\gamma)-\ell(\tilde\beta,\tilde\gamma)\}\sim\chi^2_{s-1}$ under $H_0$.","Monte Carlo evaluation uses 50,000 replications with ARL0 fixed at 200 (i.e., $\alpha=0.005$), across scenarios and sample sizes (reported mainly for n = 200 and n = 1000). Across mean-shift experiments (adding $\delta$ to the mean linear predictor), BRCC shows consistently smaller ARL1 than BRCCC and especially RCC, with RCC sometimes yielding ARL1 greater than ARL0 when the mean is near boundaries (e.g., for n = 1000: ARL1 ≈ 237 at $\delta=-0.02$ for a process with $\mu\approx0.20$, ARL1 ≈ 256 at $\delta=0.04$ for $\mu\approx0.80$, and ARL1 ≈ 587 at $\delta=-0.04$ for $\mu\approx0.08$). For dispersion increases (adding $\delta$ to the dispersion linear predictor), BRCC detects changes fastest (lowest ARL1) in all scenarios, outperforming BRCCC and RCC. In the tire-manufacturing case (ARL0=200), BRCC flags observation 6 as out-of-control while BCC and RCC do not; in the Brasília humidity case (ARL0=200), BRCC flags five atypical observations whereas RCC flags none and BCC flags only one.",None stated.,"The method assumes independent beta-distributed observations conditional on covariates; serial dependence (common in environmental time series like humidity) is not modeled, which can distort false-alarm rates and ARL in practice. The chart depends on correct specification of both mean and dispersion submodels (links/covariates); misspecification can bias estimated limits, but robustness studies beyond the compared RCC/BRCCC are limited. Implementation requires iterative MLE fitting and repeated computation of beta quantiles; while feasible, the paper does not provide guidance on computational burden or on-line updating for real-time monitoring.",None stated.,"Extend BRCC to explicitly handle autocorrelation/seasonal dependence (e.g., beta regression with ARMA errors or state-space structures) and assess its impact on in-control ARL calibration. Develop Phase I procedures for establishing in-control model/limits (including robust estimation and outlier cleaning) and self-starting/online refitting strategies. Provide open-source reference implementations (e.g., an R package) and study robustness to model misspecification and boundary-inflation (zeros/ones) via inflated beta models.",1804.01454v1,local_papers/arxiv/1804.01454v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:04:25Z
TRUE,Multivariate,EWMA|Hotelling T-squared|MEWMA,Phase II,Theoretical/simulation only,TRUE,FALSE,NA,Approximation methods|Simulation study|Markov chain|Integral equation|Other,ARL (Average Run Length)|Steady-state ARL|Expected detection delay|Other,Not discussed,TRUE,R,Not provided,http://www.netlib.org,"The paper develops accurate approximations for the steady-state (quasi-stationary) distribution of the MEWMA statistic used to detect shifts in the mean vector of multivariate normal data with known covariance. It shows that the steady-state density can be expressed as a product of two one-dimensional functions (radial part and angular part), enabling feasible numerical calculation even in higher dimensions; the derivation uses a representation of the noncentral chi-square density via the confluent hypergeometric limit function. Building on integral-equation (Fredholm) formulations and Nyström quadrature, the paper computes conditional and cyclical steady-state ARLs as well as worst-case ARLs with improved accuracy compared to earlier approaches. Numerical studies (including Monte Carlo confirmation and comparisons to Markov chain methods) demonstrate that steady-state behavior changes in non-intuitive ways as dimension p increases, affecting the relationship between zero-state, steady-state, and worst-case ARL. The paper also derives optimal MEWMA smoothing constants λ for different ARL criteria and shows MEWMA’s advantage over Hotelling’s T² Shewhart chart for small-to-moderate shifts, especially as dimension grows.","The MEWMA statistic is updated by $Z_n=(1-\lambda)Z_{n-1}+\lambda X_n$ and signals when $T_n^2=(Z_n-\mu_0)'\Sigma_Z^{-1}(Z_n-\mu_0)>h_4$, giving stopping time $N=\inf\{n\ge1:T_n^2>h_4\}$. In-control ARL satisfies a Fredholm integral equation $L^{\circ}(\alpha)=1+\int_0^h L^{\circ}(u)\,\lambda^{-2} f_{\chi^2}(u/\lambda^2\mid p,\eta\alpha)\,du$; steady-state (quasi-stationary) density solves an eigenfunction equation $\varrho\psi^{\circ}(u)=\int_0^h \psi^{\circ}(\alpha)\,\lambda^{-2} f_{\chi^2}(u/\lambda^2\mid p,\eta\alpha)\,d\alpha$. For the out-of-control case, a two-dimensional ARL integral equation in $(\alpha,\gamma)$ is used, and the steady-state density factors as $\psi(u,w)=d(w)\psi^{\circ}(u)$ with $d(w)\propto (1-w^2)^{(p-3)/2}$.","For $E_\infty(N)=200$, the in-control steady-state ARL is below 200 and decreases with dimension (e.g., for $\lambda=0.1$, $D^{\circ}$ is 192.6 at $p=2$ and 186.4 at $p=50$; $D^{\circ *}$ is very close, e.g., 192.7 at $p=2$ and 186.9 at $p=50$). For $\lambda=0.1$, the steady-state ARL values computed by the new integral-equation method match Monte Carlo results (reported with $10^9$ replications) extremely closely in Table 2 (e.g., at $p=4,\sqrt{\delta}=1$, $D\approx 11.36$ and $D^*\approx 11.38$ with matching MC). Comparisons indicate Prabhu & Runger (1997) effectively reported cyclical steady-state ARL ($D^*$) and that the proposed Nyström/Gauss–Legendre approach achieves higher accuracy with much smaller systems (r=30 leading to 30 and 900 dimensional systems). Optimal $\lambda$ depends on the ARL criterion; for change magnitude $\delta=1$ and $E_\infty(N)=200$, the minimizing $\lambda$ is smallest for steady-state $D$ and larger for worst-case $W$ (Figure 5), with optimal $\lambda$ decreasing as dimension increases.","The analysis assumes serially independent multivariate normal observations with known covariance matrix $\Sigma$, and focuses on a mean shift under a simple change-point model. The paper notes that prior Markov chain approaches for steady-state ARL are complicated and lack published software implementations, motivating alternative numerical methods. (No broader practical limitations are explicitly emphasized beyond these modeling/implementation constraints.)","Results rely heavily on the assumptions of independence and multivariate normality with known $\Sigma$; performance and steady-state behavior may differ under autocorrelation, heavy tails, or covariance changes. The methods are largely evaluated via numerical computation and simulation (with limited real-data demonstration), so practical robustness and diagnostic guidance for practitioners are not established. The design/optimization of $\lambda$ is illustrated for selected $E_\infty(N)$ and shift magnitudes; sensitivity to misspecified targets, estimated parameters, or Phase I estimation error is not addressed.","The paper suggests leveraging the decomposition idea (Lemma 1) to compute finite-change-point conditional delays $D_\tau=E_\tau[N-\tau+1\mid N\ge\tau]$ for $\tau=1,2,3,\ldots$ to study convergence of $D_\tau\to D$ and assess when the steady-state approximation is valid. It also implies further investigation of the ‘odd’ steady-state behavior for large dimension p and its implications for design.","Extend the steady-state and ARL computations to settings with unknown/estimated covariance and mean (Phase I/Phase II linkage) and assess the impact of estimation error on steady-state and worst-case ARL. Develop versions that explicitly handle autocorrelated multivariate streams (e.g., via state-space modeling or residual-based MEWMA) and study steady-state behavior under dependence. Provide open-source implementations and reproducible benchmarks (including real industrial datasets) to facilitate adoption and comparative evaluation against modern multivariate monitoring methods (e.g., robust/nonparametric MEWMA, high-dimensional shrinkage-based charts).",1808.05069v1,local_papers/arxiv/1808.05069v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:05:14Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Both,Healthcare/medical,TRUE,TRUE,TRUE,Simulation study|Case study (real dataset)|Other,False alarm rate|Other,Uses the first m = 19 days as Phase I data for estimating parameters (both centers). Control limits are set with a fixed false alarm rate α = 0.02 (≈ one false alarm every 50 days on average).,TRUE,R,Not provided,https://dx.doi.org/10.1016/j.ijmedinf.2019.03.011,"The paper proposes a data-quality monitoring scheme for telehealth vital-sign data when observations are missing and the number of available measurements varies randomly by day and vital sign. The approach aggregates each day’s multivariate measurements across available participants into a mean vector and adapts a Hotelling’s T-squared chart by modifying the covariance of the daily mean via a weighting matrix that reflects paired observation counts under a Missing At Random (MAR) assumption. Robust Phase I estimation of the mean and covariance uses the OGK estimator (via R package rrcov), and signals are diagnosed using the Mason–Young–Tracy (MYT) decomposition. Control limits are obtained by simulation with a target false alarm probability α = 0.02, and the method also reduces dimension and adjusts limits when an entire vital sign is missing on a day. Retrospective and prospective case studies in Hong Kong elder care centers show the chart detects known device capping errors (e.g., SBP capped) and flags other abnormal measurement episodes, supporting timely correction of the telehealth system.","Daily subgroup mean for vital sign j on day i is $\bar X_{ij}=\frac{1}{n_{ij}}\sum_{k\in U_{ij}} X_{ijk}$, where $n_{ij}=|U_{ij}|$ varies with missingness. Under MAR, the covariance of the daily mean vector satisfies $\Sigma_{\bar X_i}=W_i\circ\Sigma$ (Hadamard product), with $[W_i]_{jj'}=\frac{|U_{ij}\cap U_{ij'}|}{n_{ij}n_{ij'}}$. The monitoring statistic is $T_i^2=(\bar X_i-\hat\mu)^\top (W_i\circ\hat\Sigma)^{-1}(\bar X_i-\hat\mu)$, signaling when $T_i^2>\mathrm{UCL}$ (with dimension reduction and an adjusted UCL if some components of $\bar X_i$ are missing).","Control limits were set by simulation at α = 0.02, yielding UCL = 17.31 for center A and UCL = 18.59 for center B (with an example reduced UCL of 13.29 when only three vital signs are available). In center A, the chart detected the known SBP capping issue (signals from 20.02.2018–02.03.2018) but with a reported detection delay of four days; MYT decomposition attributed signals primarily to SBP (and BT also influencing the signal). In center B (prospective use), a signal on 09.03.2018 was traced to DBP being set to a maximum level and was corrected; additional later signals were linked to an unusually high HR for one participant (13.04.2018) and unusual SBP/SpO2 levels (29.05.2018). Missingness levels reported were 10.3% overall in center A and 3.9% in center B, motivating the varying-sample-size/missing-data adjustment.","They note that the Phase I covariance estimation is challenging under simultaneous outliers, missing data, and deviations from normality; they state that an estimator that can handle outliers, missing data, and slight non-normality in subgrouped data “does not (yet) exist.” They also acknowledge their control-limit simulation assumes complete data as a simplification, though they report that alternative missing-data scenarios changed UCLs by at most about 5%.","The method’s validity depends strongly on the MAR assumption and independence across individuals; if missingness is informative (MNAR) or if there is within-day or between-day dependence not removed by averaging, the covariance adjustment $W_i\circ\Sigma$ and resulting $T^2$ calibration may be biased. Control limits are obtained via a bespoke simulation procedure (not an analytic distribution), and the practical robustness of UCLs to different missingness patterns, subgroup sizes, and covariance nonstationarity is only lightly explored. Using daily averages may reduce sensitivity to localized or subgroup-specific data-quality problems (e.g., a single device/operator affecting a subset), and the chart may confound data-quality shifts with genuine population health shifts without additional diagnostics.","They suggest comparing the proposed approach to existing monitoring methods for missing observations (e.g., imputation-based approaches). They also propose extending to faster-detecting schemes such as a multivariate CUSUM with dynamic probability control limits (building on Huang et al., 2016).","Developing control limits and run-length properties that explicitly incorporate random $W_i$ (rather than approximating with complete-data simulations) would strengthen theoretical calibration under missingness. Extensions to autocorrelated/seasonal structures (e.g., day-of-week effects) and to mixed-effects or hierarchical models could better separate true physiological shifts from system measurement errors. Providing an open-source implementation (R/Python) with end-to-end handling of dimension reduction, MYT decomposition, and limit simulation would improve reproducibility and adoption.",1809.03127v2,local_papers/arxiv/1809.03127v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:05:53Z
TRUE,Univariate|Nonparametric,Shewhart|Other,Both,Manufacturing (general),TRUE,FALSE,NA,Simulation study,ARL (Average Run Length)|False alarm rate|Other,"Not discussed (Phase I described as k rational subgroups of size n, but no specific recommended values are given).",TRUE,MATLAB,Not provided,NA,"The paper develops robust Shewhart S control-charting schemes by replacing conventional Phase I estimators (sample mean and sample standard deviation) with robust location and scale estimators to mitigate the impact of outliers/contamination. For Phase I, subgroup scale is estimated using MAD, Qn (Rousseeuw–Croux), and logistic M-scale (MSLOG), and the location of subgroup scales is estimated using M-Huber, Harrell–Davis, and Hodges–Lehmann estimators, yielding multiple robust S-chart variants. Three Phase I contamination mechanisms are studied (diffuse symmetric, diffuse asymmetric, and localized contamination), while Phase II data are then monitored for variance inflation via a multiplier ϕ (e.g., ϕ=1.4). Performance is assessed via Monte Carlo simulation in MATLAB using Phase I MSE (for estimator efficiency) and Phase II unconditional ARL0 (false alarm behavior) and ARL under disturbance (detection performance), with control limits calibrated to target ARL0=370.4 under clean normal data. The results indicate robust estimators substantially reduce false alarms and improve detection robustness when Phase I data are contaminated, with Qn performing best under diffuse asymmetric contamination and MAD/MSLOG performing well under diffuse symmetric contamination; under localized contamination, sample S can remain competitive if paired with a robust location estimator.","The Shewhart S-chart control limits are set as $\mathrm{LCL}=L_n\hat\sigma$ and $\mathrm{UCL}=U_n\hat\sigma$, where $\hat\sigma$ is estimated from Phase I and $L_n,U_n$ are chosen so that $P(L_n\sigma\le \hat\sigma_i \le U_n\sigma)=1-\alpha$. Run length is treated via $\mathrm{ARL}=E(1/p)$ (estimated by Monte Carlo when parameters are estimated). Scale estimators include sample SD $S_n=\sqrt{\frac{1}{n-1}\sum (x_i-\bar x)^2}$, $\mathrm{MAD}=b_n\,\mathrm{med}(|x_i-\mathrm{med}(x)|)$, and $Q_n=c_n\,\mathrm{med}(|x_i-x_j|;i<j)$; MSLOG is given as an M-scale defined by an average $\rho$-equation with tunable breakdown point.","Control limits were calibrated so the unconditional in-control ARL under clean normal Phase I data equals 370.4. Under diffuse symmetric Phase I contamination (model 1), at high contamination (a=4) and using mean+SD, ARL0 dropped to about 78.1 (high false-alarm rate), whereas robust combinations (notably using MAD/MSLOG with robust location estimators) maintained much higher ARL0; for detection with ϕ=1.4, mean+SD ARL rose to about 480.9 at high contamination while MSLOG+Harrell–Davis achieved a lower ARL around 287.5. Under diffuse asymmetric contamination (model 2), conventional methods could yield a false signal about every 38.5 samples at high contamination, while Qn produced ARL0 about 364 (near the 370.4 design) and also gave the strongest out-of-control performance; Qn+M-Huber is highlighted as a strong choice at moderate/high contamination. Under localized contamination (model 3), SD remained relatively efficient in Phase I, but robust location estimators were important; for ϕ=1.4 at moderate contamination, pairing SD with Harrell–Davis reduced ARL to about 66.2 compared with much larger ARLs when using mean+SD.",None stated.,"The study focuses on Shewhart S-charts and variance shifts (modeled via a multiplier ϕ) and does not address autocorrelation or other common industrial complications (e.g., dynamic processes, measurement error, or nonstationarity). Control-limit design and comparisons are primarily simulation-based for specific contamination models (20% contamination with selected distributions/parameters), so conclusions may be sensitive to these choices and may not generalize to other contamination rates or mechanisms. No implementation details (e.g., algorithmic settings/iterations for MSLOG or M-estimators, random seeds) or shared code are provided, which limits reproducibility and practical adoption.",None stated.,"Extend the robust S-chart designs to autocorrelated data (e.g., integrate prewhitening/ARMA modeling or develop robust residual-based charts) and assess robustness under model misspecification. Develop and publish an implementation (e.g., MATLAB/R/Python package) with clear default tuning parameters for MSLOG and M-estimators and automated limit calibration to a target in-control ARL. Evaluate performance under broader contamination structures (different contamination proportions, leverage points, mixtures, heavy tails) and compare against additional robust SPC competitors (e.g., robust EWMA/CUSUM for scale, adaptive and distribution-free variance charts) on real industrial datasets.",1812.11132v1,local_papers/arxiv/1812.11132v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:06:29Z
TRUE,Univariate|Other,Shewhart|CUSUM|EWMA|Other,Phase II,Theoretical/simulation only,FALSE,TRUE,NA,Simulation study|Other,ARL (Average Run Length)|False alarm rate|Other,Uses subgroup sample size n = 5 in simulations; discusses that Phase I data collection can be costly/insufficient but does not give a specific recommended Phase I sample size.,TRUE,R,Not provided,NA,"The paper studies robustness and adaptivity in statistical process control when normality and independence assumptions are violated, which otherwise inflates false alarms. It reviews robust approaches (adjusting limits, robust estimators like the median/IQR, and nonparametric sequential tests) and adaptive charting ideas. The main proposed procedure is a robust adaptive scheme (Ad-CUSUM $\tilde{X}$) that applies a zone-based adaptive control-limit/shrinkage mechanism to a robust median-based CUSUM. Performance is evaluated via Monte Carlo simulation (10,000 runs) under both normal data and contaminated normal-mixture data, reporting ARLs across shift sizes. Results show the adaptive robust CUSUM improves relative ARL behavior under contamination and can be best for small shifts, while classical EWMA/Shewhart can dominate for other shift magnitudes under pure normality.","Robust median CUSUM uses subgroup medians $\tilde{X}_n$ with recursion $\tilde{C}^+_{n+1}=\max\{0,\tilde{C}^+_n-(\mu_0+\delta_0)+\tilde{X}_n\}$ and $\tilde{C}^-_{n+1}=\max\{0,\tilde{C}^-_n+(\mu_0+\delta_0)-\tilde{X}_n\}$; signal when $\tilde{C}^+_n\ge L$ (analogous for the negative side if used). The new zone-adaptive limits update, when $\bar X_n$ is within current limits, shrink one side depending on the zone $Z_k$ via $L_n(\text{LCL},s)=\text{LCL}+s\,\sigma_{\bar X}$ and $L_n(\text{UCL},s)=\text{UCL}-s\,\sigma_{\bar X}$; the opposite limit resets to its baseline. A robustness comparison index is defined as $\mathrm{RARLC}=k\,\mathrm{ARLC}(\delta)$ where $k=\mathrm{ARL}(0)/\mathrm{ARLC}(0)$.","Simulation uses 10,000 runs, subgroup size $n=5$, and shifts $\delta\in\{0,0.1,0.3,0.5,0.7,1.0,1.5\}$. Under normal data $N(\delta,1)$, in-control ARLs are about 500 for all charts; for a small shift $\delta=0.1$, Ad-CUSUM $\tilde{X}$ has ARL 124.9 vs CUSUM $\bar X$ 130.0 and EWMA $\bar X$ 136.3, while Shewhart $\bar X$ is much slower (405.3). Under contaminated data (94% $N(\delta,1)$ + 6% $N(\delta,6.25)$), classical Shewhart $\bar X$ ARL0 collapses to 87.1 (high false alarms), while Ad-CUSUM $\tilde{X}$ retains ARL0=466.2 and shows strong detection (e.g., ARL 121.5 at $\delta=0.1$). Using the RARLC index on contaminated data, Ad-CUSUM $\tilde{X}$ is best overall (e.g., RARLC 130.3 at $\delta=0.1$, 25.9 at $\delta=0.3$, 12.7 at $\delta=0.5$).","Robust charts reduce false signals under assumption violations but “usually increase the delay in the detection of assignable causes,” making them less sensitive to process changes. The paper notes that Phase I data collection to estimate parameters to an acceptable level is often too costly, and too few Phase I samples can lead to large uncertainty in parameter estimates.","The proposed Ad-CUSUM $\tilde{X}$ is evaluated only via simulation under a specific contamination model (a two-component normal mixture with fixed 6% contamination and variance 6.25), so robustness to other non-normal forms (skewness, heavy tails without mixture, adversarial outliers) is not established. Autocorrelation is discussed conceptually, but the numerical study does not appear to include autocorrelated data, so claims about dependence handling are not empirically validated. The adaptive zone scheme introduces multiple tuning choices (zone limits and shrinkage parameters), yet the paper provides limited guidance on how to select them for targeted ARL/ATS properties in practice or under parameter estimation uncertainty.",None stated.,"Extend the simulation/analysis to explicitly include autocorrelated processes (e.g., AR(1), ARMA) and assess whether the adaptive robust scheme maintains nominal in-control ARL and improved detection. Provide principled design/tuning rules (or optimization/economic design) for zone boundaries and shrinkage parameters to target specific ARL/ATS across shift sizes and contamination levels. Validate the method on real industrial datasets and release reference software to support adoption and reproducibility.",1901.03701v1,local_papers/arxiv/1901.03701v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:07:10Z
TRUE,Univariate|Other,Shewhart|Other,Both,Transportation/logistics|Service industry|Other,FALSE,TRUE,NA,Case study (real dataset)|Other,False alarm rate|Other,"Uses an initial set of samples to estimate initial control limits, then iteratively removes out-of-control points and re-estimates limits until all remaining samples are within limits; no specific Phase I sample-size numbers are recommended. Case study uses weekly data for the first 42 weeks (n=42).",TRUE,Other,Not provided,NA,"The paper applies univariate Shewhart Individuals (I) control charts to monitor project schedule and cost performance using Earned Duration Management (EDM) indices (EDI, DPI) along with the classic cost performance index (CPI). Because project performance measurements are often non-normal and autocorrelated, it proposes a two-step adjustment framework: (1) test normality via Anderson–Darling and, if needed, normalize using a Johnson transformation; (2) remove autocorrelation by checking stationarity, applying first-differencing when needed, and fitting ARIMA models to obtain residuals. Control charts are then built on the ARIMA residuals to reduce false alarms and better detect special-cause deviations. A construction-project case study with 42 weekly observations demonstrates the workflow and shows that the residual-based I-charts identify out-of-control weeks that are not obvious from the raw EDM index trajectories. The work advances prior project-control uses of SPC by explicitly addressing both non-normality and serial dependence before charting EDM-based performance measures.","EDM indices include $\mathrm{EDI}=\mathrm{TED}/\mathrm{TPD}$ and $\mathrm{DPI}=\mathrm{ED}(t)/\mathrm{AD}$ (with CPI = EV/AC). Individuals-chart limits are given as $\mathrm{UCL}=\bar{Y}+3(\sigma/\sqrt{n})$, $\mathrm{CL}=\bar{Y}$, and $\mathrm{LCL}=\bar{Y}-3(\sigma/\sqrt{n})$. When data are autocorrelated, the paper fits ARIMA models (after differencing for stationarity) and uses the model residuals as the charted values.","Normality testing (Anderson–Darling, significance level 0.20) finds EDI approximately normal (p = 0.222) but DPI and CPI non-normal; Johnson transformation increases DPI’s normality p-value to 0.5777. All three index series are non-stationary and are made stationary via 1-lag differencing, then modeled as ARIMA(4,1,1) for CPI, ARIMA(1,1,1) for DPI, and ARIMA(4,1,1) for EDI to remove autocorrelation. Individuals charts of residuals flag special-cause points including DPI out-of-control in week 3 and EDI out-of-control in week 9, indicating schedule issues not readily diagnosable from raw index values alone. The CPI residual chart shows a positive special-cause point (better-than-expected cost performance) around week 3.",None stated.,"The approach relies on modeling choices (Johnson transformation form, differencing order, ARIMA order selection) that may be subjective and can materially affect residual behavior and control-limit validity. The paper evaluates the method on a single construction-project dataset (n=42), with limited benchmarking against alternative SPC approaches for autocorrelated/non-normal data (e.g., residual EWMA/CUSUM, distribution-free charts). Control-limit formulas are presented in a way more typical of subgroup means rather than standard Individuals-chart estimation (e.g., using moving range), which may impact practical implementation details and reproducibility.","The authors suggest improving the approach by utilizing fuzzy time series to provide more practical prediction solutions under project uncertainty, especially when prior performance shows no trend.","A useful extension would be to compare residual-based Shewhart charts with memory-type charts (EWMA/CUSUM) on EDM indices for earlier detection of small sustained drifts. Developing guidance for ARIMA order selection and validating residual independence/normality more formally (with sensitivity analyses) would strengthen robustness. Providing an implementation package (e.g., in R/Python) and testing across multiple projects/industries would improve generalizability and adoption.",1902.02270v1,local_papers/arxiv/1902.02270v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:07:40Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Healthcare/medical,TRUE,FALSE,TRUE,Markov chain|Economic design|Simulation study|Case study (real dataset),Other,Uses sample size n = 1 throughout (patient-level monitoring). No Phase I reference-sample size guidance is provided.,TRUE,R,Not provided,https://www.ncbi.nlm.nih.gov/books/NBK279318/,"The paper develops Markov chain-based, cost-optimal control-chart designs tailored to healthcare monitoring, especially patient-level longitudinal tracking with one-sided deterioration. Building on Zempléni et al. (2004), it generalizes a univariate X-chart framework to allow random shift sizes (Poisson count of shifts with exponential sizes leading to Erlang/gamma mixtures), imperfect repair (modeled via a Beta-distributed remaining-distance proportion), and random/failed sampling due to non-compliance (modeled via logistic or beta-based sampling probabilities). Long-run expected cost is computed from the stationary distribution of a discretized finite-state Markov chain, and optimization selects the sampling interval h and control limit k; the objective can be expected cost alone or a weighted combination of expected cost and cost standard deviation. The authors implement the method in R and study sensitivity to parameters; they also simulate to assess the impact of sensitizing rules. A real-data motivated application to LDL cholesterol monitoring in Hungary shows the optimized visit interval and threshold can differ from medical guideline defaults, and incorporating cost variability can reduce cost standard deviation with little increase in mean cost.","A baseline expected-cost objective is given by Eq. (1): $E(C)=\frac{c_s+p_3 c_f+p_4 c_r}{h}+p_2 c_o+p_4 c_o B$, where $p_i$ are stationary probabilities of Markov states (INC/OOC/FA/TA) and $B$ is the expected undetected-shift fraction of an interval. With random shifts, the shift-size CDF at time $t$ is a Poisson–gamma mixture (Eq. (3)) $Q_t(x)=n_t(0)+\sum_{k\ge1} n_t(k)Y_k(x)$, with $n_t$ Poisson($ts$) and $Y_k$ Erlang($k,1/\delta$). Out-of-control loss uses a Taguchi-type function (squared distance); a closed-form for the expected squared distance over an interval is derived (Eq. (5)), and the overall cost is computed via the stationary distribution of a discretized transition matrix $\Pi$ and minimized over $(h,k)$ (optionally minimizing $G=pE(C)+(1-p)\sigma(C)$).","In simulations assessing sensitizing rules (parameters including $p=0.9,\sigma=1,s=0.2,\delta=2$), the optimized baseline gave $h=0.38$ and $k=1.14$ with theoretical $E(C)=37.75$ and $\sigma(C)=150.33$; a 50,000-interval simulation yielded empirical mean cost $\approx 36.51$ and alarm proportion $\approx 0.192$. Sensitizing rules based on consecutive points beyond a $\tfrac{2}{3}k$ warning limit produced only small changes (e.g., mean cost $\approx 37.42$ and $s^*\approx 171.57$ for the 3-point rule; mean cost $\approx 36.54$ and $s^*\approx 190.91$ for the 2-point rule). In the LDL application, optimizing expected cost alone produced an optimal visit interval of 56.57 days and critical LDL increase 0.143 mmol/l with average daily cost €0.469 and SD €0.562; optimizing $G$ with $p=0.9$ produced 64.76 days and 0.129 mmol/l with average daily cost €0.477 and SD €0.418.","The authors note that modeling non-compliance (sampling probability) is difficult to estimate well, so the application results should be regarded as close approximations and scenario evaluation is often required. They also state that a missing feature is more realistic modeling of the repair procedure because repair is assumed instantaneous, which may be inappropriate in many situations. They further suggest that a continuous-time model (e.g., involving time series) could be beneficial.","The approach relies on discretization of the state space (choice of $\Delta$ and $V_d$), which can materially affect accuracy and computational burden; the paper provides limited guidance on selecting these to balance bias vs. runtime. The base model assumes a normal measurement distribution with known $\mu_0$ and $\sigma$ and focuses on one-sided mean shifts with $n=1$, limiting applicability when parameters are uncertain, variance changes occur, or two-sided monitoring is needed. The optimization and cost comparisons are tailored to the chosen cost structure (Taguchi squared loss and specific repair/sampling models), and results may not generalize under alternative clinical utility/loss formulations or constraints (e.g., minimum feasible visit spacing). Code is described as implemented in R but not shared, which reduces reproducibility.","They propose extending the model to represent non-instantaneous repair/treatment dynamics, since instantaneous repair may be unrealistic. They also suggest mathematical development of a continuous model (e.g., time-series-based) as a beneficial direction.","Develop principled discretization/approximation error control (or continuous-state methods) and provide practical guidance or defaults for $\Delta$ and $V_d$ with sensitivity bounds. Extend to unknown-parameter (Phase I/II) or Bayesian/self-starting variants that propagate estimation uncertainty in $\mu_0,\sigma$ and shift/repair/compliance parameters. Add support for autocorrelated biomarker trajectories (state-space/ARIMA residual charts) and irregular visit times beyond missed-visit probabilities. Provide open-source software (R package) and benchmark against alternative healthcare monitoring approaches (e.g., Bayesian decision rules, sequential GLR/CUSUM/EWMA under drift) under comparable cost/utility models.",1903.06675v1,local_papers/arxiv/1903.06675v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:08:19Z
TRUE,Multivariate,EWMA|Other,Phase II,Manufacturing (general)|Theoretical/simulation only|Other,TRUE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ATS (Average Time to Signal)|Steady-state ARL|ARL (Average Run Length),"Not discussed (they vary subgroup sizes for Phase II monitoring: n=3,5,10 for p=2; n=11,15,20 for p=10, and use n=5 in a case study).",TRUE,R,Not provided,https://cran.r-project.org/web/packages/MSQC/MSQC.pdf,"The paper studies whether multivariate observations should be monitored individually or aggregated into subgroups when the goal is to detect shifts in the process covariance matrix (multivariate dispersion monitoring). It compares an individual-observation EWMA-type dispersion chart (MEWMS from Huwang et al., based on an EWMA of outer products and the trace) against subgroup-based dispersion charts using non-overlapping and overlapping (moving-window) subgroups, including the generalized variance chart (GVC; determinant of the sample covariance) and trace-based covariance charts. Two overlapping trace-based charts are used: OTCC (trace of the moving-window sample covariance) and OTMC (trace of a mean squared successive difference covariance estimator), with limits tuned to achieve in-control steady-state ATS0=370. Using Monte Carlo simulation (50,000 runs) under multivariate normal data, the study finds individual-observation MEWMS is quickest for sustained increases in variability, while overlapping subgroup charts (especially OTCC/OTMC) are better for decreases in variability; overlapping subgroups generally outperform non-overlapping subgroups. A small real-data illustration (industrial bivariate dataset from the MSQC R package) supports these conclusions.","Data are standardized as $\mathbf{Y}_t=\Sigma_0^{-1/2}(\mathbf{X}_t-\mu_0)$ with $\mathbf{Y}_t\sim N_p(0,I)$. The individual-observation MEWMS computes $E_t=\omega\,\mathbf{Y}_t\mathbf{Y}_t' + (1-\omega)E_{t-1}$ and signals when $\operatorname{tr}(E_t)$ exceeds time-varying limits $p\pm L\sqrt{2p\,C_t}$. Subgroup charts use $S_T=\frac{1}{n-1}Y_T^{[n]}(Y_T^{[n]})^T$ (non-overlapping or overlapping window) with statistics $\det(S_T)$ (GVC) or $\operatorname{tr}(S_T)$ (NTCC/OTCC); OTMC uses $\mathrm{MSSD}_{T'}=\frac{1}{2(n-1)}(Y_{T'}^{[n]}-Y_{T'-1}^{[n]})(\cdot)^T$ and charts $\operatorname{tr}(\mathrm{MSSD}_{T'})$.","Simulations (50,000 Monte Carlo runs) are calibrated to steady-state $ATS_0=370$ and evaluated for $p=2$ and $p=10$ under overall variance scaling $\Sigma_1=\delta\Sigma_0$, combined variance/correlation shifts, and partial shifts. For increases in dispersion ($\delta>1$), MEWMS (especially with $\omega=0.2$) yields the lowest out-of-control ATS across many scenarios; for decreases in dispersion ($\delta<1$), overlapping subgroup charts OTCC/OTMC produce signals while MEWMS can be slow due to symmetric limits. Overlapping subgrouping improves performance versus non-overlapping for moderate/large shifts (OTCC generally best among grouped-observation methods). Subgroup size effects show larger subgroups can help small/moderate shifts in some $p=2,\rho=0$ settings, but smaller subgroups are preferable for larger shifts and for higher dimension ($p=10$).","The study assumes in-control parameters $\mu_0$ and $\Sigma_0$ are known (acknowledging that in practice they are estimated in Phase I), and focuses on normally distributed data. Only two dimensions are examined in depth ($p=2$ and $p=10$), and only a limited set of subgroup sizes is explored (three choices per $p$). The paper notes it did not study the optimal subgroup size, recommending this as future work.","The comparisons mix charts with different internal dependence structures (overlapping-window statistics are serially correlated), and control limits are obtained by numerical search; reproducibility is limited because simulation/optimization details and code are not provided. The MEWMS chart is evaluated with symmetric limits even though dispersion changes can be asymmetric in practice; alternative one-sided/asymmetric designs could materially change conclusions. The out-of-control models are mainly covariance scaling and equicorrelation-type changes; more realistic covariance changes (localized eigenvalue shifts, nonstationary means, heavy tails) are not systematically studied.",The authors suggest studying the actual subgroup size that yields optimal performance (beyond the three subgroup sizes used) as future research. They also expect that redesigning the MEWMS lower control limit could improve performance for detecting decreases in dispersion.,"Extend the study to Phase I/Phase II with estimated parameters (including robust/covariance-regularized estimators for small Phase I samples and high-dimensional $p\gg n$). Evaluate robustness under non-normality, outliers, and autocorrelated base processes (beyond dependence induced solely by overlapping windows), and consider adaptive or one-sided/asymmetric designs for better detection of variance decreases. Provide open-source implementations and standardized benchmarks so practitioners can reproduce the numerical limit calibration and compare against additional modern dispersion charts (e.g., eigenvalue-based, regularized, or likelihood/GLR approaches).",1906.08038v1,local_papers/arxiv/1906.08038v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:09:02Z
TRUE,Multivariate|Nonparametric|Image-based monitoring|Other,EWMA|Other,Both,Manufacturing (general),FALSE,NA,TRUE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Detection probability|False alarm rate|Other,Phase II simulations and recommendations use a Phase I reference set of m0=100 in-control parts (authors note Chen et al. (2016) recommend at least m0≥50). For Phase I startup comparisons they use 25 IC + 25 OC parts per replication; for Phase II run-length studies they use 100 IC parts followed by OC parts until signal.,TRUE,MATLAB|R,Supplementary material (Journal/Publisher),NA,"The paper proposes an intrinsic (registration-free) SPC methodology for monitoring 3D manufactured part geometry represented as meshes/point clouds/voxels, using the spectrum of an estimated Laplace–Beltrami (LB) operator as the feature vector. The LB spectrum yields a multivariate, non-normal data stream, so the authors employ distribution-free, rank/permutation-based multivariate control schemes: a modified DFEWMA chart for Phase II monitoring and a distribution-free Phase I procedure for startup. They show the intrinsic approach avoids requiring point-to-point correspondence and can handle unequal mesh sizes, unlike prior registration-based SPC approaches. Performance is assessed via extensive simulations (ARL/SDRL, false alarm probability, and detection probabilities) comparing against an ICP-based monitoring statistic and against a registration-based Gaussian-process surface monitoring method; results show strong sensitivity of the LB-spectrum chart for global shape/size changes and competitive performance overall. A post-alarm diagnostic is provided using ICP registration to localize defects on the part surface once an alarm is signaled.","A discrete LB operator is estimated from each mesh using the (localized) mesh Laplacian, yielding an m×m matrix $L_K^t$ whose eigenvalues (lower spectrum) form the monitored vector. The localized mesh Laplacian is given by Eq. (9), a heat-kernel-weighted sum over neighbors within radius $r$, and can be written as a Laplacian matrix $L_K^t=D-W$ (Eq. (8)). Phase II uses a distribution-free EWMA on ranks: $T_{jn}(w,\lambda)$ in Eq. (13) is a standardized exponentially weighted sum of ranks of the last $w$ observations (window), and the multivariate chart statistic is $T_n(w,\lambda)=\sum_{j=1}^p T_{jn}^2$.","In-control DFEWMA performance closely matches the nominal geometric run-length distribution (e.g., with $\alpha=0.05$ nominal ARL=20, observed ARL≈20.17–20.46 for both cylinder and prototype part; Table 1). For barrel-shaped cylinder deformations, the LB-spectrum chart signals very quickly for small-to-moderate shifts (e.g., with $\alpha=0.005$ nominal ARL=200, LB ARL≈2.03 for $\delta=0.005$ and ARL≈2.00 for $\delta\ge0.5$, while the ICP-objective chart is much slower for small shifts: ARL≈39.76 at $\delta=0.005$ and ARL≈83.21 at $\delta=0.0005$; Table 2). Under spatially correlated, non-isotropic noise, LB-spectrum run-length performance is largely unchanged whereas the ICP-based chart deteriorates markedly (Table 4). For localized defects on small meshes, the ICP statistic can outperform LB-spectrum unless meshes are densified via Loop subdivision, which improves LB detection (Table 3). Phase I detection probabilities using Capizzi & Masarotto’s method show strongest power in the lower eigenvalues; including too many eigenvalues (up to the 100th) is counterproductive due to noise (Tables 7–8).","The authors note that very high noise can prevent reliable spectrum estimation and that higher eigenvalues tend to reflect geometrical noise, reducing detection power when too many are used. They state that voxel (3-manifold) extensions are not developed in this paper (focus is on surface meshes), and that further work is needed to detect changes in variance (noise level) rather than only mean geometry. They also assume no significant systematic local scanner bias (e.g., optical aberration), indicating extensions are needed if such bias is present.","The approach depends on choices of discretization and tuning parameters for the LB approximation (e.g., heat-kernel parameter $t$, neighborhood radius $r$, mesh preprocessing), but robust default-selection guidance for practitioners is limited and may be application-dependent. Monitoring eigenvalues can confound different physical defect mechanisms that produce similar spectral changes, so interpretability/diagnosis is largely deferred to post-alarm registration rather than built into the chart statistic. The reported comparisons use simulated CAD-based scenarios and limited real-data evidence; broader empirical validation on diverse scanner types and part families would strengthen generalizability. Computational cost for very large meshes (tens of thousands of points) is discussed, but wall-clock benchmarks and scalability limits under production constraints are not fully quantified.","They propose extending the framework to voxel/volumetric (3-manifold) data using tetrahedralizations and FEM-based LB approximations (e.g., for CT scans in additive manufacturing). They also call for developing charts to detect changes in variance/noise levels in addition to mean geometry changes. Additional future work mentioned includes handling systematic local scanner bias if calibration is insufficient and comparing alternative symmetrizable LB discretizations (e.g., FEM methods) against the localized mesh Laplacian used here.","Developing principled, data-driven tuning rules for LB discretization parameters ($t$, $r$) and the number of eigenvalues $p$, with robustness guarantees across mesh densities, would improve deployability. Integrating diagnostic information directly into monitoring (e.g., contribution plots, localized spectral features, or multiscale statistics) could reduce reliance on post-alarm ICP and improve actionable feedback. Extending to autocorrelated sequences of parts (tool wear trends) and to mixed-effects designs across multiple machines/lines would align with real manufacturing deployment. Providing open-source, production-ready implementations (e.g., Python/R packages) and benchmark datasets would facilitate reproducibility and adoption.",1907.00111v3,local_papers/arxiv/1907.00111v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:09:50Z
TRUE,Univariate|Other,Change-point|Other,Phase II,Theoretical/simulation only,TRUE,TRUE,NA,Simulation study|Integral equation|Other,ARL (Average Run Length)|Detection probability|Other,Not discussed,TRUE,R,Not provided,https://CRAN.R-project.org/package=spc,"The paper proposes an online (Phase II) procedure for detecting mean/level shifts in autocorrelated data streams by adapting Tsay’s (1988) offline outlier/level-shift detection based on one-step-ahead prediction errors to a moving-window framework. The method computes pointwise Wald-type statistics for candidate change points within a window of width K, updates them efficiently via a linear recursion, and uses the maximum absolute statistic in the window as the charting statistic. Because the windowed statistics are correlated, the authors propose simulation-based algorithms to select the control limit h to achieve a target in-control ARL (ARL0), exploiting run-length properties rather than multiple-testing corrections. Performance is evaluated on simulated stationary AR(1) processes with level shifts of varying size and autocorrelation, and compared to a two-sided CUSUM on one-step prediction errors. Results indicate comparable ARL1 to tuned CUSUM, but higher probability of identifying the correct change point (within ±10 steps) and easier tuning for more complex ARMA structures due to window-size search being process-independent in range.","Data are modeled as ARMA with AR representation $\Pi(B)x_t=a_t$ and one-step prediction error $e_t=x_t-\hat x_t$, where $\hat x_t=\sum_{i=1}^{p^*}\pi_i x_{t-i}$. A level shift at time $t^*$ implies $e_t\approx \tau H(B)I_t^{t^*}+a_t$ with $H(B)=\Pi(B)/(1-B)$, yielding $E(e_t)=0$ for $t<t^*$ and $E(e_{t^*+i})=\tau\eta_i$. The pointwise test statistic for candidate change point $d$ is $\lambda_{d,T}=\hat\tau_{d,T}/(\rho_{d,T}\sigma_a)$, with $\hat\tau_{d,T}=\rho_{d,T}^2\left(e_d+\sum_{i=1}^{T-d}\eta_i e_{d+i}\right)$ and $\rho_{d,T}^2=(1+\sum_{i=1}^{T-d}\eta_i^2)^{-1}$. Online monitoring uses a window $T-K+1\le d\le T$ and charting statistic $\Lambda=\|\Lambda_T\|_\infty=\max_{d}|\lambda_{d,T}|$, updated recursively via $\lambda_{d,T+1}=(\rho_{d,T+1}/\rho_{d,T})\lambda_{d,T}+(\rho_{d,T+1}\eta_{T+1-d}/\sigma_a)e_{T+1}$.","Critical values $h$ (chosen to target ARL0=370.4) were estimated via simulation for window sizes up to $K=100$ and $\phi_1\in[-0.95,0.95]$, with $h$ generally increasing with K and varying with $\phi_1$ (dropping toward about 3 as $\phi_1\to0.95$). In AR(1) simulations (20,000 repetitions), the proposed method and CUSUM (on one-step prediction errors) achieved broadly comparable ARL1 when each was tuned appropriately, but the proposed method showed higher probability of identifying the correct change point (signal within $t^*\pm10$) across horizons/window sizes. The paper reports that overall the proposed method performed about 39% better than one CUSUM tuning (setting 1) in their studied range, while being comparable to a simulation-optimized CUSUM tuning (setting 2), and in some regions (large positive $\phi_1$) it could be dramatically better (reported “up to 48 times” better than CUSUM).","The study primarily investigates stationary AR(1) processes and therefore does not fully demonstrate advantages for more general ARMA(p,q) models; the authors note that more complicated processes (e.g., with MA terms) may show clearer benefits and warrant further study. They also assume model parameters are known and the process is stationary and invertible, which may limit direct applicability in settings with parameter uncertainty or model misspecification.","The procedure’s performance depends on having a correctly specified time-series model to generate one-step prediction errors; robustness to model misspecification and parameter estimation error (common in practice) is not systematically evaluated. Control-limit selection relies on potentially heavy simulation (large N, repeated for many (K, model) settings), which could be computationally burdensome in practical deployments or for high-dimensional parameter searches. The method targets level/mean shifts; behavior under variance shifts, transient shifts, or multiple successive changes is not analyzed. No real-data case study is provided, so practical issues (data cleaning, non-Gaussian noise, missingness/irregular sampling) remain unvalidated.","The authors recommend investigating extension of the proposed moving-window method to multivariate time series. They also suggest that applying the approach to more general ARMA processes (beyond AR(1)) may reveal additional advantages, particularly where one-step prediction error means exhibit complex patterns after a level shift.","Develop Phase I/II integrated versions that estimate ARMA parameters online (or use robust/self-starting estimation) and quantify the effect of estimation error on ARL and change-point localization. Extend the approach to handle non-Gaussian innovations (e.g., heavy tails) via robust or nonparametric statistics and to accommodate missing/irregular sampling typical of streaming sensor data. Provide computational accelerations and software implementations (e.g., an R/Python package) that automate control-limit calibration and window-size selection, and validate on real industrial or IoT datasets. Generalize beyond mean shifts to variance shifts and multiple-change scenarios, including diagnostics to distinguish shift types.",1907.05453v2,local_papers/arxiv/1907.05453v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:10:40Z
TRUE,Other,GLR (Generalized Likelihood Ratio)|Change-point|Other,Phase II,Energy/utilities,TRUE,NA,FALSE,Simulation study|Other,ARL (Average Run Length)|False alarm rate|Expected detection delay|Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a real-time transmission line outage detection and localization scheme using PMU voltage phase angle data under transient (non–quasi-steady) dynamics. It derives a time-varying small-signal relationship from the AC power flow model, leading to a conditional multivariate Gaussian model for successive angle differences whose covariance depends on the current Jacobian. Outage detection is formulated as quickest change detection among multiple post-outage hypotheses (one per outage scenario), implemented via a generalized likelihood ratio (GLR) stopping rule with a threshold selected to meet a target false alarm constraint (via an ARL0-based approximation). The method supports limited PMU deployment via a selection matrix and evaluates likelihoods for many candidate outages efficiently using a recursive update. Extensive dynamic simulations on IEEE 39-bus and 2383-bus systems show fast detection (often sub-second in the 39-bus case with limited PMUs) and good identification accuracy, with performance influenced by PMU placement and grid topology.","The dynamic model discretizes the decoupled AC power-flow Jacobian relation as $\Delta P_k = J(\theta_{k-1})\,\Delta \theta_k$. With $\Delta P_k\sim\mathcal N(0,\sigma^2 I)$, this implies $\Delta\theta_k\sim\mathcal N\big(0,\sigma^2 (J(\theta_{k-1})^T J(\theta_{k-1}))^{-1}\big)$; different outages $\ell$ correspond to different Jacobians $J_\ell$ (via modified admittance/incidence matrices). The per-sample log-likelihood ratio is $Z_k(\ell)=\ln|J_\ell|-\ln|J_0|+\frac{1}{2\sigma^2}\Delta\theta_k^T\,(J_0^T J_0 - J_\ell^T J_\ell)\,\Delta\theta_k$, and the recursive GLR statistic is $W_{\ell,k}=\max\{0, W_{\ell,k-1}+Z_k(\ell)\}$ with alarm time $D=\inf\{k:\max_{\ell\in\mathcal L}W_{\ell,k}\ge c\}$ and threshold approximation $c\approx\ln(\text{ARL}_0\cdot p)$ (with $p$ PMUs).","Thresholds are set from the approximation $c=\ln(\text{ARL}_0\cdot p)$; e.g., for 39 PMUs and ARL0 = 1 day, $c=18.43$ (Table I). For a representative 39-bus case with 10 PMUs, a line-10 outage at 3 s is detected at 3.5 s (0.5 s delay) while per-sample computation is reported as ~1 ms to evaluate Jacobians plus 0.227 ms to update all outage statistics (Table II), under a 33 ms sampling period. In comparisons on selected outages (Table III), the proposed “AC-limited” GLR method is much faster than an Ohm’s-law CUSUM-type method and a DC-model method (which can miss detections under limited PMUs); e.g., for line 27, delays are about 0.0012–0.0039 s (AC-limited) vs ~3.30–3.86 s (Ohm’s-law limited). On the 2383-bus system with 1000 PMUs, detection delays for simulated outages range roughly from 1.37 s to 4.90 s for several lines, with some undetected outages reported (Table IV).","The authors note that with limited PMU deployment, diagonal elements of the reduced Jacobian $J^o$ can be inaccurate because unobservable neighboring bus measurements are unavailable and related terms are treated as zero (Remark 2). They also state that PMU placement affects detection efficiency and that finding the optimal PMU placement is beyond the scope of the paper. For the large 2383-bus system, they report that detecting single-line outages is harder, leading to longer delays and several undetected outages.","The statistical model assumes independent, homogeneous-variance Brownian increments for active power mismatches (and thus conditional Gaussian angle differences), which may be violated by real PMU noise characteristics, load correlations, and generator/control dynamics (serial dependence). Threshold calibration uses an approximate formula $c=\ln(\text{ARL}_0\cdot p)$ rather than an exact/validated in-control run-length analysis for the conditionally time-varying distribution, so achieved false-alarm rates may deviate in practice. The approach requires computing and inverting Jacobians (or effectively working with $J^T J$) for many outage scenarios; numerical stability and conditioning under stressed operating points are not deeply analyzed. Missing data, bad data, communication delays, and PMU time-alignment issues (common in operational settings) are not addressed.",They propose investigating the optimal number and placement of a limited number of PMUs to reduce detection delays and improve identification accuracy. They also propose incorporating generator dynamics into the system model to leverage more detailed physical information for improved outage detection and identification.,"Develop robust/nonparametric or heavy-tailed variants of the GLR to reduce sensitivity to non-Gaussian PMU noise and model mismatch, and extend the method to explicitly handle autocorrelated innovations (e.g., via state-space/ARMA residual monitoring). Provide principled threshold calibration (e.g., via simulation/Markov-chain/integral-equation methods) for the time-varying conditional model to guarantee ARL0 under realistic operating variability. Add mechanisms for missing/irregular PMU streams and bad-data detection to improve deployability, and release reference implementations to support replication and benchmarking.",1911.01733v2,local_papers/arxiv/1911.01733v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:11:24Z
TRUE,Multivariate|Other,Hotelling T-squared|Machine learning-based|Other,Both,Energy/utilities,NA,FALSE,TRUE,Case study (real dataset)|Other,False alarm rate|Detection probability|Expected detection delay|Other,Not discussed,TRUE,None / Not applicable,Not provided,https://www.iea.org/statistics/balances/|http://www.energystorageexchange.org/projects/data|https://arxiv.org/abs/1911.06242,"The paper proposes a novel condition-monitoring Key Performance Indicator (KPI) for hydropower plants based on a trained Self-Organizing Map (SOM) that models nominal multivariate behavior. For each new multivariate observation, the method computes a SOM distortion measure and converts it into a scalar KPI; alarms are raised when the KPI falls below a 3-sigma lower control limit computed from training (nominal) data, with additional EWMA-style smoothing over the last 12 hours. The approach also provides variable-wise contribution ratios to help localize which signals/components are driving an alarm. Performance is validated in online operation over more than one year on two Italian hydropower plants (hundreds of 1-minute sensors), reporting detection of 20+ anomalous situations and earlier/more stable warnings than a benchmark Hotelling $t^2$ chart (with fewer false positives in highlighted cases). The Hotelling $t^2$ control chart is included as a comparison method with standard Phase I limit estimation and Phase II monitoring.","SOM best-matching unit: $c=\arg\min_i\|r-m_i\|$. Distortion measure for a pattern: $DM(r)=\sum_{i=1}^D w_{ci}\|r-m_i\|$ with neighborhood weights $w_{ci}=\exp\{-d(c,i)^2/(2\sigma^2)\}$; training-average distortion $DM_\Delta=\frac{1}{N}\sum_{r\in\Delta}DM(r)$. KPI: $KPI(r)=\frac{1}{1+\left|1-\frac{DM(r)}{DM_\Delta}\right|}$ with alarm limit $LCL_{kpi}=\mu_{kpi}-3\sigma_{kpi}$ (computed on filtered KPI values from training data). Hotelling comparison: $t^2(r)=(r-\mu)C^{-1}(r-\mu)^T$ with limits $UCL=\mu_{t^2}+3\sigma_{t^2}$ and $LCL=\max(\mu_{t^2}-3\sigma_{t^2},0)$.","Using real 1-minute data (Plant A: 630 analog signals; Plant B: 60 analog signals), the system was trained on 05/01/2017–03/31/2018 and tested online 04/01/2018–07/2019 with retraining every two months. Over ~16 months of online operation (from April 2018), the authors report detection of more than 20 anomalous situations across components. In a generator-temperature sensor anomaly (Plant B, Oct 2018), the SOM-KPI produced a clear warning at onset while the Hotelling chart would have produced several false positives historically; avoided cost of a potential unit stop is estimated at 25k€–100k€. In an HV-transformer gas anomaly (Plant A, starting Apr 22 2018), the SOM-KPI detected the abnormality about 20 days earlier than Hotelling $t^2$; after operator feedback and removal of past similar periods from training, the KPI retrospectively indicated the pattern started about one month earlier.",The authors state the procedure cannot yet be implemented in a fully unsupervised fashion: iterations with plant operators are still needed when alarms are triggered to confirm anomalies and to curate training data. They also frame the method as a first step: it observes/detects faults but does not yet predict them ahead of time at an incipient stage.,"Control limits are set via a simple 3-sigma rule on filtered KPI values, without demonstrating in-control false-alarm calibration (e.g., ARL/ATS) or sensitivity to nonstationarity, seasonality, or retraining frequency; this may make alarm rates hard to control in deployment. The evaluation is largely case-study based and comparisons focus mainly on Hotelling $t^2$; other common monitoring baselines (e.g., MEWMA/MCUSUM, PCA-based $T^2$/SPE, autoencoder/anomaly-score methods) are not benchmarked. The method relies on substantial data cleaning and manual removal of fault periods; performance may degrade where operator feedback or clean nominal segments are unavailable, and retraining every two months may be operationally burdensome. Autocorrelation (1-minute sampling) is not explicitly modeled; smoothing may mask short-lived faults and complicate delay quantification.",They propose further research to reduce the need for human-in-the-loop iterations so the approach can operate more fully unsupervised. They also indicate moving from fault detection toward fully automatic predictive maintenance where faults are predicted ahead of time when still incipient.,"Develop a principled thresholding/alarm design with explicit in-control false-alarm guarantees (e.g., ARL/ATS targets), including effects of KPI smoothing and periodic retraining. Extend the approach to handle autocorrelated/nonstationary behavior explicitly (e.g., dynamic SOMs, residual modeling, or state-space preprocessing) and quantify detection delays under realistic temporal dependence. Provide broader comparative studies against modern multivariate SPC and anomaly detection (PCA/SPE, MEWMA/MCUSUM, GLR/change-point, autoencoders) and publish a reproducible software implementation (e.g., Python/R package) to support adoption.",1911.06242v1,local_papers/arxiv/1911.06242v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:12:07Z
TRUE,Univariate|Nonparametric|Other,Shewhart|Change-point|Other,Phase I,Energy/utilities|Environmental monitoring,FALSE,TRUE,NA,Simulation study|Case study (real dataset)|Other,False alarm rate|Detection probability|Other,Subgroup the residuals into m subgroups of size n successive observations; in the case study they use hourly subgroups with n = 6 (10-minute SCADA data). They also recommend a minimum segment length between step changes lmin = 5 for stability of the Phase I statistics.,TRUE,R,Not provided,https://opendata-renewables.engie.com/pages/home/,"The paper proposes a Phase I SPC workflow for wind-turbine SCADA data to uncover hidden operating-status changes. A site-specific power-curve model is fit using Multivariate Adaptive Regression Splines (MARS) while explicitly reducing serial correlation in the regression errors via iterative feasible GLS (Cochrane–Orcutt/IFGLS), producing approximately i.i.d. residuals. Phase I stability of the adjusted power-generation process is then assessed with the distribution-free RS/P (Recursive Segmentation and Permutation) chart, aimed at detecting mean/level changes covering isolated, single-step, and multi-step patterns. The method is demonstrated on real 10-minute data from the La Haute Borne wind farm, where multiple informative out-of-control segments are detected that are not obvious from the raw power curve after rough filtering. The work advances SPC for low-quality, autocorrelated SCADA streams by combining autocorrelation removal with a nonparametric Phase I change-segmentation chart with permutation-based significance control.","Power output is modeled as $Y_t=f(X_t)+u_t$ with autocorrelated errors $u_t=\sum_{\theta=1}^p a_\theta u_{t-\theta}+\varepsilon_t$, and $f(\cdot)$ is estimated by MARS as $\hat f(X_t)=\sum_{s=1}^S \alpha_s B_s(X_t)$ with spline basis products. Autocorrelation is reduced by IFGLS: iteratively estimate AR coefficients $\{a_\theta\}$ from residuals and refit $\alpha$ by least squares on transformed responses, yielding independent residuals $r_t=Y_t-E[Y_t\mid X_t]$. RS/P tests for mean level changes across subgroups via multiple change-point statistics $T_k$ aggregated as $W=\max_{k=0,\ldots,K}(T_k-u_k)/v_k$ and computes a distribution-free p-value by permutation $p=(1/L)\sum_{l=1}^L \mathbf{1}(W_l^\ast\ge W)$.","On the La Haute Borne dataset (turbine R80711), adding IFGLS to MARS reduces power-curve RMSE from 39.18 to 30.08 (dataset 1) and from 42.86 to 35.84 (dataset 2), indicating improved fit after accounting for serial correlation. For RS/P, the case study uses subgroups of size n = 6 (hourly), maximum detected change points k = 50, and significance threshold p = 0.05; iteratively removing detected out-of-control segments increases the aggregated-statistic p-value (e.g., after removing the first 4 segments in dataset 2, p increases to 0.002, and after removing 4 more segments it exceeds 0.05). Detected segments exhibit diverse mean-shift patterns (short deviations, sustained down-rating-like periods, pre-emergency shutdown behavior), supporting the method’s ability to reveal non-obvious Phase I instabilities.","The authors note they only focus on level (mean) changes in the residual process; detecting scale/variance shifts is described as non-trivial and left for further work. They also state they did not monitor the coefficients of the power-curve model (i.e., profile/parameter drift), which would require additional development. They acknowledge precise system status cannot be verified from available records, limiting definitive validation of detected segments’ causes.","The approach relies on successfully making residuals approximately i.i.d. via an AR(p) error model and Box–Ljung checks; remaining dependence, nonstationarity, or nonlinear dynamics could inflate false alarms or reduce power. The Phase I procedure removes detected segments iteratively, which can introduce selection bias and may be sensitive to tuning choices (p-order selection, subgroup size n, K, lmin, permutation count L). The method is univariate after adjustment (monitors residual power only) and may miss multivariate or structural anomalies present in other SCADA channels unless separately modeled.","They propose extending the Phase I analysis to detect scale (variance) shifts in the residual process. They also suggest monitoring the coefficients/parameters in the power-curve model (i.e., model-profile changes) as an additional and non-trivial research direction requiring further development.","Develop a unified framework that jointly monitors mean and variance (or full distribution) changes in residuals under remaining autocorrelation, possibly with robust long-run variance estimation. Extend the workflow to multivariate monitoring of multiple SCADA residual streams and to profile monitoring of the power-curve function itself (including drift diagnostics and attribution). Provide open-source, reproducible code and guidelines for selecting key tuning parameters (AR order, subgrouping, K, L) with sensitivity analyses and computational-cost tradeoffs.",1912.04045v2,local_papers/arxiv/1912.04045v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:12:46Z
TRUE,Multivariate|Nonparametric|High-dimensional|Self-starting,Shewhart|CUSUM|EWMA|MEWMA|MCUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Other,Phase II,Theoretical/simulation only|Food/agriculture,NA,TRUE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|Steady-state ARL|Other,"Focuses on individual observations (often n=1) with p-dimensional vectors; for covariance estimation in grouped-data settings notes usually n>p. For Phase I, several reviewed methods require a historical reference sample of size m (large m noted as needed for nonparametric charts), but no single fixed m is recommended in this review.",NA,None / Not applicable,Not applicable (No code used),NA,"This paper is a literature review of multivariate dispersion (covariance-matrix) control charts designed for multivariate individual observations (vector-by-vector data), covering 30 articles from 1987–2019. It classifies methods into five groups: CUSUM-type, MEWMA-type, Shewhart-type, nonparametric, and high-dimensional dispersion charts, focusing on Phase II monitoring of changes in process variability/covariance structure. The review summarizes key charting statistics used across the literature (e.g., trace, determinant-based likelihood ratios, Wilks’ statistic, norm-based summaries, penalized likelihood/LASSO approaches, spatial sign/rank methods, and dissimilarity indices) and notes typical assumptions (often multivariate normality for parametric charts). It highlights under-explored areas, particularly CUSUM dispersion charts, nonparametric dispersion charts, and high-dimensional settings, and discusses practical issues such as control-limit calibration, sensitivity to decreases in variance, and the impact of parameter estimation. The paper concludes with a set of research directions including better control-limit computation, robustness to non-normality/outliers, steady-state performance assessment, and methods for autocorrelated or transient shifts.","The review adopts the common standardization $\mathbf{Y}_i=\Sigma_0^{-1/2}(\mathbf{X}_i-\mu_0)$ so that in control $\mathbf{Y}_i\sim N(\mathbf{0},I_p)$. A central MEWMA covariance recursion is $S_i^{EWMA}=\lambda \mathbf{Y}_i\mathbf{Y}_i' + (1-\lambda)S_{i-1}^{EWMA}$ (often with $S_0=I_p$), with monitoring based on summaries such as $\operatorname{tr}(S_i^{EWMA})$ or Alt’s likelihood ratio form $c_i=\operatorname{tr}(S_i^{EWMA})-\log|S_i^{EWMA}|-p$. For CUSUM likelihood-ratio dispersion monitoring (Healy), a typical form is $T_i=\max\{T_{i-1}+\log[f_B(X_i)/f_G(X_i)],0\}$, which under proportional covariance shifts simplifies to an update based on quadratic forms minus a reference constant.","The paper does not report new numerical experiments; instead it synthesizes findings from 30 prior studies. It notes that many reviewed papers evaluate charts using run-length metrics (e.g., $ARL_0$, $ARL_1$, SDRL, MRL) and that some methods show good zero-state performance but may degrade in steady-state (the review explicitly calls for more steady-state evaluation). It highlights reported comparative conclusions from the literature, such as modified MCUSUM-type schemes for time series covariance changes performing well across simulated shifts (Bodnar & Schmid, 2017) and spatial-sign/rank nonparametric EWMAs being robust under non-normality (Li et al., 2013) with varying advantages for larger shifts (Huwang et al., 2019). It also summarizes that high-dimensional enhancements (e.g., parallelized Monte Carlo tuning for control limits and specialized high-dimensional tests) enable application to larger p (e.g., Gunaratne et al. optimize limits for up to p=15 in their study).","The authors note that the review is limited to charts specifically designed to monitor the covariance matrix with multivariate individual observations and focused on Phase II applications, excluding charts primarily designed for the mean vector (e.g., Hotelling $T^2$, standard MEWMA for means). They also emphasize that many methods assume independent observations and (for most parametric charts) multivariate normality, which may not hold in practice. They point out practical drawbacks in existing methods, including symmetric control limits used for skewed statistics and weak performance for detecting decreases in variances.","Because this is a narrative review, the search and screening process is not fully reproducible (e.g., no PRISMA-style flow, no complete search strings, and no explicit inclusion/exclusion auditing), so completeness and selection bias are possible. The review synthesizes performance claims across papers but does not normalize comparisons across a common simulation design (same shifts, same $ARL_0$, same initialization/steady-state settings), which can make cross-paper performance statements hard to interpret. It also provides limited practitioner guidance on how to choose among chart families for specific covariance-change structures beyond qualitative discussion.","They explicitly call for more research on CUSUM-type dispersion charts, nonparametric dispersion charts, and high-dimensional covariance monitoring methods. They recommend developing reliable algorithms/simulation methods for correct (often non-symmetric) control limits, improving sensitivity to decreases in variances, and designing charts for transient shifts. They also stress evaluating steady-state performance (not only zero-state), investigating robustness to outliers/non-normality, and studying the impact of parameter estimation (unknown in-control parameters) in practical deployment, including methods for dependent/autocorrelated observations.","Develop unified benchmark suites (open datasets + standardized shift scenarios) for covariance monitoring with individual observations, enabling fair, cross-paper comparisons under matched $ARL_0$ and steady-state initialization. Create accessible software implementations (e.g., R/Python packages) with calibrated control-limit estimation routines (including bootstrap/resampling for nonparametric charts and high-dimensional approximations) to lower adoption barriers. Extend reviewed dispersion charts to handle missingness/irregular sampling and to provide post-signal diagnostics that localize which variances/covariances changed, especially in high-dimensional sparse-change regimes.",1912.09755v1,local_papers/arxiv/1912.09755v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:13:32Z
TRUE,Multivariate|Other,Shewhart|Other,Both,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,FALSE,Markov chain|Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Other,"Phase II subgroup/sample sizes considered are n ∈ {5, 10, 15} (with multivariate dimension p ∈ {2, 3, 4}); the illustrative case study uses n = 5. Phase I is used to estimate the in-control MCV γ0 (example: γ0 = 0.089115), but no minimum Phase I sample-size guidance is provided.",TRUE,None / Not applicable,Not provided,NA,"The paper develops one-sided, “pure” run-rules (r-out-of-s) control charts to monitor the multivariate coefficient of variation (MCV) for p-variate normal data, using the Nikulin–Voinov definition $\gamma=(\mu^T\Sigma^{-1}\mu)^{-1/2}$ and the sample statistic $\hat\gamma=(\bar X^T S^{-1}\bar X)^{-1/2}$. It proposes applying supplementary run rules (2-of-3, 3-of-4, 4-of-5) to the existing Shewhart–MCV chart to improve detection of changes in the MCV, separately for decreases (lower-sided) and increases (upper-sided). Statistical performance (ARL/SDRL) is computed via an embedded Markov-chain approach (Brook–Evans style), with control limits chosen to attain a fixed in-control ARL (ARL0 = 370.4). Extensive numerical studies across $n\in\{5,10,15\}$, $p\in\{2,3,4\}$, and $\gamma_0\in\{0.1,\dots,0.5\}$ show run-rules charts typically reduce out-of-control ARL relative to Shewhart–MCV (especially for smaller/moderate shifts), and can be competitive with or better than the Run Sum MCV chart when shift size is uncertain (evaluated via EARL over shift intervals). A spring-manufacturing example illustrates that run-rules MCV charts signal an out-of-control condition that the Shewhart–MCV chart misses.","MCV definition: $\gamma=(\mu^T\Sigma^{-1}\mu)^{-1/2}$ and sample MCV: $\hat\gamma=(\bar X^T S^{-1}\bar X)^{-1/2}$. The CDF of $\hat\gamma$ is expressed via a noncentral F distribution: $F_{\hat\gamma}(x)=1-F_F\big(\tfrac{n(n-p)}{(n-1)px^2}\,\big|\,p,n-p,\delta\big)$ with $\delta=n\mu^T\Sigma^{-1}\mu=n/\gamma^2$. Run-rules signaling is r-out-of-s observations beyond a single control limit (lower-sided: $\hat\gamma<LCL^-$; upper-sided: $\hat\gamma>UCL^+$), with ARL computed from the transient-state matrix $Q$ of a Markov chain: $ARL=q^T(I-Q)^{-1}\mathbf 1$ (and SDRL via second-moment formulas). Control limits are solved to satisfy $ARL(\tau=1)=ARL_0$ (target 370.4), where shifts are modeled as $\gamma_1=\tau\gamma_0$.","With ARL0 fixed at 370.4, tables of computed limits show both LCL− and UCL+ depend on $(n,p,\gamma_0)$; larger p generally yields smaller limits. Across many simulated scenarios, run-rules charts often yield substantially smaller ARL1 than Shewhart–MCV for moderate/small shifts; e.g., the paper reports broad positive improvement indices (ΔA, ΔE) in most cases, while Shewhart can be better for very large shifts (negative ΔA for extreme τ such as 0.50 or 1.50 in some settings). For unknown shift sizes, EARL comparisons over τ∈[0.5,1) (decreases) and τ∈(1,2] (increases) generally favor run-rules charts; recommended choices are RR−4,5 for decreasing shifts and RR+2,3 for increasing shifts. In the spring-manufacturing Phase II example (n=5, p=2, γ0≈0.0891), the upper Shewhart–MCV chart with UCL=0.1691 fails to signal, while run-rules charts signal (e.g., RR+2,3 signals with points #4 and #5 above its UCL=0.1296).",None stated.,"The approach assumes i.i.d. multivariate normal observations and uses the noncentral-F distribution for $\hat\gamma$; robustness to non-normality, serial correlation, and estimation error in Phase I parameters (especially for small Phase I samples) is not developed. Only one-sided, single-limit “pure” run-rules schemes (2/3, 3/4, 4/5) are explored; two-sided monitoring and broader/adaptive run-rule families could change conclusions. The paper does not provide implementation code or software, which may hinder reproducibility of the Markov-chain computations and numerical solving for limits.",None stated.,"Extend the MCV run-rules charts to autocorrelated and/or non-normal multivariate processes (e.g., via robust estimators, bootstrap calibration, or copula/elliptical models). Develop two-sided and adaptive run-rules designs (or self-starting variants) with explicit treatment of Phase I estimation uncertainty and recommended Phase I sample sizes. Provide open-source software to compute limits/ARL/EARL (including the larger-state Markov chains such as RR4,5) and to support practitioner-facing design/diagnostics.",2001.00996v2,local_papers/arxiv/2001.00996v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:14:15Z
TRUE,Univariate|Other,Shewhart|Other,Phase II|Both,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,FALSE,Markov chain|Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Expected detection delay|Other,"Not discussed (the paper studies subgroup sizes n ∈ {5, 15} and considers repeated measurements per item m ∈ {1, 3, 5, 7, 10}, but does not give general Phase I sample-size guidance).",TRUE,None / Not applicable,Not provided,NA,"The paper studies run-rules control charts for monitoring the squared coefficient of variation (CV^2) when measurements are contaminated by measurement error. To avoid ARL-bias that arises with two-sided run-rules charts under the asymmetric distribution of CV^2, it proposes using two separate one-sided r-out-of-s run-rules charts (lower-sided to detect decreases and upper-sided to detect increases). In-control mean and standard deviation of the sample CV^2 are approximated via Breunig’s formulas, and run-length properties (ARL, SDRL) are computed using embedded Markov chains for 2-out-of-3, 3-out-of-4, and 4-out-of-5 rules. A linear covariate measurement error model (with precision and accuracy components) is incorporated by replacing the latent CV with the observed CV implied by the error model; results show both error types degrade performance (increase ARL/EARL), and repeated measurements per item (larger m) provide little improvement. The proposed one-sided CV^2 run-rules charts outperform prior two-sided run-rules CV charts and compare favorably to a VSI Shewhart CV^2 chart under comparable error conditions; an illustrative sintering-process dataset shows run-rules charts signal where a Shewhart chart fails.","The chart monitors the sample squared CV, \(\hat\gamma_i^2\), using one-sided run-rules with limits \(\mathrm{LCL}^- = \mu_0(\hat\gamma^2) - k_d\,\sigma_0(\hat\gamma^2)\) and \(\mathrm{UCL}^+ = \mu_0(\hat\gamma^2) + k_u\,\sigma_0(\hat\gamma^2)\), where \(\mu_0(\hat\gamma^2) \approx \gamma_0^2\left(1-\frac{3\gamma_0^2}{n}\right)\) and \(\sigma_0(\hat\gamma^2)\) is given by Breunig’s approximation (Eq. 5). The distributional model used is \(n\hat\gamma^2\sim F_{1,n-1}(\lambda=n\gamma^2)\) (noncentral F), giving \(F_{\hat\gamma^2}(x\mid n,\gamma)=1-F_F\left(\frac{n}{x}\mid 1,n-1,n\gamma^2\right)\). Run length is computed from a Markov chain with transient matrix \(Q\) as \(\mathrm{ARL}=\mathbf{q}^\top (I-Q)^{-1}\mathbf{1}\) and \(\mathrm{SDRL}=\sqrt{2\mathbf{q}^\top (I-Q)^{-2}Q\mathbf{1}-\mathrm{ARL}^2+\mathrm{ARL}}\). Under measurement error, the observed CV is \(\gamma^* = \frac{\sqrt{B^2 b^2 + \eta^2/m}}{\theta + B(1+a\gamma_0)}\,\gamma_0\), and the same noncentral-F CDF is used with \(\gamma\) replaced by \(\gamma^*\).","With \(\mathrm{ARL}_0=370.4\), the proposed one-sided \(\mathrm{RR}_{2,3}\!-
\gamma^2\) chart yields \(\mathrm{ARL}_1=95.9\) for \(\gamma_0=0.05\), \(n=5\), and a 10% increase (\(\tau=1.10\)), improving over the prior two-sided RR-CV chart reported in the literature (\(\mathrm{ARL}_1=101.6\) in Castagliola et al., 2013, as cited). Measurement errors increase ARL/EARL: for example (\(n=5,\gamma_0=0.05,B=1,m=1,\theta=0.05,\tau=0.8\)), \(\mathrm{ARL}_1\) changes from about 93.12 at \(\eta=0\) to 93.20 at \(\eta=0.3\) (small effect), while increasing \(\theta\) produces a clearer degradation (e.g., \(\mathrm{RR}_{3,4}\!-
\gamma^2\), \(n=5,\gamma_0=0.1,\eta=0.28,\tau=1.3\): \(\mathrm{ARL}_1\) increases from 26.56 at \(\theta=0\) to 29.19 at \(\theta=0.05\)). Repeated measurements per item provide negligible benefit: in one setting (\(n=5,B=1,\gamma_0=0.05,\eta=0.28,\theta=0.05,\tau=0.8\)) the \(\mathrm{RR}^+_{2,3}\!-
\gamma^2\) chart has \(\mathrm{ARL}_1\approx 9.07\) for both \(m=1\) and \(m=10\). In a real sintering-process example with assumed error parameters (\(\eta=0.28,\theta=0.05,B=1,m=1\)) and expected \(\tau=1.25\), the upper one-sided run-rules charts signal starting around sample #12, while the corresponding Shewhart CV chart does not signal.","The authors note that the approximation for the sample CV CDF used in the background is sufficiently precise only when \(\gamma<0.5\), and they rely on this being typical in practice. They also state that many additional control-limit values for other parameter combinations are omitted from the paper and are available only upon request.","The proposed charts and performance calculations rely on normality and i.i.d. sampling; robustness to autocorrelation, heavy tails, or non-normal industrial data is not assessed. The measurement error model assumes known constants (A, B) and normal measurement noise; in practice these must be estimated and may be misspecified, which can materially affect in-control ARL. Implementation guidance is limited (no software or step-by-step algorithm for practitioners), and the empirical validation is based on a single illustrative dataset rather than multiple diverse case studies.",None stated.,"Develop self-starting or Phase-I/Phase-II integrated versions that account for estimation of \(\gamma_0\) and measurement-error parameters (A, B, \(\sigma_M\)) from historical data. Study robustness and/or nonparametric alternatives for CV monitoring under non-normality and autocorrelation, and provide open-source software to facilitate adoption and reproducibility.",2001.01821v1,local_papers/arxiv/2001.01821v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:15:07Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Healthcare/medical|Other,NA,NA,NA,Case study (real dataset)|Other,Other,Not discussed (study uses 4 test users for usability evaluation; no Phase I sample size guidance for SPC parameter estimation).,TRUE,R|Other,Not provided,https://eralpdogu.shinyapps.io/msstatsqc/|https://drive.google.com/drive/folders/1jbR5DOv4uOae99K4-oU10Q4T3rLcXXDb?usp=sharing,"This paper evaluates the usability and plot interpretability of MSstatsQC, an open-source tool that performs longitudinal system suitability monitoring for targeted proteomics using control charts and related visual summaries. The study has four proteomics-domain test users complete task-based workflows (data import/metric selection, control chart interpretation, decision-rule setting, and interpretation of summary plots such as boxplots, decision maps, river plots, and radar plots). The SPC component is primarily the use of XmR-style control charts to monitor both mean and dispersion of suitability metrics over time, along with decision-rule-based flags visualized in decision maps. Results highlight key usability failures (especially data import due to insufficient format guidance and lack of error messages) and interpretability issues (insufficient plot titles/axis labels and missing plot explanations). The authors recommend interface changes such as clearer import guidance with pop-up errors, improved plot labeling and interpretive hints, and tab numbering to reflect workflow order.",Not applicable (the paper is a usability/interpretability evaluation and does not present new SPC chart formulas).,"None of the four test users were able to successfully complete Task 1 (data import/format validation) because the required file-format guidance was not conspicuous and no error message appeared for incorrect formats. All test users completed the control-chart interpretation task (Task 2), but at least one expressed confusion and users requested explicit explanations for chart elements (e.g., red/blue points, threshold line) plus axis labels and bold titles. Some users misunderstood that changing decision rules should affect boxplots; two users expected boxplots to change after decision-rule updates, suggesting workflow/tab placement confusion. The least rated usability attribute was convenience with an average rating of 2.7/5.",The authors note difficulty recruiting qualified proteomics researchers as test users because the community is small and participation in a 40-minute study without reward is unattractive. They also state that designing tasks that adequately cover both usability and interpretability while remaining feasible within 30–40 minutes was challenging.,"The study uses only four participants, limiting generalizability and making results sensitive to individual differences; no formal qualitative coding framework (e.g., thematic analysis with inter-rater reliability) is described. SPC performance of the underlying control charts (e.g., ARL/false-alarm behavior, robustness to non-normality/autocorrelation typical in instrument time series) is not assessed, so recommendations focus on UI rather than statistical adequacy. The paper does not specify how representative the chosen tasks/datasets are across different labs/instruments or whether users’ prior SPC/control-chart knowledge influenced interpretability outcomes.","The authors propose redesigning MSstatsQC based on the usability findings and then re-evaluating the revised software with another usability test to demonstrate improved convenience, satisfaction, comprehension, and aesthetics. They also propose developing general rules/techniques that proteomics software designers can follow to design their own usability studies.","A useful extension would be to couple usability findings with statistical validation of MSstatsQC’s monitoring choices (e.g., calibration of false-alarm rates under realistic autocorrelation and drift) and provide guidance on selecting chart parameters/decision rules. Larger, more diverse multi-site user studies (including novice vs expert SPC users) with structured qualitative analysis and quantitative usability metrics would strengthen conclusions. Providing an open-source reproducible usability-study package (tasks, datasets, scripts, anonymized logs) would enable benchmarking against other QC tools (e.g., AutoQC, SProCoP) and track improvements across versions.",2002.00511v1,local_papers/arxiv/2002.00511v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:15:40Z
TRUE,Multivariate|Nonparametric|High-dimensional|Self-starting|Other,Shewhart|CUSUM|EWMA|Hotelling T-squared|MEWMA|MCUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Machine learning-based|Other,Both,Manufacturing (general)|Semiconductor/electronics|Healthcare/medical|Service industry|Theoretical/simulation only,TRUE,NA,NA,Simulation study|Approximation methods|Exact distribution theory|Other,ARL (Average Run Length)|ATS (Average Time to Signal)|False alarm rate|Detection probability|Expected detection delay|MRL (Median Run Length)|Other,Not discussed (notes Phase I requires a large dataset for good parameter estimation; highlights need for research on required Phase I sample size for covariance monitoring).,NA,None / Not applicable,Not applicable (No code used),NA,"This paper is a literature review of multivariate statistical process monitoring methods for detecting changes in a process covariance (dispersion) matrix, an area less developed than mean-vector monitoring. It organizes the literature into four groups: monitoring covariance alone when subgroup size n≥p, monitoring covariance when n<p (including individual observations), joint monitoring of mean vector and covariance, and related topics such as diagnosis, robustness/nonparametrics, and Phase I analysis. The review emphasizes Phase II monitoring under multivariate normality with independent subgroups, while also summarizing robust/nonparametric and high-dimensional/sparse approaches (e.g., penalized likelihood/LASSO, thresholding, eigenvalue-based methods). It highlights key charting statistics used across the literature (e.g., generalized variance |S|, trace-based measures, likelihood-ratio statistics, EWMA/CUSUM-based schemes) and common performance criteria such as ARL0/ARL1 and ATS. The paper concludes with open problems, including guidance for practitioners, misleading signals in joint schemes, tuning-parameter selection in sparse methods, Phase I estimation effects, non-normal and autocorrelated data, and high-dimensional settings.","The review sets up monitoring as hypothesis tests on the covariance matrix: $\Sigma=\Sigma_0$ vs $\Sigma\neq\Sigma_0$ (two-sided) or one-sided PSD alternatives $\Sigma\succcurlyeq\Sigma_0$ / $\Sigma\preccurlyeq\Sigma_0$. It defines Phase I estimators (combined mean and covariance) and presents representative charting statistics used in the literature, e.g., Alt/Smith LRT-type Shewhart statistic $R_i=-(n-1)\left[p+\ln(|S_i|/|\Sigma_0|)-\operatorname{tr}(\Sigma_0^{-1}S_i)\right]$. It also summarizes penalized precision-matrix estimation used for sparse covariance changes, e.g., $\hat\Omega_i(\lambda)=\arg\min_{\Omega\succ0}\{\operatorname{tr}(\Omega S_i)-\ln|\Omega|+\lambda\|\Omega\|_1\}$ and PLR-type monitoring statistics based on $\hat\Omega_i(\lambda)$, with control limits often set via Monte Carlo to meet a target ARL0.","Not applicable (review paper; no single set of new quantitative results). The paper notes that many studies compare methods primarily via in-control/out-of-control ARL (ARL0/ARL1) and sometimes ATS, and it cites examples where particular methods outperform others under certain shift patterns (e.g., VMIX vs VMAX for small variance shifts; MEWMA/EWMA-type schemes better for small shifts than Shewhart-type charts). It also reports qualitative findings from the literature that generalized-variance charts can miss compensating eigenvalue changes and that some schemes are sensitive to mean shifts (e.g., MEWMS spuriously signaling covariance changes under mean shifts).","The authors state the review mainly focuses on Phase II control charts for multivariate normal processes with independent subgroups, and only also considers related topics (diagnostics, robust charts, Phase I methods). They note that covariance monitoring is challenging because summarizing a covariance matrix is nontrivial and parameter estimation burden grows quickly with dimension (p≥3), making methods diverse and less accessible to practitioners.","As a review centered on developments since 2006, coverage and depth may be uneven across subtopics (e.g., fewer details on economic design, implementation guidance, and software availability). Because the paper aggregates results across many sources, it does not provide standardized benchmarking (common scenarios, consistent ARL0 settings, identical shift models), which limits direct practitioner-ready recommendations. It also does not provide implementation artifacts (code/templates) to operationalize the classification into a decision tool for method selection.","The paper calls for broader and more systematic comparisons of covariance-monitoring charts to guide practitioners; more study of misleading-signal probabilities in joint mean/covariance schemes; and more adaptive-chart designs (variable sampling intervals/sizes) for dispersion monitoring. It highlights the need for principled tuning-parameter selection in sparsity/penalized-likelihood methods and more research on distinguishing mean shifts from covariance shifts with better diagnostics. It also emphasizes research on Phase I estimation effects and required Phase I sample sizes, robust/nonparametric methods beyond bivariate cases, methods for autocorrelated multivariate data, and leveraging high-dimensional techniques and variable selection for modern applications.","A practical next step would be a reproducible, open benchmark suite (standard data-generating processes, shift families, and reporting of ARL/ATS/SDRL) to enable fair cross-paper comparisons and meta-analysis. Developing unified software (e.g., an R/Python package) implementing the major covariance-monitoring families with calibrated limits would substantially improve accessibility and uptake. Additional work could formalize decision rules for choosing among sparse vs dense detectors (possibly adaptive/ensemble charts) and provide diagnostic post-signal tools that localize which covariance elements/eigen-structures changed, especially under high-dimensional constraints.",2002.06159v2,local_papers/arxiv/2002.06159v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:16:23Z
FALSE,Other,Machine learning-based|Other,Phase II,Network/cybersecurity|Other,TRUE,NA,FALSE,Simulation study|Other,Other,Not discussed,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a real-time anomaly detection approach for data-center log-based predictive maintenance using an evolving fuzzy-rule-based classifier (eGFC) trained online on features extracted from sliding time windows of log-entry rates. A control-chart-inspired labeling scheme tags each window’s mean log rate into four severity classes (normal, low, medium, high) based on bands defined by multiples of the standard deviation around the global mean of window means. The eGFC model evolves its rule base (creating, updating, merging, and deleting Gaussian fuzzy granules) to adapt to non-stationary streaming behavior without requiring an initial rule set. Empirical results on StoRM backend logs from the INFN Tier-1 Bologna data center show multi-class classification accuracy up to about 92.48% (60-minute windows) with a compact rule base (~13 rules on average) and sub-second runtime. Overall, SPC concepts are used primarily as an automated labeling/thresholding heuristic within a machine-learning monitoring pipeline rather than as a contribution to SPC/control-chart methodology.","Key elements include: (i) window mean $\mu_j=\frac{1}{n}\sum_{i=1}^n u_i$ and global mean $\bar\mu=\frac{1}{m}\sum_{j=1}^m \mu_j$; (ii) banding via $\sigma_k(\mu)=k\sqrt{\frac{1}{m}\sum_{j=1}^m(\bar\mu-\mu_j)^2}$ and tagging $\mu_j$ by whether it falls within $[\bar\mu-\sigma_k,\bar\mu+\sigma_k]$ for $k=1,2,3,4$; (iii) eGFC rule form with Gaussian membership functions and online updates of modal values and dispersions via recursive formulas (Eqs. (6)–(7)) plus an adaptive activation threshold $\rho$ updated by average dispersion ratios (Eqs. (8)–(9)).","Across 5 runs on datasets of 1,436 samples (5 attributes, 4 classes) derived from StoRM logs, average accuracy (99% CI) was 92.48% ± 1.21 for 60-minute windows with 13.42 ± 4.32 rules and 0.36 ± 0.10 s runtime. For 30-, 15-, and 5-minute windows, accuracies were 88.01% ± 4.96, 82.57% ± 5.64, and 81.97% ± 5.02, respectively, with ~16–18 rules and ~0.41–0.49 s runtime. A shown confusion-matrix example reports ~94.2% accuracy, with most misclassifications occurring between neighboring severity classes, especially Class 1 vs Class 2.","The authors state that analyzing the message type/content is out of scope, focusing only on timestamps/log-rate summaries rather than semantic log content. They also note that although eGFC can handle partially labeled data, they assume a fully labeled dataset in this paper. They emphasize the class imbalance induced by the control-chart probability bands, implying the online classification problem is unbalanced.","The control-chart tagging uses a global mean and standard deviation of window means, which is not a standard Phase II SPC design with fixed in-control parameters and could be unstable under strong non-stationarity or seasonality. The approach evaluates accuracy and model compactness but does not report SPC-relevant detection metrics (e.g., false-alarm rate, in-control ARL/ATS, detection delay), making monitoring trade-offs hard to compare to control-chart methods. The evaluation appears limited to one service/log type (StoRM backend ATLAS) and does not demonstrate robustness across different services, machines, or varying workload regimes.",They propose to identify the types of log messages associated with anomalous time windows and to investigate autonomous feature extraction procedures to better select and process relevant log components while minimizing computational cost.,"A natural extension would be to quantify monitoring performance using detection-delay/false-alarm metrics (e.g., ATS, expected delay) and calibrate thresholds for target false-alarm rates under non-stationary baselines. Incorporating explicit handling of autocorrelation/seasonality (e.g., residual charts or state-space modeling of log-rate) could make the labeling and detection more reliable. Providing an open-source implementation and evaluating across multiple services and data centers would improve reproducibility and generalizability.",2004.13527v1,local_papers/arxiv/2004.13527v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:16:59Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Both,Manufacturing (general)|Semiconductor/electronics|Other,TRUE,FALSE,FALSE,Simulation study|Case study (real dataset),False alarm rate|Other,"Uses Phase I training data size N and online window length W. Examples in simulations use N=5000 training samples; online monitoring uses W in ranges such as [7,10] (numerical example) and [5,10] (CSTR example).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes moving-average Hotelling’s $T^2$ control charts with multiple window lengths (a bank of MA-$T^2$ charts) to detect intermittent faults (small magnitude, short duration) in a multivariate statistical process monitoring framework. It derives distributional results for the MA-$T^2$ statistic and introduces a guaranteed-detectability concept tailored to intermittent faults, extending prior detectability notions focused on permanent faults. Necessary and sufficient conditions are obtained for guaranteed detectability of both appearance and disappearance of intermittent faults, including constraints linking the window length to active/inactive fault durations and fault magnitude. The method integrates alarms across multiple window lengths to reduce false alarms, compensate missing alarms, and infer fault appearance/disappearance time intervals, summarized in an online algorithm. Effectiveness is demonstrated via simulation on a Gaussian numerical example and a simulated CSTR process with intermittent sensor faults.","The MA mean and covariance estimates are $\bar x_f(k)=\frac{1}{W}\sum_{i=1}^W x^f_{k-W+i}$, $\bar x=\frac{1}{N}\sum_{i=1}^N x_i$, and $S=\frac{1}{N-1}\sum_{i=1}^N (x_i-\bar x)(x_i-\bar x)^T$. The charting statistic is $T_k^2(W)=(\bar x_f(k)-\bar x)^T S^{-1}(\bar x_f(k)-\bar x)$, which under in-control conditions follows a scaled $F$ distribution: $T_k^2(W)\sim \frac{p(N+W)(N-1)}{NW(N-p)}F(p,N-p)$. The control limit is $\delta_W^2=\frac{p(N+W)(N-1)}{NW(N-p)}F_\alpha(p,N-p)$, and intermittent-fault detectability requires (among other conditions) $W\le \tau_q^r$ and magnitude/window inequalities such as $\|S^{-1/2}\xi_q f_q\|>2\delta_W$ (or a duration-scaled variant).","The paper proves that guaranteed detectability of an intermittent fault’s disappearance by MA-$T^2$(W) holds iff the window length does not exceed the subsequent inactive duration: $W\le \tau_q^r$ (Lemma 2). For appearance detectability, it derives necessary-and-sufficient inequalities that depend on whether the window is shorter than the fault’s active duration (Lemmas 3–4), and consolidates them into global conditions (Theorems 1–3). It provides optimality properties for two important window lengths: $W^*=\left\lceil \frac{N}{\frac{N+1}{4\delta^2}\|S^{-1/2}\xi_q f_q\|^2-1}\right\rceil$ (minimizes alarm delay when detection is possible for given direction/magnitude) and $W^\#=\min\{\tau_{q-1}^r,\tau_q^o,\tau_q^r\}$ (supports detectability across faults sharing durations). Simulation studies (Gaussian 2D and CSTR) show improved detection and better inference of appearance/disappearance intervals when using multiple windows (e.g., W in [7,10] and [5,10]) compared with small W (e.g., 1–2) which yields many false/missing alarms.","The paper assumes current process data are independent (no serial correlation), noting that dynamic MSPM methods are not considered under their stated independence assumption. It also notes that practical implementation may not know exact intermittent-fault parameters and suggests using lower bounds when available, implying reliance on some prior parameter information for window selection.","The approach relies on multivariate normality and an $F$-based control limit derived from that assumption; robustness to heavy tails/outliers is not analyzed. Multiple-window monitoring increases multiplicity/false-alarm risk; while heuristic intersection/duration rules mitigate this, there is no formal overall false-alarm (familywise) calibration for the bank. The evaluation appears based on simulated examples (Gaussian and simulated CSTR) rather than real industrial datasets, so real-world efficacy under nonstationarity/autocorrelation remains uncertain.","The paper suggests extending MA-TCCs(M) to multi-mode processes by incorporating prior multi-mode knowledge or multi-mode modeling techniques such as Gaussian mixture models (GMM) and hidden Markov models (HMM). It also notes applicability to detecting intermittent modes/patterns viewed as intermittent faults, including discriminating them from false alarms and inferring their appearance/disappearance times.","Develop robust/nonparametric or heavy-tail-resistant versions of MA-$T^2$ banks (e.g., using robust covariance estimators) and study their detectability conditions. Extend the framework to autocorrelated or irregularly sampled data (e.g., integrating time-series modeling or using residual-based charts) and provide end-to-end false-alarm calibration across multiple window lengths. Provide open-source implementations and benchmark on real industrial intermittent-fault datasets to validate performance and tuning guidance.",2005.06825v2,local_papers/arxiv/2005.06825v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:17:39Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Phase II,Manufacturing (general)|Energy/utilities|Theoretical/simulation only|Other,NA,TRUE,FALSE,Simulation study|Other,False alarm rate|Detection probability|Other,"Uses Phase I/training data of N sets of W consecutive observations (independent across sets via long intervals). Examples: numerical study uses N=5000 sets of length W=10; other MSPM baselines use 50,000 consecutive observations for training. No general minimum N guidance beyond needing N sufficiently large and Np (for covariance invertibility).",TRUE,None / Not applicable,Not provided,NA,"The paper develops an optimally weighted moving-average Hotellings $T^2$ control chart (OWMA-TCC) for multivariate statistical process monitoring with weakly stationary, autocorrelated observations, targeting intermittent fault (IF) detection. Unlike standard MA/EWMA schemes (equal/exponential weights), it chooses an optimal weight vector using estimated auto- and cross-correlation information within a finite window to maximize an IF detectability criterion. The authors derive guaranteed detectability conditions for IF appearance/disappearance, formulate weight selection as a constrained optimization problem, and provide nonlinear fixed-point equations for the optimal weights with existence proven via Brouwers fixed-point theorem; they also show the optimal weights are symmetric in stationary settings and reduce to equal weights when data are independent. For Gaussian observations they derive the $T^2$ statistic distribution and analytic control limit; for non-Gaussian weakly stationary processes they propose empirical/KDE limits while retaining the weight optimality in a mean-separation sense. Simulations on a multivariate AR(1) example and a CSTR benchmark show OWMA-TCC detects intermittent small/short faults more clearly and with FAR aligned to the nominal level, outperforming PCA/MA-PCA, DPCA, CVA, MW-KD, and MA-based Mahalanobis distance approaches under autocorrelation.","Within a window of length $W$, the weighted mean is $\tilde X^f_k=\sum_{j=1}^W a_j X^f_{k-j+1}$ with constraint $\sum_{j=1}^W a_j=1$; Phase I estimates are $\tilde X=\frac1N\sum_{i=1}^N \tilde X_i$ and $\tilde S_W=\frac{1}{N-1}\sum_{i=1}^N (\tilde X_i-\tilde X)(\tilde X_i-\tilde X)^T$. The charting statistic is $\tilde T^2_k(W)=(\tilde X^f_k-\tilde X)^T\tilde S_W^{-1}(\tilde X^f_k-\tilde X)$ with Gaussian-case control limit $\delta^2=\frac{p(N^2-1)}{N(N-p)}F_{\alpha}(p,N-p)$. Detectability for IF direction $\xi_q$ and magnitude $f_q$ requires $\|\tilde S_W^{-1/2}\xi_q f_q\|>2\delta$, motivating the optimization $\max_{a} \beta(a)=\tfrac12\|\tilde S_W^{-1/2}\xi_q\|^2$ subject to $\sum a_j=1$, yielding nonlinear fixed-point equations $\hat T(a^*)a^*=b$.","In the numerical AR(1) study with window W=10 and significance level $\alpha=0.01$, OWMA-TCC and MA-TCC both achieve FAR \approx 0.25% on the first 400 in-control test samples, but OWMA-TCC signals much more distinctly during intermittent faults (MA-TCC tends to hover near the limit). Static MA-PCA with W=10 suffers a high FAR (reported 11%) under autocorrelated data, illustrating non-independence issues; several DPCA/CVA configurations show weaker intermittent-fault sensitivity. In the CSTR study (sampling interval 3s), MA-based Mahalanobis distance with W=10 has FAR \approx 10.25% while OWMA-TCC(W=10) has FAR \approx 0.25% and clearer continuous alarming during IFs. Under non-Gaussian noise (uniform), OWMA-TCC using empirical limits still shows the best separation versus MW-KD and other baselines.",None stated.,"The method relies on accurate estimation of lagged auto/cross-covariances and assumes the Phase I sets are independent (obtained via long enough spacing), which may be operationally expensive or infeasible. Optimal weights are derived relative to a specified IF direction (or assumed known/representative direction), and the paper does not fully address robustness when the fault direction/magnitude/duration are misspecified or vary over time. Control-limit computation for non-Gaussian data is delegated to empirical/KDE approaches without a detailed study of their calibration, sample-size sensitivity, or computational burden in high dimensions.","The authors propose combining OWMA with recursive methods, other statistics, kernel methods, and dynamic data modeling methods and exploring other selection criteria to handle slightly varying operating points, varying noise levels, nonlinear and nonstationary processes, and faults with unknown characteristics.","Develop a fully self-starting/online Phase I-to-Phase II approach that updates covariance/weight estimates without requiring independently spaced training batches, and study the impact on false-alarm control. Extend OWMA-TCC to high-dimensional settings (p comparable to or larger than N) via regularized covariance estimation and provide theory/simulation for robustness. Provide open-source implementations and benchmark studies on additional real industrial datasets, including comparisons to modern change-point and ML-based detectors under autocorrelation and non-Gaussianity.",2005.06832v2,local_papers/arxiv/2005.06832v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:18:20Z
TRUE,Multivariate|Profile monitoring|Functional data analysis|High-dimensional,Hotelling T-squared|Other,Both,Manufacturing (general),TRUE,FALSE,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length)|Detection probability,Not discussed (examples used: simulation uses M=200 samples; case study training set includes 308 normal samples and 69 samples for each of 5 fault classes).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a real-time monitoring and fault diagnosis framework for high-dimensional multi-channel (tensor) sensor profile data using uncorrelated multilinear discriminant analysis (UMLDA) for feature extraction. The extracted low-dimensional, uncorrelated discriminant features are monitored with a multivariate Hotelling $T^2$ control chart, with limits based on the $F$ distribution and a specified Type I error rate (e.g., $\alpha=0.01$ in simulations). The approach targets detecting process changes/faults reflected in profile shape changes, mean shifts, sinusoidal superimpositions, localized weakening in segments, parameter mean shifts, and increased noise variance. Performance is evaluated via Monte Carlo simulations using benchmark signals (Blocks/Heavy sine/Bumps) and via a real forging-process case study with missing-part faults, showing improved detection compared to VPCA/UMPCA/MPCA-based feature extraction. The method is positioned as a tensor-native, supervised alternative to PCA-based multiway monitoring that better exploits class information and cross-channel relationships for faster/more accurate signaling.","UMLDA extracts scalar features via tensor-to-vector projections: $y_{ml}=\mathcal{X}_m\times_1 \mathbf{v}^{(1)T}_l\times_2 \mathbf{v}^{(2)T}_l\cdots\times_N \mathbf{v}^{(N)T}_l$, choosing EMPs to maximize the Fisher discriminant criterion $F_l^y=S_{B,l}^y/S_{W,l}^y$ with an additional uncorrelated-feature constraint. Monitoring uses a Hotelling chart on the feature vector $\mathbf{g}_{new}\in\mathbb{R}^J$: $T^2=(\mathbf{g}_{new}-\bar{\mathbf{g}})^T\mathbf{S}^{-1}(\mathbf{g}_{new}-\bar{\mathbf{g}})$, where $(\bar{\mathbf{g}},\mathbf{S})$ come from in-control training features. Control limits are set from the $(1-\alpha)$ quantile of an $F$ distribution for $T^2$ (as stated) with degrees of freedom $J$ and $M-J$.","In Monte Carlo simulations with targeted $\alpha=0.01$ and 1000 experiments per projection setting, the proposed UMLDA-based monitoring generally achieves the smallest (best) ARLs for scenarios involving benchmark-signal changes (mean shift, sine superimposition, localized weakening). For scenarios involving shifts in the model-parameter distribution or increased noise variance, MPCA is a close competitor and is slightly better in 3 out of 20 tested cases (4 channels × 5 shift magnitudes). In the real forging case study (missing-part detection), the number of detected out-of-control samples (out of 345 fault samples) is 163 (VPCA), 222 (UMPCA), 224 (MPCA), and 311 (UMLDA), indicating substantially higher detection for UMLDA. Reported average online monitoring time per sample is on the order of milliseconds; for 1000 samples: 1.3e-3 s (VPCA), 9.5e-5 s (UMPCA), 5.4e-4 s (MPCA), and 1.4e-3 s (UMLDA), supporting real-time feasibility.",None stated.,"The $T^2$ charting limits rely on distributional assumptions for the extracted features (normality/$F$-approximation) and do not explicitly address serial dependence typical of waveform/profile data, which can inflate false alarms if samples are autocorrelated. The method appears to require labeled fault classes to train UMLDA (supervised), which may be unavailable for many SPC Phase I settings and may limit generalization to novel/unknown fault types. Implementation details affecting practical performance (feature/segment selection strategy, robustness to mis-segmentation, and sensitivity to Phase I estimation uncertainty) are not fully developed as formal guidance or guarantees.",None stated.,"Extend the approach to explicitly handle autocorrelated profiles (e.g., integrate time-series modeling or use residual-based charts) and assess impacts on in-control ARL under dependence. Develop semi-supervised/unsupervised or novelty-detection variants that do not require labeled fault classes, enabling broader Phase I/Phase II deployment. Provide robust/bootstrapped control-limit estimation for the $T^2$ chart under non-normal extracted features and small Phase I sample sizes, and release a reference software implementation for reproducibility and adoption.",2005.12585v1,local_papers/arxiv/2005.12585v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:18:58Z
TRUE,Multivariate|Profile monitoring,EWMA|MEWMA,Phase II,Healthcare/medical,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length),For simulations/monitoring at monthly intervals they generate datasets of size 500 patients and state this size “fulfills the asymptotic assumption” of the score-based MEWMA chart; control limit set for in-control ARL = 200 assuming Phase I parameters known without error.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a multi-stage, risk-adjusted monitoring method for healthcare procedures that have temporally ordered intermediate and final outcomes. It develops a score-based multivariate EWMA (MEWMA) control chart that monitors the vector of regression coefficients in a graphical/GLM model linking outcomes to upstream outcomes, process variables, and patient risk factors, thereby accounting for dependence among outcomes and case-mix. The chart uses exponentially weighted standardized likelihood score contributions to improve sensitivity to small, gradual changes, and signals when a Hotelling-type statistic based on the MEWMA vector exceeds a control limit chosen to achieve a target in-control ARL. Performance is evaluated via Monte Carlo simulation under several shift types (additive coefficient shifts, paired coefficient shifts, mean shifts, and odds-ratio shifts) with results summarized using out-of-control ARLs. The approach is demonstrated using parameter estimates obtained from real maternity-unit data (Southmead Hospital, Bristol) to ground the simulation scenarios.","A multistage GLM is specified for each outcome $Y_v$ via $g_v(\mu_{vt})=\alpha_v+x_{vt}^\top\beta_v+y_{pa(v),t}^\top\gamma_v+z_{vt}^\top\delta_v$; for the motivating example all outcomes are binary with logit link. The log-likelihood is $\ell(\Theta)=\sum_{v=1}^V\sum_{t=1}^n\{y_{vt}(\theta_v^\top U_{vt})-\log(1+\exp(\theta_v^\top U_{vt}))\}$ and the score for block $v$ is $s(\theta_v)=\sum_{t=1}^n u_{vt}[y_{vt}-\exp(\theta_v^\top U_{vt})/(1+\exp(\theta_v^\top U_{vt}))]$. The MEWMA update is $W_t=RS_t+(I-R)W_{t-1}$ and the charting statistic is $T_t^2=W_t^\top\Sigma_{W_t}^{-1}W_t$, signaling when $T_t^2>h$ (with $h$ chosen by simulation for a target in-control ARL).","The control limit is calibrated to achieve an in-control ARL of 200 (assuming Phase I parameters are known without error), and out-of-control performance is reported as ARL under different shift scenarios using 5,000 Monte Carlo replications per setting. For additive shifts in process-effect coefficients, ARL decreases as shift size increases; e.g., shifting $\beta_{24}$ by factor $c$ reduces ARL from 195.7 (c=0.2) to 66.3 (c=4.0), while shifting $\beta_{23}$ yields ARL 197.9 (c=0.2) to 77.3 (c=4.0). For upstream-outcome effects, shifting $\gamma_{34}$ shows strong sensitivity (e.g., ARL 196.8 at c=0.2 to 53.4 at c=4.0). Simultaneous shifts in pairs of coefficients (e.g., $(\beta_{23},\beta_{24})$ or $(\gamma_{23},\gamma_{24})$) are detected faster than single-coefficient shifts, as shown by lower ARL curves in Figures 2–3 relative to the single-shift dotted baselines.",None stated.,"Control limits are obtained by simulation under an assumption that Phase I parameters are known “without error,” but the effect of parameter-estimation uncertainty on in-control ARL and false-alarm behavior is not analyzed. The method appears to assume independent patient observations (no explicit handling of temporal correlation, seasonality, or practitioner-level clustering), which can materially affect MEWMA calibration in healthcare. Implementation details (e.g., chosen smoothing parameter(s) $r$, how $\Sigma_S$ is estimated in practice, and computational guidance/software) are limited, and no publicly available code is provided.",None stated.,"Extend calibration and performance evaluation to incorporate Phase I estimation error (e.g., using bootstrap or Bayesian/shrinkage estimation of $\Theta$ and $\Sigma_S$) and study its impact on in-control ARL. Develop variants that explicitly handle dependence structures common in healthcare (serial correlation, seasonality, practitioner/hospital random effects) and missing/irregularly observed stage outcomes. Provide diagnostic tools to localize which stage/coefficients drive a global signal and release an implementation package to support routine deployment.",2006.14737v1,local_papers/arxiv/2006.14737v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:19:35Z
TRUE,Univariate|Bayesian,CUSUM|EWMA,Phase II,Theoretical/simulation only,NA,FALSE,FALSE,Simulation study,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|ATS (Average Time to Signal)|Other,"Simulation/sample-size sensitivity uses subgroup/sample size n = 10 (main simulations) and also studies n ∈ {5, 10, 20, 30}. Simulations use m = 10,000 iterations and designs target ARL0 ≈ 370.",TRUE,None / Not applicable,Not provided,NA,"The paper develops and compares Bayesian versions of EWMA and CUSUM control charts where the chart centerline and limits are computed from the posterior predictive distribution under different Bayesian loss functions. It studies three loss functions—squared error (SELF), precautionary (PLF), and Linex (LLF)—and evaluates how the loss function choice affects monitoring performance. The Bayesian framework is worked out for a Normal likelihood with a conjugate Normal prior and for a Poisson likelihood with conjugate Gamma prior, and also considers a Poisson likelihood with an Exponential prior to illustrate generality. Control limits retain the standard EWMA/CUSUM forms but replace classical mean/SD with posterior-predictive Bayes-estimator-based quantities, and performance is assessed primarily via ARL and SDRL along with ATS and SDTS. Extensive Monte Carlo simulations (m = 10,000) calibrate charts to ARL0 ≈ 370 and compare out-of-control behavior across shift sizes and hyperparameter/sample-size settings.","Bayesian EWMA uses posterior-predictive mean/SD in limits: $UCL/LCL=\mu_{LF}\pm L\,\sigma_{\bar Y}\sqrt{\tfrac{\tau}{2-\tau}}$, $CL=\mu_{LF}$, with statistic $z_i=\tau(\bar y\mid x)+(1-\tau)z_{i-1}$. Bayesian CUSUM uses $UCL/LCL=\pm h\,\sigma_{\bar Y}$, $CL=\mu_{LF}$, with statistic $c_i=[(\bar y\mid x)-\mu_{LF}]+c_{i-1}$. Here $\mu_{LF}$ is the posterior-predictive Bayes estimator under SELF/PLF/LLF and $\sigma_{\bar Y}$ is the SD of the Bayes-estimator posterior-predictive distribution.","Charts are calibrated to in-control performance around $ARL_0\approx370$ (e.g., Normal-conjugate Bayesian CUSUM uses $h=6$ to achieve this). Under Normal conjugacy, a small mean shift (e.g., $\delta=0.25$) reduces ARL dramatically (e.g., about 382 at $\delta=0$ down to about 25 at $\delta=0.25$ in reported settings), and ARL continues decreasing toward ~2 as $\delta$ approaches 2.5. Hyperparameter sensitivity results indicate the Bayesian CUSUM performance is relatively stable across the tested prior means/variances, with LLF often yielding smaller ATS/SDTS than SELF/PLF. Sample-size sensitivity (Normal conjugate) shows improved detection with larger n when $h$ is reduced accordingly (e.g., for SELF at $\delta=1$, ARL drops from ~8.82 at n=5 to ~3.81 at n=20 and ~3.00 at n=30 in the reported table).",None stated.,"The work appears largely simulation-based with limited/no real-data case study, so practical performance in realistic manufacturing/service settings is not validated. Autocorrelation/serial dependence is not modeled, which can materially affect EWMA/CUSUM false-alarm and detection properties. The Phase I problem (estimating in-control parameters from historical data with uncertainty/outliers) is not treated; charts are effectively assessed in a Phase II setting with assumed model structure and specified priors. Software/code is not provided, which may hinder reproducibility of the simulation design/calibration (e.g., achieving $ARL_0=370$ across settings).",None stated.,"Extend the Bayesian EWMA/CUSUM constructions to autocorrelated processes (e.g., ARMA errors) and quantify robustness of ARL/ATS under model misspecification. Develop Phase I Bayesian estimation procedures (including robust priors) and propagate parameter uncertainty into Phase II limits and run-length properties. Provide publicly available software (e.g., an R/Python package) implementing calibration to target $ARL_0$ and facilitating prior elicitation and loss-function selection. Add real-world case studies (e.g., healthcare rates or manufacturing counts) and comparisons against classical and robust/nonparametric competitors under non-normal and overdispersed count data.",2007.09844v1,local_papers/arxiv/2007.09844v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:20:09Z
TRUE,Univariate|Other,CUSUM|Change-point,Phase II,Network/cybersecurity|Other,NA,FALSE,FALSE,Simulation study|Case study (real dataset),Detection probability|False alarm rate|Expected detection delay|Other,Not discussed (trial window $T_f$ is 30 days; experiments use 18 consumers and define anomaly thresholds as 10%–100% of consumers within a trial window).,TRUE,None / Not applicable,Not provided,https://aws.amazon.com/ec2/instance-types/|https://www.cs.ucsb.edu/~rich/workload/,"The paper proposes an Event-Condition-Action (ECA) framework to detect and manage changes in long-term IaaS performance “signatures” derived from aggregated free-trial user experiences. Anomaly-based event detection is performed by comparing a new trial time series to the current signature using shape-based similarity (e.g., Pearson correlation, cosine similarity, Euclidean distance); frequent anomalies within a fixed trial window trigger re-evaluation. For signature change detection (condition/action), the authors recompute a new signature from recent trial users and apply a univariate CUSUM control chart to detect a change relative to the existing signature’s mean and standard deviation, then update/replace the affected signature segment. A feedback (self-adjustment) loop adapts the anomaly-frequency threshold based on true/false positives to reduce false alarms and detection delay. Experiments using public workload traces and benchmark-derived performance data report tradeoffs between false positives, detection delay, and detection accuracy as similarity and anomaly thresholds vary, showing detection delays often on the order of one to two trial windows under suitable thresholds.","Performance anomaly is detected by comparing normalized trial experience $E'_Q=\{q'_t\}$ with signature segment $S_Q=\{s_t\}$ via similarity, e.g., Euclidean distance $\sqrt{\sum_{t=1}^n (s_t-q'_t)^2}$, Pearson correlation, or cosine similarity; a similarity threshold $T_S$ is initialized as $\min_{i=1..N} S(E_i,S_Q)$. Signature change detection uses CUSUM recursion: $UL_i=\max(0,UL_{i-1}+x_i-m_x-\tfrac{1}{2}ns_x)$ and $LL_i=\min(0,LL_{i-1}+x_i-m_x+\tfrac{1}{2}ns_x)$; a change is signaled if $UL_i>cs_x$ or $LL_i<-cs_x$ (with $m_x,s_x$ taken from the existing signature over the trial window).","Using experiments with 100 simulations, a 360-day horizon, trial window $T_f=30$ days, and 18 consumers, the paper reports how false positives and detection delay vary with similarity thresholds (0.1–0.9) and anomaly thresholds (10%–100% of consumers). Average detection delay is shown to be about 30–55 days for certain anomaly-threshold settings, while broader settings yield average delays up to ~180 days due to some undetected changes. With an evaluation window $T_w=60$ days, detection accuracy increases with higher similarity thresholds (reported roughly from ~40% up to ~95%), and decreases sharply as anomaly thresholds increase (reported from ~95% down to <10% in some settings). The results illustrate a clear tradeoff: lower thresholds increase sensitivity (shorter delays) but raise false positives, motivating the proposed self-adjustment mechanism.","Due to difficulty obtaining long-term real-world workload traces and performance datasets, the evaluation uses publicly available traces and benchmark results to mimic long-term cloud behavior rather than end-to-end measurements from multiple real providers over long periods. The paper also notes (in the signature definition section) that it focuses on a single QoS attribute (throughput) for simplicity, treating signatures as two-dimensional and leaving multi-attribute signatures for future work.","The CUSUM component is applied after re-aggregating trial data into a new signature, so detection performance depends heavily on trial-user participation and aggregation choices; robustness to sparse/biased trial samples is not analyzed. The approach relies on threshold tuning (similarity and anomaly-frequency) and reports large delays when changes are missed; there is no formal guarantee on false alarm rate (in-control ARL) or detection optimality typical in SPC. Serial dependence and seasonality in cloud performance time series are not explicitly modeled, which can distort similarity measures and CUSUM signaling. Implementation details (e.g., exact CUSUM parameterization, control-limit calibration, and reproducibility artifacts) are insufficient for straightforward replication.",The authors state they plan to conduct experiments on a larger scale to evaluate the impact of the approach in long-term selection. They also indicate extending signatures beyond a single QoS attribute (supporting more than two dimensions / multiple QoS metrics) in future work.,"Provide SPC-style calibration of CUSUM parameters and thresholds to achieve target in-control ARL/false-alarm rates, and study steady-state vs. zero-state performance. Extend the method to handle autocorrelation/seasonality (e.g., residual charts after time-series modeling) and missing/irregular trial observations. Develop a multivariate change-detection approach for multi-QoS signatures (e.g., MCUSUM/MEWMA or high-dimensional variants) and add diagnostic tools to localize which QoS components changed. Release a reference implementation and reproducible benchmark suite to enable fair comparison with alternative online change-point methods (e.g., GLR, Bayesian online change-point detection).",2007.11705v2,local_papers/arxiv/2007.11705v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:20:46Z
TRUE,Univariate|Other,Shewhart|Change-point|Other,Phase II,Food/agriculture|Manufacturing (general),TRUE,FALSE,FALSE,Simulation study|Other,ARL (Average Run Length)|Other,"Short-run setting assumes a finite number of inspections I over horizon H; they tabulate designs for I ∈ {10, 30, 50} with sample sizes n ∈ {1, 5, 7, 10, 15} (also mentions n up to 30 in performance tables). No minimum Phase I sample size is given; coefficients of variation and correlation are treated as known.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes two one-sided Shewhart-type control charts (denoted Sh−RZ and Sh+RZ) to monitor the ratio of two (possibly correlated) normal variables in short production runs with a finite monitoring horizon. The charting statistic is the ratio of sample means, and control limits are obtained from an approximation to the ratio distribution’s CDF/quantiles, parameterized by coefficients of variation, correlation, and sample size. Because traditional run length is not appropriate for finite-horizon production, the authors evaluate performance using truncated run length (TRL) and its expectation, truncated average run length (TARL), and provide ready-to-use tables of probability-based control limits. Numerical studies explore how n, the number of planned inspections I, coefficients of variation, and correlation affect control limits and TARL under small ratio shifts (decrease for Sh−RZ, increase for Sh+RZ). A food-industry muesli-mixture illustration (pumpkin vs. flaxseed weights) demonstrates detection of a 1% upward ratio shift using Sh+RZ.","The monitoring statistic at inspection i is the ratio of sample means: $\hat Z_i=\bar X_i/\bar Y_i=\frac{\sum_{j=1}^n X_{i,j}}{\sum_{j=1}^n Y_{i,j}}$. One-sided Shewhart limits are quantiles of the approximate ratio distribution: for Sh−RZ, $\mathrm{LCL}^- = F^{-1}_{\hat Z}(\alpha_0\mid n,\gamma_X,\gamma_Y,z_0,\rho_0)$ and $\mathrm{UCL}^- = +\infty$; for Sh+RZ, $\mathrm{LCL}^+=0$ and $\mathrm{UCL}^+=F^{-1}_{\hat Z}(1-\alpha_0\mid n,\gamma_X,\gamma_Y,z_0,\rho_0)$. Finite-horizon performance uses TARL: $\mathrm{TARL}_0=\frac{1-(1-\alpha)^{I+1}}{\alpha}$ and $\mathrm{TARL}_1=\frac{1-\beta^{I+1}}{1-\beta}$, with $\alpha$ and $\beta$ computed from $F_{\hat Z}$ at the chosen limit(s).","The paper tabulates probability-based control limits (LCL−, UCL+) designed to satisfy $\mathrm{TARL}_0=I$ for I ∈ {10,30,50} across correlations ρ0 ∈ {0, ±0.4, ±0.8}, CVs (γX,γY) ∈ {(0.01,0.01),(0.2,0.2),(0.01,0.2),(0.2,0.01)}, and sample sizes n ∈ {1,5,7,10,15}. Control limits tighten as n increases and widen as I increases; e.g., for γX=γY=0.01, ρ0=−0.8, I=10: (LCL−,UCL+)=(0.9615,1.0401) at n=1 vs (0.9899,1.0102) at n=15. TARL1 decreases with increasing n for small shifts (τ near 1), indicating faster detection with larger subgroup sizes; for I=10, γX=γY=0.01, ρ0=−0.8, τ=0.99 or 1.01, TARL1 drops from about 8.2 at n=1 to about 2.0–2.1 at n=15. In the food illustration with n=5, ρ0=0.8, H=16 hours and I=15 inspections, the Sh+RZ chart uses UCL+ = 1.01421 and signals an out-of-control point at sample 11 after a simulated 1% ratio increase.","The authors note that traditional run length measures are not suitable for finite-horizon (short-run) processes and therefore replace them with truncated run length/TARL. They also treat key in-control parameters (coefficients of variation, correlation, and nominal ratio) as known and constant for design/implementation, reflecting practical constraints in short runs.","The approach relies on an approximate CDF/quantile for the ratio of two (correlated) normal variables and on assumptions such as normality and proportionality of standard deviation to the mean (constant CV), which may be violated in practice and could affect false-alarm properties. Parameter estimation is not developed (no Phase I estimation strategy), yet short runs often lack reliable prior knowledge of γX, γY, and ρ0; the impact of estimation error on TARL is not assessed here. The evaluation is largely table-based and does not report implementation details (e.g., numerical inversion accuracy for the quantile), and no software is provided to reproduce tables.",The conclusion suggests extending the short-run ratio-monitoring framework to one-sided EWMA-type and CUSUM-type ratio charts for short production runs.,"Develop Phase I/empirical-Bayes or self-starting versions that estimate γX, γY, and ρ online and quantify the impact of estimation uncertainty on TARL and false alarms. Study robustness to non-normality, outliers, and autocorrelation common in short-run sampling, and provide diagnostic tools to separate shifts in numerator vs. denominator. Provide open-source software to compute limits for arbitrary (I,n,γX,γY,ρ0) and to simulate TARL under general shift scenarios, including changes in correlation and variance structure.",2010.01297v1,local_papers/arxiv/2010.01297v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:21:31Z
TRUE,Profile monitoring|Functional data analysis|Other,EWMA|Other,Both,Environmental monitoring,NA,NA,NA,Simulation study|Case study (real dataset)|Other,False alarm rate|Detection probability|Other,Not discussed,TRUE,None / Not applicable,Not provided,https://ypen.gov.gr/perivallon/poiotita-tis-atmosfairas/dedomena-metriseon-atmosfairikis-rypansis/|https://www.who.int/news-room/fact-sheets/detail/ambient-(outdoor)-air-quality-and-health,"The paper develops a functional-profile monitoring framework for detecting interpretable “shape shifts” by combining Fréchet means (as generalized averages in metric spaces) with a deformation model, specifically the Shape Invariant Model (SIM). Phase I estimates the typical in-control profile as a SIM-constrained Fréchet mean (via a regularized optimization problem and an alternating splitting–projection numerical scheme) and estimates empirical in-control distributions of deformation features via profile registration. Phase II proposes an EWMA-type monitoring procedure adapted to metric-space/functional data, termed Exponentially Weighted Fréchet Moving Average (EWFMA), with two stages: (i) monitor underlying-shape deviance after removing estimated SIM deformations, and (ii) monitor deformation-process deviance via a Fréchet-mean-based EWMA on modeled profiles; if out-of-control, diagnose via EWMA charts on individual SIM parameters (amplitude/phase/location). Control limits are set using empirical quantiles from the induced in-control distributions rather than parametric formulas. A real case study monitors daily air-pollutant concentration profiles in Athens and reports classification performance for detecting hazardous days under WHO threshold labeling, with overall accuracies roughly in the 82–96% range depending on pollutant.","Fréchet mean: $x_F=\arg\min_{z\in\mathcal M}\sum_{j=1}^n w_j d^2(z,x_j)$. SIM deformation model: $f_j(t)=\beta_j+\alpha_j f_0\big((t-\zeta_j)/\kappa_j\big)+\epsilon_j(t)$. Shape-monitoring EWMA: $\mathrm{De}^s_j=\lambda D^s_j+(1-\lambda)\mathrm{De}^s_{j-1}$ with $D^s_j=d^2(\hat f_{0,j},f_0)$, and deformation-monitoring EWMA: $\mathrm{De}^\theta_j=\lambda D^\theta_j+(1-\lambda)\mathrm{De}^\theta_{j-1}$ with $D^\theta_j=\|\hat f(\theta_j)-f_0\|_2^2$; EWFMA profile update is defined via a weighted Fréchet-mean variational problem in the model space.","In the Athens air-quality case study (train: 2001–2004; test: 2005–2007; Oct–Dec), overall (two-stage) monitoring accuracies reported are 87.22% (CO), 82.58% (NO2), 88.35% (O3), and 95.86% (SO2). The deformation-process stage shows high true-classification rates for CO/O3/SO2 (about 90–97%) with near-zero Type I error per the table, while NO2 is harder (about 85% deformation-stage accuracy). Within-class results in Table 2 indicate OOC detection is 100% for CO/O3/SO2 in the “both” (overall) scheme, and 78.26% for NO2; IC correct-classification ranges roughly from ~82.6% to ~94.8% (overall scheme). The authors also grid-search $\lambda$ values (0.01–0.05, 0.10–0.90, 0.95–0.99) and choose a per-pollutant value to reduce both type-I and type-II errors, but do not provide a single universal optimal $\lambda$.","The paper notes the method’s conservative tendency in some cases: it can yield higher Type II errors (misclassifying OOC as IC) while Type I errors are almost zero. It also acknowledges that reducing Type II errors may require incorporating additional covariates (e.g., temperature/humidity), modeling interdependencies among deformation features within/across pollutants, or using a different deformation model than SIM for pollutants with qualitatively different yet in-control patterns.","Control limits are based on empirical quantiles from the Phase I sample, but the paper does not thoroughly analyze sensitivity to Phase I sample size, representativeness, or concept drift (e.g., changing seasonal/long-term pollution patterns). The evaluation largely uses a single real dataset and a WHO-threshold-based labeling scheme; this may not align perfectly with “shape shift” notions and could bias reported classification results. Practical implementation details (e.g., computational cost, convergence diagnostics, and robustness to missing/irregular hourly measurements) are not fully developed, even though such issues are common in environmental sensor data.","The authors propose reducing Type II errors by adding more information (e.g., environmental covariates like temperature and humidity), more careful modeling of interdependencies in deformation features within and across pollutants, and considering alternative deformation models beyond SIM (e.g., landmark deformation models) when in-control profiles exhibit more diverse shapes.","Provide principled design guidance for selecting $\lambda$ and empirical-quantile control limits to target specified in-control false alarm rates (e.g., via bootstrap/Markov-chain approximations of run-length). Extend the framework to explicitly handle autocorrelated/seasonal functional profiles (common in environmental monitoring) and to missing/irregularly sampled curves. Release reference software (e.g., an R/Python implementation) and benchmark against functional PCA/GLR-type competitors using standardized functional monitoring datasets.",2010.02968v4,local_papers/arxiv/2010.02968v4.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:22:10Z
FALSE,Other,EWMA|Other,Phase II,Energy/utilities,NA,FALSE,NA,Simulation study,Other,Not discussed,TRUE,Python|Other,Not provided,NA,"The paper proposes a decentralized ADMM-based optimization framework for short-term generation maintenance and unit commitment in power systems that preserves privacy of inter-regional network flow information using differential privacy. Privacy is achieved by injecting carefully designed exponential noise into shared phase-angle messages; via the linear DC power flow relationship, this induces Laplace-distributed noise on line flows and yields formal ε-differential privacy guarantees for flows (including under mixing of information across iterations/regions). To improve convergence under noisy, iterative coordination—especially with mixed-integer (nonconvex) subproblems—the method adds an EWMA-based consensus mechanism for phase angles and flows. For stability and stopping, it introduces a CLT-based control-chart alarm rule applied to aggregated phase-angle residual quantities over a lookback window, used as a convergence criterion within each region. Experiments on IEEE 118-bus instances (8- and 12-region decompositions) show that the privacy-preserving decentralized solutions have objective gaps typically under about 5% relative to a centralized non-DP benchmark across many noise and convergence-limit settings, with an MPI/HPC implementation using Gurobi for regional MIQPs.","The core privacy mechanism perturbs each shared phase angle as $T(\theta_{b,t})=\theta_{b,t}+\alpha_{b,t}$ with $\alpha_{b,t}\sim \mathrm{Exp}\big(\omega/(|\Gamma_{(b_1b_2)}|\,\varepsilon)\big)$; then the induced flow message on tie-line $(b_1,b_2)$ is $M'(\theta_{b_1,t},\theta_{b_2,t})=\Gamma_{(b_1b_2)}(T(\theta_{b_1,t})-T(\theta_{b_2,t}))=f_{b_1b_2,t}+\psi_t$ with $\psi_t\sim \mathrm{Lap}(0,\omega/\varepsilon)$, giving $\varepsilon$-DP for flows. Consensus uses EWMA filtering of received noisy messages, e.g., $\tilde\theta_{b,k}=\eta\hat\theta_{b,k}+(1-\eta)\tilde\theta_{b,k-1}$ and similarly for $\tilde f$, with averaged consensus terms $\bar\theta,\bar f$ used in ADMM multiplier updates. The control-chart criterion aggregates residuals $\Theta=\sum_{k=1}^{S_w}(\theta_{u,k,r}+\alpha_{u,k,r}-\theta_{u,k,r'}-\alpha_{u,k,r'})$ and triggers alarms when $|\Theta|>\sqrt{2\tilde\omega^2/S_w}$.","On IEEE 118-bus tests with 8- and 12-region decompositions, the decentralized DP approach produced solution quality (relative objective gap to a centralized non-DP benchmark) typically under about 5% in most examined settings of noise scale and convergence limit (surface plots for multiple CL values). The flow perturbation magnitude (2-norm between true and DP flow values at convergence) increased monotonically with the noise scale, indicating increased privacy protection as noise increases. With convergence limit $CL=0.1$, lookback sizes of 10 vs 20 showed that smaller lookback can increase variance in optimality gap (fewer iterations/messages), while larger lookback tends to stabilize the gap. Reported mean runtimes (including nonconverged cases capped by a time limit) ranged roughly from ~1,000s at low noise to several thousand seconds at higher noise (e.g., for 8 regions: mean ~1030s at scale 1.5e-2 up to ~9218s at scale 30e-2; for 12 regions: mean ~664s at 1.5e-2 up to ~7052s at 15e-2).","The authors note sensitivity of ADMM performance to the penalty parameter $\rho$, observing non-monotone behavior where optimality gap can decrease with increasing noise for some regimes, which they attribute to $\rho$ tuning. They state that methods for automatically tuning $\rho$ exist mainly for convex problems and that applicability to mixed-integer (nonconvex) formulations remains unexplored; hence $\rho$ was chosen empirically via repeated trials. They also cap optimality gaps for experiments that did not converge within a fixed runtime limit, implying convergence is not always achieved under high noise/tight limits.","The control-chart component is used as a convergence/stopping heuristic rather than a classical SPC monitoring design with in-control/out-of-control ARL calibration, so false-alarm behavior and statistical guarantees of the stopping rule are not characterized in SPC terms. The privacy accounting across many ADMM iterations is not summarized as an overall privacy budget consumption over time (e.g., composition of ε over iterations), which can matter in iterative DP protocols. The method is tied to the DC power flow linear relationship (phase angles to flows); extending to AC-OPF/nonlinear settings may be nontrivial. Empirical evaluation focuses on IEEE 118-bus decompositions; broader benchmarking across different grids, decompositions, and parameter settings (including robustness to communication delays/failures) is not demonstrated.",They state that automatically adjusting/tuning the ADMM penalty parameter $\rho$ during runtime to improve mixed-integer decentralized performance is a key component of their future work.,"Provide a formal privacy budget analysis across iterations/regions using composition theorems and report effective $(\varepsilon,\delta)$ guarantees for full runs. Calibrate the CLT-based control-chart stopping rule with explicit error/false-stopping probabilities (or ARL-like metrics) and study its sensitivity to dependence across iterations introduced by EWMA/ADMM. Extend the approach to nonlinear AC power flow/OPF settings and evaluate how privacy noise propagates through nonlinearities. Release an open-source reference implementation (e.g., mpi4py + Gurobi model builder) with reproducible experiment scripts and standardized benchmarks across multiple IEEE cases.",2010.09099v1,local_papers/arxiv/2010.09099v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:22:50Z
TRUE,Multivariate|Other,CUSUM|EWMA|Hotelling T-squared|Other,Both,Transportation/logistics|Network/cybersecurity|Theoretical/simulation only,FALSE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Conditional expected delay|Other,Phase I: generate 2500 in-control temporal graphs (each graph N=100 nodes) and use a burn-in of 1000 time points; for the real-data example Phase I uses 2018 with z=7 (358 observations). Phase II in the real-data example uses 2019–2020 (486 observations after the z=7 windowing).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an online network surveillance framework that combines temporal exponential random graph models (TERGM) with multivariate statistical process control to detect anomalous changes in evolving networks. The monitored signals are low-dimensional network characteristics derived either from TERGM parameter estimates $\hat\theta_t$ or averaged network statistics $\hat s_t$, enabling real-time monitoring despite the high dimensionality of adjacency matrices. Two multivariate control charts are calibrated and compared: Crosier’s MCUSUM and the MEWMA chart, both based on Mahalanobis-distance-type statistics, to detect shifts in the mean of the monitored characteristic vector. Because temporal dependence and sliding windows induce autocorrelation that invalidates standard normal-theory limits, upper control limits are obtained via Monte Carlo simulation targeting specified in-control ARL values. Performance is assessed by ARL and conditional expected detection delay under simulated anomalies, and the method is demonstrated on daily U.S. flight networks where the charts flag changes associated with holiday travel peaks and the COVID-19 disruption.","Networks are modeled via ERGM/TERGM: $P_\theta(Y)=\exp(\theta^\top s(Y))/c(\theta)$ and $P_\theta(Y_t\mid Y_{t-1{:}t-v})=\exp(\theta^\top s(Y_t,Y_{t-1{:}t-v}))/c(\theta,Y_{t-1{:}t-v})$. Monitoring uses a $p$-vector of estimated characteristics $\hat c_t$ (either $\hat\theta_t$ or $\hat s_t=\frac1z\sum_{n=0}^{z-1}s(Y_{t-n})$), with Mahalanobis-type statistic $D^{(1)}_t=(\hat c_t-c_0)^\top\Sigma^{-1}(\hat c_t-c_0)$. Crosier MCUSUM uses $C_t=\{(r_{t-1}+\hat c_t-c_0)^\top\Sigma^{-1}(r_{t-1}+\hat c_t-c_0)\}^{1/2}$ with update $r_t=0$ if $C_t\le k$ else $(r_{t-1}+\hat c_t-c_0)(1-k/C_t)$, and charting $D^{(2)}_t=r_t^\top\Sigma^{-1}r_t$. MEWMA uses $\ell_t=\lambda(\hat c_t-c_0)+(1-\lambda)\ell_{t-1}$ with $D^{(3)}_t=\ell_t^\top\Sigma_{\ell_t}^{-1}\ell_t$ and $\Sigma_{\ell_t}=\frac{\lambda}{2-\lambda}(1-(1-\lambda)^{2t})\Sigma$.","Upper control limits (UCLs) are calibrated by Monte Carlo simulation for ARL0 ∈ {50, 75, 100} and window sizes z ∈ {7, 14} for both MEWMA and MCUSUM, and tables of UCLs are provided for multiple choices of $\lambda$ (MEWMA) and $k$ (MCUSUM). In Phase II simulations (250 replications) with change time $\tau=101$ and target ARL0=50, conditional expected detection delay (CED) generally decreases as anomaly intensity increases; MEWMA often yields smaller CED than MCUSUM, though both are competitive with well-chosen $\lambda$ or $k$. For “point-change” anomalies that increase mutual edges (Type C), charts built on TERGM parameter estimates $\hat\theta_t$ can detect moderate events much faster than charts based on averaged statistics $\hat s_t$ (e.g., for $\zeta=0.01$ with z=7, MEWMA minimum CED 3.16 using $\hat\theta_t$ vs 17.83 using $\hat s_t$). In the U.S. flight-network case study (Phase I: 2018; Phase II: 2019–2020; z=7), both MCUSUM (k=1.5) and MEWMA (λ=0.9) signal anomalous periods including summer 2019 travel peaks and a sustained shift beginning in late March 2020 associated with COVID-19-related travel disruption.","The authors note that TERGM term selection is difficult and that TERGM is not suitable for very large networks. They also state that temporal-dependence statistics depend on the chosen temporal lag and the time-window size, so reliable modeling relies heavily on domain knowledge. They further point out that multivariate control charts work best when the number of monitored variables is not too large (typically up to about 10).","The approach depends on repeated TERGM fitting in (near) real time; even with MPLE, this may be computationally burdensome for many practical networks or for richer model specifications, and the paper does not provide runtime/complexity benchmarks. Control limits are calibrated by Monte Carlo under specific simulation regimes and parameter settings, so transferability across network sizes, term sets, and dependence structures may be limited without substantial re-calibration. The monitoring statistic is global (Mahalanobis-distance based), so once a signal occurs, additional diagnostic methods are needed to localize which terms, nodes, or subgraphs drove the change; this is acknowledged implicitly but not developed as a full diagnostic workflow.","They suggest extending the approach by implementing STERGM to separate formation and dissolution processes for clearer interpretation of changes. They call for research on when it is reliable to base monitoring on averaged network statistics $\hat s_t$ versus TERGM parameter estimates $\hat\theta_t$, and for considering alternative estimators for $\hat s_t$. They also mention extending the monitoring to allow the covariance matrix $\Sigma$ to differ between in-control and out-of-control states and exploring adaptive control charts to improve anomaly detection performance.","Developing formal diagnostic procedures (e.g., contribution plots, term-wise decomposition, or node/edge attribution) tailored to TERGM-based monitoring would improve interpretability after a signal. Robust/nonparametric variants could reduce sensitivity to misspecified TERGM terms, heavy-tailed estimation errors, or time-varying variance, and would lessen dependence on Monte Carlo calibration. Providing open-source software and standardized simulation benchmarks (varying N, lag v, window z, term sets, and change types) would greatly aid reproducibility and practical adoption.",2010.09398v2,local_papers/arxiv/2010.09398v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:23:47Z
TRUE,Multivariate|Nonparametric|Other,CUSUM|Shewhart|GLR (Generalized Likelihood Ratio)|Machine learning-based|Other,Both,Environmental monitoring|Other,FALSE,TRUE,TRUE,Simulation study|Other,ARL (Average Run Length)|False alarm rate|Other,"Not discussed (no fixed Phase I sample size in subgroups; Phase I uses a panel of N=21 series over a long historical window 1947–2013, selecting IC pools P1 (12 stations) and P2 (6 stations) by clustering, and uses K=200 nearest neighbours for estimating IC patterns).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a comprehensive nonparametric SPC scheme to monitor a panel of serially correlated time series with time-varying mean/variance, strong noise, no clearly in-control (IC) period, and potentially many missing values. Phase I selects stable IC subsets from the panel via robust criteria (median-based reference and clustering) and estimates IC longitudinal mean/variance patterns using temporal smoothing and KNN to cope with missingness. Phase II standardizes each series by the estimated IC patterns and monitors it mainly with a two-sided CUSUM chart whose control limits are calibrated nonparametrically via (moving) block bootstrap to preserve autocorrelation. After an alarm, the method adds support vector regression/classification to estimate shift magnitude and classify shift form (e.g., jump, trend/drift, oscillatory), trained on bootstrap-generated synthetic shifts. The approach is demonstrated on daily sunspot-number station data, detecting known historical deviations and many smaller persistent shifts, and is positioned as a robust panel-monitoring extension of dynamic screening/SPC methods (e.g., Qiu & Xiang).","Panel model uses a multiplicative structure with common signal removal by dividing by the panel median: $\hat\eta_e(i,t)=X(i,t)/\operatorname{med}_{1\le i\le N}X(i,t)$, and optional level removal via long-window moving average filtering. IC mean/variance patterns are estimated over a smoothing window and IC subset $P_2$: $\hat\mu_0(t)$ and $\hat\sigma_0^2(t)$ (Eq. 5), then residuals are standardized as $\hat\epsilon(i,t)=(\hat\eta(i,t)-\hat\mu_0(t))/\hat\sigma_0(t)$ (Eq. 6/11). Monitoring uses a two-sided CUSUM: $C_j^+=\max\{0,C_{j-1}^+ + \hat\epsilon(t)-k\}$ and $C_j^-=\min\{0,C_{j-1}^- + \hat\epsilon(t)+k\}$ with signaling when $C_j^+>h^+$ or $C_j^-<h^-$. Control limits $h$ are tuned to a target $ARL_0$ via bootstrap/block-bootstrap resampling; after an alarm, SVMs predict shift size $\hat\delta=f(V_\tau)$ from the last $m$ residuals.","In the sunspot application, the authors set a target in-control average run length $ARL_0=200$ and use $k=\delta_{\min}/2$ with $\delta_{\min}=1.5$, yielding two-sided CUSUM control limits at approximately $\pm 8.5$. For SVM post-signal diagnostics trained on 63,000 simulated instances (80% train / 20% validation), the SVR achieves MAPE ≈ 26 and NRMSE ≈ 0.26 on the validation set, while the SVC achieves about 86% classification accuracy across three shift-form classes (jump/trend/oscillating). A simulation comparison shows block-bootstrap calibration better matches the intended $ARL_0$ than a parametric ARMA+distributional fit under complex autocorrelation (parametric $ARL_0$ can be much smaller/larger than target depending on correlation sign), while both approaches are similar when the true model is simple ARMA(1,1). Case-study figures illustrate detection of known prominent deviations in specific observing stations and many smaller persistent shifts.","They note that traditional control charts do not provide the nature of the shift, motivating their SVM add-on, and that classification results “can be rich” and require careful analysis to interpret (e.g., linking trends to instrument ageing or oscillations to alternating observers). They also state SVM accuracy could be improved by simulating a wider range of deviation forms (e.g., different power-law functions and sinusoids with varying frequencies). They mention periodic recalibration may be needed if the panel/processes evolve over time.","The approach relies on the panel median and clustering to define IC pools; if a large fraction of series are simultaneously affected by common-mode deviations, the median-based reference and IC selection may be biased, reducing detection power. The SVM diagnostic layer is trained on synthetic shifts superposed on bootstrap-resampled IC residuals; real shift forms outside the simulated library (or different missingness mechanisms) may degrade magnitude/form estimation. Computational cost and parameter sensitivity (block length, smoothing windows, clustering choices, KNN K, SVM hyperparameters) could be substantial in other domains, yet no software implementation or runtime study is provided. Missing data handling (reset or propagation rules; simple interpolation for SVM inputs) assumes benign missingness and may be fragile under informative/modeled missingness.","They plan to expand the monitoring from sunspot counts $N_s$ to other components ($N_g$ and the composite $N_g+10N_s$) to build a fully automated online monitoring system that can send appropriate alerts to stations. They also state the methodology will be extended to monitor process variances (not just mean/long-term bias). They suggest further investigations to interpret detected shift forms (e.g., associating trends with instrument ageing or oscillations with observer alternation).","Developing principled methods for choosing block length and smoothing windows (e.g., data-driven selection with sensitivity/robustness analysis) would improve transferability beyond the sunspot setting. Extending the bootstrap-calibrated charts to provide steady-state/conditional performance metrics under frequent re-starts and missingness would strengthen guarantees for long-running monitoring. A fully distribution-free multistream/panel false-discovery control layer (e.g., controlling FDR across many series) could complement per-series CUSUM signaling. Releasing reproducible software (e.g., an R/Python package) and benchmarking against alternative robust panel monitoring methods would increase practical adoption and validate performance across domains.",2010.11826v1,local_papers/arxiv/2010.11826v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:24:33Z
TRUE,Univariate|Other,Hotelling T-squared|Machine learning-based|Change-point,Both,Healthcare/medical|Transportation/logistics|Theoretical/simulation only,NA,FALSE,FALSE,Simulation study|Other,ARL (Average Run Length)|False alarm rate|Other,Phase I uses 2500 in-control samples to estimate $Q_0$ (via mean $\bar Q$) and $\Sigma_0$ (via sample covariance matrix $S$). Daily quantiles are computed from 10 to 100 simulated accidents per day; Phase II monitoring period is 100 days.,TRUE,Python,Not provided,NA,"The paper discusses statistical learning for change point and anomaly detection in dynamic graph (network) data, emphasizing a hybrid approach that combines statistical process control (SPC) with deep learning. It proposes an enhanced network monitoring procedure where a control chart flags potential deviations and, conditional on a signal, a graph convolutional network (GCN) classifies the network state to help explain the cause of the anomaly. The SPC component is a control chart for quantile function values applied to daily ambulance response-time quantiles (0.8 and 0.95), using a chi-square-based threshold calibrated to a target in-control ARL. A simulation study on a road-network/ambulance-service setting generates in-control and multiple out-of-control scenarios (manpower shortage, construction works, traffic jams), then evaluates chart signaling and GCN classification performance. The work advances SPC for network monitoring by explicitly coupling detection (control charts) with automated post-signal diagnosis (graph deep learning).","The control statistic is a quadratic form on estimated daily quantiles: $a_t=(\hat Q_t-Q_0)^\top\Sigma_0^{-1}(\hat Q_t-Q_0)$, where $\hat Q_t=(\hat Q_{0.8,t},\hat Q_{0.95,t})^\top$ (so $c=2$). In Phase I, $Q_0$ is estimated by the sample mean $\bar Q$ and $\Sigma_0$ by the sample covariance matrix $S$ from 2500 in-control samples. For large samples, $a_t\sim\chi^2_c$ in control, so the control limit is set to $\chi^2_{\alpha}(c)$ with $\alpha=1/\text{ARL}$ (here ARL=1000).","Phase I calibration uses ARL = 1000, giving control limit $\chi^2_{0.001}(2)=13.82$. The GCN is trained on 2500 labeled graphs (balanced across 4 classes) with an 800-graph validation set; early stopping selects epoch 102 with weighted F-scores of 93% (training) and 87% (validation). In Phase II over 100 simulated days, the control chart shows one false signal in the final 10 in-control days (slightly above the limit). On the Phase II-derived test set, the GCN achieves an overall weighted F-score of 83%, performing very well on classes 0, 2, and 3 but less well on class 1 (manpower shortage).",None stated.,"The control chart relies on an asymptotic $\chi^2$ approximation for $a_t$ (“for sufficiently large number of samples”), but daily quantiles are computed from as few as 10 accidents, which may undermine the nominal false-alarm control. The study is simulation-based with a simplified road network (18 nodes, 25 edges), so generalizability to real, larger, noisy, and evolving networks (with changing topology and data quality issues) is unclear. The hybrid procedure’s operational aspects (how often to run the GCN, latency/compute constraints, and how misclassification affects decisions) are not fully analyzed in SPC terms (e.g., impact on run-length or downstream actions).","The authors suggest expanding joint applications of machine learning with classical statistical tools for network monitoring and highlight open questions such as how to represent graph data and convolve information, which approach to use in which case, and how to measure performance. They also discuss (but caution against) the idea of omitting control charts and using GCN-like models for the entire monitoring procedure due to complexity and training-time constraints.","Develop finite-sample or bootstrap-based control limits for quantile-chart statistics to maintain in-control false-alarm properties when the number of daily events is small or variable. Extend the framework to autocorrelated/seasonal response-time data and evolving graph topology, including robust handling of missing/irregularly sampled network attributes. Provide an end-to-end SPC evaluation of the full hybrid system (detection + classification), including economic/utility-based performance, misclassification costs, and guidance on when to trigger diagnostic models.",2011.06080v1,local_papers/arxiv/2011.06080v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:25:06Z
TRUE,Profile monitoring|Multivariate,Hotelling T-squared|Other,Both,Semiconductor/electronics|Pharmaceutical|Manufacturing (general),TRUE,FALSE,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length)|False alarm rate|Detection probability|Other,Not discussed (examples use Phase I with 100 profiles of size n=5 in simulation; semiconductor Phase I uses 18 profiles with n=11; pharmaceutical dissolution guidance notes sample size 12 tablets per batch).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a profile monitoring approach for simple linear profiles by estimating the intercept and slope via the maximum entropy principle (ME) and comparing it to standard linear regression (LR) estimation. For each profile, the estimated coefficient vector (intercept, slope) is converted to a scalar monitoring statistic using a Hotelling $T^2$ chart, with control limits set either by an F-distribution approximation or by an empirical $(1-\alpha)$ quantile of Phase I $T^2$ values. Performance is evaluated via Monte Carlo simulation using ARL$_0$, ARL$_1$, and Type II error $\beta$ over a range of small shifts affecting the intercept, slope, or both. Two real case studies are provided: semiconductor DRIE trench profile monitoring (Phase I/II) and a pharmaceutical dissolution profile example; the ME-based approach is reported to better highlight differences in the pharmaceutical data where LR is less effective.","Profiles follow $\tilde{y}_j = a + b\tilde{x} + \tilde{\varepsilon}_j$ with normal errors. ME-based coefficient estimates use expectations under the ME-estimated joint density: $\hat b_{j,ME}=\frac{E_j[(X-E X)(Y-E_jY)]}{E[(X-E X)^2]}$ and $\hat a_{j,ME}=E_j(Y)-\hat b_{j,ME}E(X)$ (analogous LR formulas use sample moments). Monitoring uses Hotelling statistics $T^2_{j,ME}=(\mathbf{m}_j-\bar{\mathbf{m}})'S_m^{-1}(\mathbf{m}_j-\bar{\mathbf{m}})$ and $T^2_{j,LR}=(\mathbf{l}_j-\bar{\mathbf{l}})'S_l^{-1}(\mathbf{l}_j-\bar{\mathbf{l}})$, with UCL either F-based or empirical quantiles $q_{ME},q_{LR}$.","In simulation with 100 Phase I profiles of size $n=5$ from $Y=2+3X+\varepsilon$ (normal, var 0.1) and fixed $X=(2,2.2,2.4,2.1,2.7)$, reported control limits were $UCL_F=6.303865$, $UCL_{ME}=5.591411$, and $UCL_{LR}=5.80042$; estimated in-control ARLs included about 17 (F-based) versus about 13 (ME-quantile) and about 15 (LR-quantile). For the semiconductor DRIE data, reported Phase I limits were $UCL_F=8.150644$ and empirical limits $UCL_{ME}=UCL_{LR}=4.857291$; in Phase II, sample 14 has $T^2\approx 5.77$ and is signaled by the empirical ME/LR limit but not by the F-based limit. In the pharmaceutical dissolution example, with limits at confidence 0.9973 ($UCL_F=26.97728$, $UCL_{ME}=6.812622$, $UCL_{LR}=5.883782$), the ME-based $T^2$ values show clear separation across batches and are described as better matching known similarity/dissimilarity statements than LR.",None stated.,"The approach relies on normal-error assumptions for the profile model and uses Hotelling $T^2$ with covariance estimated from Phase I, so performance may degrade under non-normality, outliers, or serial dependence between profiles. The maximum entropy estimation requires choosing moment/constraint sets; results may be sensitive to these choices and computational details, but systematic robustness/sensitivity analysis is not presented. Software/implementation details (solver, numerical integration, convergence criteria) are not provided, making reproducibility and computational burden hard to assess.",None stated.,"Evaluate robustness under non-normal and heavy-tailed errors, outliers, and autocorrelated profiles; and consider robust covariance estimation for Phase I. Develop guidance for selecting ME constraints and provide sensitivity studies and diagnostics for constraint adequacy. Provide open-source software and scalable algorithms (e.g., for higher-order/multiple linear profiles) and assess performance versus alternative profile monitoring methods (e.g., MEWMA/MCUSUM on coefficients or GLR change-point approaches).",2012.14289v4,local_papers/arxiv/2012.14289v4.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:25:37Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Both,Manufacturing (general)|Environmental monitoring|Other,FALSE,NA,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length)|Other,Not discussed (examples assume Phase I consists of the first 20 samples in the manufacturing dataset and the first 70 samples in the flood dataset; simulation uses repeated ARL estimation 1000 times).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a multivariate dependence-aware control chart by estimating an unknown joint distribution using maximum copula entropy, so that the dependence structure in process variables is preserved without assuming multivariate normality. A maximum-entropy copula density is fitted under constraints that enforce valid copula properties and match dependence measures (Spearman’s rho and Blest rank correlations), and it is combined with maximum-entropy marginal densities via Sklar’s theorem to obtain the joint density. Monitoring is performed using the Hotelling $T^2$ statistic, but the upper control limit (UCL) is obtained numerically from the fitted joint density rather than via the usual $F$ approximation (which relies on normality). Performance is evaluated using average run length (ARL) calculations under various mean shifts, showing sensitivity to small shifts, and the approach is illustrated on two real datasets (a manufacturing quality dataset and flood-event duration/volume data). Results are compared conceptually and in examples against traditional Fisher-distribution-based $T^2$ limits, with the copula-entropy approach yielding more appropriate, dependence-sensitive UCLs.","The maximum-copula-entropy copula density is of exponential-family form: $c(u,v)=\exp\{-1-\lambda_0-\sum_{i=1}^r\lambda_i(u^i+v^i)-\lambda_{r+1}uv-\lambda_{r+2}(u^2v+uv^2)-\lambda_{r+4}u^2v^2\}$ on $[0,1]^2$, where Lagrange multipliers are set to satisfy copula-validity constraints and dependence-moment constraints (e.g., matching Spearman’s $\rho$ and Blest measures). The joint density is constructed via Sklar: $f_{X,Y}(x,y)=c(F_X(x),F_Y(y))f_X(x)f_Y(y)$, with marginals $f_X(x)=\exp(-\lambda_0-\sum_i\lambda_i g_i(x))$ from univariate maximum entropy. Monitoring uses $T^2=(\mathbf{X}-\boldsymbol\mu)'\Sigma^{-1}(\mathbf{X}-\boldsymbol\mu)$ and the UCL solves $P(T^2\le \text{UCL})\ge 1-\alpha$ by integrating $f_{X,Y}$ over the $T^2$ ellipsoidal region.","A simulation study reports ARL values (recomputed 1000 times) across five dependence settings and multiple mean/shift scenarios; with nominal $\alpha\approx0.05$, reported ARL0 values are typically above the baseline 20 (e.g., for $\mu_X=2,\mu_Y=1$, ARL0 ranges about 22.4–31.4 depending on dependence group). For shift detection, ARL1 decreases sharply with larger mean shifts; for $\mu_X=2,\mu_Y=1$ and dependence group 1, ARL1 is about 4.171 when $(\delta_X,\delta_Y)=(1,1)$. In the first real-data example (manufacturing), the 95% UCL is 3.03649 using the copula-based density versus 7.716048 using a dependency-ignoring maximum-entropy joint density, indicating the latter is less sensitive; an iterative Phase I cleaning yields a final UCL of 2.87983. In the flood example, an initial UCL of 6.85875 is refined to 6.89478 after removing four Phase I outliers; the paper notes samples 81 and 84 are flagged in stage 1 but fall in-control in stage 2.",None stated.,"The method requires solving a potentially high-dimensional nonlinear system for Lagrange multipliers and then numerically integrating the fitted joint density to obtain the UCL; computational burden and numerical stability are not systematically analyzed. The approach is demonstrated primarily for the bivariate case, and scalability to higher-dimensional monitoring (p>2) is not addressed. Dependence is enforced via a small set of rank-based measures (Spearman’s rho and selected Blest measures), which may not uniquely identify the copula or capture tail dependence relevant to rare events. The paper does not provide implementation details (software, algorithms, convergence checks), which limits reproducibility and practical adoption.",None stated.,"Extend the maximum-copula-entropy construction and resulting control limits to higher-dimensional multivariate monitoring (p>2), including efficient computation of UCLs in higher dimensions. Develop robust and self-starting variants that update dependence and marginal estimates online and handle autocorrelation common in process data. Provide open-source software and benchmark comparisons against modern nonparametric and copula-based SPC charts (e.g., kernel/empirical copula approaches) under diverse dependence (including tail dependence). Study theoretical properties (in-control false-alarm control, consistency of estimated UCLs) and sensitivity to constraint choice and dependence-measure estimation error.",2012.14759v5,local_papers/arxiv/2012.14759v5.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:26:26Z
TRUE,Univariate,EWMA,Both,Semiconductor/electronics|Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,FALSE,Simulation study|Integral equation|Approximation methods|Other,ARL (Average Run Length)|MRL (Median Run Length)|Detection probability|False alarm rate|Other,Phase I uses m subgroups of size n (example throughout: n=5; m varies). Authors recommend m ≥ 50 to detect small variance changes; suggest m ≥ 100 based on CDF profiles for practical performance.,TRUE,R,Package registry (CRAN/PyPI)|Not provided,https://cran.r-project.org/|https://kassandra.hsu-hh.de/apps/knoth/s2ewmaP/,"The paper studies EWMA control charts for monitoring process variance using the sample variance S^2 when the in-control variance σ0^2 must be estimated from Phase I data, focusing on false-alarm behavior under parameter uncertainty. It argues that calibrating charts by the unconditional in-control ARL can be misleading (heavy-tailed unconditional run-length distributions can inflate ARL while increasing early false alarms), while guaranteeing a minimum conditional ARL is conservative and difficult to communicate. The author proposes calibrating by controlling the unconditional probability of a false alarm within a planned monitoring horizon \(\bar{l}\), i.e., targeting an unconditional run-length quantile via \(P(L\le \bar{l})=\alpha\). Numerical algorithms are developed to adjust one-sided (upper) and two-sided EWMA S^2 limits for normally distributed subgroup data, using accurate run-length distribution computation based on integral recursions and a collocation approach that outperforms Markov-chain discretization in accuracy/speed. Extensive numerical studies examine how the EWMA smoothing constant \(\lambda\) and Phase I size m affect required limit widening, false-alarm profiles, and out-of-control detection; the paper recommends larger Phase I samples (about m≥50) for small shifts and suggests \(\lambda\in\{0.1,0.2\}\) as practical choices.","Subgroup sample variance: \(S_i^2=\frac{1}{n-1}\sum_{j=1}^n (X_{ij}-\bar X_i)^2\). EWMA recursion: \(Z_i=(1-\lambda)Z_{i-1}+\lambda S_i^2\) with \(Z_0=z_0=\sigma_0^2\) (estimated in practice). Stopping times: upper \(L_{\text{upper}}=\min\{i\ge1: Z_i>c_u\}\); two-sided \(L_{\text{two}}=\min\{i\ge1: Z_i>c_u\ \text{or}\ Z_i<c_l\}\). Phase I estimator: pooled variance \(\hat\sigma_0^2=\frac{1}{m}\sum_{i=1}^m s_i^2\). Proposed calibration targets an unconditional RL quantile: \(P(L\le \bar l)=\alpha\). Unconditional survival function integrates over Phase I uncertainty: \(p_{l,\text{unc}}(z;\sigma^2,c_u)=\int_0^{\infty} f_{\hat\sigma_0^2}(s^2)\, p_l(z;\sigma^2/s^2,c_u)\,ds^2\), with conditional recursion \(p_1(z_0)=\int_{(1-\lambda)z_0}^{c_u}\delta(z_0,z)dz\), \(p_l(z_0)=\int_{(1-\lambda)z_0}^{c_u} p_{l-1}(z)\delta(z_0,z)dz\).","The paper shows that unconditional IC run-length distributions can be heavy-tailed for small Phase I sizes (e.g., for an upper EWMA S^2 with n=5, λ=0.1, unadjusted: small m increases early false-alarm probabilities, and tails can be extreme; e.g., probabilities like \(P(L>10^5)\) are non-negligible for m=10). Under the proposed design with \(\bar l=1000\) and \(\alpha=0.25\), control limits are widened relative to the known-parameter case; the widening becomes small for moderate/large m (from about m≈50 onward, limits are close to the known-σ0^2 limits; overall widening is typically on the order of a few to ~10% depending on λ and m). For m=50 and n=5, λ=0.1, upper-chart example: known-σ0^2 IC ARL is about 3461, while after calibration to \(P(L\le 1000)=0.25\) the unconditional IC ARL becomes very large (reported \(>8\times 10^5\)); OOC ARLs also increase (e.g., σ=1.2: 38.4 → 84.8; σ=1.5: 8.05 → 9.52). Two-sided examples with m=50 show more moderate IC ARL inflation (e.g., λ=0.1: IC ARL 3453 → 6803) but OOC ARLs for small shifts can roughly double/triple compared with known-parameter calibration. The collocation-based RL computation achieves much higher accuracy than Markov-chain approximation at comparable/less computation (e.g., N=50 collocation ≈ higher accuracy than N=500 Markov chain; about 1s vs 22s in a reported configuration).","All theoretical/numerical developments assume normally distributed subgroup data and use the pooled variance estimator \(\hat\sigma_0^2\) in the calculations; the author notes that alternative (e.g., robust) estimators would make computations more complicated. For the two-sided chart, determining both limits (c_l, c_u) is more complex and time-consuming than the upper one-sided case; the fully ‘unbiased’ two-sided adjustment needs considerably more computing time than the proposed ‘quasi-unbiased’ alternative. The approach widens limits to control false alarms, which the paper notes necessarily deteriorates out-of-control detection, especially for small shifts when Phase I size is small.","The method is developed for subgrouped variance monitoring via S^2; it does not address individual-observation dispersion charts or broader dispersion models (e.g., nonnormal/gamma/lognormal) beyond normality. It assumes independence within and across subgroups (no treatment of autocorrelation), which is common in metrology/time-ordered data and can materially affect false-alarm and RL properties. The proposed calibration requires choosing \(\alpha\) and horizon \(\bar l\); guidance is heuristic and may be application-dependent, and the resulting unconditional IC ARL can become extremely large (potentially undesirable for practitioners expecting ARL-based interpretability). Code is referenced via an R package and supplementary material, but the paper does not provide a dedicated reproducible repository link for the paper’s experiments/figures beyond pointing to CRAN and mentioning supplementary material.",None stated.,"Extend the quantile-based false-alarm calibration framework to nonnormal dispersion monitoring (e.g., gamma/lognormal) and to robust/contaminated Phase I settings with explicit robustness guarantees. Incorporate autocorrelation (e.g., ARMA residual-based monitoring or state-space models) and evaluate how estimation uncertainty plus dependence jointly affects unconditional RL quantiles. Provide a principled strategy for selecting \((\bar l,\alpha)\) tied to economic design or service-level constraints, and develop self-starting/adaptive re-estimation variants (periodic updating) with theoretical guarantees for the proposed quantile criterion. Add broader comparative benchmarks against bootstrap-based conditional-guarantee methods for EWMA variance charts (runtime/accuracy tradeoffs) and publish fully reproducible code/workflows for all figures and tables.",2101.04011v1,local_papers/arxiv/2101.04011v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:27:30Z
TRUE,Univariate|Other,Shewhart,Both,Healthcare/medical|Theoretical/simulation only,FALSE,FALSE,NA,Exact distribution theory|Simulation study,False alarm rate|Other,"Not discussed (the paper studies small-sample behavior via simulations, e.g., (n1,n2)=(1,1),(2,3),(5,5),(10,10), but does not give Phase I sample-size recommendations).",TRUE,R,Not provided,https://CRAN.R-project.org/package=hypergeo|https://support.minitab.com/en-us/minitab/20/|https://CRAN.R-project.org/package=rQCC,"The paper revisits geometric-distribution-based g and h control charts for monitoring the number of conforming cases between nonconformities, emphasizing that the process parameter p is typically unknown and must be estimated. It shows that an estimator commonly used in the quality-engineering literature and software as the “MVU” estimator is not actually unbiased, and it derives the correct minimum-variance unbiased (MVU) estimator for p using Rao–Blackwellization with the complete sufficient statistic. The note derives exact first and second moments (and hence biases/MSE behavior) of competing estimators (ML, the commonly-used biased “MVU”, and the correct MVU) in terms of Gauss hypergeometric functions, and confirms findings with Monte Carlo simulations. It also addresses a practical restriction of conventional g/h charts—balanced subgroup sizes—by providing construction formulas for unbalanced samples (varying subgroup sizes) and gives ML-based and MVU-based control limits using appropriate estimators for the mean and variance parameters of the geometric model. The main practical implication is that practitioners should avoid the incorrectly-labeled MVU estimator and use the correct MVU estimator for p, while estimating the center line via the MVU/ML estimator of the mean (the grand average) and using corresponding variance estimators when forming limits under unequal subgroup sizes.","Shifted geometric model: $P(Y=y)=p(1-p)^{y-a}$ for $y=a,a+1,\ldots$. ML estimator: $\hat p_{\mathrm{ml}}=1/(\bar Y-a+1)$. Correct MVU estimator: $\hat p_{\mathrm{mvu}}=(n-1)/(\sum_{i=1}^n Y_i-na+n-1)=((n-1)/n)/(\bar Y-a+1-1/n)$; for unequal subgroup sizes with total $N=\sum_i n_i$ and grand mean $\bar{\bar X}$, $\hat p_{\mathrm{mvu}}=((N-1)/N)/(\bar{\bar X}-a+1-1/N)$. Unbalanced-sample h-chart limits: $\mathrm{UCL/LCL}=\mu\pm g\sqrt{\sigma^2/n_k}$ with $\mu=(1-p)/p+a$ and $\sigma^2=(1-p)/p^2$; g-chart limits: $n_k\mu\pm g\sqrt{n_k\sigma^2}$. Estimation used for chart construction: $\hat\mu=\bar{\bar X}$, $\hat\sigma^2_{\mathrm{ml}}=(\bar{\bar X}-a)(\bar{\bar X}-a+1)$, $\hat\sigma^2_{\mathrm{mvu}}=\frac{N}{N+1}(\bar{\bar X}-a)(\bar{\bar X}-a+1)$.","The note proves an ordering of the three p-estimators for $0<p<1$: $\hat p_b<\hat p_{\mathrm{mvu}}<\hat p_{\mathrm{ml}}$, implying systematic underestimation by the commonly-used “MVU” ($\hat p_b$) and overestimation by ML. It derives exact expressions for $E(\hat p_{\mathrm{ml}})$ and $E(\hat p_b)$ in terms of $\,_2F_1(\cdot)$, showing $\mathrm{Bias}(\hat p_{\mathrm{mvu}})=0$ while the others can be substantial for small total sample size $N$ and large p. Monte Carlo (10,000 reps) shows severe empirical bias for $\hat p_b$ when sample sizes are small and p is large (e.g., with $(n_1,n_2)=(1,1)$, bias of $\hat p_b$ is about -0.434 at $p=0.9$), while $\hat p_{\mathrm{mvu}}$ bias stays near zero across tested p and sample-size scenarios. The paper also provides hypergeometric-function forms for second moments (including $\,_3F_2$) to compute MSE/variance exactly, and uses these to compare theoretical and empirical MSE curves.","The note emphasizes that conventional g and h charts assume balanced samples (equal subgroup sizes), which is often restrictive in practice; this motivates their unbalanced-sample construction method. It also notes that although one could plug $\hat p_{\mathrm{mvu}}$ into the mean/variance formulas to form limits, MVU estimators do not have the ML invariance property, so such plug-in limits cannot be regarded as MVU-based limits.","The proposed h/g limits rely on an asymptotic normal approximation for subgroup means/totals, which may be inaccurate for very small subgroup sizes and for highly discrete/zero-inflated geometric data—exact (discrete) limit calibration or ARL evaluation is not provided. The work focuses on i.i.d. Bernoulli-trial assumptions (independence within/between subgroups); many healthcare event processes exhibit temporal correlation or nonstationarity, which could distort false-alarm properties. The paper provides bias/MSE comparisons for parameter estimators but does not present control-chart run-length (ARL/ATS) performance comparisons of the resulting charts under in-control and out-of-control regimes, so practical detection-speed implications are not quantified.",The authors state that they developed the rQCC R package and plan to add these (updated/unbalanced-sample) g and h control charts in the next update so practitioners can use the results more easily.,"Provide full in-control and out-of-control run-length (ARL/ATS) evaluations for the ML- vs MVU-based unbalanced g/h charts, including exact/discrete calibration of control limits to target false-alarm rates. Extend the approach to settings with overdispersion or dependence (e.g., renewal-process models, autocorrelated event indicators) and develop robust or Bayesian estimators for p under model misspecification. Implement and benchmark the methods in software with reproducible examples and guidance on how much Phase I data is needed to stabilize estimates under realistic healthcare sampling patterns.",2101.07575v2,local_papers/arxiv/2101.07575v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:28:20Z
TRUE,Multivariate|High-dimensional|Other,Change-point|Other,Phase II,Semiconductor/electronics|Manufacturing (general),NA,FALSE,TRUE,Simulation study|Other|Case study (real dataset),Detection probability|Conditional expected delay|False alarm rate|Other,"Monitoring can start with at least 6 observations (each side of split point has ≥3) for the change-point statistic; in the moving-window version, window sizes W = 20, 30, 40 are studied and each window requires at least W observations.",TRUE,Other,Public repository (GitHub/GitLab),https://github.com/wyfwzz/Supplementary-code-for-A-change-point-based-control-chart...-,"The paper proposes a change-point based control chart to detect sparse mean shifts in high-dimensional (p≫n) data streams under heteroscedasticity and possible nonnormality. The chart uses a supremum (max-over-variables) two-sample mean-difference statistic and then maximizes it over all candidate split points within the current sample/window; a moving window is introduced to control computational cost and improve sensitivity. Control limits are obtained via a data-driven bootstrap to meet a prespecified false alarm probability (FAP) over a monitoring horizon, rather than targeting ARL. Performance is evaluated via Monte Carlo experiments under four models (baseline mean shift, heteroscedastic variances over time, correlated covariance, and heavy-tailed t distribution), using detection rate and conditional expected detection delay, and results indicate robustness to heteroscedasticity and nonnormality for large sparse shifts. A post-signal diagnostic procedure estimates the change-point and identifies shifted variables, and a semiconductor SECOM case study demonstrates effectiveness in a real high-dimensional process.","For split point k, the non-studentized statistic is $T^{NS}_{n,k}=\max_{1\le r\le p}\sqrt{\frac{k(n-k)}{n}}\,|\bar X_{k,r}-\bar X_{n-k,r}|$. The charting statistic is $U^{NS}_n=\max_{3\le k\le n-3}T^{NS}_{n,k}$, and the estimated change-point is $\hat\tau^{NS}_n=\arg\max_k T^{NS}_{n,k}$. With moving window size W, $U^{NS}_{n,W}=\max_{3\le k^*\le W-3}\max_{1\le r\le p}\sqrt{\frac{k^*(W-k^*)}{W}}\,|\bar X_{k^*,r}-\bar X_{W-k^*,r}|$, signaling when $U^{NS}_{n,W}>h_{p,W}$ where $h_{p,W}$ is set by bootstrap for a target FAP.","In simulations (typically n=100, step size s=5, B=10000 bootstraps for limits), the chart’s detection rate is low for small shifts (e.g., δ=1) but becomes near 1 for larger sparse shifts (e.g., δ=2) across p=20–100 and sparsity v=10%–25%. Under heteroscedastic, dependent, and heavy-tailed models (p=100, W=40, τ=25), detection rates for δ=1.5 are ~0.955–0.999 and for δ=2 are 1, with conditional expected detection delays around ~15 observations when τ≤W. In the SECOM case study (416 variables after preprocessing), detection rate is 1 for τ=10, 25, 50 and all W=20,30,40, with detection occurring in the first window when W>τ. Compared with the distribution-free EWMA chart of Chen et al. (2016), DFEWMA shows higher detection but also higher empirical false alarm probability and requires a large reference sample (m0=100), whereas the proposed method does not.","The authors note that a limitation of the proposed supremum-based method is that the signal is driven by a single variable (the maximum), so abnormal behavior in other shifted variables can be ignored in diagnosis. They also highlight a practical trade-off in choosing the window size W: larger W can improve sensitivity but can increase detection delay, especially when the true change-point τ is small relative to W.","The approach relies on bootstrap-calibrated control limits whose validity depends on having representative in-control data and approximate independence between windowed statistics (motivating a step size choice); performance may degrade if strong serial dependence persists. The paper primarily targets mean shifts; variance/covariance changes are treated as nuisance (robustness) rather than explicitly monitored, and design guidance for selecting W and s is mostly empirical. The GitHub URL in the paper appears truncated/ellipsized, which may hinder reproducibility if the exact repository link is not recoverable from the PDF.",The paper suggests improving sensitivity of the proposed method as a future research direction. One proposed avenue is to adapt/add a variable selection algorithm before starting monitoring to enhance performance (and potentially diagnosis) in high-dimensional settings.,"Developing an explicit method for autocorrelated/high-frequency sensor streams (e.g., incorporating dependence-robust resampling or modeling) would strengthen applicability in modern monitoring. Extending the framework to jointly detect sparse changes in covariance/variance (not just mean) and providing principled, data-adaptive selection rules for window size W and step size s (e.g., optimizing expected delay under constraints) would improve practical deployment. Providing a packaged software implementation (e.g., an R/Python package) and more real industrial case studies would enhance reproducibility and practitioner uptake.",2101.09424v1,local_papers/arxiv/2101.09424v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:29:05Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Energy/utilities,NA,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,False alarm rate|Other,"GP training uses 2500 randomly selected points from the first two years of turbine operation (assumed healthy). X-bar chart limits are estimated from a 6-month period immediately after the training period using repeated random subsampling (20 times) to compute robust mean/SD, then 3σ limits.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a structural health monitoring (SHM) scheme for operational wind-turbine blades that uses Gaussian Process (GP) regression to learn pairwise relationships between nominally identical blades’ first edgewise natural frequencies, with ambient temperature as an additional input feature. After training the GPs on an assumed healthy period (first two years), the method predicts one blade’s frequency from another’s and uses the prediction residuals as a damage-sensitive signal when correlations degrade. Diagnostics are performed by applying X-bar (Shewhart) control charts to 28-day averages of GP residuals, with 3σ thresholds estimated from a subsequent 6-month “baseline” period to reduce overly tight limits from training data. The approach is validated on one synthesized example and two real onshore turbine SCADA datasets where blade damage was known to occur, demonstrating early detection. In one case study the method signaled approximately six months before the damage was identified and remedied; in another, about three months prior.","GP regression model: $\mathbf{y}=f(\mathbf{X})+\varepsilon$ with zero-mean GP prior $f\sim GP(0,K)$ and covariance built as $K_\theta=k_{SE}+k_{BL}+k_N$ (squared-exponential + Bayesian linear + noise). Predictions use $\bar f^*=K_*^T K_\theta^{-1} \mathbf{y}$ with predictive covariance $K_{**}-K_*^T K_\theta^{-1}K_*$. Residuals are $r_{eA}=\bar f_A-f_A^*$, etc., and X-bar chart limits are computed as $\mathrm{thr}=\bar\mu\pm 3\bar\sigma$ from subsampled residuals in a 6-month baseline window.","In a real-turbine case study (Site A), the X-bar charts of 28-day averaged GP residuals exceeded 3σ limits roughly 6 months before the recorded damage identification/remedial action. In another real-turbine case (Site B), the 3σ limits were exceeded about 3 months before remedial activity. The paper also notes that after repair/replacement the inter-blade correlations change and residuals may not return within the original limits, implying retraining is needed after maintenance.","The authors note that damage type and location are not disclosed due to confidentiality, and that the synthesized example does not reflect true blade physics. They also caution that GPs should not be used for extrapolation, and prediction accuracy degrades as inputs move away from the training domain near damage. They state that after remedial action, correlations can change because blade properties differ, so retraining becomes essential.","The X-bar chart design and limit estimation are heuristic (28-day averaging; 3σ limits via subsampling) and the in-control false alarm performance/ARL is not quantified, making SPC performance hard to compare or tune. The approach assumes blades experience sufficiently similar EOVs and that relative-frequency relationships are stable in the absence of damage; persistent asymmetric loading or sensor/estimation bias could mimic damage. Autocorrelation and nonstationarity in residuals are not explicitly modeled, which can distort Shewhart chart signaling properties.","The authors suggest the correlation-based methodology could be applied to different signal types (e.g., strains and displacements), though these may require additional preprocessing and complementary features.","Quantify in-control/out-of-control chart performance (e.g., ARL/ATS) under realistic autocorrelation and seasonal effects, and compare against alternative monitoring statistics (EWMA/CUSUM) on residuals for earlier detection. Develop a formal Phase I/Phase II framework for selecting the training/baseline windows and for post-maintenance retraining/adaptation. Provide an open-source implementation and guidance for selecting kernel structure, training size, and robust residual modeling to improve transferability across turbines and fleets.",2101.11711v1,local_papers/arxiv/2101.11711v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:29:37Z
TRUE,Univariate|Bayesian|Other,Shewhart|Other,Phase II,Transportation/logistics,TRUE,FALSE,NA,Simulation study,Detection probability|False alarm rate|Other,Not discussed (online monitoring uses a rolling/accumulated set of n real-time sensor measurements up to time t* but no recommended n is given).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a real-time monitoring framework for automated-vehicle car-following that tracks deviations of the actual time gap from a user-specified desired setting. It models spacing as a random-coefficients (random effects) linear model, where the time gap is treated as a stochastic coefficient, and derives a closed-form Bayesian updating rule to infer the posterior distribution of the coefficients using streaming sensor measurements of spacing and speed. A univariate Shewhart control chart is then used to set lower/upper control limits around a baseline (desired) time-gap distribution and to signal when observed/updated time-gap estimates fall outside the acceptable range, prompting an adjustment of the time-gap setting. The approach is demonstrated via simulation using a disturbance-rich leader trajectory built from NGSIM data for a platoon of five connected automated vehicles. Results show early vehicles in the platoon exceed control limits multiple times under the initial setting, and that changing the desired time gap (and limits) can reduce observed variability in time gap.","Spacing is modeled as a random-coefficient linear model: $S_i(t)=s_0+\tau_i V_i(t)+\epsilon_i(t)$ with $\epsilon_i(t)\sim \mathcal N(0,\sigma^2)$ and $\Gamma_i=[s_0,\tau_i]^T\sim \mathcal N(\mu_b,\Sigma_b)$. Given real-time data $S_a^*=Z_a^*\Gamma_a+E_a^*$, the posterior is Gaussian $\Gamma_a\mid S_a^*\sim \mathcal N(\mu_a^*,\Sigma_a^*)$ with $\Sigma_a^*=(\Sigma_b^{-1}+Z_a^{*T}Z_a^*/\sigma^2)^{-1}$ and $\mu_a^*=\Sigma_a^*\,(Z_a^{*T}S_a^*/\sigma^2+\Sigma_b^{-1}\mu_b)$. Control limits use a Shewhart form around the baseline time-gap distribution: $\mathrm{LCL}=\mu_{\text{desired}}-L\sigma_{\text{desired}}$, $\mathrm{CL}=\mu_{\text{desired}}$, $\mathrm{UCL}=\mu_{\text{desired}}+L\sigma_{\text{desired}}$.","In the simulation with baseline time-gap distribution $\mathcal N(1.6,0.125)$ and 95% limits (2$\sigma$), the control limits are reported as LCL=1.35, CL=1.6, UCL=1.85. Vehicles 1–2 exceed these bounds, with reported extrema of about 1.92 (max) and 1.28 (min), including multiple out-of-bounds events within a short time window (four times within 50 seconds). After changing the desired time gap for vehicle 1 from 1.6 s to 1.0 s (and updating limits to LCL=0.75, CL=1.0, UCL=1.25), the maximum deviation from the desired time gap is reported to drop to 0.17 from 0.32. Later vehicles (3–5) show smaller variations under both settings, consistent with disturbance damping along the platoon.","The authors note that the study could be enhanced using real experimental autonomous-vehicle data to better analyze uncertainty in time gap. They also suggest that a non-linear modeling approach might yield more accurate real-time time-gap estimates. They further state that time gap depends on other control parameters (e.g., feedback/feedforward gains) that are not considered, and that incorporating additional performance metrics would improve the monitoring methodology.","The monitoring uses Shewhart limits around a baseline normal distribution, which is not optimized for small persistent shifts and may be sensitive to non-Gaussian posterior behavior or heavy-tailed disturbances. Serial dependence is likely in high-frequency vehicle trajectories; the approach appears to treat errors/updates as effectively independent and does not study the impact of autocorrelation on false-alarm rates. The evaluation is limited to a simulation scenario and does not report standard SPC metrics such as in-control/out-of-control ARL or ATS, making performance hard to compare against established SPC alternatives (e.g., EWMA/CUSUM). Implementation details (windowing/online update cadence, choice of n at each update, and robustness to sensor dropouts) are not specified, which affects deployability.",They propose validating the framework with real experimental AV data to analyze time-gap uncertainty more systematically. They suggest exploring non-linear modeling to improve real-time estimation accuracy. They also mention incorporating the dependence of time gap on controller parameters (feedback/feedforward gains) and adding other performance metrics to strengthen the overall monitoring methodology.,A natural extension is to account explicitly for time-series dependence (state-space or Bayesian filtering with autocorrelated errors) and recalibrate control limits to maintain a desired false-alarm rate under autocorrelation. Developing EWMA/CUSUM-style Bayesian charts (or posterior predictive-based GLR/change-point detectors) would likely improve sensitivity to small sustained degradations in tracking performance. Robust or nonparametric limits (or heavy-tailed observation models) could improve resilience to outliers and rare disturbance events. Providing open-source implementation and benchmarking via ARL/ATS against standard SPC and change-point methods on multiple leader-disturbance scenarios and real sensor datasets would strengthen evidence and adoption.,2102.00375v1,local_papers/arxiv/2102.00375v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:30:15Z
TRUE,Univariate,EWMA|Other,Phase II,Theoretical/simulation only,TRUE,FALSE,FALSE,Simulation study|Integral equation|Other,ARL (Average Run Length)|Expected detection delay|Steady-state ARL|Conditional expected delay,Not discussed.,TRUE,R,Package registry (CRAN/PyPI),https://cran.r-project.org/web/packages/spc/index.html,"This preprint critiques generally weighted moving average (GWMA) control charts for monitoring shifts in a process mean under i.i.d. normal observations with known and constant standard deviation. The authors argue GWMA has practical disadvantages: no recursive update (requiring storage of all past data) and lack of a Markov property, which prevents standard Markov-chain/integral-equation approaches and forces heavy Monte Carlo simulation for limits and performance. They show that earlier GWMA-vs-EWMA comparisons can be unfair, and that appropriately tuned EWMA charts (e.g., matching asymptotic variances) provide comparable or better detection performance than GWMA across a range of mean shifts. Performance is assessed using zero-state ARL and conditional/steady-state metrics (CED and steady-state ARL), with EWMA quantities computed via integral-equation methods implemented in the R package spc and GWMA quantities estimated by extensive simulation. The paper concludes there is no statistical-performance justification for using GWMA over the simpler, established EWMA chart.","The GWMA statistic is a weighted average of all past observations and the target mean: $G_t=\sum_{i=1}^t (q^{(i-1)\alpha}-q^{i\alpha})X_{t-i+1}+q^{t\alpha}\mu_0$, with in-control variance $\mathrm{Var}(G_t)=Q_t\sigma_0^2$ where $Q_t=\sum_{i=1}^t (q^{(i-1)\alpha}-q^{i\alpha})^2$ and asymptotic $Q=\lim_{t\to\infty}Q_t$ (no closed form). Control limits are $\mu_0\pm L_G\sqrt{Q_t}\,\sigma_0$ (or $\mu_0\pm L_G\sqrt{Q}\,\sigma_0$ asymptotically). The competing EWMA uses $Z_t=(1-\lambda)Z_{t-1}+\lambda X_t$ with asymptotic scaled variance $Q_E=\lambda/(2-\lambda)$ and EWMA limits based on $\mu_0\pm L_E\sqrt{\mathrm{Var}(Z_t)}$.","Recomputing comparisons for $q=0.75$ shows GWMA advantages reported in Sheu & Lin (2003) disappear when EWMA is fairly tuned (e.g., by matching asymptotic variances $Q=Q_E$). For example, for a $0.5\sigma$ mean shift, Table 2 reports ARL \approx 40.76 (GWMA $\alpha=0.5$) versus 34.60 (matched EWMA $\lambda=0.152$), and for a $1.0\sigma$ shift, 11.74 (GWMA $\alpha=0.5$) versus 8.90 (EWMA $\lambda=0.152$). The authors also compare conditional expected delay $D_\tau$ profiles and the conditional steady-state ARL $D$, finding the matched EWMA designs typically yield similar or smaller delays than GWMA across small-to-moderate shifts, with GWMA only showing slight advantages in some larger-shift cases. Overall, an EWMA with $\lambda=0.152$ is reported to “dominate uniformly” the considered GWMA designs in the presented zero-state ARL comparisons.","The paper’s comparisons focus on the standard GWMA/EWMA framework with i.i.d. $N(\mu,\sigma^2)$ data and an assumed known, constant $\sigma_0$, with attention primarily to detection of mean shifts. For steady-state comparisons for GWMA, the authors note exact/analytical computation is difficult because $\{G_t\}$ is not first-order Markov and limits may vary with time, so they rely on large-scale Monte Carlo simulation (using $D_{100}$ as a substitute for $D$).","Because the paper is a critique and uses specific design-matching (equalizing asymptotic variances) as the primary fairness criterion, conclusions may depend on that matching choice; other optimality criteria (e.g., constrained optimization over ARL curves or economic design) are not explored. The study assumes independence and normality with known $\sigma_0$; robustness to non-normality, autocorrelation, parameter estimation (Phase I uncertainty), and other common SPC complications is not evaluated. Real-data case studies are not provided, so practical impact in applied settings is inferred from simulations/analytical calculations rather than demonstrated empirically.",None stated.,"Extend the critique to settings with estimated parameters (Phase I/II interplay), autocorrelated data, and non-normal distributions to assess whether any GWMA variants offer robustness benefits relative to EWMA under realistic violations. Provide benchmarked, reproducible implementations for GWMA performance evaluation (including efficient approximations) to standardize fair comparisons across the GWMA literature. Investigate alternative matching/design criteria (e.g., minimizing worst-case steady-state delay over a shift range, or economic design) to determine whether any GWMA tuning yields meaningful advantages under practitioner-relevant objectives.",2107.00224v1,local_papers/arxiv/2107.00224v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:30:53Z
TRUE,Multivariate|High-dimensional|Other,MEWMA|Other,Both,Healthcare/medical,TRUE,NA,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|False alarm rate|Expected detection delay|Other,Not discussed,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a dynamic modeling-and-monitoring framework for the emergence and progression of multiple chronic conditions (MCC) driven by patient risk factors. It extends functional continuous-time Bayesian networks (FCTBNs) by casting the Poisson-regression edge parameters as latent states in a nonlinear state-space model estimated via an Extended Kalman Filter (EKF), yielding a dynamic FCTBN (D-FCTBN). For monitoring, it introduces a “tensor control chart” that treats predicted risk-factor coefficients as a 3-way tensor (parents × children × risk factors), extracts low-rank features via multilinear PCA (MPCA), and charts the reconstruction-error features using a MEWMA statistic to detect meaningful behavioral changes. The approach is evaluated using both simulated data (with injected behavioral changes) and real longitudinal cohort data (CCHC; 385 patients) involving 5 chronic conditions and 7 risk factors (4 modifiable lifestyle, 3 demographic). Results show the chart can signal changes after a small number of observations in several scenarios, supporting proactive detection of lifestyle-driven increases in MCC risk.","(i) FCTBN conditional intensities use Poisson regression, e.g., $\log q_{x_i,x_j\mid u}=\beta_{0}+\sum_{k=1}^m z_k\,\beta_{k}$ (Eq. 5) and $q=\exp(z^T\beta)$. (ii) EKF observation/state model for coefficients: observation $q^t_{x_i\mid u}=\exp(z_t^T\beta^t_{x_i\mid u})$ (Eq. 9) and state transition $\beta_t = F\beta_{t-1}+\varepsilon_t$ with Gaussian noise (Eq. 10–12). (iii) Monitoring uses MPCA reconstruction error (Eqs. 17–18) and MEWMA: $Z_i=\lambda x_i+(1-\lambda)Z_{i-1}$ and $T_i^2=Z_i^T S_I^{-1}Z_i$ (Eqs. 19–21), with control limits based on EWMA variance (Eq. 22).","In simulation, with control-chart parameters set (via simulation) to approximately $\lambda=0.15$ and width $L=1.5$, the MEWMA-based tensor chart shows no signals in an in-control 48-month run (supporting low Type I error). When a single lifestyle factor is changed at month 17, the chart signals after about 3 observations (month ~20) for diet change and about 4 observations (month ~21) for drinking change. When two behavioral factors are changed simultaneously (introduced around month 19 in the simulated scenario), the first out-of-control signal appears much later (around month 39), which the authors attribute to interaction effects. In the real case example, a behavioral change around year 13 leads to an out-of-control signal after one observation (year 14).","The paper notes practical constraints from the real cohort data availability, using yearly intervals for real experiments due to limited consecutive visit data, while simulated experiments use monthly observations. It also notes EKF can require additional steps/corrections to avoid divergence and discusses stability conditions, implying sensitivity to EKF tuning and bounded-noise assumptions.","The proposed monitoring scheme relies on MPCA feature extraction and MEWMA charting of reconstruction-error vectors, but the choice of tensor ranks/feature dimensions and their robustness to misspecification is not fully validated or benchmarked against alternative multivariate monitoring approaches (e.g., direct Hotelling $T^2$ on coefficients, likelihood-based change detection). The EKF assumes Gaussian process/measurement noise and linear state evolution for regression coefficients, which may be restrictive for abrupt behavioral changes or nonstationary dynamics. The paper reports detection examples but does not provide standard SPC run-length summaries (e.g., calibrated in-control ARL and comprehensive out-of-control ARL curves) across shift magnitudes, which limits comparability to established MEWMA/EWMA designs.",None stated,"Future work could (i) provide a full ARL/ATS design and calibration study for the tensor-MEWMA chart (including steady-state ARL) and compare against competing multivariate/tensor monitoring methods, (ii) develop robust/nonparametric variants to reduce sensitivity to Gaussian-noise and model-misspecification assumptions, and (iii) address irregular sampling and missing visits explicitly (common in longitudinal healthcare data), potentially via state-space models with uneven time steps or particle filtering. Packaging the method in reproducible software (e.g., R/Python) with guidance on selecting MPCA ranks and EKF hyperparameters would also improve practical adoption.",2107.13394v1,local_papers/arxiv/2107.13394v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:31:31Z
TRUE,Univariate|Other,Shewhart|Change-point,Both,Manufacturing (general)|Food/agriculture|Other,TRUE,TRUE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Other,"Phase I parameters of the VAR(1) model are to be estimated from Phase I data; the paper notes this should be done with “sufficiently enough collected data” but gives no specific Phase I sample-size rule. Phase II examples/simulations use subgroup sizes n ∈ {2,5,7,10,15} (and an illustrative example with n = 5).",TRUE,R|SAS,Not provided,NA,"The paper studies how serial autocorrelation affects the Shewhart-RZ control chart, which monitors the ratio of two (approximately) normal variables using probability control limits because the ratio distribution lacks moments. Autocorrelation within subgroups is modeled via a bivariate VAR(1) process for (X,Y), and closed-form expressions are derived for the mean/covariance of subgroup means under VAR(1), leading to adjusted coefficients of variation and correlation used in the ratio distribution. Control limits are defined as quantiles of the ratio distribution using an existing approximation for the inverse CDF, and Phase II performance is evaluated mainly through ARL (and an overall EARL metric when shift size is random). Extensive numerical studies show that increasing autocorrelation parameters (Φ11, Φ22) inflates ARL1 (worse detection) relative to the independent case, i.e., autocorrelation degrades the chart’s ability to detect shifts in the mean ratio. The paper also illustrates implementation on a furnace-pressure VAR(1) dataset for parameter estimation and a food-industry ratio-monitoring example with autocorrelated simulated data.","The monitored statistic is the subgroup ratio $\bar Z_i=\bar X_i/\bar Y_i$ (Eq. 12). Autocorrelation is modeled by VAR(1): $W_{i,j}=\mu_W+\Phi(W_{i,j-1}-\mu_W)+\varepsilon_{i,j}$ with $\varepsilon_{i,j}\sim N_2(0,\Sigma_\varepsilon)$ (Eq. 1). Probability limits are $\mathrm{LCL}=F_Z^{-1}(\alpha/2\mid \gamma_{\bar X},\gamma_{\bar Y},\bar\omega,\bar\rho)$ and $\mathrm{UCL}=F_Z^{-1}(1-\alpha/2\mid\cdot)$ (Eqs. 13–14), and with independence between successive subgroup statistics the run length is geometric with $\mathrm{ARL}_1=1/(1-\beta)$ where $\beta=F_Z(\mathrm{UCL}\mid\cdot)-F_Z(\mathrm{LCL}\mid\cdot)$ (Eqs. 15–16).","Using ARL0 = 200 (α = 0.005) across scenarios, simulations show ARL1 increases (detection worsens) as autocorrelation increases. For example, with n=5, $(\gamma_X,\gamma_Y)=(0.01,0.01)$, $\rho_0=\rho_1=-0.8$, and a small downward shift $\tau=0.99$, ARL1 increases from 23.1 at $(\Phi_{11},\Phi_{22})=(0.1,0.1)$ to 59.7 at $(0.7,0.7)$; both exceed the no-autocorrelation benchmark reported in prior work (19.1). The paper also reports EARL surfaces over $(\Phi_{11},\Phi_{22})$ showing the same degradation pattern (larger autocorrelation → larger EARL) for both decreasing and increasing shift ranges. In the food-industry illustrative example with n=5 and $\Phi_{11}=\Phi_{22}=0.5$, the chart’s limits are LCL = 0.9723582 and UCL = 1.0284276 and it signals at samples #14–#15 for a 2% ratio increase.","The authors assume that while observations within each subgroup are autocorrelated, the subgroup ratio statistics $\bar Z_i$ are independent because the sampling interval is “large enough to eliminate any dependence” between successive $\bar Z_i$. They also restrict much of the analysis to the case of no lagged cross-dependence (often taking $\Phi$ diagonal, $\Phi_{12}=\Phi_{21}=0$) and note that Phase I estimation should be done with sufficiently large data but do not develop Phase I implementation in detail.","The method relies on (approximate) bivariate normality and on an approximation to the inverse CDF of the ratio; robustness to non-normality/heavy tails or to poor approximation accuracy (especially outside the stated CV range) is not assessed. Treating successive subgroup statistics as independent may be unrealistic when sampling intervals are short, so the geometric-run-length/ARL calculations could be optimistic in highly autocorrelated settings. Comparisons are largely within the Shewhart-RZ family; the paper does not benchmark against time-series-residual charts, ARMA/VAR modeling plus residual Shewhart/EWMA, or other ratio-monitoring schemes under dependence.","The authors suggest designing more advanced control charts to mitigate the negative impact of autocorrelation on Shewhart-RZ performance, investigating autocorrelation effects for EWMA-RZ charts along similar lines, and developing Phase I implementations for RZ-type control charts.","Extend the framework to allow dependence between successive subgroup statistics (e.g., explicit ARMA structure in $\bar Z_i$) and compute steady-state ARL/ATS under such dependence. Develop robust/nonparametric or Bayesian versions for ratio monitoring under autocorrelation and evaluate sensitivity to VAR(1) model misspecification and Phase I estimation error. Provide open-source implementations (e.g., an R package) and include broader benchmarks against model-based residual charts and alternative ratio or log-ratio transformations under serial dependence.",2108.05239v1,local_papers/arxiv/2108.05239v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:32:13Z
TRUE,Univariate|Other,Hotelling T-squared|Other,Both,Manufacturing (general)|Other,TRUE,TRUE,NA,Exact distribution theory|Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Detection probability,"Phase I uses I historical in-control batches; simulation varies I from 30 to 100 (also examples with I=30) and application varies I = 10, 30, 100, 200, 300, 500. Batch (time series) length T is varied from 100 to 1000 in simulation; real data uses T=500.",TRUE,R,Package registry (CRAN/PyPI),https://www.R-project.org/,"The paper proposes a batch-process monitoring and fault diagnosis approach using control charts built on ARMA model coefficient estimates, explicitly accounting for within-batch serial correlation and batch-to-batch variability. In Phase I, an ARMA(v,w) model is fit separately to each in-control batch via OLS; the across-batch mean vector and covariance matrix of coefficient estimates are then used as references. For Phase II monitoring, a modified Hotelling $T^2$ chart on the coefficient vector is derived, using an exact/asymptotic scaled F distribution that incorporates estimation uncertainty from finite numbers of batches; after a signal, a set of per-coefficient modified t charts is used to diagnose which ARMA coefficients changed. The method targets changes in process dynamics (AR/MA parameters) and can also respond to mean/intercept shifts, and it avoids iterative/complex estimation by using closed-form distributional results. Performance is demonstrated via Monte Carlo simulations (showing lower out-of-control ARLs than residual-mean charts with EWMA enhancement for dynamic shifts) and via a real engine-noise time-series dataset where an AR(12) model (first three AR coefficients) separates two operating conditions with high detection rates even for small Phase I sample sizes.","The ARMA model is $x_t=\phi_0+\sum_{j=1}^v\phi_j x_{t-j}+\epsilon_t+\sum_{k=1}^w\theta_k\epsilon_{t-k}$ with parameter vector $\beta=[\phi_0,\phi_1,\ldots,\phi_v,\theta_1,\ldots,\theta_w]$. Phase I combines batchwise OLS estimates $\hat\beta_i$ into $\bar\beta=\frac1I\sum_{i=1}^I\hat\beta_i$ and $S_{\hat\beta}=\frac{1}{I-1}\sum_{i=1}^I(\hat\beta_i-\bar\beta)(\hat\beta_i-\bar\beta)'$. The monitoring statistic is $T^2_\beta=(\hat\beta_i-\bar\beta)'S_{\hat\beta}^{-1}(\hat\beta_i-\bar\beta)$ with $(\text{scaled})\;T^2_\beta\sim F_{p,I-p}$, and diagnostic charts use $t_\beta=(\hat\beta^*_i-\bar\beta^*)/S_{\hat\beta^*}$ with a scaled $t_{I-1}$ distribution.","In Monte Carlo studies (1000 replications per scenario; Phase I batches I varied; batch length T from 100 to 1000; $\alpha=0.01$), the proposed $T^2_\beta$ chart generally yields substantially smaller out-of-control ARLs than a residual-mean chart (with EWMA used to improve residual detection) when disturbances affect AR/MA parameters (process dynamics). The paper reports ARL0 values close to the nominal (e.g., $\alpha=0.01\Rightarrow$ ARL0≈100) and shows faster ARL1 reduction as disturbance magnitude increases for $T^2_\beta$ compared with residual-based charts. In the real FordA-train application (T=500; reference group +1), an AR(12) fit produced residuals passing Ljung–Box (100% uncorrelated) and Shapiro–Wilk normality (95% at 5% level); using only the first three significant AR coefficients in $T^2_\beta$ gave high detection of the -1 group: for $I=500$, mean detection rates $r_1$ were about 0.89 (α=0.10), 0.84 (α=0.05), and 0.70 (α=0.01). Empirical false alarm rates $r_0$ approached the nominal α as I increased (e.g., around 0.08/0.05/0.02 for α=0.10/0.05/0.01 at I=500).",None stated.,"The approach is developed and evaluated one variable at a time (univariate ARMA); it does not directly model cross-variable correlations typical of multi-sensor batch processes, so practitioners may need multiple charts and multiplicity control. The theory relies on (exact or large-T) normality of coefficient estimators and independence across batches; strong non-normality, model misspecification, or batch-to-batch dependence could distort the stated F/t control limits. Diagnostic t-charts can be confounded by covariance among coefficients (the authors note occasional false alarms due to coefficient covariance), suggesting limited fault isolation when parameters are highly correlated or when multiple coefficients shift simultaneously.",None stated.,"Extend the method to multivariate settings (e.g., VAR/ARMA with multiple variables per batch) with joint monitoring of coefficient matrices and structured covariance estimation. Develop robust/nonparametric variants that relax normality assumptions and handle heavy tails or outliers common in industrial sensor data. Add explicit treatment of batch-to-batch dependence and within-batch nonstationarity (trends/cycles/regime changes), and provide open-source implementations (e.g., the promised CRAN package) with guidance on model-order selection and multiple-testing control for the diagnostic t-charts.",2109.00952v1,local_papers/arxiv/2109.00952v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:32:59Z
TRUE,Univariate|Other,CUSUM|EWMA|Change-point|Other,Phase II,Theoretical/simulation only,TRUE,FALSE,NA,Simulation study|Markov chain|Integral equation|Other,ARL (Average Run Length)|Conditional expected delay|Steady-state ARL|Other,Not discussed,TRUE,R,Not provided,https://cran.r-project.org/web/packages/spc/index.html,"This paper critiques a set of recently proposed “compound” or “memory-based” process monitoring schemes that modify or combine standard control charts (notably EWMA and CUSUM), such as mixed EWMA–CUSUM charts, EWMA/CUSUM charts with run rules, and recursively defined statistics (e.g., DEWMA/TEWMA, DMA/TMA/QMA, DPM). The authors argue these ad hoc constructions are inadequately justified and often employ unreasonable weighting patterns that can overweight older observations, leading to poor performance for shifts occurring after monitoring has been underway. Performance is assessed using more realistic measures than the common zero-state ARL, emphasizing conditional expected delay (CED) and the conditional steady-state ARL, and showing that apparent zero-state advantages often disappear or reverse. For several representative compound charts, they demonstrate that properly tuned conventional competitors (especially standard EWMA or CUSUM) match or outperform them in steady-state and worst-case behavior with less complexity. The work is primarily methodological/critical and provides guidance on fair comparisons (e.g., matching effective weighting/variance and using steady-state metrics) rather than proposing a new chart.","The benchmark EWMA is $Z_0=\mu_0$, $Z_i=(1-\lambda)Z_{i-1}+\lambda X_i$ with exact (time-varying) limits signaling when $|Z_i-\mu_0|>c_E\sqrt{\{1-(1-\lambda)^{2i}\}\,\lambda/(2-\lambda)}$. The mixed EWMA–CUSUM (MEC) replaces $X_i$ in a two-sided CUSUM by an EWMA $Q_i=(1-\lambda_q)Q_{i-1}+\lambda_q X_i$, with $M_i^+=\max\{0,M_{i-1}^+ + Q_i-a_i\}$ and $M_i^-=\max\{0,M_{i-1}^- - Q_i-a_i\}$, where $a_i=a^*\sigma_{Q,i}$ and $\sigma_{Q,i}=\sqrt{\lambda_q/(2-\lambda_q)\,\{1-(1-\lambda_q)^{2i}\}}$; signal when $\max(M_i^+,M_i^-)>b^*\sigma_{Q,i}$. They emphasize CED $D_\tau=E_\tau(L-\tau+1\mid L\ge\tau)$ and the conditional steady-state ARL $D=\lim_{\tau\to\infty}D_\tau$ as key evaluation criteria.","For MEC charts, the paper shows that when the CUSUM reference value is chosen fairly as $k=a^*\sigma_{Q,\infty}=a^*\sqrt{\lambda_q/(2-\lambda_q)}$, the simpler CUSUM achieves similar zero-state performance and is at least as good in steady-state, while dominating for medium and large shifts (illustrated for $\delta=0.5$ and $\delta=1.5$). It also highlights that MEC can have a worse worst-case ARL than its zero-state ARL, unlike standard CUSUM where zero-state equals worst-case. For run-rules CUSUM/EWMA proposals, re-estimated ARLs (with much larger Monte Carlo replication) and fairer comparisons indicate no compelling advantage over a well-chosen standard CUSUM/EWMA. For DMA/MA and DEWMA/TEWMA families, the authors show that older-data-heavy weighting leads to inferior CED/steady-state ARL; e.g., an EWMA with matched asymptotic variance (e.g., EWMA $\lambda\approx0.05$ vs DEWMA $\lambda=0.1$ at in-control ARL 200) has clearly lower steady-state delays across shifts, and MA generally beats DMA under steady-state optimization. For DPM, CED profiles suggest inability to detect delayed changes well, with CED growing large for $\tau\gtrsim 30$ even when zero-state ARL looks favorable.",None stated.,"The article’s quantitative conclusions rely on selected illustrative compound charts (five are studied in detail) and on specific tuning rules (e.g., matching asymptotic variance/weighting) that, while reasonable, are not the only possible fairness criteria; different matching objectives (e.g., worst-case ARL, specific shift-optimality) could alter some comparisons. The scope is largely restricted to independent normal observations with known variance (the main analytical setup), so the critique is not fully validated for autocorrelated, nonstationary, or highly non-normal industrial data streams where some compound heuristics might behave differently. The work uses simulation extensively, but code for reproducing the studies is not provided, which limits reproducibility and independent verification of the Monte Carlo designs.",The paper suggests refocusing monitoring goals toward detecting changes of practical importance (not merely statistical significance) and points readers to related discussion in Woodall and Faltin (2019).,"A useful extension would be to replicate the steady-state/worst-case comparisons under realistic dependence (e.g., ARMA residual monitoring) and under parameter estimation (Phase I-to-II effects) to test whether the critique persists when practitioners must estimate baselines. Providing open, standardized benchmark code and datasets for fair chart comparisons (including steady-state ARL/CED and worst-case metrics) would improve reproducibility and reduce “weak competitor” issues. Another direction is to formalize fairness criteria (e.g., matching effective kernel/weighting, steady-state in-control behavior, or worst-case constraints) and derive principled tuning methods so that comparisons across chart families are unambiguous.",2110.10680v1,local_papers/arxiv/2110.10680v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:33:40Z
TRUE,Multivariate|High-dimensional|Nonparametric|Other,Hotelling T-squared|Other,Both,Manufacturing (general)|Healthcare/medical|Theoretical/simulation only|Other,TRUE,TRUE,FALSE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"Focuses on limited Phase I sample size with individual observations in high-dimensional settings (p>m); examples/simulations use m ranging roughly from 20–400 with p from about 10–200, and real examples include m=24, p=314 (VDP) and m=357, p=30 (breast cancer benign set). No single minimum m is prescribed.",TRUE,R,Not provided,http://ftp.cs.wisc.edu/|http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/,"The paper proposes a Phase I methodology for monitoring high-dimensional multivariate processes with individual observations when the Phase I sample size m is small relative to the dimension p and data may contain outliers. It introduces a modified Mahalanobis-distance statistic that uses only the diagonal of the covariance matrix (variance scaling) and standardizes it using correlation-trace terms tr(ρ^2) and (via Cornish–Fisher adjustment) tr(ρ^3) to improve tail accuracy for small α. A robust Phase I estimation procedure is developed using a reweighted minimum diagonal product (RMDP/MDP) approach to estimate μ, the diagonal variance matrix D, and consistent high-dimensional estimators of tr(ρ^2) and tr(ρ^3), along with a finite-sample correction factor derived and tuned via simulation. Performance is evaluated primarily via Phase I outlier detection error rates (swamping/type I and masking/type II) and detection power for mean shifts under different correlation structures and contamination rates. Two real data applications (wood board vertical density profiles; diagnostic breast cancer data) illustrate the method’s practical use and show it can flag likely outliers/mean shifts where classical high-dimensional alternatives struggle.","The chart uses the modified Mahalanobis distance with diagonal scaling: $M_i^2=(X_i-\mu)^\top D^{-1}(X_i-\mu)=\sum_{j=1}^p (X_{ij}-\mu_j)^2/\sigma_{jj}$. It standardizes $M_i^2$ as $U_i=\{M_i^2(\mu,D)-p\}/\sqrt{2\,\mathrm{tr}(\rho^2)}$ and then applies a Cornish–Fisher correction to obtain a tail-accurate statistic $Z_i$ with threshold $z_\alpha$ (Eqns. (4)–(5)). Consistent estimators for $\mathrm{tr}(\rho^2)$ and $\mathrm{tr}(\rho^3)$ are based on the sample correlation matrix $R$, e.g., $\widehat{\mathrm{tr}(\rho^3)}=\mathrm{tr}(R^3)-\frac{3p}{m}\mathrm{tr}(R^2)+\frac{2p^3}{m^2}$ (Eq. (8)), combined with a finite-sample correction factor $c_{p,m}=1+\frac{2p}{m}\big(\mathrm{tr}(R^2)-p^2/m\big)^{-1/2}$ (Eq. (7)).","Simulations (typically 10,000 replications) show the Cornish–Fisher adjusted statistic $Z_i$ matches the standard normal tail behavior much better than $U_i$ for moderate p (e.g., p=10–200), improving false-alarm calibration. Table 1 reports that incorporating Cornish–Fisher adjustment substantially reduces inflated in-control false-alarm rates; for example with m=200, p=30, Scenario 2 and nominal α=0.05, simulated α improves from 0.091 (without CF) to 0.060 (with CF). The proposed Phase I chart’s in-control false-alarm rate converges quickly to nominal α as m increases across tested p (e.g., p=30–100), with slower convergence only under very high correlations (e.g., a=0.9 in an AR(1)-like structure). In contaminated Phase I settings, detection power increases with mean-shift magnitude δ and decreases as contamination rate r increases; compared with an MCD-based Phase I T^2 chart (Vargas 2003), the proposed method attains much closer-to-nominal type I error and generally higher power, especially for larger p (e.g., p=100).","The authors note that the method relies on asymptotic approximations and Assumption 1 (correlations not cumulatively large), and that very high correlations can slow convergence of the false-alarm rate to its nominal level. They also state the Cornish–Fisher adjustment is derived under normality, and the method’s performance for non-normal distributions is not fully developed (they suggest using transformations and note this as an area for future work).","Because the chart replaces Σ with its diagonal D, it can lose efficiency when correlation structure is strong or when shifts manifest primarily through correlated directions rather than marginal changes; even though tr(ρ^2) and tr(ρ^3) enter the standardization, the numerator still ignores off-diagonal covariance. The robust estimation procedure (MDP/RMDP) involves algorithmic choices (γ via h, α/2 reweighting thresholds) and may be sensitive to tuning and to very small m (e.g., m close to the minimum h), but systematic guidance for choosing these parameters in practice is limited. The paper focuses largely on mean shifts; extensions to variance/covariance changes, mixed shifts, or autocorrelated observations over time (beyond using correlated-variable structures) are not fully addressed. No open-source implementation is provided, which may hinder reproducibility and practitioner uptake.","They propose extending the method to non-normal high-dimensional processes, to highly correlated multivariate processes (including improved distributional approximations such as Welch–Satterthwaite ideas), and to cases where batch/subgroup size is greater than one. They also mention improving control limits for distributions other than normal as a general approach beyond simulation-based thresholds tied to (m,p,ρ).",Developing a self-starting/online version that updates robust parameter estimates sequentially would broaden applicability when Phase I reference data are scarce or evolving. Providing diagnostic tools to identify which variables drive signals (post-signal interpretation) and integrating sparse mean-shift localization could make the approach more actionable in high-dimensional monitoring. A more comprehensive robustness study under heavy-tailed and skewed distributions (with and without marginal transformations) and under serial dependence (time-series autocorrelation) would clarify operating characteristics in modern sensor streams. Packaging the method in an R/Python library with reproducible simulation scripts and default tuning recommendations would materially improve adoption.,2110.13689v2,local_papers/arxiv/2110.13689v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:34:34Z
TRUE,Multivariate|Self-starting|High-dimensional,Shewhart|Hotelling T-squared|Other,Both,Semiconductor/electronics|Theoretical/simulation only,TRUE,TRUE,TRUE,Approximation methods|Simulation study|Case study (real dataset)|Other,ARL (Average Run Length),"They note initial Phase I samples can affect Phase II performance and report simulations suggesting a Phase I sample size of about 200–300 is appropriate. They also recommend using larger m when using smaller α (e.g., α=0.005).",TRUE,R,Not provided,http://archive.ics.uci.edu/ml/datasets/SECOM,"The paper proposes a Phase II monitoring scheme for high-dimensional multivariate processes with individual observations when Phase I sample size is small relative to dimension (p≫m). It introduces a Shewhart-type charting statistic based on a modified Mahalanobis distance that uses only the diagonal of the covariance matrix (variance estimates) to avoid singular covariance inversion, and standardizes it using traces of powers of the correlation matrix. To obtain accurate control limits in the small-tail region typical for control charts, it applies Cornish–Fisher (Edgeworth-based) quantile expansion to adjust the normal approximation of the statistic. A unified Phase I/II self-starting procedure is provided, using robust RMDP-based parameter estimation to mitigate Phase I outliers and updating estimates sequentially as new in-control observations arrive. Performance is assessed primarily by ARL via Monte Carlo simulation under independent and correlated (AR-type) structures and with/without contamination, and the method is illustrated on the SECOM semiconductor dataset.","The modified distance replaces Σ with its diagonal D: $M_i^2=(X_i-\mu)^\top D^{-1}(X_i-\mu)=\sum_{j=1}^p (X_{ij}-\mu_j)^2/\sigma_{jj}$. The charting statistic is standardized using correlation-matrix traces: $U_i=(M_i^2-p)/\sqrt{2\,\mathrm{tr}(\rho^2)}$, and the upper control limit is adjusted via Cornish–Fisher, e.g. first-order $\omega_{\alpha,p}\approx z_\alpha+\frac{4\,\mathrm{tr}(\rho^3)(z_\alpha^2-1)}{3\,[2\,\mathrm{tr}(\rho^2)]^{3/2}}$, leading to a signal rule of the form $Z_i>z_\alpha$ after subtracting the CF correction term.","Using 10,000 Monte Carlo replications, simulations show that Cornish–Fisher adjusted limits yield in-control ARLs close to nominal (e.g., nominal ARL0=200 at α=0.005; with CF, ARL0≈193–207 across p=10–200, while without CF ARL0 can be far lower, implying excess false alarms). For out-of-control scenarios where 20% of variables shift, CF-adjusted ARL1 values are generally much closer to nominal than the unadjusted approach across scenarios and dimensions. Sensitivity analysis with AR(1)-type correlation ($\sigma_{ij}=a^{|i-j|}$) indicates ARL0 remains near nominal for a wide range of a, with some inflation at very high correlation (e.g., a=0.9) and small p. In a comparison against an RMCD-based robust Hotelling-type chart (assuming very large m for fair setup), the proposed RMDP-based chart shows smaller ARL1 (faster detection) and is less degraded by Phase I contamination (r=0.1, 0.2) than RMCD.","The authors note that for a few cases with very high correlation (e.g., $a=0.9$) and small p, simulated ARL0 can deviate (be slightly larger than nominal), and this may increase ARL1. They also indicate that some nonconforming items in the SECOM example are not well detected by a mean-shift-focused chart, suggesting nonconformance may arise from distributional changes beyond mean shifts. They state that initial Phase I samples can affect Phase II performance (motivation for self-starting/learning time).","The method relies on using only marginal variances (diagonal D) for the distance, so it may lose power when mean shifts manifest primarily through correlated directions captured by off-diagonal covariance structure. The Cornish–Fisher correction requires estimating traces like $\mathrm{tr}(\rho^2)$ and $\mathrm{tr}(\rho^3)$; estimation error (especially with small m and/or model misspecification) could impact achieved ARL, yet finite-sample robustness of these trace estimators is not fully characterized here. Although they discuss AR/MA correlation structures informally, the core derivations assume i.i.d. observations over time; explicit treatment of serial dependence in time (autocorrelation across sampling epochs) is not developed. Practical implementation details (e.g., packaged software, computational complexity for very large p, and guidance for choosing robust-tuning parameters) are limited.","They suggest addressing the high-correlation shortcoming by adopting a Welch–Satterthwaite $\chi^2$ approximation for the weighted sum of $\chi^2(1)$ variables, and/or determining a finite-sample correction coefficient for highly correlated multivariate processes via extensive simulation so the chart attains nominal ARL0. They also emphasize that high-dimensional process monitoring remains a promising area where “much more work is needed.”","Developing versions that explicitly handle serial correlation over time (e.g., vector ARMA dynamics) and irregular sampling would broaden applicability. Extending the approach beyond mean shifts to variance/covariance shifts or general distributional changes (possibly via combined location-scale monitoring) could improve detection in examples like SECOM where nonconformance may not be mean-driven. Providing open-source implementation (an R package) and automated Phase I cleaning/outlier diagnosis tools would improve adoption and reproducibility. A theoretical and empirical study of robustness under non-normal heavy tails (without marginal normal-score transforms) and under strong cross-correlation (including power analyses) would clarify operating characteristics.",2110.13696v2,local_papers/arxiv/2110.13696v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:35:21Z
TRUE,Multivariate|Other,CUSUM|MCUSUM|Other,Both,Transportation/logistics|Network/cybersecurity|Theoretical/simulation only,NA,TRUE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),"Uses Phase I in-control data of size m = 1000 to estimate the in-control mean vector, covariance matrix, and correlation coefficients (with m >> B). In the metro case study, Phase I parameters are estimated from the first eleven months of 2017 data.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes an online SPC method for monitoring serially correlated directed networks by transforming network flow counts into a transition probability matrix and then applying a weighted multivariate CUSUM scheme. To address serial dependence without specifying a parametric time-series model, it extends the spring-length-based approach to multivariate row vectors and constructs an asymptotically uncorrelated/standardized statistic via a Cholesky-based decorrelation using estimated short-memory autocovariances up to a data-adaptive order B (capped by Bmax). Row-wise two-sided CUSUM statistics are combined into a single network-level chart using weights proportional to each node’s total outgoing transitions, emphasizing more active nodes. Performance is evaluated by Monte Carlo simulation (10,000 replications) using ARL and SDRL under scenarios with single and double serial correlation and with correlated rows, and compared against top-1 CUSUM variants and an EWMA method for networks. A real metro-traffic case study (half-hour transaction networks over 14 stations) demonstrates Phase I false-alarm robustness and Phase II detection sensitivity relative to competitors.","Network observation at time t is a transition count matrix N(t) with entries n^{(t)}_{ij}, transformed to a transition probability matrix P(t) with p^{(t)}_{ij}=n^{(t)}_{ij}/n^{(t)}_i and \sum_j p^{(t)}_{ij}=1. For row i, a two-sided CUSUM is computed using a (decorrelated) score \tilde e_{i,t} and reference value k_i=\tfrac12(\mu_{1,i}-\mu_{0,i})^T\Sigma_i^{-1}(\mu_{1,i}-\mu_{0,i}), updating \tilde C^+_{i,t}=\max(0,\tilde C^+_{i,t-1}+\tilde e_{i,t}-k_i) and \tilde C^-_{i,t}=\min(0,\tilde C^-_{i,t-1}+\tilde e_{i,t}+k_i), with \tilde C_{i,t}=\max(\tilde C^+_{i,t},-\tilde C^-_{i,t}). The network statistic is a weighted sum \tilde C_t=\sum_{i=1}^K w_i^{(t)}\tilde C_{i,t} where w_i^{(t)}=n_i^{(t)}/\sum_i n_i^{(t)}, and a signal occurs when \tilde C_t>h; the decorrelation uses estimated autocovariances \hat\gamma_i(q) and an adaptive short-memory order B updated via spring length (capped by Bmax).","Simulations use 10,000 replications and evaluate ARL/SDRL under multiple dependence settings (serial correlation in N(t), in P(t), in totals n_i^{(t)}, and combined double-correlation), with a typical design ARL0 \approx 200 and Bmax=4. Across scenarios, the proposed weighted spring-length CUSUM (WSCUSUM) is reported to have strong robustness: SDRLs are consistently smaller than corresponding ARLs, and ARL1 decreases with increasing shift magnitude and (often) with larger transition counts. In the metro case study, control limits (for ARL0=200) are reported as: WSCUSUM h=638.5, TCUSUM h=62.63, DTCUSUM h=18300, DTCUSUM-n h=0.28, NEWMA L=17.26; Phase I back-testing shows TCUSUM/NEWMA raise alerts while WSCUSUM does not, and in Phase II (Nov 11–15, 2019) WSCUSUM and TCUSUM signal at the 4th observation while DTCUSUM signals at the 19th and DTCUSUM-n/NEWMA do not show sustained increase.","The authors note that estimating the autocovariance terms \gamma(B) for B\le Bmax can require a large Phase I sample size m, especially when Bmax and the number of nodes are large, motivating future work for small-m decorrelation. They also acknowledge that while the method is designed to be robust for complex serial correlation and small shifts, it is not as sensitive as conventional control charts for detecting large shifts, suggesting a need to better balance robustness and sensitivity.","The approach relies on user-specified/estimated out-of-control mean vectors \mu_{1,i} (or shift direction) and covariance matrices \Sigma_i; in many real network-monitoring settings these are unknown or time-varying, which can degrade performance. The decorrelation assumes short-memory dependence with a finite lag cutoff and uses Phase I-based autocovariance estimation; if dependence is long-range, nonstationary, or changes after the shift, the “asymptotically uncorrelated” statistic may be misspecified. The weighted aggregation \sum_i w_i^{(t)}\tilde C_{i,t} may mask localized anomalies in small but critical nodes (low weight) and provides limited diagnostic localization unless additional node-level outputs are monitored.","They propose developing a decorrelation method for settings with small in-control sample size m, since large m is needed to estimate \gamma(B) reliably when Bmax and network size are large. They also suggest improving sensitivity for large shifts while maintaining robustness under complex serial correlation, i.e., designing a method that better balances robustness and sensitivity.","Provide guidance/automatic selection for Bmax and the weighting scheme (or robust alternatives) and study sensitivity of ARL to these design choices. Extend the method to handle missing/irregularly sampled network snapshots and to provide built-in diagnostics to identify which nodes/rows/edges drive a signal. Develop and release software (e.g., an R/Python implementation) with reproducible simulation scripts and practical parameter-estimation procedures for \mu_{0,i}, \Sigma_i, and \mu_{1,i} when only limited Phase I data are available.",2111.02653v2,local_papers/arxiv/2111.02653v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:36:19Z
TRUE,Univariate|Other,Shewhart|EWMA|CUSUM|Change-point|Other,Phase II,Theoretical/simulation only,TRUE,FALSE,FALSE,Simulation study|Markov chain|Integral equation|Approximation methods|Other,ARL (Average Run Length)|Conditional expected delay|Steady-state ARL|Other,Not discussed,TRUE,R,Package registry (CRAN/PyPI),https://cran.r-project.org/package=spc,"This preprint critically reassesses “synthetic-type” control charts (2-of-(H+1) run/scan-rule charts), arguing that their proclaimed detection superiority is largely an artifact of head-start initialization and of focusing on zero-state ARL. Under an i.i.d. normal mean-shift change-point model with known and constant variance (in-control ARL calibrated to 500), the paper compares eight synthetic-type variants (with and without head-start) against standard EWMA charts, and also examines Shewhart-combination (“improved”) versions. Performance is evaluated using conditional expected delay (CED) profiles and both zero-state and conditional steady-state ARL; Markov-chain models provide exact results for synthetic-type charts and numerical approximations are used for EWMA and combo charts. Results show EWMA (notably with λ≈0.25) uniformly dominates synthetic-type charts in steady-state performance, especially for small-to-moderate shifts, while synthetic-type head-start versions can look good only for very early changes. The paper also clarifies confusions and errors in parts of the literature regarding steady-state ARL computation and emphasizes that steady-state (or worst-case) measures should be central in assessing monitoring designs.","The EWMA statistic is defined by $Z_0=\mu_0$ and $Z_i=(1-\lambda)Z_{i-1}+\lambda X_i$, with a signal when $|Z_i-\mu_0|>c_E\sqrt{\frac{1-(1-\lambda)^{2i}}{2-\lambda}}\,\sigma_0$ (variable limits). The mean-shift change-point model is $\mu_t=\mu_0$ for $t<\tau$ and $\mu_t=\mu_1=\delta$ for $t\ge \tau$ with known $\sigma=\sigma_0=1$. For Markov-chain chart models with transient-state matrix $Q$, the ARL vector is $\ell=(I-Q)^{-1}\mathbf{1}$ and conditional/cyclical steady-state ARLs are computed as $D_i=\psi_i^\top\ell$ for appropriate quasi-stationary/restart vectors $\psi_i$. For the “true” synthetic chart, signal probability is $p(k;\delta)=1-[\Phi(k-\delta)-\Phi(-k-\delta)]$ with $q=1-p$ and explicit formulas for $\ell$ and steady-state vectors are given.","With in-control ARL fixed at 500, CED profiles for head-start synthetic-type charts show pronounced maxima around $\tau=H+1$ and large gaps between zero-state and steady-state performance, indicating that zero-state ARL can be misleading for head-start designs. Across shifts (e.g., $\delta=1,2,3$), the EWMA chart (especially with $\lambda=0.25$) yields CED/steady-state ARL curves below all synthetic-type counterparts for most change-point locations, with synthetic-type charts only competitive for very early changes (roughly $\tau\le 3$) and larger shifts (e.g., $\delta\ge 2$). ARL-envelope comparisons for the #4 synthetic-type charts (R4/S4) optimized over $H\in\{1,\dots,200\}$ show EWMA($\lambda=0.25$) uniformly dominates in conditional steady-state ARL over $0<\delta\le 5$. Adding a Shewhart rule improves large-shift performance ($\delta\gtrsim 3$) for both synthetic-type and EWMA combos, but Shewhart–EWMA remains better for small shifts ($\delta\le 2$). The paper also notes that commonly used (and in some papers incorrect) steady-state vector formulas yield only small numerical differences in a demonstrated in-control example (H=3, k≈2.2238 giving steady-state ARLs around 536 vs. zero-state 500/538 depending on initialization).",None stated,"The empirical conclusions are based on an idealized i.i.d. normal mean-shift model with known constant variance; results may differ under autocorrelation, variance shifts, heavy tails, or parameter estimation effects typical of Phase I/II practice. The study focuses primarily on ARL/CED-based criteria and does not address economic design, diagnostic interpretability, or robustness to model misspecification beyond the considered scenarios. Comparisons for EWMA and combo charts rely on numerical approximation routines (not exact Markov chains as for synthetic-type charts), so very small differences between close competitors could depend on approximation settings.",None stated,"Extend the critique/comparison to settings with autocorrelation (e.g., ARMA residual charts), non-normal data, and joint mean–variance changes to assess whether synthetic-type designs retain any niche advantages under realistic departures from i.i.d. normality. Incorporate Phase I parameter estimation uncertainty and self-starting implementations to compare practical in-control/out-of-control performance, including false-alarm reinitialization policies. Provide open, reproducible benchmarking code/scripts and standardized testbeds (including real case studies) to facilitate transparent comparisons across modern chart proposals.",2112.02641v1,local_papers/arxiv/2112.02641v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:36:59Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Healthcare/medical,TRUE,FALSE,FALSE,Markov chain|Economic design|Other,Other,Uses an X-chart setup with fixed sample size N = 1; focuses on optimizing sampling interval h and one-sided critical value K. No Phase I reference-sample size recommendations are discussed.,NA,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=Markovchart,"The paper develops closed-form expressions for the expected squared deviation from target (integrated over a sampling interval) when process mean shifts follow special mixture distributions, within a Markov chain-based economic design framework for control charts. It considers a univariate, one-sided X-chart setup with sample size N=1 and optimizes chart parameters (critical value K and sampling interval h) for cost optimality, motivated by healthcare monitoring. The main technical contribution is a closed-form formula for the key cost term based on a Taguchi-type squared loss when the shift-size distribution is a two-component mixture (illustrated with exponential–geometric mixtures, and then generalized to arbitrary two-component mixtures with finite moments). The framework assumes Poisson shift arrivals and derives expressions needed for average cost calculations used in Markov-chain economic design. The authors state that these formulas speed up computations in the Markovchart R package and enable handling non-trivial mixture shift distributions in cost-optimal chart design.","The expected cost per unit time is expressed as $E(C)=C\cdot P$, where $C$ is a vector of per-state costs and $P$ is the stationary distribution from a discretized Markov chain. The key derived quantity is $C^2_{h,j}=\int_{t_0}^{t_0+h} \mathbb{E}\{(H_{t_0,j}(t))^2\}\,dt$, i.e., the integral over a sampling interval of the expected squared shift (Taguchi loss). For an exponential–geometric mixture shift model, the paper derives a closed form for $\mathbb{E}[(X+JY+j)^2]$ and then for $C^2_{h,j}$; it also provides a general two-component-mixture result $C^2_{h,0}=\tfrac{1}{6}h^2 s\big(3(m_X^2+v_X+\zeta(m_Y^2+v_Y-m_X^2-v_X))+2h(m_X-\zeta(m_X-m_Y))^2 s\big)$.","A closed-form expression is obtained for the integrated squared deviation term $C^2_{h,j}$ under Poisson shift arrivals and an exponential–geometric mixture shift-size distribution, replacing numerical convolution/integration for this cost component. The paper also derives a general closed form for $C^2_{h,0}$ for any two-component mixture where the stacked-shift components have finite means and variances, showing the dependence on $(m_X,v_X,m_Y,v_Y)$ and mixture weight $\zeta$. As a consistency check, the general formula reduces to the known pure-exponential case when $\zeta=0$ (recovering $h^2 s\delta(\delta+hs\delta/3)$ under their parametrization) and becomes independent of $\zeta$ when the two components have identical first two moments. The authors claim these formulas speed up Markovchart computations (no run-length/ARL numerical comparisons are reported in the note).","The note focuses on deriving closed forms for a main term in the cost calculations and does not detail the discretisation/implementation aspects of the Markov-chain algorithm, stating that the theoretical results presented do not need discretisation. It also treats special cases (e.g., Poisson shift arrivals; particular mixture constructions) to demonstrate the approach before giving a two-component finite-moment generalization.","The work is largely theoretical and does not present simulation studies, ARL/ATS/false-alarm operating characteristics, or empirical case studies validating the impact of mixture shifts on overall chart performance or cost in realistic settings. The framework assumes independent shifts, known shift-size distribution and parameters, and (in the motivating setup) normal measurement error; robustness to misspecification, autocorrelation, and nonstationary shift intensities is not assessed. The mapping from the derived cost term to fully optimized chart designs depends on the discretized-state Markov chain and other cost components (e.g., false alarms/repairs), which are not quantified here, limiting direct practical guidance on parameter choices beyond enabling faster computation.","The authors state they plan to return to implementation of these results later, and suggest the formulas have potential applications to other mixture cases such as normal and Fréchet mixtures (light-tailed + heavy-tailed shift components).","Extending the derivations to settings with autocorrelated observations or time-varying shift intensities (non-homogeneous Poisson or renewal processes) would improve applicability in healthcare monitoring. Providing full economic-design case studies (including optimized $(K,h)$, sensitivity analysis to mixture parameters, and comparisons versus non-mixture assumptions) would quantify practical benefits. Developing/benchmarking run-length properties (in-control false alarm behavior and out-of-control detection delay) under mixture shifts, and offering robust or Bayesian parameter-uncertainty handling for shift-size distributions, would strengthen deployment guidance.",2112.05940v1,local_papers/arxiv/2112.05940v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:37:36Z
TRUE,Multivariate|High-dimensional|Other,EWMA|GLR (Generalized Likelihood Ratio)|Other,Both,Semiconductor/electronics|Manufacturing (general),NA,FALSE,TRUE,Simulation study|Case study (real dataset),ARL (Average Run Length)|Other,"Simulation uses sample size N = 100 per sample and EWMA smoothing parameter λ = 0.1. In the case study Phase II, the subgroup/sample size is N = 4 (46 samples total). Phase I size is described via 1,463 in-control observations used to estimate IC probabilities (after dichotomization) but no general minimum-sample guidance is given.",TRUE,None / Not applicable,Not provided,http://archive.ics.uci.edu/ml/datasets/SECOM,"The paper proposes a global monitoring scheme for simultaneously monitoring a large number (high-dimensional) heterogeneous categorical data streams, allowing both nominal streams (multinomial probability-vector changes) and ordinal streams (latent continuous location shifts). For each stream it constructs an EWMA-smoothed likelihood ratio test (LRT) statistic: a multinomial LRT for nominal data and a 1-df ordinal LRT based on an ordinal log-linear/latent-variable formulation for ordinal data. To remove heterogeneity across streams (different numbers of levels and parameters), each local EWMA-LRT statistic is transformed via its asymptotic chi-square CDF to an approximately i.i.d. Uniform(0,1) variable in control. The set of p uniformized statistics at each time is then aggregated using Zhang’s (2002) powerful likelihood-ratio goodness-of-fit (GOF) test to form the global charting statistic T_k, which signals when T_k exceeds a control limit chosen to achieve a target in-control ARL. Simulations (ARL comparisons vs max- and sum-aggregation competitors) show T_k is generally best or near-best across sparse and dense change regimes, and a SECOM semiconductor case study demonstrates Phase II signaling on dichotomized sensor streams.","Nominal local LRT for stream i at sample k: $R_{ik}=2\sum_{j=1}^{h_i} n_{ijk}\ln\{n_{ijk}/(N\pi^{(0)}_{ij})\}$. Ordinal local LRT: $R_{ik}=(\alpha_i^\top n_{ik})^2/(N\,\alpha_i^\top\Lambda_i\alpha_i)$ with $\Lambda_i=\mathrm{diag}(\pi_i^{(0)})-\pi_i^{(0)}(\pi_i^{(0)})^\top$ and (under normal latent variable) $\alpha_{ij}=[\phi(\Phi^{-1}(c^{(0)}_{i,j-1}))-\phi(\Phi^{-1}(c^{(0)}_{ij}))]/\pi^{(0)}_{ij}$. EWMA counts: $w_{ik}=(1-\lambda)w_{i,k-1}+\lambda n_{ik}$, substituted into the LRT forms to get $A_{ik}$. Normalization: $U_{ik}=F_{\chi^2_{df(i)}}\big((2-\lambda)A_{ik}/\lambda\big)\approx U(0,1)$, then global GOF statistic $T_k$ is Zhang (2002) LR-GOF function of ordered $U_{(i)k}$ and signals if $T_k>L$.","Simulation design uses $p=1000$, $N=100$, $\lambda=0.1$, and sets $\mathrm{ARL}_0=370$ (control limits found via bisection); performance is compared by out-of-control ARLs against $Q_k=\max_i U_{ik}$ and $S_k=\sum_i U_{ik}$. Across multiple nominal-only, ordinal-only, and mixed nominal/ordinal scenarios, the proposed $T_k$ yields smaller (better) OC ARLs in most configurations, while $Q_k$ tends to be better only for very sparse changes and $S_k$ for very dense changes. In the SECOM case study (461 dichotomized streams; Phase II sample size $N=4$; target $\mathrm{ARL}_0=500$ with log control limit 3.889), the chart signals at the 22nd sample and stays above the limit thereafter. Reported tables provide many OC ARLs (with standard errors) demonstrating these comparative patterns for various shift magnitudes and numbers of affected streams.","The authors note that while the global statistic is sensitive to various changes, diagnosing which streams are out-of-control and identifying root causes remains an open problem. They also assume all categorical data streams shift at the same time, which may not hold in practice.","The method relies on (approximate) chi-square calibration of the EWMA-LRTs and independence across streams to justify i.i.d. Uniform(0,1) inputs; performance under strong cross-stream dependence could deviate without additional modeling/calibration. For ordinal streams the implementation assumes normally distributed latent variables (or suggests logistic as an alternative), so misspecification of latent distributions/thresholding could affect power and false-alarm control. Control limits are obtained by Monte Carlo/bisection but no ready-to-use analytic limit or packaged implementation is provided, which may hinder deployment for different p, level structures, or dependence settings.",They propose future research on diagnosing out-of-control streams and root-cause identification after a global alarm. They also suggest relaxing the assumption that all streams shift simultaneously and developing efficient monitoring for a large number of categorical streams with multiple change-points.,"Developing post-signal localization procedures (e.g., p-value/FDR-based identification on $U_{ik}$ with multiplicity control) would make the method more actionable. Extending the framework to explicitly handle cross-stream and within-stream dependence (e.g., copula/graphical models or block bootstrap calibration) and providing robust calibration when the chi-square approximation is poor (small N, rare categories) would strengthen reliability. Providing open-source software and guidance for choosing $\lambda$, N, and updating/estimating IC probabilities (Phase I uncertainty) would improve practical uptake.",2112.09077v1,local_papers/arxiv/2112.09077v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:38:21Z
TRUE,Univariate|Other,EWMA,Phase II,Food/agriculture|Theoretical/simulation only,FALSE,FALSE,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length),"Focuses on small subgroup sizes in Phase II monitoring; simulations use n = 5, 10, 15, 20 per time point (with T = 5000). Real-data example uses n = 50 cans per sample (30 samples, with first 24 treated as IC).",TRUE,R,Public repository (GitHub/GitLab),https://github.com/lchen723/SPC-ME-R-code.git,"The paper proposes a measurement-error-corrected EWMA p-control chart for monitoring the proportion of nonconforming items when binary observations are subject to misclassification. Using a 2×2 misclassification (classification) matrix Π with misclassification probabilities (π10, π01), the authors derive a corrected proportion and corrected EWMA statistic whose expectation matches the true in-control proportion p0. The resulting corrected EWMA p-chart has asymmetric control limits (with LCL set to 0 and an adjusted UCL) and is positioned as suitable for small subgroup sizes where normal approximations for proportions are unreliable. Performance is evaluated primarily via Monte Carlo estimation of control-limit constants to achieve a target ARL0 (e.g., 370) and comparison of ARL1 versus a naive (uncorrected) EWMA p-chart, showing the naive chart can have substantially worse detection (larger ARL1) under misclassification while the corrected chart closely matches the performance with true (unobserved) data. A real-data illustration on orange-juice can nonconformance data (from R package qcr) demonstrates how sensitivity-analysis choices for Π affect limits and signals, and shows the corrected chart can detect out-of-control points more reliably across smoothing parameters.","Observed (misclassified) IC proportion satisfies $p_0^* = \pi_{11}p_0 + \pi_{10}(1-p_0)$. The proposed correction inverts the misclassification effect to define $p_0^{**} = \dfrac{p_0^* - \pi_{10}}{1-\pi_{10}-\pi_{01}}$ and $X_{it}^{**}=\dfrac{X_{it}^* - \pi_{10}}{1-\pi_{10}-\pi_{01}}$. The corrected EWMA statistic is $\text{EWMA}_{0,t}^{**}=\lambda \hat p_{0,t}^{**}+(1-\lambda)\text{EWMA}_{0,t-1}^{**}$ with variance $\operatorname{Var}(\text{EWMA}_{0,t}^{**})=\dfrac{p_0^*(1-p_0^*)\lambda\{1-(1-\lambda)^{2t}\}}{n(1-\pi_{10}-\pi_{01})^2(2-\lambda)}$, yielding an adjusted UCL $\text{UCL}^{**}=p_0^{**}+L^{**}\sqrt{\operatorname{Var}(\text{EWMA}_{0,t}^{**})}$ (and LCL set to 0).","Across extensive simulations (e.g., ARL0 fixed at 370; n = 5–20; λ = 0.05 or 0.2; misclassification set to π00=π11=π with π=0.95 or 0.99), the corrected chart’s UCL values essentially match those obtained using the true (unobserved) data, while the naive misclassified chart produces wider limits (larger UCL*). For ARL1, the naive chart consistently yields larger ARL1 than both the true-data benchmark and the corrected chart, with the gap especially pronounced for small shifts (δ = 0.1) and small n (e.g., Table 5 shows ARL1≈208 vs ARL1*≈252 for p0=0.05, n=5, δ=0.1 under π=0.95, λ=0.05). In contrast, corrected ARL1 values are close to the true-data ARL1 throughout the tables, supporting the claim that the correction recovers detection performance under misclassification. In the orange-juice can example, the naive chart yields larger limits than the corrected chart (e.g., Table 9: for λ=0.05, UCL* = 0.126 vs UCL** = 0.080 when π=0.95; for λ=0.20, UCL* = 0.151 vs UCL** = 0.101 when π=0.95), and the corrected chart is less sensitive to λ in detecting out-of-control points.","The authors note that the misclassification matrix Π is typically unknown in practice; much of the paper’s implementation relies on sensitivity analysis by specifying Π at different levels. They state that Π can be estimated only if auxiliary information (e.g., external validation data) is available, otherwise one must explore plausible Π values.","The main development is for Phase II with the in-control proportion treated as known; practical deployment often requires Phase I estimation of p0 and possibly Π, which would add extra uncertainty and may affect achieved ARL0. The approach assumes constant misclassification probabilities over time and across inspectors/equipment; time-varying or state-dependent misclassification could degrade performance. The proposed correction can yield corrected values outside [0,1] at the observation/proportion level (since $X_{it}^{**}$ is an affine transform of $X_{it}^*$), which may complicate interpretation and boundary handling in practice and is not discussed. Comparisons focus on a naive EWMA p-chart; broader comparisons to alternative robust/bayesian/misclassification-aware monitoring approaches are not provided.","They propose (i) exploring SIMEX as an alternative measurement-error correction strategy for SPC, (ii) extending the corrected EWMA p-chart idea to distribution-free continuous-variable monitoring methods that convert observations to binary indicators, and (iii) extending to profile monitoring settings such as logistic regression profiles where similar correction ideas could be applied for monitoring nonconforming products.","Developing a full Phase I/Phase II framework that jointly estimates p0 and Π (with uncertainty propagation) and provides control-limit adjustments to maintain nominal ARL0 would broaden practical applicability. Extending the method to handle autocorrelated/clustered binary data (e.g., repeated items, time dependence) and to adaptive or time-varying misclassification rates would address common real monitoring conditions. Providing an exact/Markov-chain-based ARL approximation (beyond Monte Carlo calibration) could reduce computation and improve design transparency. Packaging the method into a documented R package with functions for Π estimation from validation samples and for sensitivity analysis workflows would improve adoption.",2203.03384v1,local_papers/arxiv/2203.03384v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:39:13Z
TRUE,Multivariate|Other,MEWMA,Phase II,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,NA,Markov chain|Simulation study|Other,ATS (Average Time to Signal)|ARL (Average Run Length)|Other,Not discussed (examples/scenarios include n = 1 and p = 3; monitoring is described for samples of size n collected each period).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Phase II variable-sampling-interval (VSI) MEWMA control chart for monitoring compositional data (VSI MEWMA-CoDa) using the isometric log-ratio (ilr) transformation to map compositions from the simplex to an unconstrained Euclidean space. The chart monitors shifts in the compositional center (mean) by applying a MEWMA to ilr-transformed sample means and signaling when a quadratic form exceeds an upper control limit; a warning limit determines whether the next sampling interval is long or short. Average Time to Signal (ATS) for the VSI design is computed using a Markov-chain approach (with a 1D chain for in-control ATS and a modified 2D chain for out-of-control ATS). The authors present an optimization procedure to select the smoothing constant, control limit, and long sampling interval to minimize out-of-control ATS subject to constraints on in-control ATS and average sampling interval. Numerical comparisons show the VSI MEWMA-CoDa consistently reduces out-of-control ATS relative to the fixed-sampling-interval MEWMA-CoDa benchmark, with larger gains for small-to-moderate shifts and shorter short-interval settings.","Compositional observations are transformed via $\mathrm{ilr}(x)=\mathrm{clr}(x)B^\top$ to $\mathbb{R}^{p-1}$. The MEWMA recursion is $W_i=r(\bar X_i^*-\mu_0^*)+(1-r)W_{i-1}$, and the plotted statistic is $Q_i=W_i^\top \Sigma_{W_i}^{-1}W_i$ with $\Sigma_{W_i}=\frac{r}{n(2-r)}\Sigma^*$ (asymptotic form). VSI sampling uses a warning limit UWL and UCL: use $h_L$ if $Q_i\le \mathrm{UWL}$ and $h_S$ if $\mathrm{UWL}<Q_i\le \mathrm{UCL}$; signal if $Q_i>\mathrm{UCL}$. ATS is obtained by Markov-chain formulas $\mathrm{ATS}=s^\top(I-P)^{-1}h$ (with appropriate 1D/2D constructions).","In the reported comparison scenario (notably $n=1$, $p=3$, $\mathrm{ATS}_0=200$, $E_0(h)=1$, and $h_S\in\{0.1,0.5\}$), the VSI MEWMA-CoDa yields uniformly smaller out-of-control ATS than the fixed-sampling MEWMA-CoDa ARL/ATS benchmark across shifts $\delta\in\{0.25,0.5,\ldots,2.0\}$. For example, at $\delta=0.25$ the FSI value is 64.6 versus VSI ATS1 = 56.8 for $h_S=0.1$ (and 63.5 for $h_S=0.5$). At $\delta=0.5$, VSI ATS1 is 19.9 for $h_S=0.1$ versus 26.4 (FSI) and 23.5 for $h_S=0.5$. Gains diminish for large shifts (e.g., $\delta\ge 1.75$) but VSI remains better (e.g., at $\delta=2.0$, 2.4 vs 3.5 for $h_S=0.1$).",None stated.,"The method relies on a multivariate normal model for ilr-transformed compositions and (implicitly) independent sampling over time; robustness to non-normality, heavy tails, and serial dependence is not evaluated. Performance evidence is primarily numerical (Markov-chain-based computations) with no real industrial case study, so practical impact for specific applications is uncertain. The design/optimization is illustrated for limited scenarios (e.g., small p such as p=3 and particular $h_S$ choices), and guidance for selecting the ilr basis/contrast matrix B in practice is limited despite its impact on $\mu^*$ and $\Sigma^*$.","The authors suggest extending the approach to a VSI MCUSUM-CoDa chart, studying the impact of measurement error on these charts, and further investigating alternative transformations of compositional data prior to charting. They also note that online monitoring of compositional data in real-life applications is a worthwhile direction.","Develop robust/nonparametric or heavy-tail-resistant variants of VSI MEWMA-CoDa (e.g., based on robust covariance estimation in ilr space) and assess sensitivity to the choice of ilr basis. Extend the method to handle autocorrelated compositional streams (e.g., via residual charts after compositional time-series modeling) and to high-dimensional compositions (large p) with regularized covariance. Provide open-source software and more extensive real-data case studies (e.g., food/chemical compositions) with diagnostic tools for identifying which parts drive an out-of-control signal.",2203.15438v1,local_papers/arxiv/2203.15438v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:39:39Z
TRUE,Univariate|Other,Other,Phase II,Other|Theoretical/simulation only,FALSE,NA,FALSE,Simulation study|Other,False alarm rate|Other,Not discussed,TRUE,C/C++|Other,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher),https://github.com/Khaled-Janada/AC_Charts|https://doi.org/10.5281/zenodo.6512066,"The paper proposes the Angular Control Chart (ACC), a probability-limits time-between-events (TBE) control-chart design for monitoring reliability of fully repairable multi-state systems (MSSs) with multiple state transitions. Instead of plotting raw time-to-failure (TTF) on a single t-chart (which cannot share limits across different state-transition rates), ACC maps each observed TTF to an angle $\theta=\arctan(T_C/t)$ on a state-specific horizontal “state line” located at the median $T_C$. Control limits are expressed as Angular Control Limits (ACL/ALCL/AUCL) derived from distribution quantiles using an acceptable false-alarm probability (default $c=0.27\%$), yielding a standard ACC when distributions share form/shape and a generalized ACC with state-specific zig-zagging ACL segments when shapes differ. The method supports several nonnegative continuous distributions (exponential, Weibull/Rayleigh, lognormal, Fr\u00e9chet; gamma numerically) and is demonstrated via simulated MSS examples including cumulative TTF over $r$ failures (an ACC analogue of a $t_r$-chart). The authors provide open-source C# software (AC Charts Software) to implement ACCs.","ACC plots each observation using the angle $\theta=\arctan(T_C/t)$ where $t$ is the observed TTF and $T_C=F^{-1}(1/2)$ is the median of the assumed in-control distribution. Angular control limits use quantile ratios $\rho(a,b)=F^{-1}(a)/F^{-1}(b)$ with $T_L=F^{-1}(c/2)$ and $T_U=F^{-1}(1-c/2)$, giving $\theta_C=45^\circ$, $\theta_L=\arctan(\rho(1/2,c/2))$, and $\theta_U=\arctan(\rho(1/2,1-c/2))$ (standard design). For exponential transitions $F^{-1}(p)=-\alpha\ln(1-p)$ so ACL angles depend only on $c$ (e.g., with $c=0.27\%$, $\theta_L\approx 89.89^\circ$, $\theta_U\approx 5.99^\circ$); for Weibull/lognormal/Fr\u00e9chet, ACLs depend on the shape parameter, and for gamma they are obtained numerically.","With $c=0.27\%$ on a linear scale, the exponential ACC has distribution-scale-independent limits $\theta_L\approx 89.89^\circ$ and $\theta_U\approx 5.99^\circ$ (and Rayleigh limits listed as $\theta_L\approx 87.47^\circ$, $\theta_U\approx 17.95^\circ$). For Weibull and Fr\u00e9chet distributions, the paper derives closed-form expressions showing ACL angles depend on the shape parameter $\beta$ (e.g., Weibull: Eqs. (16)\u2013(17); lognormal: $\theta_L=\arctan(\exp(3\beta))$, $\theta_U=\arctan(\exp(-3\beta))$ when $c=0.27\%$). Simulation examples show the ACC can flag state-specific improvements/degradations via points beyond AUCL/ALCL and can also indicate overall system change via imbalance of points above vs. below the 45\u00b0 center line (e.g., Example III reports 33/50 points above ACL, interpreted as overall improvement). Example II (cumulative TTF over $r=2$) produced fewer out-of-control points than Example I, suggesting potential sensitivity changes when aggregating over $r$ failures.","The authors note that when ACC is used to monitor cumulative TTF every $r$ occurrences (an analogue to the $t_r$ concept), further investigation is needed on how $r$ affects ACC sensitivity, and this use is restricted to distributions with a limiting sum distribution. They also state two obstacles remain under study: clarifying the chronological order of failure events and handling potential coincidences where two or more observation points overlap on the chart.","The method’s design and decision rules rely on choosing and fitting an in-control parametric distribution (and, in the generalized design, state-specific shape/location), but the paper does not fully address parameter-estimation uncertainty (Phase I) and its impact on false-alarm rates in Phase II. Performance evaluation is largely illustrative/simulation-based without comprehensive run-length metrics (e.g., ARL/ATS) across a grid of shifts, so comparative efficacy versus established TBE charts (EWMA/CUSUM/group-runs) is not rigorously quantified. Practical implementation may be challenging for high numbers of states due to chart crowding and the need for reliable state assignment and repair assumptions (instant repair, no minor failures), which may not hold in many MSS applications.",They propose studying the effect of the parameter $r$ (when monitoring cumulative TTF every $r$ occurrences) on the sensitivity of the ACC. They also indicate ongoing work to address two practical issues: representing the chronological order of failures and resolving overlapping/coincident plotted points.,"A natural extension is a formal Phase I/Phase II framework for ACCs, including robust/biased-corrected estimation of distribution parameters per state and adjusted limits that preserve nominal false-alarm probabilities under estimation error. Additional work could provide standard SPC performance summaries (ARL/ATS/SDRL, steady-state ARL) and head-to-head comparisons against memory-type TBE charts (CUSUM/EWMA) and nonparametric alternatives for MSS reliability data. Software extensions could include automated distribution selection/diagnostics, handling censoring/truncation common in reliability data, and methods for autocorrelated or condition-dependent failures.",2205.02024v1,local_papers/arxiv/2205.02024v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:40:29Z
TRUE,Multivariate|High-dimensional|Other,CUSUM|Change-point|Other,Phase II|Both,Healthcare/medical,FALSE,NA,TRUE,Simulation study|Case study (real dataset),ARL (Average Run Length)|False alarm rate|Expected detection delay|Other,Case study treats the first 15 years as in-control (Phase I) to set control limits to achieve ARL0 = 50 via Monte Carlo simulation; no general Phase I sample size recommendations beyond this setting.,TRUE,R|MATLAB,Not provided,https://www.cdc.gov/mmwr/mmwr_nd/index.html|https://wonder.cdc.gov/nndss/nndss_annual_tables_menu.asp|https://www.mathworks.com/matlabcentral/fileexchange/27374-b-splines|http://triad.gatech.edu,"The paper proposes Poisson-assisted Smooth Sparse Tensor Decomposition (PoSSTenD) for rapid detection and localization of sparse spatio-temporal “hot-spots” in multivariate count data (e.g., infectious disease counts across states, diseases, and years). Counts are modeled as Poisson with mean equal to population size times an infection rate, and the log-rate is additively decomposed into a smooth global trend (modeled via Tucker decomposition with spline bases) plus sparse local hot-spots (modeled via identity bases and LASSO sparsity). Hot-spot occurrence is detected sequentially by constructing a CUSUM chart on a standardized statistic derived from Pearson residuals projected in the estimated hot-spot direction, with penalty parameter selected from a grid and standardized using Phase I estimates. Hot-spot locations (state × disease at the alarm time) are identified from nonzero/large entries of the estimated hot-spot tensor slice, optionally using thresholding (the paper illustrates order-thresholding). Performance is validated with 1000 Monte Carlo replications (reporting ARL1 and localization precision/recall/F-measure) and a real CDC dataset where PoSSTenD signals a hot-spot year (2017) and produces sparse localized outbreaks compared to several baselines.","Observation model: $Y_{i,j,t}\sim\text{Poisson}(R_{i,j,t}N_{i,j,t})$ with log-rate decomposition $\log R_{i,j,t}=U_{i,j,t}+H_{i,j,t}$. Global trend and hot-spots use Tucker forms $U=\mathcal\vartheta_m\times_1 B_{m,1}\times_2 B_{m,2}\times_3 B_{m,3}$ and $H=\mathcal\vartheta_h\times_1 B_{h,1}\times_2 B_{h,2}\times_3 B_{h,3}$, leading to a vectorized Poisson GLM with design matrices $(B_{m,1}\otimes B_{m,2}\otimes B_{m,3})$ and $(B_{h,1}\otimes B_{h,2}\otimes B_{h,3})$. Parameters are estimated by penalized negative log-likelihood with L1 penalty on hot-spot coefficients: $F(\theta_m,\theta_h)=\sum_i\{-y_i(x_i^\top\theta_m+z_i^\top\theta_h)+n_i\exp(x_i^\top\theta_m+z_i^\top\theta_h)\}+\lambda\|\theta_h\|_1$. Detection uses standardized test statistic and one-sided CUSUM: $W_t^+=\max\{0, W_{t-1}^+ + \tilde P_t^+(\lambda_t^*)-d^*\}$, alarm if $W_t^+>L$.","In simulations matched to the motivating dimensions (49 states × 10 diseases × 26 years) with sparse hot-spots after $\tau=15$, PoSSTenD achieves short detection delays: ARL1 up to 2.015 (increasing population) and 4.278 (decreasing population) for small hot-spots $\delta=0.05$, and ARL1 ≈ 1.0 for large hot-spots $\delta=0.2$ in both scenarios. Localization accuracy varies with signal size; e.g., for decreasing population and $\delta=0.2$, PoSSTenD reports precision 81.13%, recall 53.52%, and F-measure 64.41%. Baselines (e.g., PCA monitoring and Hotelling $T^2$) have substantially larger ARL1 or fail to signal within the study horizon; LASSO-only approaches struggle to separate global trend from sparse hot-spots. In the CDC case study (10 diseases, 49 states, 1993–2018), with control limits calibrated to ARL0 = 50 using the first 15 years as in-control, PoSSTenD signals in 2017 and yields sparse state×disease hot-spot localizations.","The authors note that the CUSUM optimality guarantee is for normally distributed data; for non-normal data (as here) optimality may not hold though CUSUM remains reasonable. For localization, they acknowledge potential high false positive rate from nonzero estimates and propose using significance testing or thresholding (they use thresholding in the paper). They also indicate the paper’s focus is detection/localization rather than model fitting/prediction, and do not compare to general GLM/ARMA forecasting approaches.","Control limits and standardization of the detection statistic rely on Phase I assumptions (first 15 years in control) and Monte Carlo calibration; robustness to Phase I contamination or limited Phase I length is not thoroughly analyzed. The approach depends on correct specification/availability of exposure offsets $N_{i,j,t}$ and Poisson variance structure; overdispersion (negative binomial), zero inflation, or reporting artifacts could degrade performance without explicit accommodation. Temporal dependence is not explicitly modeled in the observation noise beyond the smooth trend bases, so residual serial correlation could affect false alarm rates and CUSUM calibration. Computational complexity and scalability to much larger tensors (e.g., daily county-level data with many categories) is implied but not benchmarked with runtime/memory results.","The paper states the method can be extended from order-3 tensors to order-$d$ tensors ($d\ge3$) by adding corresponding dimensions and bases, highlighting this as a key advantage. It also notes that if clustered hot-spots are of interest, spline bases for the hot-spot component (instead of identity) could be used. For localization, the authors suggest improving false positive control via formal significance testing or alternative thresholding strategies.","Extend the Poisson model to handle overdispersion/zero inflation (e.g., negative binomial or quasi-Poisson) and assess impact on ARL calibration. Develop theory or fast approximations for ARL0/ARL1 (beyond Monte Carlo) and study robustness under Phase I estimation error/contamination. Incorporate explicit autocorrelation or state-space structure for the global trend and residuals, with corresponding control limit adjustments. Provide open-source software (e.g., R/Python package) and runtime benchmarks, and explore adaptive/self-starting monitoring where the smooth trend is updated online without a fixed Phase I window.",2205.10447v2,local_papers/arxiv/2205.10447v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:41:19Z
TRUE,Profile monitoring|Nonparametric,Other|Change-point,Both,Manufacturing (general)|Healthcare/medical|Transportation/logistics|Other,FALSE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate,"Uses Phase I historical profiles of size $m$ (known IC) and monitoring window size $w\le m$; simulations vary $m\in\{20,40\}$ with $w\in\{10,20,40\}$ (often via ratios $m/w\in\{1,2\}$). Profile size per time $n$ varies (e.g., $n\in\{128,256,512\}$ in quadratic simulations); robot dataset uses $n=90$ with only about $m\approx 11\text{--}13$ historical IC profiles after cleaning and $w\in\{4,5,6\}$.",TRUE,R,Not provided,NA,"The paper proposes an Eigenvector Perturbation (EP) Control Chart for fast, nonparametric profile monitoring where each time point yields a noisy profile $y_t$ observed at random design points $x_{ti}$, aiming to simultaneously achieve extremely large in-control ARL (ARL0) and very small detection delay (ARL1). The method forms a sliding-window sample correlation matrix of the most recent $w$ profiles and monitors the $\ell_2$ distance between its leading eigenvector and the in-control eigenvector $\sqrt{1/w}\,\mathbf{1}$; a change-point (mix of IC/OOC profiles in the window) induces a block-structured correlation matrix with a different leading eigenvector. To avoid failure once the window becomes fully OOC, it replaces the oldest profiles in the window with sampled historical IC profiles and uses the maximum perturbation across several replacement sizes $k_1$. Control limits are calibrated from IC data using a bootstrap/“quantile trick” approach rather than Monte Carlo run-length simulation, enabling targets like ARL0 $>10^6$. Extensive simulations and a robot-arm sensor case study show the EP chart can maintain near-zero FAR with ARL0 exceeding $10^6$ while often achieving ARL1 close to 1 and being computationally faster than competing nonlinear profile monitoring methods.","Profiles follow $y_{ti}=f_t(x_{ti})+\epsilon_{ti}$. At time $t$, form the $w\times w$ sample correlation matrix $R$ of profiles $y_{t-w+1},\dots,y_t$; compute its leading eigenvector $v_1$ and use monitoring statistic $S_t=\|v_1-\sqrt{1/w}\,\mathbf{1}\|_2$ (and in practice $\max_{k_1\in K}\|v^{(k_1)}-\sqrt{1/w}\,\mathbf{1}\|_2$ after replacing $k_1$ oldest profiles with historical IC profiles). Signal when $S_t>U$, where $U$ is an upper control limit estimated from IC bootstraps (fitting a Normal to bootstrapped $S$ values and taking a high quantile).","Simulations report that, with bootstrap-based calibration, the EP control chart achieves ARL0 exceeding $10^6$ (often reported as ARL0 $>5\times 10^6$) while maintaining ARL1 essentially equal to 1 across many scenarios (including cases where the change-point occurs very late, e.g., $\tau=10^4$). In a multivariate-predictor comparison, competing methods calibrated to ARL0 of 200–370 show noticeable ARL1 inflation, whereas EP keeps ARL1 at 1 even when calibrated to ARL0 $>5\times 10^6$ (with observed FAR near 0). Stress tests show degradation under very heavy-tailed errors (few moments), very small $n$, and when IC and OOC functions are highly correlated (e.g., $\rho(f,h)\approx 0.9$) and other signal conditions are weak; EP is robust to moderate within-profile AR(1) correlation. In the robot-arm dataset (n=90; about 18 IC profiles after cleaning), EP achieved ARL1=1 in all simulated scenarios with FAR < 0.02 in the few cases with small $(w,m)$, while PCA had no false alarms but average ARL1 around 1.46–2.12 depending on ARL0 calibration.","The authors note performance can degrade in difficult regimes such as small profile size $n$, small $m$ or $m/w$, low variance ratio $\mathrm{Var}[f]/\sigma^2$, and when the correlation contrast between IC–IC and IC–OOC (or OOC–OOC) profiles is small (e.g., $\rho(f,h)\approx 1$, or cases like $h\propto f$ or $h=f+c$). They also report sensitivity to very heavy-tailed noise: performance is unsatisfactory when errors have fewer than about five moments, aligning with typical EP theory assumptions.","Control-limit calibration relies on a fitted Normal approximation to bootstrapped IC monitoring statistics; if the IC distribution of the maximum-over-$k_1$ statistic is skewed/heavy-tailed, this may miscalibrate FAR/ARL0, especially for extreme quantiles targeting ARL0 $\gg 10^6$. The method’s core signal is a change in cross-profile correlation structure; shifts that do not materially alter correlations (or that change correlations uniformly across the window) may be hard to detect, and diagnostic guidance for identifying the root cause after a signal is limited. The algorithm requires storing and resampling historical IC correlation submatrices and repeatedly computing leading eigenvectors for multiple $k_1$ values, which may become nontrivial for very large $w$ or high-frequency streaming without careful engineering (though claimed fast for tested settings).","The authors suggest extensions including monitoring not just correlations but also scenarios such as $\nu=1$ in their OOC model, improving performance in regimes with small within-profile sample size $n$ (contrasting with methods needing large $m$), and modifying the EP control chart to handle more general autocorrelation structures beyond the equicorrelated error structure considered. They also mention relaxing the need for dependence between predictors $x_r,x_s$ across time points (for $r\ne s$) as a valuable direction.","Developing a distribution-free or extreme-value-based control-limit method (instead of Normal-fit quantiles) could better justify ARL0 calibration at ultra-low FAR levels and improve robustness to IC non-Gaussianity. Extensions to multichannel functional profiles with missing/irregular sampling and explicit handling of concept drift in IC behavior (adaptive updating of the IC reference) would improve practicality in real streaming applications. Providing open-source software (e.g., an R package) and standardized benchmarks against additional profile-monitoring baselines (including modern functional data and high-dimensional monitoring methods) would strengthen reproducibility and adoption.",2205.15422v2,local_papers/arxiv/2205.15422v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:42:07Z
TRUE,Multivariate|Profile monitoring|Functional data analysis|High-dimensional,Hotelling T-squared|Other,Both,Manufacturing (general)|Transportation/logistics,TRUE,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"Notes that small Phase I sample size relative to number of variables can cause overfitting; suggests splitting Phase I into training and tuning sets. In simulations they use Phase I n=4000 (training=1000, tuning=3000); in the real case study Phase I n=919 (training=460, tuning=459).",TRUE,R,Supplementary material (Journal/Publisher),https://doi.org/10.1080/00401706.2024.2327346,"The paper proposes RoMFCC, a robust multivariate functional control charting framework for monitoring multivariate functional (profile) data in Industry 4.0 settings contaminated by both functional casewise and cellwise outliers. The framework combines (i) a functional univariate filter that flags cellwise outliers and replaces them with missing components, (ii) robust multivariate functional imputation of missing values, (iii) robust multivariate functional PCA (RoMFPCA) via ROBPCA on basis coefficients for robust dimensionality reduction, and (iv) Phase II monitoring using Hotelling’s T² and SPE charts on the RoMFPCA model. Performance is evaluated via extensive Monte Carlo studies under contaminated Phase I samples and mean-shift out-of-control conditions, showing RoMFCC maintains controlled false alarm rates and higher detection compared to scalar-feature multivariate charts and non-robust/iterative functional competitors. A real case study on resistance spot welding dynamic resistance curves (10-dimensional functional profiles) demonstrates improved out-of-control detection (reported 72.3% flagged in Phase II) and bootstrap CIs that dominate competing methods. The work advances SPC/profile monitoring by providing an integrated, scalable robustness strategy for simultaneous cellwise+casewise contamination in multivariate functional data.","RoMFCC represents standardized multivariate functional data via basis expansion and robust multivariate FPCA: \(\hat X_i(t)=\hat\mu(t)+\hat D(t)\sum_{l=1}^L \hat\xi_{il}\,\hat\psi_l(t)\). Cellwise outlier filtering uses functional distances \(D^{\mathrm{fil}}_i=\sum_{l=1}^{L_{\mathrm{fil}}}(\hat\xi^{\mathrm{fil}}_{il})^2/\hat\lambda^{\mathrm{fil}}_l\) and flags the largest \(\lfloor n d_n\rfloor\) based on tail discrepancies vs a \(\chi^2\) reference. Missing components are imputed by minimizing a RoMFPCA-based quadratic form leading to \(\hat c_i^{m}=-(C_{m,m})^{+}C_{m,o}c_i^{o}\), with stochastic augmentation \(c_i^{\mathrm{imp}}=\hat c_i^{m}+\varepsilon_i\). Monitoring uses \(T^2=\sum_{l=1}^{L_{\mathrm{mon}}}(\xi^{\mathrm{mon}}_l)^2/\lambda^{\mathrm{mon}}_l\) and \(\mathrm{SPE}=\|Z-\hat Z\|_H^2\), with chi-square limits for \(T^2\) and Jackson–Mudholkar limits for SPE (with Šidák correction for joint signaling).","Simulation: across scenarios with Phase I contamination by functional cellwise or casewise outliers (contamination probability 0.05; additional results at 0.1), RoMFCC shows the highest true detection rates while keeping mean false alarm rates near the nominal \(\alpha=0.05\), and its performance is described as largely insensitive to contamination model/level. Competing approaches (Hotelling-type charts on scalar summaries; non-robust functional MFCC; iterative removal variants) degrade as contamination severity increases, especially under cellwise contamination. Real case study (resistance spot welding, 1839 items, 10 DRC profiles): RoMFCC flags 72.3% of Phase II observations as out-of-control; bootstrap comparison on Phase II gives \(\widehat{TDR}=0.723\) with 95% CI \([0.695,0.753]\), exceeding MFCC (0.541 [0.511,0.574]), iterMFCC (0.632 [0.595,0.664]), and non-functional alternatives (e.g., RoMCC 0.513 [0.481,0.547]).",None stated.,"The monitoring limits for the score-based \(T^2\) chart rely on (approximate) multivariate normality of retained scores and independence assumptions for the Šidák correction; robustness to heavy tails or strong deviations from these assumptions is not fully established. Performance is mainly demonstrated for mean-shift-like out-of-control patterns inspired by welding data; sensitivity to covariance/shape changes, drift, or autocorrelated functional observations is not directly addressed. The framework is multi-step (filtering, imputation, robust FPCA, dual charts) and may require careful tuning (e.g., \(\delta_{\mathrm{fil}},\delta_{\mathrm{imp}},\delta_{\mathrm{mon}}\)) and computational resources for very high-frequency/streaming applications.","Suggests incorporating RoMFDI into a multiple imputation framework (repeating imputations and combining RoMFPCA covariance estimates, e.g., by averaging) to better account for uncertainty introduced by single stochastic imputation. Also notes that if the complete-case set is sufficiently large, the RoMFPCA model need not be updated at every imputation iteration (implying potential computational improvements).","Extend RoMFCC to explicitly handle autocorrelation/serial dependence in functional profiles (e.g., dynamic FPCA, state-space residual monitoring) and assess steady-state/ATS-style properties under dependence. Develop diagnostic tools to localize which functional components/time regions drive alarms (post-signal interpretation) and integrate fault isolation for multivariate profiles. Provide open-source, production-ready software (e.g., an R package/vignette) with guidance for parameter selection, scalability, and real-time deployment; broaden benchmarking to additional out-of-control types (variance/covariance shifts, localized shape changes, drifts) and more industrial datasets.",2207.07978v3,local_papers/arxiv/2207.07978v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:42:53Z
TRUE,Multivariate|Profile monitoring|Functional data analysis|Nonparametric,Hotelling T-squared|Other,Phase II,Environmental monitoring|Transportation/logistics|Other,NA,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,False alarm rate,"Package is Phase II-focused: assumes a clean in-control reference dataset is available. In the built-in simulation generator, defaults are nI = 1000 reference profiles plus ntun = 1000 additional in-control tuning profiles (and nII = 60 Phase II profiles).",TRUE,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=funcharts,"The paper introduces the R package funcharts, an off-the-shelf toolkit for statistical process monitoring (SPM) of multivariate functional data (profiles), including adjustment for scalar/function responses influenced by functional covariates. For monitoring multivariate functional quality characteristics without covariates, it uses MFPCA to obtain scores and constructs Hotelling’s T² and squared prediction error (SPE) charts; unlike earlier work assuming normal scores, control limits are computed nonparametrically via empirical quantiles with Bonferroni correction. For covariate-adjusted monitoring, it implements regression-based schemes: scalar-on-function regression adds a third chart for the scalar prediction error, and function-on-function regression (functional regression control chart, FRCC) monitors functional residuals (standard or studentized) using T² and SPE on residual scores. The package also provides real-time versions of these procedures for partially observed profiles up to an intermediate domain point. Demonstrations include a simulation generator with several mean-shift types and a real case study monitoring ship CO2 emissions per mile adjusted for speed, wind components, and trim, showing clear post-intervention shifts in residual-based charts.","Functional observations are represented via MFPCA, retaining the first M components and reconstructing $\hat X_{ip}(t)=\sum_{m=1}^M \xi_{im}\,\psi_{mp}(t)$. Monitoring without covariates uses $T_i^2=\sum_{m=1}^M \xi_{im}^2/\lambda_m$ and $\mathrm{SPE}_i=\sum_{p=1}^P\int_T (X_{ip}(t)-\hat X_{ip}(t))^2dt$. Limits for $T^2$ and SPE are set by empirical quantiles (with Bonferroni splitting of $\alpha$ across charts); contribution decompositions are provided for diagnostics.","In the simulation illustrations (default nI=1000, ntun=1000, nII=60), the multivariate functional T²/SPE charts show no out-of-control (OC) points in the first (in-control) block of 20 and multiple OC points in blocks with induced mean shifts, with stronger shifts producing nearly all signals. In the scalar-on-function case, the added regression prediction-error chart signals several OC points for moderate scalar shifts and essentially all points for larger shifts, while T²/SPE behavior on covariates matches the no-covariate case (with higher limits due to Bonferroni across three charts). In the ship-navigation case study (159 pre-intervention voyages as reference; Phase II after VN1257), both T² and SPE charts on studentized residuals show a clear shift after the energy-efficiency initiative, with many Phase II points exceeding limits at overall $\alpha=0.01$ (split across two charts). Real-time charts for a selected voyage show statistics crossing UCL primarily in the middle of the voyage, corresponding to unusually low conditional CO2-per-mile residuals.",None stated.,"The paper/package is primarily Phase II and assumes an already “clean” in-control reference set; it provides limited guidance on robust Phase I cleaning/parameter estimation under contamination. Many procedures rely on empirical-quantile limits that can be sensitive to limited reference sample sizes and to dependence between successive profiles; serial correlation across voyages/production cycles is not explicitly modeled. The real-time approach recomputes limits on truncated domains and may be computationally heavy in practice without careful caching/approximation; performance guarantees for sequential, repeated looks (beyond Bonferroni at a fixed time point) are not developed.",None stated.,"Extend the package/methods to explicitly handle autocorrelated sequences of profiles (e.g., functional time-series modeling or residual-whitening before charting) and to provide Phase I tools for robust reference-set cleaning and limit estimation under contamination. Develop principled repeated-monitoring error control for real-time/online use beyond pointwise Bonferroni (e.g., sequential FWER/FDR control or run-length calibrated limits). Provide additional benchmarking against alternative functional/profile monitoring approaches and add scalable implementations for larger n/p (e.g., randomized SVD, streaming FPCA), along with automated shift diagnosis/localization tools.",2207.09321v2,local_papers/arxiv/2207.09321v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:43:35Z
TRUE,Univariate|Other,Other,Phase II,Healthcare/medical,FALSE,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"The paper fixes an experiment length of N = 180 observations (30 days × 6 prompts/day) and uses a triggering start S* = 6 to ensure enough prior data for Beta parameter estimation; it notes that too-small S* can cause computational/estimation issues. For real-data inclusion, users with fewer than 6 interactions were excluded.",TRUE,R,Public repository (GitHub/GitLab),https://github.com/AI-for-Better-Living/adc-tinnitus,"The paper proposes adaptive EMA triggering algorithms inspired by control-chart ideas to request a burdensome secondary task when a self-reported variable is at statistically “extreme” values, while controlling participant burden. A univariate, distribution-based chart is built by fitting a Beta distribution to the participant’s accumulated self-reports via method-of-moments and triggering when the new observation falls outside symmetric Beta quantiles at level α. The design parameters (start point S* and significance level α) are selected via a simple design-optimization utility targeting an expected number v of triggers; a second algorithm adapts α over time using an online estimate of adherence (missingness), effectively widening thresholds for less-adherent participants to collect enough extreme observations. Performance is evaluated in simulations and on real tinnitus app data, comparing against random and static-threshold schedules using F1 score and a utility based on deviation from the desired trigger count. The adherence-adaptive algorithm shows statistically significant improvements in F1 and utility over baselines and over the non-adaptive version, especially when adherence varies strongly.","At time t, estimate Beta parameters from past data using method-of-moments: $\hat\delta_t=\hat\mu_t\,\nu_t$ and $\hat\xi_t=(1-\hat\mu_t)\,\nu_t$ where $\nu_t=\hat\mu_t(1-\hat\mu_t)/\hat\sigma_t^2-1$. Trigger if $x_t<z_{\alpha/2}(\hat\delta_{t-1},\hat\xi_{t-1})$ or $x_t>z_{1-\alpha/2}(\hat\delta_{t-1},\hat\xi_{t-1})$, where $z_\beta(\cdot)$ is the Beta quantile. With adherence, estimate $\hat\chi(t)=\frac{1}{t}\sum_{i=1}^t a_i$ and $\hat N_0(t)=\hat\chi(t)N$, then set $\alpha^*(t)=v/(\hat N_0(t)-S^*+1)$ (with boundary cases) and use this in the quantile thresholds; a stopping rule caps total triggers (e.g., ≤10).","Across both simulated data and real tinnitus data, Algorithm 2 (adherence-adaptive) is reported to stochastically dominate competitors in both F1 score and utility $u_1=(\sum_i w_i-v)^2$ via Wilcoxon–Mann–Whitney tests. For simulated data, p-values for Algorithm 2 > random and Algorithm 2 > static are extremely small for F1 (1.4×10^-154 and 5.6×10^-131) and for utility (1.6×10^-165 and 9.1×10^-121); Algorithm 2 > Algorithm 1 yields p = 1.5×10^-3 (F1) and 3.2×10^-5 (utility). For real data, Algorithm 2 > random and Algorithm 2 > static yield p = 4.1×10^-46 and 5.6×10^-42 for F1, and p = 3.3×10^-46 and 1.9×10^-13 for utility; Algorithm 2 > Algorithm 1 yields p = 1.2×10^-9 (F1). The real-data results suggest larger gains for the adherence-adaptive method due to higher adherence variability than assumed in simulation.","The authors state that the algorithms rely on strong statistical assumptions that may not hold in real settings: the outcome and adherence processes are assumed i.i.d. and independent, and the distribution of the outcome is assumed known a priori (with Beta used in the paper). They note the approach requires access to distribution quantiles and that estimation can encounter computational issues if the start point S* is too small (e.g., negative parameter estimates). They also caution that the adherence-adaptive definition of “extreme” values is inconsistent across participants and must be accounted for in subsequent analyses.","The triggering rule is framed as a control-chart-like procedure, but it does not analyze classic SPC properties such as in-control ARL/false-alarm calibration under parameter estimation, nor does it provide run-length distributions. Using a Beta model with method-of-moments can be brittle under rounding, heaping, or bounded self-report behavior; robustness to model misspecification is not systematically studied. The evaluation relies on F1/utility defined against a “ground truth” derived from the same quantile-thresholding framework, which can bias comparisons toward methods aligned with that definition and away from alternative notions of “extremes.”","They propose incorporating time dependence (e.g., autoregressive models) for both the outcome and adherence to better match longitudinal dynamics. They suggest extending the approach to multivariate settings where multiple quantities of interest are monitored, potentially including objective wearable-sensor measures, and exploring hierarchical methods for improved performance. They also note the current work adapts triggering of the secondary task only, and propose future work on adaptive sampling for the primary (lighter) EMA prompts to further reduce burden.","A natural extension is to provide SPC-style performance analysis (e.g., calibrated false-alarm rate and run-length/ATS behavior) under parameter estimation and missingness, including sensitivity to the chosen stopping cap. Developing a distribution-robust or nonparametric version (e.g., using empirical/quantile tracking or conformal prediction) would reduce reliance on a known parametric family and quantile availability. It would also be valuable to add diagnostic/interpretability tools to explain triggers (e.g., contribution of adherence vs. symptom extremity) and to validate prospectively in a live deployment rather than retrospective replay.",2207.12331v1,local_papers/arxiv/2207.12331v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:44:17Z
TRUE,Multivariate|Nonparametric|Other,Shewhart|Change-point|Other,Both,Other|Theoretical/simulation only,FALSE,NA,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Detection probability|False alarm rate|Other,"Phase I reference samples are class-specific subsets of correctly classified training embeddings: $R_c \subseteq \{m_t:t=1,\ldots,T, y_t=c\}$ of size $|R|$ (same across classes). The paper studies $|R|=100$ (toy example), $|R|\in\{2000,3000,4000\}$ for CIFAR-10 (single-class reference samples), $|R|\in\{400,500,600\}$ for TREC, and $|R|\in\{50,60,70\}$ for sonar; also explores merged reference samples (e.g., 30000 for CIFAR-10).",TRUE,R|Other,Not provided,https://www.cs.toronto.edu/~kriz/cifar.html|https://cogcomp.seas.upenn.edu/Data/QA/QC/,"The paper proposes a real-time statistical process monitoring (SPM) approach to detect nonstationarity (concept drift/novelty) during deployment of artificial neural networks by monitoring latent feature representations (embeddings) from an intermediate layer. Because the embedding distribution is unknown, it uses nonparametric multivariate control charts based on data depth and normalized ranks, focusing mainly on the Shewhart-type $r$ chart for individual observations (and also discussing the batch-wise $Q$ chart). For each predicted class, a reference sample of correctly classified Phase I embeddings is built, depths are computed with respect to that reference set, and a depth-based rank statistic is compared to a lower control limit equal to the chosen false-alarm probability $\alpha$. The method is evaluated on a toy Gaussian example and three real-data ANN settings (CNN on CIFAR-10 with CIFAR-100 classes as OOC, LSTM text classification on TREC with held-out categories as OOC, and an FNN on sonar data with synthetic OOC), and compared against benchmarks such as LOF, KDEOS, isolation forest, Mahalanobis distance, and NOF. The authors find asymmetric projection depth combined with the $r$ chart to be the most reliable across scenarios, while emphasizing practical issues such as misclassification-driven signals and reference-sample selection/size trade-offs.","Embeddings $m_i\in\mathbb{R}^k$ are monitored via a depth-based rank statistic for class $c$: $r_c^{\,\cdot}(m_i)=\frac{|\{t\in R_c: D_c^{\,\cdot}(m_t)\le D_c^{\,\cdot}(m_i)\}|}{|R_c|}\in[0,1]$, where $D$ is a data depth (Halfspace/Tukey, robust halfspace, Mahalanobis, (a)symmetric projection, simplicial). The Shewhart-style decision rule is $\text{signal if } r_c^{\,\cdot}(m_i)\le \alpha$ (LCL=$\alpha$, no UCL). For batches of size $n$, the $Q$ chart uses $Q_c^{\,\cdot}=\frac{1}{n}\sum_{j=1}^n r_c^{\,\cdot}(m_{ij})$ with LCL computed from $\alpha$ (closed form for small $\alpha$ or numerically).","In the toy example with $|R|=100$ and $\alpha=0.05$, most depth-based $r$ charts achieve perfect out-of-control detection (CDR=1.00) except symmetric projection depths and simplicial depth (CDR=0.00). On CIFAR-10 (Experiment 1), with reference samples chosen by highest softmax scores, increasing $|R|$ reduces in-control signal rate (e.g., PD2 SR decreases from 0.49 at $|R|=2000$ to 0.30 at $|R|=4000$) while out-of-control detection moderately decreases (PD2 CDR from 0.91 to 0.78). Benchmarks (LOF/NOF/MDis/iForest) show broadly comparable behavior but the recommended depth-based method (asymmetric projection depth with Nelder–Mead, i.e., PDa2/PD2 depending on symmetry) provides the best SR–CDR trade-off in their comparisons. The paper reports misclassification-conditional signal rates in Phase II, showing very high SR on misclassified points (e.g., in Experiment 1 average SR|M about 0.92) versus much lower SR on correctly classified points (SR|C around 0.25–0.49 depending on $|R|$). For $Q$ charts with batch sizes $n=3$ and $n=5$, LCL increases (e.g., 0.22 and 0.29 respectively) and SR often becomes very high, while CDR can improve (notably in Experiment 2).","The authors note that high Phase II signal rates can arise from misclassification and from difficulty selecting representative class-wise reference samples, and that a larger reference sample does not necessarily improve detection. They also highlight substantial computational cost for depth computations in higher dimensions (e.g., embeddings in $\mathbb{R}^{16}$) and that improving software/runtime is important for applicability to state-of-the-art ANN deployments. They further indicate that performance would improve if additional information to detect misclassification were available.","The approach relies on having a clean set of correctly classified Phase I embeddings per class; in many deployments, label noise or limited validation may make it hard to guarantee “correctly classified” reference samples, which risks reference contamination and inflated false alarms. The method largely assumes independence of monitored embeddings over time; if embeddings are autocorrelated (common in streaming/temporal data), FAR/ARL properties can deviate substantially. Although benchmarks are included, the comparisons are framed within the same $r$-chart signaling rule; alternative sequential detectors (e.g., CUSUM/EWMA/GLR on suitable scores) might offer better small-shift sensitivity or steadier false-alarm control but are not explored. Finally, the method signals drift but provides limited guidance for diagnosing drift type/location in embedding space beyond visualization, which may limit actionability in complex production systems.","They propose studying how Phase I SPM techniques (e.g., multivariate mean-rank charts) can better support Phase I analysis and reference-sample quality, and when/how reference samples should be updated or augmented online while avoiding out-of-control contamination. They suggest exploring improved train/test splitting strategies that preserve distributional similarity, as well as data splitting/compression trade-offs for fast yet reliable training and monitoring. They also call for developing an additional method to detect misclassification (to complement monitoring), investigating window-based monitoring, and extending the framework to class-imbalanced settings and to semi-/unsupervised learning models; they note that different drift types warrant further method development and comparison.","A natural extension is to incorporate explicit control of in-control ARL under estimated-reference uncertainty (Phase I estimation error), e.g., via bootstrap-calibrated limits or self-starting depth-rank charts. Another direction is to adapt the monitoring to autocorrelated embeddings (e.g., via residual modeling or time-series robust depths) and to irregular sampling/missingness. For scalability, developing fast approximate depth updates (streaming/coreset methods) or learning a surrogate outlyingness score that preserves depth order could enable sub-second monitoring in high-dimensional embeddings. Finally, integrating post-signal diagnostics (e.g., attribution of which embedding dimensions/neurons drive low depth, and mapping back to input features) would improve interpretability and support root-cause analysis.",2209.07436v2,local_papers/arxiv/2209.07436v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:45:22Z
TRUE,Multivariate|Other,CUSUM|Other,Phase II,Transportation/logistics|Energy/utilities|Other,NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,ATS (Average Time to Signal)|False alarm rate|Detection probability|Other,Training uses k samples from nominal (anomaly-free) data; in experiments both methods were trained with 24 h of 1 Hz data and tested on a different 24 h of data (per cell group).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an online anomaly detection approach for large Li-ion battery packs that generates mean-based residuals (per cell within a cell group) from real-time voltage and temperature signals and then applies PCA to capture cell-to-cell inconsistencies. The PCA-reconstructed normalized residuals are converted to a scalar anomaly score via RMSE and monitored using a one-sided CUSUM control chart with control limits set from nominal training data. A baseline “direct” approach (filter residuals then CUSUM each residual) is used for comparison. The method is validated on real locomotive battery-pack data (825 cells arranged into 25 groups of 11) plus statistically tested using synthetic injected anomalies (internal short circuit, air-flow anomaly, sensor lead faults, voltage dropout). Reported results show low false-positive rates (<3%) and improved detection performance versus direct thresholding/CUSUM, including faster detection and fewer missed/false-negative anomalies; temperature residuals enable detection of module balancing events in ~14 min that voltage residuals can miss.","Mean-based residual for cell i in a group: $x_i(t)=X_i(t)-\mu_X(t)$ with $\mu_X(t)=\frac{1}{n}\sum_{i=1}^n X_i(t)$. Z-score normalization uses training means/SDs: $z_i(t)=\frac{x_i(t)-\mu_{X_r,i}}{\sigma_{X_r}}$. PCA reconstruction uses truncated left singular vectors $U_r$: $\hat z(t)=U_rU_r^T z(t)$, and the anomaly score is based on the reconstruction error (scalarized via RMSE) and monitored with CUSUM: $C^+[t]=\max\{0, C^+[t-1]+(y[t]-\mu_c)-K\}$ (and $C^-$ for the direct method), with $K=4\sigma_c$ and 5$\sigma_c$ control limits (one-sided in PCA method).","On 24 h nominal data across 25 cell groups, average false-positive rates were 1.9% (direct) and 2.9% (PCA). For 13 module balancing events (all 11 cells with mild ESC), temperature-PCA detected anomalies in 13.5 min on average with 2.3% false-negative rate, while voltage-PCA was ineffective (99% FNR). Across five synthetic anomaly families, the PCA method improved detection time by 56%, false-negative rate by 42%, and missed anomaly rate by 60% relative to the direct method (Table 2). The PCA method detected all anomalies with voltage deviations >4 mV and temperature deviations >0.15 °C with zero missed-anomaly rate; tracing accuracy exceeded 95% for deviations >7 mV or >0.3 °C.","The authors note that detection sensitivity depends strongly on sensor/data-acquisition quality; lower-sensitivity/noisier sensors would require higher CUSUM thresholds to avoid false positives, reducing the ability to detect small deviations (e.g., 4 mV, 0.15 °C). They also state the PCA approach can fail when all cells in a group exhibit identical anomaly signatures (though argued to be unlikely in practice), and that voltage PCA needs retraining after balancing events due to changed nominal cell-to-cell voltage relationships.","The approach relies on having representative anomaly-free training data (24 h per group in experiments) and assumes cell groups are sufficiently similar; performance may degrade under strong heterogeneity, aging divergence, or changing operating regimes not captured in training. The method uses manually tuned low-pass filter cutoffs and fixed CUSUM design choices (e.g., $K=4\sigma_c$, 5$\sigma_c$ limits), and results may be sensitive to these hyperparameters without an automated design/robustness analysis. Autocorrelation/1 Hz time-series dependence is not explicitly modeled in the CUSUM design, so nominal ARL/false-alarm properties under serial correlation are not characterized. Code and implementation details (e.g., PCA update/retraining logic, windowing, computational load) are not provided, limiting reproducibility.",None stated.,"Extend the monitoring design to explicitly handle autocorrelated residuals (e.g., time-series prewhitening or CUSUM designs with correlated data) and quantify in-control ARL/false-alarm behavior under realistic dependence. Develop adaptive/self-starting or online-updated PCA/CUSUM schemes to reduce retraining burden and better track drift (aging, seasonal/environmental changes) while controlling false alarms. Provide open-source implementation and benchmark datasets (or privacy-preserving surrogates) to enable reproducible comparison against alternative multivariate SPC methods (e.g., $T^2$/SPE charts, MEWMA/MCUSUM, robust PCA) and modern ML baselines.",2210.15773v2,local_papers/arxiv/2210.15773v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:46:15Z
FALSE,Univariate|Other,Shewhart|Change-point,Phase II,Finance/economics,NA,FALSE,FALSE,Case study (real dataset)|Other,Other,Not discussed (uses rolling windows of 3 months ≈ 63 trading days to compute connectedness measures and tree distances).,TRUE,R|Other,Not provided,NA,"The paper studies dynamic networks of Swedish stock returns (OMX30 constituents) by building return-connectedness graphs using two dissimilarity measures: Pearson correlation coefficient dissimilarity (PCCD) and generalized variance decomposition dissimilarity (GVDD) from a VAR model. For each rolling three-month window, the dense network is converted into a rooted hierarchical clustering tree (single-linkage; GVDD is symmetrized via max of reciprocal entries), and day-to-day structural change is summarized by a one-dimensional time series of distances between consecutive trees, computed via an information-theoretic generalized Robinson–Foulds/“clustering information distance” framework. A Shewhart-style thresholding (mean and 5×SD) is applied to the resulting tree-distance series to flag abnormal jumps, interpreted as warnings of market-structure changes (notably around early 2020/COVID-19). The paper also analyzes graph “centers” over time and finds Investor is most frequently central under both PCCD and GVDD, while tree distances under GVDD are typically larger/more volatile than under PCCD. Overall, SPC is used as an ancillary detection tool on a derived financial-network statistic rather than being the paper’s primary methodological focus.","Connectedness/dissimilarity: PCCD uses $h^{t,\mathrm{PCCD}}_{ij}=\sqrt{2(1-\rho^t_{ij})}$, where $\rho^t_{ij}$ is the rolling-window Pearson correlation. GVDD is built from a VAR/MA representation and generalized variance decomposition shares $\hat h^t_{ij}$, then converted to a dissimilarity via $h^{t,\mathrm{GVDD}}_{ij}=\sqrt{2(1-\hat h^t_{ij})}$. Monitoring statistic is the sequential tree-distance time series between hierarchical clustering trees; an SPC-style rule compares distances to the series mean and a threshold at $5\times\mathrm{SD}$ (Shewhart-type).","Using 28 Swedish stocks (2017-03-31 to 2022-03-30) with a 3-month (~63 trading day) rolling window, Investor is the most frequent network center under both PCCD and GVDD, with Sandvik AB second. Tree-distance series from PCCD and GVDD exhibit positive serial dependence; a fitted VAR(2) indicates both distance series depend positively on their own lags, while the OMX index has a negative significant effect at lag 1 on both distance series, and the distances do not predict OMX. Applying a mean and 5×SD Shewhart-style threshold to tree-distance series shows most large jumps occur after the beginning of 2020, with more exceedances under GVDD; the paper notes 7 out of 20 exceedances (above the plotted threshold) occur in 2020 during COVID-19.",None stated.,"The Shewhart charting step is informal (mean and 5×SD) without calibration to a desired in-control false-alarm rate (ARL) or discussion of run-length performance, so detection properties are unclear. The monitored statistic (tree distances) is autocorrelated (the authors even fit VAR models), but standard Shewhart assumptions of independence are not addressed; this can distort false-alarm rates. The paper does not develop or compare alternative SPC schemes (e.g., EWMA/CUSUM) that are typically better for sustained/small changes, nor does it provide sensitivity analyses for window length, linkage choice, or distance metric choice.",None stated.,"Calibrate the monitoring rule to achieve specified in-control ARL/false-alarm probability under realistic dependence (e.g., via block bootstrap or time-series modeling of tree distances). Compare Shewhart to memory charts (EWMA/CUSUM) on the tree-distance statistic and evaluate detection delay for different market-regime shifts. Provide reproducible software and robustness studies for choices such as window size, VAR order for GVDD, symmetrization method, linkage type, and tree-distance metric, and validate on additional markets or crisis periods.",2210.16679v1,local_papers/arxiv/2210.16679v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:46:49Z
TRUE,Univariate|Nonparametric|Other,Shewhart,Both,Manufacturing (general)|Theoretical/simulation only|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|Detection probability|False alarm rate|Other,Phase I examples use m = 15 subgroups with unequal sizes (Plans 1–4) and equal sizes (Plan 5). Phase II monitoring uses subgroup size n_k = 10 in simulations; real-data example monitors with n_k = 5 using 25 subgroups of varying sizes.,TRUE,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=rQCC,"The paper develops robust Shewhart $\bar{X}$ control charts that handle both (i) data contamination (outliers) and (ii) unequal subgroup sample sizes, a setting where conventional $\bar{X}$ charts can perform poorly. It proposes optimal pooling (best linear unbiased estimators, BLUEs) for Phase I estimation of the in-control location and scale when subgroup sizes vary, using weighting based on each estimator’s variance (for location) and expectation/variance (for scale). Robust versions are constructed by pairing robust location estimators (median, Hodges–Lehmann) with robust scale estimators (MAD, Shamos), with finite-sample unbiasing factors, and then plugging these into standard $\bar{X}$ chart limits. Extensive Monte Carlo studies compare ARL/SDRL/percentiles of run length under clean data and under a single-point contamination scenario, showing classical mean/SD charts are extremely sensitive to one outlier, while the proposed robust charts maintain stable in-control performance and superior robustness. A real dataset on piston ring diameters illustrates that robust limits are far less affected by injected contamination than Montgomery-style pooled/standard $\bar{X}$ limits.","Robust pooled (BLUE) location estimator for unequal subgroup sizes: $\hat\mu_C=\dfrac{\sum_{i=1}^m \hat\mu_i/\nu_i^2}{\sum_{i=1}^m 1/\nu_i^2}$, where $\nu_i^2$ is the variance of the standardized estimator under $N(0,1)$ (Eq. 5). BLUE scale estimator: $\hat\sigma_C=\dfrac{\sum_{i=1}^m (\gamma_i/\tau_i^2)\,\hat\sigma_i}{\sum_{i=1}^m (\gamma_i^2/\tau_i^2)}$ with $\gamma_i=E(\hat\sigma_i)/\sigma$ and $\tau_i^2=\mathrm{Var}(\hat\sigma_i)/\sigma^2$ under $N(0,1)$ (Eq. 11; simplifies to inverse-variance weighting when $\gamma_i=1$, Eq. 12). Phase II $\bar{X}$ chart limits for subgroup size $n_k$: $\mathrm{UCL}=\hat\mu\pm g\,\hat\sigma/\sqrt{n_k}$ with $g=3$ used for Shewhart-style limits.","In simulations with Phase I having 15 subgroups of unequal sizes (e.g., Plan-1: five each of sizes 3, 10, 17) and Phase II subgroup size $n_k=10$, pooling type C (BLUE) yields in-control ARLs closest to the nominal target (ARL0 ≈ 370) across methods; e.g., under no contamination Plan-1, Method-I (mean/SD) ARL improves from 476.1 (A) and 456.5 (B) to 366.8 (C). Under a single contaminated Phase I observation (adding $\delta=100$), classical Method-I ARLs explode (e.g., Plan-1: 6178.6 (A), 6587.0 (B), 66089.1 (C)), whereas robust methods remain comparatively stable (e.g., Plan-1 Method-III (HL/Shamos) ARL 466.7 with pooling C). The paper also reports SDRL, 99th percentile of run length, and skewness, showing severe right-skew and variability for contaminated classical charts and much more stable distributions for robust charts. A real piston-ring dataset example shows control limits from robust methods change far less than Montgomery/Method-I limits when a single observation is perturbed across $\delta\in[73,74]$.","The proposed robust $\bar{X}$ charts (and the BLUE pooling rules) rely on the key assumption that the underlying in-control distribution is normal, which limits applicability because the true distribution is typically unknown. The authors note that while transformations could be used to achieve approximate normality, recent work highlights pitfalls of nonlinear transformations for SPC, motivating further study under non-normality.","The work focuses on Shewhart $\bar{X}$ monitoring; it does not develop corresponding robust unequal-$n$ charts for small-shift detection (e.g., EWMA/CUSUM) where robustness and weighting could interact differently with serial dependence. Autocorrelation is assumed absent (iid within and across subgroups), so performance under common industrial autocorrelation or time-varying variance is unknown. Practical implementation depends on access to (or tabulation of) finite-sample variances/unbiasing factors for robust estimators across subgroup sizes; if subgroup sizes are large or vary widely, maintaining these tables and validating approximations could be nontrivial.","The authors state ongoing work to investigate performance of the proposed robust X-bar charts when the underlying distribution departs from normality (e.g., skewed distributions). They also mention the need to address non-normality without relying uncritically on nonlinear transformations due to potential dangers/pitfalls discussed in the recent literature.","Extend the BLUE pooling idea and robust estimators to memory-type charts (robust EWMA/CUSUM) with unequal sample sizes and evaluate detection delays for small mean shifts. Develop versions that explicitly handle autocorrelated data (e.g., via residual charts, time-series modeling, or block bootstrap calibration) and irregular sampling/missingness beyond unequal subgroup sizes. Provide a dedicated software vignette/benchmark suite (with reproducible simulation scripts) covering broader contamination models (multiple outliers, heavy tails) and estimating performance under parameter estimation error (conditional/steady-state ARL) for common Phase I sample size regimes.",2212.10731v1,local_papers/arxiv/2212.10731v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:47:34Z
TRUE,Univariate|Other,CUSUM|GLR (Generalized Likelihood Ratio)|Other,Phase II,Healthcare/medical|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Detection probability|False alarm rate|Other,"Not discussed (control limits are calibrated by simulation using an assumed Poisson patient arrival rate $\psi$ over a monitoring horizon `time`; examples include `psi=1` patient/day over `time=365` days, and `n_sim` typically 200 for Bernoulli/BK and 20 (default) for CGR, often increased to 300 in the application).",TRUE,R,In text/Appendix,https://CRAN.R-project.org/package=success,"The paper introduces the R package success for constructing risk-adjusted control charts tailored to survival (time-to-event) outcomes, motivated by hospital performance monitoring where dichotomizing survival times can delay detection. The package implements discrete-time methods (risk-adjusted funnel plots and Bernoulli CUSUM) and continuous-time survival monitoring methods: the Biswas–Kalbfleisch continuous-time CUSUM (BK-CUSUM) and the Continuous Time Generalized Rapid Response CUSUM (CGR-CUSUM). BK-CUSUM requires a user-specified hazard-ratio shift parameter, while CGR-CUSUM estimates the post-change hazard ratio via maximum likelihood, reducing sensitivity to misspecification but potentially being unstable early or in low-volume settings. Control limits for the CUSUM charts are chosen primarily by simulation to control the type I error over a fixed time horizon, due to limited analytical results for continuous-time survival charts. The paper demonstrates use on a large simulated multi-hospital dataset and on a breast cancer clinical-trial-based dataset, comparing detection timing across charts and highlighting practical interpretation differences between discrete-time and continuous-time monitoring.","Bernoulli CUSUM: $S_n=\max(0,S_{n-1}+W_n)$ with $W_n=X_n\ln\{p_1(1-p_0)/[p_0(1-p_1)]\}+\ln\{(1-p_1)/(1-p_0)\}$ (or using odds ratio $e^\theta$). BK-CUSUM (continuous time): $\mathrm{BK}(t)=\max_{0\le s\le t}\{\theta_1 N(s,t)-(e^{\theta_1}-1)\Lambda(s,t)\}$ where $N(s,t)=N(t)-N(s)$ and $\Lambda(s,t)=\Lambda(t)-\Lambda(s)$. CGR-CUSUM: $\mathrm{CGR}(t)=\max_{1\le \nu\le n}\{\hat\theta_{\ge\nu}(t)N_{\ge\nu}(t)-[\exp(\hat\theta_{\ge\nu}(t))-1]\Lambda_{\ge\nu}(t)\}$ with $\hat\theta_{\ge\nu}(t)=\max\{0,\log(N_{\ge\nu}(t)/\Lambda_{\ge\nu}(t))\}$.","Control limits are calibrated by simulation to achieve a target type I error (e.g., $\alpha=0.05$) within a chosen horizon (e.g., 1 year). An example Bernoulli CUSUM control limit reported is $h=5.56$ for `time=365`, `alpha=0.05`, `followup=30`, `psi=1`, and detecting an odds ratio of 2 (`theta=log(2)`). In the EORTC-based application, simulated control limits (with `n_sim=300`) vary by centre volume category; for estimated arrival rates $\psi\approx 0.9,2.1,11$, the reported limits are (Ber,BK,CGR) = (2.29,3.23,4.90), (2.93,3.89,5.51), (4.71,6.43,6.49). Detection-time comparisons show that continuous-time charts can signal earlier than the discrete-time Bernoulli CUSUM due to avoiding follow-up delay, while CGR-CUSUM can have early instability/spikes from MLE uncertainty and BK-CUSUM sensitivity depends on the chosen shift parameter.","The authors note that analytical results for continuous-time survival control charts are lacking, motivating simulation-based control-limit selection. They state that CGR-CUSUM can be unstable early in monitoring and may provide unreliable values for low-volume hospitals because the MLE of the hazard ratio needs sufficient failure information to stabilize. They also discuss that BK-CUSUM requires prespecifying a shift size (hazard ratio), and misspecifying it can substantially delay detection.","The paper focuses on methods and software rather than a systematic benchmarking study; comparisons are illustrative and may not cover a broad range of censoring mechanisms, non-proportional hazards, or model misspecification scenarios common in practice. The approach relies on correct specification of the risk-adjustment models (logistic/Cox PH) and Poisson arrival assumptions for control-limit simulations; robustness to departures (overdispersion, time-varying covariate effects, informative censoring) is not fully assessed. Control-limit calibration by simulation may be computationally heavy and may not guarantee nominal error under real-world heterogeneity across units or changing case-mix over time.",None stated.,"Provide broader robustness studies for CGR/BK under non-proportional hazards, time-varying effects, informative censoring, and non-Poisson arrivals, including guidance for practitioners on diagnosing assumption violations. Develop faster and/or more exact control-limit/false-alarm approximations (e.g., Markov chain/integral-equation approximations) to reduce reliance on expensive simulations, especially for CGR-CUSUM. Extend the software to handle correlated outcomes over time (seasonality/autocorrelation), irregular sampling, and missing covariates, and add more real-data case studies plus a reproducible benchmarking suite.",2302.07658v2,local_papers/arxiv/2302.07658v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:48:18Z
TRUE,Univariate|Other,CUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Other,Phase II,Theoretical/simulation only|Other,NA,FALSE,NA,Exact distribution theory|Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a CUSUM-type change-detection procedure with observation-adjusted control limits (CUSUM-OAL), where the threshold is a decreasing function of a running/sliding average of log-likelihood ratios, enabling adaptive lowering of limits as evidence accumulates. It also discusses an optimal sequential likelihood-ratio-type test based on the sum of log-likelihood ratios (SLR) and a modified version with a dynamic linear boundary to achieve finite in-control ARL. The authors establish limiting relationships showing that, as a tuning parameter grows, sequences of CUSUM-OAL tests converge to the optimal SLR-type test and to a combined procedure min{CUSUM, SLR}. They provide theoretical approximations/estimations for in-control and out-of-control ARLs under Kullback–Leibler-based regimes (“small/medium/large” changes). Extensive Monte Carlo simulations (including normal mean-shift scenarios) indicate substantially smaller out-of-control ARLs for CUSUM-OAL than conventional CUSUM, particularly for small mean shifts, at the cost of larger variability (SD) in the in-control state.","Conventional one-sided CUSUM stopping time: $T_C(c)=\min\{n\ge 0: \max_{1\le k\le n}\sum_{i=n-k+1}^n Z_i\ge c\}$ with $Z_i=\log\{p_{v_1}(X_i)/p_{v_0}(X_i)\}$. The proposed CUSUM-OAL replaces the constant limit by an observation-adjusted limit: $T_C(cg)=\min\{n\ge 0: \max_{0\le k\le n}\sum_{i=n-k+1}^n Z_i\ge c\,g(\hat Z_n)\}$ (and a sliding-average variant $\hat Z_n(ac)$). The SLR-type test is $T_{SLR}=\min\{n\ge 1: \sum_{j=1}^n Z_j\ge c\}$, and a modified version uses a linear boundary $T_{SLR}(r)=\min\{n\ge 1: \sum_{j=1}^n Z_j\ge n(\mu_0-r)\}$.","In the normal mean-shift simulation with common in-control ARL0 ≈ 1000 (Table 1), conventional CUSUM has ARL1 ≈ 439 for a small shift 0.1, while CUSUM-OAL with large tuning (e.g., $u=10^4$) has ARL1 ≈ 7.51, close to the combined/limiting procedures (≈ 7.47–7.52). For moderate shifts (e.g., 0.5), CUSUM ARL1 ≈ 38.91 versus CUSUM-OAL (large $u$) ≈ 1.89–1.92. For large shifts (e.g., 3.0), all fast procedures are near 1.01 while CUSUM is 2.61. The paper also reports standard deviations of run length alongside ARLs and notes that CUSUM-OAL can have much larger in-control variability than conventional CUSUM.","The authors note that although CUSUM-OAL greatly reduces out-of-control ARLs, it has larger standard deviations than the conventional CUSUM test in the in-control state. They also emphasize that the post-change distribution is usually unknown and motivate using a parameter region $V$ (with a distribution $Q$ on $V$) to represent plausible post-change scenarios when estimating ARLs.","The method depends on selecting the adjustment function $g(\cdot)$ and tuning parameters (e.g., $u,a,ac$), but practical guidance for choosing these in general applications (beyond targeted ARL0 calibration and examples) is limited and may affect robustness. Results are largely developed for i.i.d. observations with likelihood-ratio scores; performance under model misspecification (wrong $p_{v_0},p_{v_1}$), heavy tails, or serial dependence is not established. Simulations focus heavily on normal mean shifts and may not fully reflect performance for variance/shape changes or non-exponential-family settings. Implementation requires computing $Z_i$ (log-likelihood ratios), which may not be straightforward in complex or high-dimensional data settings without additional modeling choices.",None stated.,"Develop data-driven/default choices for $g(\cdot)$ and tuning parameters with guaranteed ARL0 control and robust performance across a range of post-change distributions. Extend theory and evaluation to autocorrelated processes (e.g., ARMA), nonparametric/robust score constructions, and composite post-change models where $p_{v_1}$ is not fixed. Provide open-source software (e.g., an R/Python package) and practical design algorithms (control-limit calibration, parameter selection) to improve adoption. Add more real-data case studies and comparisons to modern adaptive CUSUM/GLR and Bayesian quickest-detection methods under comparable false-alarm constraints.",2303.04628v1,local_papers/arxiv/2303.04628v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:48:58Z
TRUE,Univariate|Other,EWMA|Shewhart,Both,Semiconductor/electronics,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|Conditional expected delay,"In the illustrative example, Phase I uses T0 = 50 observations to estimate the in-control Poisson mean (Poi(1.48)) and design charts; the remaining 150 observations are monitored in Phase II. No general minimum Phase I sample-size guidance is given.",TRUE,R,Not provided,NA,"The paper proposes two new classes of generalized EWMA control charts for monitoring univariate Poisson count processes using the Stein–Chen identity that characterizes the Poisson distribution. The AB-EWMA chart estimates the mean via a ratio of exponentially weighted moments involving a user-chosen weight function f, targeting scenarios where mean shifts may occur with distributional changes. The ABC-EWMA chart additionally normalizes by an EWMA of the counts, making it sensitive to pure distribution-family changes (e.g., model misspecification) as well as mean shifts. Performance is evaluated primarily via Monte Carlo simulations (10^4 replications) using zero-state ARL and conditional expected delay (CED) under Poisson in-control and overdispersed out-of-control models (negative binomial and zero-inflated Poisson) for different choices of f. A semiconductor particle-counts case study illustrates that the Stein–Chen charts, especially ABC-EWMA, detect overdispersion/model misspecification substantially earlier than the ordinary Poisson EWMA chart.","Ordinary Poisson EWMA: $Z_0=\mu_0$, $Z_t=\lambda X_t+(1-\lambda)Z_{t-1}$. AB-EWMA constructs EWMAs $A_t$ of $X_t f(X_t)$ and $B_t$ of $f(X_t+1)$ with $A_0=\mu_{f;0}(1;0)$, $B_0=\mu_{f;0}(0;1)$, then charts $Z_t^{AB}=A_t/B_t$ (motivated by $\mu=\mathbb{E}[Xf(X)]/\mathbb{E}[f(X+1)]$ under Poisson). ABC-EWMA adds $C_t$ as an EWMA of $X_t$ with $C_0=\mu_0$ and charts $Z_t^{ABC}=A_t/(B_t C_t)$ (motivated by $1=\mathbb{E}[Xf(X)]/(\mu\,\mathbb{E}[f(X+1)])$), using symmetric two-sided limits around $\mu_0$ (AB) or 1 (ABC).","In simulation with $\lambda=0.10$ and target in-control $\text{ARL}_0\approx370$, the ordinary Poisson EWMA is best when the out-of-control condition is a pure Poisson mean shift. Under overdispersion (negative binomial or zero-inflated Poisson, dispersion index $I=5/3$), ABC-EWMA yields markedly smaller out-of-control ARLs than ordinary EWMA and generally also improves on AB-EWMA; AB-EWMA advantages are described as limited and scenario-dependent. CED results for late change points (e.g., CED(100)) are close to zero-state ARLs, indicating little sensitivity to the change-point location. In the semiconductor particle-counts example (Phase I: $T_0=50$, Poi(1.48)), first alarm times are: ordinary EWMA at $t=31$, c-chart at $t=13$, AB-EWMA at $t=11$, and ABC-EWMA at $t=10$ for $f(x)=|x-1|$ and at $t=7$ for $f(x)=|x-1|^{1/4}$ or $f(x)=\ln x$.","Only a limited set of in-control/out-of-control scenarios and weight functions are evaluated due to page limits, and more comprehensive performance analyses are recommended. The work assumes i.i.d. counts and notes that autocorrelated counts/time-series settings require further research. The effect of Phase I parameter estimation on chart performance is not analyzed and is deferred to future work.","Control limits are chosen to be symmetric for simplicity; this may yield biased ARL profiles for particular directional alternatives and may not be optimal for specific applications. The paper relies on simulation for ARL/CED evaluation rather than providing numerical/Markov-chain/integral-equation computations for run-length distributions, which could improve precision and reproducibility. Practical guidance for selecting the weight function $f$ in ambiguous real-world settings (beyond a few examples) and sensitivity to tuning choices (e.g., $\lambda$, truncation $M$ in moment approximations) is not fully developed.","The author suggests broader performance studies including additional out-of-control scenarios such as underdispersion and non-Poisson equidispersion. Extensions to other in-control count distributions using available Stein identities are proposed, as well as analogous constructions for continuously distributed variables using continuous Stein identities. The effect of Phase I parameter estimation on Stein-type EWMA performance is highlighted as an important topic. Incorporating autocorrelation and applying the approach to count time series is proposed.","Provide a principled, data-driven method for choosing or adapting the weight function $f$ (e.g., adaptive or ensemble/parallel charts with multiple $f$’s) with guaranteed false-alarm control. Develop analytic or semi-analytic run-length approximations (e.g., Markov-chain or integral-equation methods) for AB/ABC charts to reduce reliance on Monte Carlo and enable fast design. Study robustness to misestimation of $\mu_0$ and to departures from independence, including explicit modeling of autocorrelation (e.g., INAR/GLM residual charts) and irregular sampling/missingness.",2305.19006v1,local_papers/arxiv/2305.19006v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:49:42Z
FALSE,Other,Shewhart|Other,Both,Healthcare/medical|Theoretical/simulation only,TRUE,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,Other,Not discussed.,TRUE,R,Not provided,NA,"The paper proposes SpcShrink, a wavelet-domain denoising method that sets level-dependent thresholds via an iterative procedure inspired by SPC control-chart limit re-estimation. At each wavelet level, coefficients are treated as observations and an in-control model under $H_0:c_{j,k}=0$ implies $w_{j,k}$ is (approximately) Gaussian noise; coefficients outside estimated control limits are removed and limits are recomputed until all remaining coefficients fall within limits, yielding the final threshold. Significance levels are increased with scale ($\alpha_j=j\alpha_1$), producing decreasing control-limit distances across scales and capturing the typical decay of noise across wavelet scales. The free parameter $\alpha_1$ is tuned by Monte Carlo optimization (using SNR) and the method is compared to VisuShrink, SureShrink, BayesShrink, and S-median. Experiments on synthetic benchmark signals plus two biomedical signals (IPD and ECG) show SpcShrink generally achieves higher SNR/SNR-gain and competitive visual quality, with suggested defaults $\alpha_1=1.5\%$ (maximize SNR gain) and $\alpha_1=1.0\%$ (better smooth visual output).","Model: $y=x+n$ with white Gaussian noise $n\sim N(0,\sigma^2)$; wavelet coefficients $w=W y=c+z$ where $z\sim N(0,\sigma^2)$. Hypothesis per level: $H_0:c_{j,k}=0$ vs $H_1:c_{j,k}\neq 0$. Iterative limits per level $j$: estimate $s_j=\sqrt{\frac{1}{N_j-1}\sum_{k=1}^{N_j}(w_{j,k}-\bar w_j)^2}$, set $\text{LCL}=-d_j s_j$, $\text{UCL}=d_j s_j$ with $d_j=\sqrt{2}\,\mathrm{erfc}^{-1}(\alpha_j)$ and $\alpha_j=j\alpha_1$; remove coefficients outside $[\text{LCL},\text{UCL}]$ and repeat until all are inside, then set threshold $\lambda_j=d_j s_j$.","Monte Carlo search over $\alpha_1\in\{0.1\%,0.2\%,...,5\%\}$ (Daubechies-8, $J_0=5$, soft thresholding, $M=1500$ total runs across Blocks/Bumps/Doppler at input SNR 5/15/25/35 dB) produced optimal $\alpha_1^*$ values 1.1%, 1.4%, 1.7%, 1.8% (mean 1.5%). Using $\alpha_1=1.5\%$ gives control distances $d_1=2.432,d_2=2.170,d_3=2.005,d_4=1.881,d_5=1.780$. In synthetic experiments over input SNR 5–35 dB (1 dB steps; 93,000 simulated signals), SpcShrink outperformed competing thresholds in most scenarios; reported SNR gains exceeded 12 dB for low input SNR (<10 dB) in some cases. For real biomedical data, SpcShrink converged in tens of iterations (e.g., IPD: 40–56 iterations; ECG: 51–52 iterations) and produced visually competitive denoising compared with classical methods.",None stated.,"Although motivated by control charts, the method is not an SPC monitoring chart for sequential process control; it uses control-limit ideas as a heuristic for wavelet-threshold selection, so typical SPC properties (ARL/false-alarm run-length guarantees under repeated monitoring) are not developed. The approach relies on approximate Gaussianity/whiteness of wavelet-domain noise; performance may degrade for non-Gaussian, colored, or dependent noise and for signals where the null distribution of coefficients is not well-approximated by $N(0,\sigma^2)$. Comparisons focus on a small set of classical wavelet thresholds and a limited number of real datasets; broader benchmarking (e.g., modern Bayesian/FDR or correlated-noise wavelet methods) and sensitivity analyses for parameter choices ($J_0$, wavelet family, hard vs soft) are limited.",None stated.,"Extend SpcShrink to explicitly handle colored/autocorrelated or heteroskedastic noise (e.g., estimate level-dependent noise models beyond Gaussian/white assumptions) and assess robustness under non-Gaussian errors. Develop theoretical operating-characteristic guarantees (e.g., false retention/removal rates for coefficients, risk bounds) analogous to run-length/false-alarm control in SPC. Provide an open-source implementation/package and study computational scaling and stopping-rule variants, plus broader empirical validation on diverse biomedical and imaging datasets.",2307.10509v1,local_papers/arxiv/2307.10509v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:50:17Z
FALSE,Multivariate|Other,Other,NA,Other,NA,NA,NA,Case study (real dataset)|Other,NA,Not discussed,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/rostyhn/MolSieve,"This paper presents MolSieve, a progressive visual analytics system for exploring and comparing extremely long molecular dynamics (MD) simulation trajectories (materials-focused, e.g., ParSplice ensembles). The system simplifies MD trajectories into super-states and transition regions using GPCCA clustering and an analyst-set membership-probability threshold, then progressively computes analyst-defined properties via Python scripts. For exploration, MolSieve uses coordinated views including transition-region “control charts” (moving-average charts with ±1 SD bands) and optional multi-variate control charts to highlight anomalous sub-regions, plus compact distribution summaries for super-states and a state-space aggregation chart. The approach is evaluated through real case studies with domain experts on platinum nanoparticle simulations and a tungsten defect dataset, demonstrating faster identification and comparison of structural-change regions than manual frame-by-frame workflows. Control charts are used as a visual analytics device within the system, but the paper does not develop SPC theory, chart design optimality, or run-length properties as its primary contribution.","The trajectory simplification assigns each state a GPCCA cluster-membership probability vector; a state is labeled “super-state” if its maximum membership probability exceeds an analyst-chosen threshold (default 0.75), otherwise it is part of a transition region. Transition-region charts plot a moving average of each analyst-defined property; values are colored based on whether the moving average is more than 1 standard deviation above or below the region mean (otherwise within limits). The State Space Chart divides a transition region into 10 segments and highlights states whose observed frequency in a segment exceeds an expected baseline of $1/(\#\text{unique states in segment})$.","MolSieve is demonstrated on multiple ParSplice trajectories, including nano-particle simulations with up to 18,463,872 timesteps and 24,457 unique states, and another with 13,348,978 timesteps and 53,018 unique states; a tungsten defect dataset has ~866 timesteps and 241 states. The paper reports system performance in terms of loading/preprocessing times (e.g., cached load times on the order of ~6–15 seconds for the nano-particle datasets; total preprocessing times reported in Table 1) and shows that experts could identify and compare transition regions and confirm changes using domain analyses (e.g., NEB). Similar-region search can surface candidate regions with nontrivial overlap in unique states (example shown with ~12% similarity). No SPC detection metrics such as ARL/false-alarm rates are reported.","The authors note that the GPCCA-based simplification can sometimes produce too many regions, leading to clutter that requires zooming and could be mitigated with improved level-of-detail rendering. They also report limitations in color encoding when many state IDs or clusters are displayed, including possible color overlap even after adding state clustering. They further mention missing functionality such as listing the most frequent states within a super-state and limited support for direct comparison of individual states.","Because “control charts” are used primarily as visualization cues (moving averages with ±1 SD coloring), the approach does not provide statistical guarantees (e.g., calibrated false-alarm probability) or principled limit-setting under dependence typical in MD trajectories. The anomaly highlighting depends on analyst choices (moving-average window, properties/scripts, thresholds), which may reduce reproducibility across analysts and datasets. Evaluation is largely qualitative via case studies; there is no systematic benchmark against alternative automated change/transition detection methods or ablation of MolSieve components.","The authors plan to address cramped visual encoding space and the need for additional customization, including potentially customizable simplification and distance functions. They propose switching rendering from SVG to WebGL for scalability and improving the 3D rendering pipeline to support additional analyses and rendering techniques. They also mention adding selection recall, a direct state-comparison view, and better 3D rendering support, and express interest in supporting biological simulations to broaden generality.","A natural extension would be to integrate statistically grounded change-point detection methods (with calibrated false-alarm control) alongside the current visual “control chart” cues, especially under autocorrelation. Providing quantitative validation of detection accuracy (precision/recall on labeled transitions, or domain-validated event catalogs) and comparing to automated MD event detection baselines would strengthen evidence. Packaging the property/visualization scripting interface with templates and reproducible configuration files could improve standardization across studies.",2308.11724v2,local_papers/arxiv/2308.11724v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:50:54Z
TRUE,Univariate|Other,Shewhart|Machine learning-based|Other,Both,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,NA,TRUE,NA,Simulation study|Case study (real dataset),Detection probability|False alarm rate|Expected detection delay|Other,"Simulation: T=500 with first 350 (of first 400 in-control points) used for training; remaining 50 in-control + 100 out-of-control used for testing. Case studies: Vibration—Phase I Jan to early Aug 2021 for training, Phase II Aug–Dec 2021 for monitoring; Energy—first 2 weeks (Oct 24–Nov 7, 2022) for training and last 3 weeks for monitoring.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a predictive monitoring control chart for time series data with time-varying variability (heteroscedasticity) using an LSTM-based forecasting framework. In Phase I, multiple LSTMs are trained via bootstrapping to produce a less-biased one-step-ahead prediction and an estimate of model (epistemic) uncertainty; a separate ANN is trained to estimate time-varying data-noise variance, enabling decomposed uncertainty quantification. A Shewhart-type chart is then constructed using pointwise prediction intervals whose limits vary over time according to the combined estimated uncertainties, and Phase II signals when an observation falls outside these limits. Performance is evaluated in simulations on AR(1)-GARCH(1,1) data with mean shifts, showing improved detection versus NN-based benchmarks/ablations under matched false-alarm probability. Two case studies (escalator vibration and escalator energy consumption) demonstrate the method’s ability to flag changes and abnormal events in real sensor streams with nonstationary variability.","The time series is modeled as $t_i=f(\vec{x}_i)+\epsilon_{x_i}$ with total variance decomposition $\sigma^2_{\text{total}}=\sigma^2_{\hat f}+\sigma^2_{\epsilon_{x_i}}$. Bootstrapped LSTM predictors $f_j$ are averaged to form $\hat f(\vec{x}_i)=\frac{1}{b}\sum_{j=1}^b f_j(\vec{x}_i)$ and model-uncertainty variance $\hat\sigma^2_{\hat f}=\frac{1}{b-1}\sum_{j=1}^b(f_j(\vec{x}_i)-\hat f(\vec{x}_i))^2$. The Shewhart-type limits are time-varying: $\text{UCL}_{i+w}=\hat f(\vec{x}_i)+z\,s(\vec{x}_i)$ and $\text{LCL}_{i+w}=\hat f(\vec{x}_i)-z\,s(\vec{x}_i)$ where $s(\vec{x}_i)=\sqrt{\hat\sigma^2_{\epsilon_i}+\hat\sigma^2_{\hat f}}$ and $\hat\sigma^2_{\epsilon_i}$ is estimated by an ANN trained with a log-likelihood-style loss.","Simulation uses AR(1)-GARCH(1,1) series (T=500) with a mean shift at $\tau=401$ and shifts $\delta\in\{0.25,0.5,0.75,1.0,1.5,2.0\}$; hyperparameters are tuned so in-control false alarm probability is near 0.02 (reported FAPs roughly 0.017–0.024 depending on method and $\phi$). Across most scenarios the proposed method has the highest detection rate (DR) and smaller conditional expected delay (CED) than ablated variants and an RNN-residual benchmark, with especially clear advantages under strong autocorrelation (e.g., $\phi=0.9$). For moderate/large shifts (often $\delta\ge 1.0$), DR approaches 1.0 for the proposed method and detection delays drop to a few time steps for larger shifts. Case studies report 29 alarms on escalator vibration data with the first alarm on Sept 7 (linked to maintenance on Sept 6), and 13 alarms on escalator energy data capturing two unexpected shutdowns (Nov 15 and Nov 21) plus mean-shift periods.",None stated.,"The chart is Shewhart-type on one-step-ahead prediction intervals, so it may be less sensitive than memory charts (e.g., CUSUM/EWMA) for very small persistent shifts, especially given the low recall reported for $\delta\le 1.0$. The method depends on multiple design choices (window length $w$, bootstrap size $b$, resample size $n$, early stopping, NN architectures) yet provides limited guidance for practitioners on robust tuning and computational cost tradeoffs. Evaluation focuses on mean-shift detection; performance for variance shifts, autocorrelation structure changes, or heavy-tailed innovations is not established. Software/code is not shared, which limits reproducibility.",The authors note the method can be developed to monitor multivariate data.,"Extend the approach to memory-type signaling (e.g., EWMA/CUSUM on standardized prediction errors) while retaining time-varying uncertainty estimates to improve sensitivity to small shifts. Provide theoretical or calibrated guarantees for in-control false alarm rates under dependence/heteroscedasticity (e.g., conformal or block-bootstrap calibration) rather than relying on simulation tuning. Add missing-data/irregular-sampling handling common in sensor streams and develop diagnostics to attribute alarms to mean vs variance vs model-drift causes. Release an implementation (e.g., Python/R package) and benchmark against classical SPC for autocorrelated/heteroscedastic processes (residual charts, GLR/change-point methods).",2309.01978v1,local_papers/arxiv/2309.01978v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:51:37Z
TRUE,Univariate|Other,Shewhart|Other,Both,Healthcare/medical,FALSE,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate,"Phase I uses k reference samples/subgroups of size m each (total n = m×k). Simulation examples use k=20 with m=25 or 40; the healthcare illustration uses k=5 with m=25. Hybrid censoring parameters include r and x0 (e.g., r=15 or 20 for m=25; r=30 or 35 for m=40; x0=55 or 70; illustration uses r=15, x0=7.6).",TRUE,R,Not provided,NA,"The paper proposes two Phase I/II monitoring schemes for quantiles of the generalized Weibull (GW) distribution when lifetime/reliability data are hybrid censored: a bootstrap-based hybrid-censored chart (BHC) and an asymptotic Shewhart-type hybrid-censored chart (SHC). Parameters are estimated by maximum likelihood under type-I hybrid censoring using an EM algorithm; asymptotic covariance is obtained via the observed Fisher information using the missing information principle to support SHC limits. Because the sampling distribution of estimated quantiles is not available in closed form, BHC sets control limits from parametric bootstrap empirical quantiles at a specified false alarm rate. Performance is evaluated primarily by in-control and out-of-control average run length (ARL) and SDRL across quantiles (p=0.1, 0.5, 0.9), FAR levels, subgroup sizes, and censoring plans, and the method is illustrated on a healthcare dataset (bladder cancer remission times). The authors report that the proposed hybrid-censoring schemes detect shifts effectively (frequency and speed) and can recover several existing percentile charts as special cases (Weibull, generalized exponential, Rayleigh, Burr type X; and type-I/type-II censoring).","The GW p-th quantile is $\xi_p = F^{-1}(p;\theta,\alpha)=\left[\ln\left(\frac{1}{1-p^{1/\alpha}}\right)\right]^{1/\theta}$ (with scale fixed at $\lambda=1$). The BHC chart uses parametric bootstrap: repeatedly sample from $F(x\mid\hat\theta,\hat\alpha)$, re-estimate $(\theta^*,\alpha^*)$ under hybrid censoring, compute $\hat\xi_p^*=F^{-1}(p;\hat\theta^*,\hat\alpha^*)$, and set LCL/UCL as empirical $\nu/2$ and $1-\nu/2$ quantiles. The SHC chart uses asymptotic normality of $\hat\xi_{p,m}$ with $SE_{\xi_p,m}=\sqrt{\frac{1}{m}\nabla\xi_p^T(\hat\Theta_n)\,I_n^{-1}(\hat\Theta_n)\,\nabla\xi_p(\hat\Theta_n)}$ and limits $\bar\xi_p\pm z_{1-\nu/2}SE_{\xi_p,m}$.","In simulations (e.g., $\theta=0.51,\alpha=11.1$, $B=5000$ bootstraps and 5000 Monte Carlo runs), the in-control ARL values for BHC are close to nominal $1/\nu$ (e.g., for $\nu=0.0027$ nominal ARL=370, simulated ARLs in Table 1 are generally near 370 across censoring schemes and quantiles). The paper reports that out-of-control ARL decreases sharply under small/medium parameter shifts; for example, with $\Delta\alpha=0$ and a 4% decrease (increase) in $\theta$, the ARL for the 50th quantile is reduced by about 77.6% (36.3%). In the healthcare illustration monitoring the 90th percentile with $m=25$, $r=15$, $x_0=7.6$, $\nu=0.0027$, BHC limits are UCL=10.564 and LCL=3.742 (CL=6.524) while SHC limits are UCLSH=11.864 and LCLSH=8.802 (CLSH=10.333); under a 15% decrease in $\theta$, BHC produced 6 OOC signals with the first at test sample 1 and SHC’s first signal also at test sample 1. Compared to type-I and type-II bootstrap special cases on the same data, the hybrid-censoring BHC/SHC schemes are reported to signal faster and more frequently.",None stated.,"The methods are fully parametric and rely on correct specification of the generalized Weibull model (with scale fixed at $\lambda=1$), so robustness to model misspecification or heavy-tailed/outlier contamination is unclear. The SHC chart depends on large-sample asymptotics and accurate observed-information estimation under hybrid censoring, which may be unreliable for small subgroup sizes or heavy censoring. The paper reports R-based simulations but does not provide code, which limits reproducibility and practical adoption.",None stated.,"Provide publicly available software (e.g., an R package) implementing the EM estimation and both chart designs, including automatic selection of hybrid censoring parameters and bootstrap settings. Study robustness/extensions to model misspecification (e.g., semi-/nonparametric percentile monitoring) and to dependence/autocorrelation common in time-ordered process data. Develop diagnostic tools to identify whether detected shifts correspond primarily to changes in $\theta$ vs. $\alpha$ and explore adaptive or CUSUM/EWMA versions for faster detection of small sustained shifts.",2309.11776v1,local_papers/arxiv/2309.11776v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:52:25Z
TRUE,Profile monitoring|Bayesian,Other,Phase II|Both,Energy/utilities,NA,FALSE,TRUE,Simulation study|Case study (real dataset),False alarm rate|ARL (Average Run Length)|Other,"Sequential/rolling segments use window length N_w = 500 (short-term) or 1000 (medium-term), with update size N_u = N_w/2. A starting posterior is fit using the first 1000 samples covering a full wind-speed range before sequential updating.",TRUE,None / Not applicable,Not provided,https://doi.org/10.5281/zenodo.5946808,"The paper proposes an online, directional profile-monitoring method for wind turbine power curves when each time segment contains an incomplete wind-speed range. It introduces Copula-based Variational Inference (CVI) to estimate and sequentially update the posterior distribution of monotone I-spline regression coefficients for the power curve, using the prior-to-posterior update from the previous segment to stabilize estimation with limited data. For monitoring, it designs a directional control chart based on a new statistic, the KL-divergence factor, comparing the current posterior of coefficients against an in-control (IEC-standard) distribution and a shifted (degradation) alternative. An alarm is triggered when the derived statistic exceeds a threshold chosen to achieve a specified false-alarm rate / in-control ARL (e.g., ARL≈200 discussed). Using SCADA data from the Penmanshiel wind farm, the method improves power-curve modeling accuracy versus baselines and detects degradation more accurately (higher precision/recall/F1) with few false alarms.","Power-curve profile model: \(\mathbf{Y}_t = \mathbf{Z}_t\boldsymbol\beta_t + \mathbf{e}_t\), with monotonicity enforced via I-splines \(f(x)=\sum_j \beta_j I_j(x)\) and \(\beta_j\ge 0\). CVI variational posterior uses a Gaussian copula with log-normal marginals, e.g., \(q(\Theta_t)=c_{\Sigma}(F(\Theta_t))\prod_j \mathrm{LN}(\beta_{j,t};u_{j,t},\sigma^2_{j,t})\prod_i \mathrm{LN}(\tau_{i,t};c_{i,t},d^2_{i,t})\). Monitoring uses KL-divergence factor leading to statistic \(\Lambda_t\) (Eq. 27), a ratio of KL terms between the current lognormal posterior \(\mathrm{LN}(\mu_\beta,\Sigma_\beta)\) and (i) the in-control reference \(\mathrm{LN}(\mu_0,\Sigma_0)\) and (ii) a degraded alternative \(\mathrm{LN}(\mu_0-\mathbf d,\Sigma_1)\); signal if \(\Lambda_t>h\).","For sequential power-curve prediction, CVI achieves the lowest RMSE and MAE among benchmarks (MGR, GPR, WCDF) on Penmanshiel 02 and 08 for both N_w=500 and N_w=1000 (Table 1). For degradation detection, CVI outperforms LWZ (WCDF+T^2), GPR-based monitoring, and LLR+GLR across precision/recall/F1; e.g., for Penmanshiel 08 with N_w=1000, CVI reports recall 0.926, precision 1.0, F1 0.962 (Table 2). The paper uses degradation tolerance \(\mathbf d=0.1\mu_0\) (10% efficiency drop) and a threshold example \(h=1\) in experiments; threshold selection is linked to controlling false-alarm rate or setting an in-control ARL such as 200.",None stated.,"The monitoring threshold selection is described as empirical (so performance may depend on simulation design and assumed in-control distribution), and the paper does not report standard run-length metrics (e.g., full ARL/SDRL curves) for multiple shift magnitudes/directions beyond the chosen 10% setting. The approach relies on a specific profile model class (monotone I-splines) and parametric posterior forms (Gaussian copula + lognormal marginals), so robustness to model misspecification (non-monotone behavior, heavy tails, serial dependence) is unclear.","Future work includes extending from single-turbine incomplete power-curve monitoring to monitoring multiple turbines simultaneously, and monitoring profile characteristics of different turbine components to support fault diagnosis and better operational understanding.","Provide comprehensive run-length evaluation (IC/OC ARL, SDRL, steady-state ARL) under a range of degradation magnitudes and alternative directions, and study robustness to autocorrelation typical in SCADA streams. Develop/open-source an implementation package and practical guidance for choosing \(\mathbf d\) and \(h\) (including adaptive or economically optimal thresholds), and extend the Bayesian profile model to incorporate additional covariates (temperature, air density, direction) or multivariate/profile-to-profile correlation across turbines.",2311.02411v1,local_papers/arxiv/2311.02411v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:53:09Z
TRUE,Univariate|Profile monitoring|Functional data analysis|Other,Shewhart|Machine learning-based|Other,Phase II,Transportation/logistics|Theoretical/simulation only,NA,FALSE,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length),"Not discussed (but simulations use Phase I reference data split into training/validation/tuning; e.g., 4000/1000/10000 IC samples in simulation; HVAC case study uses 1853 voyages split 740/186/927).",TRUE,R,Public repository (GitHub/GitLab),https://github.com/unina-sfere/FNNCC,"The paper proposes the Functional Neural Network Control Chart (FNNCC) for profile monitoring when a scalar quality characteristic (response) depends on functional (profile) covariates and possibly scalar covariates. An FNN model is fit in Phase I to learn a potentially nonlinear scalar-on-function relationship; Phase II monitoring is then performed on the residuals, using empirical quantile-based control limits estimated from a tuning subset of the in-control reference data. The method targets detection of mean shifts in the scalar response while adjusting for functional covariate information, and it studies robustness when the covariate mean function also shifts. Performance is assessed by Monte Carlo simulation comparing out-of-control ARL (ARL1) at a fixed in-control ARL0 against a functional regression control chart (FRCC; linear) and an unadjusted Shewhart chart (SCC), showing clear gains for nonlinear response–covariate relationships. A railway HVAC case study demonstrates practical use, successfully signaling a diagnosed compressor fault and returning to in-control after repair; code and data are provided online.","The FNNCC monitors residuals from a fitted functional neural network: for observation i, the residual is $e_i=y_i-\hat y_i$, and for a new point $e_{new}=y_{new}-\hat y_{new}$. Control limits are set nonparametrically as empirical quantiles of the in-control residual distribution from a tuning set: $\mathrm{LCL}=Q_{\alpha/2}(e)$ and $\mathrm{UCL}=Q_{1-\alpha/2}(e)$, and an alarm is triggered if $e_{new}$ falls outside these limits. The first functional hidden layer neuron uses functional weights via $h^{(1)}_{ik}=g\left(\sum_{p=1}^P\int_{\mathcal T}\gamma_{kp}(t)X_{ip}(t)dt+\sum_{j=1}^J w^{(1)}_{kj}z_{ij}+b^{(1)}_k\right)$, with $\gamma_{kp}(t)$ approximated by basis expansions.","With $\mathrm{ARL}_0=20$ (\u03b1=0.05), simulations over five scenarios (one linear and four nonlinear mappings) show FNNCC matches FRCC in the linear scenario and substantially improves detection in nonlinear scenarios. For example, in Scenario C at a small shift $\Delta\mu_y=0.5s_y^*$, reported $\mathrm{ARL}_1$ values are 3.46 (FNNCC) versus 11.83 (FRCC) and 11.89 (SCC). In Scenario B, at $\Delta\mu_y=0.5s_y^*$, FNNCC achieves $\mathrm{ARL}_1=9.01$ versus 18.06 for FRCC, while for $\Delta\mu_y=1.5s_y^*$ performance improves to 1.02 (FNNCC) versus 1.84 (FRCC). A covariate mean shift can degrade residual-chart performance (increase ARL1), but FNNCC remains more sensitive than FRCC across scenarios; the HVAC case study flags an out-of-control voyage associated with a known compressor failure and later voyages return within limits after repair.","The paper focuses on prospective residual monitoring (Phase II) and assumes an in-control Phase I reference dataset is available; it notes Phase I analysis is crucial but is not the focus. It also notes that interpretability of conventional neural networks is challenging, motivating the FNN-based approach (while not framing this as a limitation of FNNCC itself). No other explicit methodological limitations are clearly stated.","Control limits are empirical quantiles from a finite tuning set, so achieving the nominal type-I error/ARL0 may require large Phase I samples and may be sensitive to how the Phase I data are split (training/validation/tuning) and to random splitting variability. The approach effectively becomes a Shewhart-type chart on residuals (no memory), so it may be less efficient for very small persistent shifts than EWMA/CUSUM-style residual charts. The method appears to assume independent residuals across time/voyages; serial dependence (common in Industrial IoT/condition monitoring) could inflate false alarms or reduce detection performance unless explicitly modeled.","The authors suggest extending FNNCC to different and more sophisticated monitoring statistics, noting that integrating neural networks into functional data analysis is promising for nonlinear learning with profile covariates.","Develop memory-type versions (e.g., EWMA/CUSUM) on FNN residuals and study their steady-state and transient ARL properties. Provide formal guidance for Phase I sample sizing and for uncertainty in estimated control limits under data splitting and hyperparameter tuning. Extend to autocorrelated/streaming settings (online updating/self-starting) and to multivariate responses or simultaneous monitoring of response and functional covariate shifts, including diagnostics to localize which covariate/region of the functional domain drives alarms.",2311.11050v1,local_papers/arxiv/2311.11050v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:53:51Z
TRUE,Univariate|Other,CUSUM|Change-point|Other,Both,Energy/utilities|Theoretical/simulation only,TRUE,TRUE,FALSE,Simulation study|Case study (real dataset),Expected detection delay|False alarm rate|Other,"In simulations: burn-in 300, model-fitting 600, Phase I m=200, Phase II=100 (50 in-control + 50 out-of-control). For the real-data study, Phase I uses years 2018–2019 and Phase II uses years 2020–2022; no general minimum-sample-size guidance is provided.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a statistical monitoring framework for Temporal Edge Network (TEN) processes, where the network structure is fixed but time series evolve on edges (e.g., cross-border electricity flows). TEN edge series are converted into a node-based representation (edges become vertices) and modeled with GNARX (generalized network autoregressive model with exogenous variables), producing one-step-ahead forecast residuals. Monitoring is performed via residual-based Page-type CUSUM control charts applied to centered squared forecast errors to detect changes affecting mean and/or variance. A simulation study varies changes in GNARX parameters (autoregressive, neighbor, and exogenous effects) and compares different constructed adjacency structures (Erdős–Rényi vs. SBM) using a “cumulative change intensity” function to summarize system-wide signaling. The approach is illustrated on European cross-border physical electricity flows (ENTSO-E), detecting periods associated with COVID-19 disruptions and the 2022 energy crisis/war-related disruptions under several flow-aggregation choices.","TEN series are modeled with GNARX and monitored using forecast errors $u_{\iota,t+1}=x_{\iota,t+1}-\hat x_{\iota,t+1}$. The monitoring statistic adapts Page’s CUSUM to centered squared residuals: $Q_{\iota}(m,k)=\sum_{t=m+1}^{m+k}(u_{\iota,t}-\hat b)^2-\frac{k}{m}\sum_{t=1}^{m}(u_{\iota,t}-\hat b)^2$, and $D_{\iota}(m,k)=\max_{0\le a\le k}|Q(m,k)-Q(m,a)|$. A signal occurs when $D_{\iota}(m,k)$ crosses the upper control limit $UCL=\hat\sigma_{\iota}\,\zeta_{\alpha}\, g(m,k,\nu)$ (with $\nu=0$ used).","In simulation, the method shows essentially no signaling during in-control portions and detects introduced changes after the change-point, with detection speed increasing with change magnitude. Changes in the neighbor-effect parameter $\beta$ are reported as the hardest to detect (wider uncertainty and longer run length than for changes in $\alpha$ or $\gamma_1$). For the real ENTSO-E case study (weekly aggregation; 76 bilateral exchanges), Phase II monitoring flags 27 (M1), 33 (M2), and 39 (M3) country-pairs with detected change points; using a system-level threshold $W=0.2$ corresponds to at least 16 exchanges signaling. Detected periods align with COVID-19 disruptions (roughly 15.03.2020–30.05.2021) and war/energy-crisis-related disruptions beginning February 2022, with examples including RU–UA and BY–UA around early March 2022 and Baltic-state related changes later in 2022.","The framework is applied only to TENs observed at discrete times; extending to continuous-time monitoring is left open. The authors note uncertainty about when time-varying adjacency matrices should be introduced and whether they improve monitoring. They also state that whole-network monitoring via their cumulative change intensity could be replaced by a suitable multivariate control chart. Finally, effectiveness depends on the GNARX model being suitable; if data are not well represented (e.g., count data), alternative/extended modeling is required.","The monitoring runs many parallel univariate charts (one per edge/flow) and then aggregates signals, but the paper does not appear to control the overall false alarm rate across multiple charts (family-wise/FDR) or discuss dependence between chart statistics. Control-limit calibration details (e.g., how $\zeta_\alpha$ is obtained in finite samples, and sensitivity to Phase I estimation error) are limited, which can strongly affect false alarms in practice. Missing data are a stated issue in the electricity dataset selection, but the framework does not incorporate missingness/irregular sampling directly, potentially biasing Phase I estimates and residual properties. The approach relies on model-based residual whitening; model misspecification (seasonality, regime shifts, heavy tails) could inflate false alarms, and robustness checks (e.g., non-Gaussian errors) are not emphasized.","The authors propose research on extending monitoring from discrete to continuous time. They suggest investigating when to use different adjacency matrices over time and whether this improves monitoring performance. They also point to developing a suitable multivariate control chart (or other multivariate monitoring tool) to monitor an entire TEN more directly than via aggregated univariate charts. Additionally, they note the need for alternative/extended modeling when GNARX is not suitable (e.g., for count data).","Develop principled multiple-testing/error-control methods for simultaneous monitoring of many flows (e.g., FDR-controlled signaling or global charting statistics) and quantify how cross-chart dependence affects false alarm rates. Provide robust variants using heavy-tailed or nonparametric residual monitoring, and evaluate performance under model misspecification and strong seasonality typical of energy systems. Incorporate missing-data handling (imputation with uncertainty, state-space/irregular-time GNARX) and assess sensitivity of Phase I calibration to missingness. Release reproducible software (e.g., an R/Python package) that implements TEN construction choices (line graph vs. random graphs), GNARX fitting, and residual-based CUSUM calibration for practitioners.",2312.16357v1,local_papers/arxiv/2312.16357v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:54:32Z
TRUE,Univariate|Other,Shewhart,Both,Manufacturing (general)|Healthcare/medical|Network/cybersecurity|Theoretical/simulation only,FALSE,FALSE,FALSE,Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Other,"Phase I sample size m is studied at m ∈ {100, 200, 500, 1000, 2000, 5000}; the authors suggest Phase I samples with at least 1000 observations for reliable estimation, and note that even >5000 may be needed in some zero-inflated settings.",TRUE,R,Upon request,https://www.R-project.org/,"The paper studies Shewhart-type attribute control charts for zero-inflated Poisson (ZIP) and zero-inflated binomial (ZIB) processes when in-control parameters are unknown and must be estimated from a Phase I sample (Case U). It compares maximum likelihood estimators and method-of-moments estimators for the zero-inflation and count parameters, then quantifies how parameter estimation affects chart performance under an unconditional (marginal) run length perspective. Performance is evaluated via Monte Carlo simulation for many Phase I sample sizes, reporting unconditional in-control and out-of-control ARL and SDRL, and showing large discrepancies versus the known-parameter (Case K) benchmarks, especially for small m and large zero inflation. To improve practice, it proposes “adjusted” design constants L* (used in Shewhart Lσ-type limits) chosen so the unconditional IC ARL in Case U matches a target (often the Case K IC ARL / nominal ARL0≈370.4). Results indicate very large Phase I samples (often m≥1000) are needed; using L* can recover near-nominal ARL while reducing SDRL compared with unadjusted limits.","ZIP pmf: $f_{ZIP}(x\mid\phi,\lambda)=\phi+(1-\phi)f_P(0\mid\lambda)$ for $x=0$, and $(1-\phi)f_P(x\mid\lambda)$ for $x\ge1$; ZIB pmf analog with $f_B(x\mid n,p)$. Shewhart-type limits use mean±$L$sd with integer rounding, e.g. for ZIP: $UCL=\left\lceil\lambda_0(1-\phi_0)+L\sqrt{\lambda_0(1+\lambda_0\phi_0)(1-\phi_0)}\right\rceil$, $LCL=\max\left(0,\left\lfloor\lambda_0(1-\phi_0)-L\sqrt{\lambda_0(1+\lambda_0\phi_0)(1-\phi_0)}\right\rfloor\right)$. With known parameters, run length is geometric with $ARL=1/(1-\beta)$ and $SDRL=\sqrt{\beta}/(1-\beta)$ where $\beta=P(LCL\le Y\le UCL)$; in Case U they use unconditional ARL/SDRL by averaging over the estimator distribution via simulation and tune an adjusted constant $L^*$ to hit a target IC ARL.","Using 50,000 Monte Carlo runs, the paper shows unconditional IC ARL/SDRL can differ dramatically from Case K when parameters are estimated; for example (ZIP, MLE) at $(\phi_0,\lambda_0)=(0.8,4)$ Case K ARL≈234.04, but with m=200 and unadjusted L, ARL≈566.39 and SDRL≈1116.81. Adjusting the design constant to $L^*=4.02$ for that case yields IC ARL≈234.34 and SDRL≈390.11, much closer to target with reduced variability. Similar inflation of ARL/SDRL occurs for ZIB; e.g., (MLE) $(\phi_0,n,p_0)=(0.9,250,0.01)$ has Case K ARL≈242.82, while m=200 gives ARL≈828.37 and SDRL≈2962.55. Across many settings, MLE and MoM results are nearly identical for m≥500, but can differ materially for m=100–200; using $L^*$ produces OOC ARLs closer to Case K than using unadjusted L.","The authors note that the joint distribution of the parameter estimators (e.g., $(\hat\phi,\hat\lambda)$ or $(\tilde\phi,\tilde\lambda)$) is not known, so unconditional performance must be assessed via Monte Carlo simulation. They also note practical difficulties for small Phase I samples when zero inflation is large, since a Phase I sample may contain only zeros, making estimation impossible.","Results focus on i.i.d. observations and do not address serial dependence common in count processes (e.g., time series of incidents), which can distort ARL properties. The work is limited to Shewhart-type charts; for small shifts, memory charts (EWMA/CUSUM) for zero-inflated counts might be more effective and could change conclusions about required Phase I sizes. Guidance for selecting L* relies on extensive simulation and does not provide a closed-form or fast approximation method, which may limit ease of deployment without software.",None stated.,"Develop analytical/approximation methods for unconditional ARL/SDRL (or for calibrating $L^*$) to reduce reliance on large Monte Carlo runs, and provide packaged software to compute adjusted limits. Extend the framework to autocorrelated or nonstationary zero-inflated processes (e.g., ZIP/ZIB with ARMA/GLMM structure) and to memory charts (EWMA/CUSUM) under estimated parameters. Consider robust/diagnostic procedures for Phase I when the sample may be all zeros or contaminated by out-of-control data, and study the impact of model misspecification (model error) in addition to stochastic estimation error.",2401.10605v1,local_papers/arxiv/2401.10605v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:55:17Z
TRUE,Univariate|Other,EWMA|Other,Both,Healthcare/medical|Theoretical/simulation only,FALSE,TRUE,NA,Simulation study|Markov chain|Case study (real dataset),ARL (Average Run Length),Not discussed (Phase I example uses 16 historical series; simulations use R = 10^4 replications).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Stein EWMA control charts for monitoring count processes with Poisson, negative binomial, or binomial in-control marginals, including both i.i.d. counts and AR(1)-type autocorrelated count time series (INAR/IINAR/BinAR). The method leverages Stein identities to construct an EWMA-based ratio statistic expected to fluctuate around 1 in control, and it introduces the choice of a bounded weight function f to target specific distributional departures beyond mean shifts. Different f choices are advocated to increase sensitivity to zero inflation, overdispersion, or underdispersion, enabling targeted diagnosis by running multiple charts in parallel. Performance is assessed primarily via simulated zero-state ARLs (target ARL0 ≈ 370) across various out-of-control scenarios (mean shifts, dispersion changes, zero inflation) and dependence levels, and a health-surveillance case study (emergency department counts) illustrates earlier and more diagnostic signaling than traditional c/EWMA charts. The paper concludes that Stein EWMA charts substantially improve detection for distributional changes, especially overdispersion/zero inflation, and proposes shifted-PMF weights for the challenging underdispersion setting.","The proposed Stein EWMA charts compute exponentially weighted recursions for A_t = \lambda X_t f(X_t) + (1-\lambda)A_{t-1}, C_t = \lambda X_t + (1-\lambda)C_{t-1}, and B_t as an EWMA of the Stein-identity right-hand-side term (distribution-specific). The charting statistic is a ratio Z_t^S expected to be near 1 in control: for Poisson, Z_t^S = A_t/(B_t C_t) with B_t tracking f(X_t+1); for NB and Bin variants, Z_t^S includes additional factors (\nu + C_t) or (n - C_t) in the numerator and B_t tracks (\nu+X_t)f(X_t+1) or (n-X_t)f(X_t+1). Symmetric control limits are used: LCL = 1-L and UCL = 1+L, with L calibrated to achieve a target in-control ARL (typically ≈ 370) at a fixed smoothing \lambda (set to 0.10 in the study).","Across simulations (R = 10^4) with ARL0 calibrated near 370, Stein EWMA charts outperform ordinary EWMA for distributional changes such as overdispersion and zero inflation, while ordinary EWMA remains best when only the mean shifts and the in-control distributional form is preserved. Recommended weights are f(x)=|x-1| for overdispersion and f(x)=|x-1|^{1/4} for zero inflation; for underdispersion, f(x)=1/(x+1) works for low means but can fail for higher means, and a shifted in-control PMF weight f(x)=p(x+2) yields substantially improved underdispersion detection. Autocorrelation (e.g., \rho=0.5 AR(1)-like dependence) generally increases required control limits and worsens ARLs, but Stein EWMA remains superior for dispersion/zero-inflation changes. In a healthcare case study with a Poisson-INAR(1) in-control model (\mu_0=2.1, \rho_0=0.78), Stein EWMA charts for overdispersion/zero inflation signal very early (t=6) on an anomalous day where c-chart and ordinary EWMA signal at t=23 and t=171, and underdispersion-focused Stein EWMAs detect another anomalous day missed by c/EWMA (signals at t=35 and t=74).","The author notes that monitoring underdispersion is particularly demanding and “requires additional research activity,” as some weight choices (e.g., inverse weights) can perform poorly depending on the mean level. The study primarily uses zero-state ARL (change point \tau=1) justified by prior evidence, rather than extensively analyzing steady-state/conditional-delay performance for later change points. The paper also limits design to symmetric control limits to accommodate multiple possible out-of-control scenarios.","The approach requires specifying an in-control marginal distribution (Poisson/NB/Bin) even for time series settings; misspecification of the marginal or of dependence structure could degrade performance, and robustness to such misspecification is not systematically studied. Chart calibration and ARL evaluation rely heavily on Monte Carlo simulation (often with fixed \lambda=0.10), and the paper provides limited guidance on selecting \lambda adaptively or on computational burden for practitioners. Running multiple Stein EWMAs in parallel for diagnosis raises multiplicity/false-alarm concerns, but joint false-alarm control is not developed beyond a brief remark. No software implementation or reproducible code is provided, which may hinder adoption and independent verification.","The paper proposes developing Stein CUSUM charts for count processes, motivated by the often superior out-of-control performance of CUSUM relative to EWMA. It also suggests a residuals-based Stein approach (analogous to earlier residual CUSUM work) to extend applicability to models specified via conditional (rather than marginal) Poisson/NB/Bin distributions. Finally, it points to extending Stein EWMA/CUSUM ideas to continuous-variable monitoring using Stein identities for continuous distributions.","Derive and compare steady-state ARL/conditional expected delay for various change-point locations under autocorrelation to validate the zero-state proxy more broadly for Stein charts. Develop principled, data-driven selection of the weight function f (and possibly \lambda) tailored to practitioner-specified alternatives, including composite alternatives and multi-objective designs. Study robustness to marginal and dependence misspecification (e.g., INAR vs other count TS, or over/under-dispersion beyond modeled families) and incorporate robust/parameter-estimation (Phase I) uncertainty into control-limit calibration. Provide open-source implementations (e.g., R/Python) and computational shortcuts for calibration (e.g., Markov-chain/integral-equation approximations) to facilitate routine use in practice.",2401.11789v1,local_papers/arxiv/2401.11789v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:56:05Z
TRUE,Multivariate|Profile monitoring|Functional data analysis,EWMA|Hotelling T-squared|Other,Both,Transportation/logistics|Manufacturing (general)|Other,NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Other,"Simulation study uses a Phase I sample of 2500 observations (training 1000, tuning 1500) and Phase II composed of 200 sequences with change-point at 100; case study uses Phase I = 568 profiles split equally into training/tuning and Phase II sets of sizes 220, 368, and 184. No general minimum-sample guidance is provided beyond these study designs.",TRUE,R,Supplementary material (Journal/Publisher),https://doi.org/10.1080/00224065.2024.2383674,"The paper proposes an Adaptive Multivariate Functional EWMA (AMFEWMA) control chart for monitoring multivariate functional data (profiles), extending adaptive EWMA ideas to the profile-monitoring setting. The method forms an EWMA-like recursion with time- and component-varying weights determined by score functions of the current functional residuals, so it behaves like an EWMA for small deviations and like a Shewhart-type chart for large deviations. Monitoring is performed via a functional Hotelling-type statistic based on a truncated multivariate functional PCA (MFPCA) representation of the AMFEWMA statistic and an estimated covariance operator. Parameters (notably λ and k) are selected via an optimization scheme targeting good ARL across both small and large mean shifts while meeting a specified in-control ARL0, with the control limit calibrated by bootstrap. Extensive Monte Carlo studies (two out-of-control scenarios with multiple severity levels) show AMFEWMA* achieves near-best ARL across shift magnitudes compared with a multivariate functional EWMA and an MFPCA-based Shewhart/Hotelling chart. A case study on resistance spot welding dynamic resistance curves (automotive body-in-white) demonstrates practical performance across electrode wear levels.","The adaptive functional EWMA recursion is $\mathbf Y_n(t)=(\mathbf I-\Lambda_n(t))\mathbf Y_{n-1}(t)+\Lambda_n(t)\mathbf X_n(t)=\mathbf Y_{n-1}(t)+\Lambda_n(t)\mathbf E_n(t)$ with $\Lambda_n(t)=\mathrm{diag}(w(E_{n1}(t)),\dots,w(E_{np}(t)))$ and $w(u)=\eta(u)/u$ for proposed score functions $\eta_1$ or $\eta_2$ (piecewise-linear or smooth) controlled by parameters $(\lambda,k)$ and thresholds $C_j(t)=k\sigma_j(t)$. The monitoring statistic is a functional Hotelling-type quadratic form $V_n^2=\sum_{i=1}^p\sum_{j=1}^p\int\!\int Y_{ni}(s)K^*_{ij}(s,t)Y_{nj}(t)\,ds\,dt$ with $K^*_{ij}(s,t)=\sum_{l=1}^L \rho_l^{-1}\psi_{li}(s)\psi_{lj}(t)$ from MFPCA eigenpairs $(\rho_l,\psi_l)$. Control limit $h$ is chosen to achieve a target in-control ARL0 using a Phase I bootstrap calibration.","In simulation (two out-of-control scenarios with severity levels SL=1..6, target ARL0=20), AMFEWMA* attains ARL close to the best competitor at each SL, matching MFEWMA with small $\lambda$ for small shifts and matching the Shewhart/Hotelling approach for large shifts; it also has the smallest Relative Mean Index (RMI) among methods (Scenario 1 RMI: 0.27; Scenario 2 RMI: 0.49). In the resistance spot welding case study (p=10 DRCs per body), AMFEWMA* with $(\lambda^*,k^*)=(0.5,4)$ yields ARL 2.92 (wear level 1), 1.30 (wear level 2), and 1.085 (wear level 3), staying very close to the best-performing comparator in each wear regime. The competing best ARLs are MFEWMA $\lambda=0.5$ for wear level 1 (2.905) and Shewhart for wear levels 2 and 3 (1.25 and 1.025).",None stated.,"The chart design and control-limit calibration rely heavily on bootstrap resampling and tuning/training splits; performance may depend on the representativeness and size of the Phase I sample and the specific bootstrap settings (e.g., sequence length, number of sequences). The approach assumes independent profiles over time (no explicit modeling of serial correlation) and does not address missing/irregularly sampled functional observations beyond the initial smoothing step. Computational cost and sensitivity to smoothing/basis choices and to the choice of $L$ (variance-explained threshold) could affect practical deployment, but comprehensive robustness analyses to these choices are limited.",None stated.,"Extend AMFEWMA to explicitly handle autocorrelated/streaming functional data (e.g., via functional time-series models or residual-based charts) and to missing/irregular sampling without requiring dense regular grids. Develop theoretically grounded or faster control-limit approximations (beyond bootstrap) and study robustness to smoothing/basis selection and to Phase I contamination. Provide packaged software/workflows for practitioners (including diagnostics to interpret which channels/time regions drive a signal) and evaluate the method on additional real multichannel profile applications beyond resistance spot welding.",2403.03837v2,local_papers/arxiv/2403.03837v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:56:51Z
FALSE,Univariate,Other,Phase I,Service industry|Other,NA,FALSE,FALSE,Case study (real dataset),Other,"Uses 5 repeated observations (time study measurements) per activity; data sufficiency test at 95% confidence and 5% precision yields required N′ values around 3.36–4.98, concluding N=5 is sufficient for each activity.",FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is a time-and-motion (stopwatch) study to standardize working time for retail product display activities at PT XYZ Branch in Bandung, Indonesia, focusing on two product types (X Milk and Y Bread). The authors map the display processes, collect repeated time observations, and apply a data sufficiency test (95% confidence, 5% precision) to confirm that five observations per activity are adequate. They use simple control charts (control diagrams with UPL/LCL) to assess consistency of observed times and identify time deviations in several activities, especially for X Milk. They then compute performance rating using the Westinghouse method (rating factor 1.14), apply an allowance of 12.5% (60 minutes break over 480 minutes working time), and calculate normal and standard times. Final standard times are reported as 15.83 minutes per 20 units for X Milk and 9.18 minutes per 20 units for Y Bread, intended to improve operational consistency and control.","Data sufficiency test: $N' = \left[\frac{k}{s}\frac{\sqrt{N\sum x_i^2-(\sum x_i)^2}}{\sum x_i}\right]$ with $k=2$ (95% confidence) and $s=0.05$ (5% precision). Normal time: $N_t = (\text{Cycle Time})\times (\text{Rating Factor})$ where the rating factor is derived from Westinghouse components (skill, effort, conditions, consistency). Standard time: $S_t = N_t\times \frac{100\%}{100\%-\text{allowance}\%}$ with allowance computed as $(\text{break time}/\text{working time})\times 100\%$.","Allowance is calculated as $60/480\times 100\% = 12.50\%$ and the Westinghouse rating factor is set to 1.14 (Good/C1 for all four components), applied across all activities and products. Data sufficiency results show required sample sizes $N'$ between 3.36 and 4.74 for X Milk activities and 4.42 to 4.98 for Y Bread activities, so $N=5$ observations are deemed sufficient. Control-chart checks (UPL/LCL) indicate time deviations for X Milk in multiple activities, with an example showing points outside limits on the 2nd and 4th observation for carrying products. Total standard time for the X Milk display process is 15.83 minutes per 20 units (activity-level standard times: 2.04, 2.52, 3.71, 1.76, 2.06, 3.74 minutes). Total standard time for the Y Bread display process is 9.18 minutes per 20 units (1.13, 3.33, 1.58, 3.14 minutes).",None stated.,"The control-chart use is descriptive and not a primary SPC contribution; chart type/parameters (e.g., basis for UPL/LCL, subgrouping rationale, and assumptions) are not fully specified, limiting reproducibility. The study uses a small number of repeats (five) and appears to pool/assume a single constant rating factor (1.14) for all activities and both products, which may not reflect task-to-task or worker-to-worker variability. Results are from a single branch/store context and may not generalize to other stores, staffing levels, congestion conditions, or demand patterns. No economic impact, sustained monitoring plan, or validation that standard times improve performance over time is provided.",None stated.,"Future work could validate the proposed standard times prospectively (Phase II-style) over longer periods and across multiple stores/shifts to assess stability under varying workload and congestion. Robust/replicated rating-factor estimation (multiple raters, inter-rater reliability) and task-specific allowances could improve accuracy and fairness. A clearer, formal SPC framework (explicit chart type, rational subgrouping, and signal rules) could be used to continuously monitor adherence to standard times and identify assignable causes. Providing reproducible calculation templates or software (e.g., spreadsheets/R scripts) would improve adoption and auditability.",2403.09138v1,local_papers/arxiv/2403.09138v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:57:25Z
TRUE,Univariate|Other,Shewhart|Other,Phase I|Phase II,Healthcare/medical|Pharmaceutical|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,Not discussed (authors note dispersion estimates can be inaccurate for small numbers of historical clusters and only fit models to Ames subsets with at least 5 historical control groups).,TRUE,R,Package registry (CRAN/PyPI),https://cran.r-project.org/package=predint,"The paper develops historical control limits (HCL) for clustered, overdispersed Poisson-type count data, motivated by pre-clinical (Ames assay revertant counts) and medical (multiple-sclerosis relapse counts) quality control. It proposes prediction-interval-based HCL under (i) quasi-Poisson variance inflation and (ii) negative-binomial (gamma–Poisson mixture) models, explicitly incorporating variable exposure/cluster sizes via offsets. To address strong right-skewness and to achieve equal-tail error rates, it introduces a parametric bootstrap calibration algorithm that separately calibrates lower and upper interval coefficients to hit the desired tail coverages. Monte Carlo studies compare eight HCL methods and show that traditional Shewhart-style limits (c/u charts) and mean±2SD heuristics fail to achieve nominal coverage/type-I error under overdispersion, while bootstrap-calibrated prediction intervals control coverage best, even with small historical sample sizes. Real-data demonstrations illustrate improved Shewhart-type charts using calibrated limits for Ames historical controls and for patient/center relapse monitoring.","Counts are modeled with offsets: $E(Y_h)=n_h\lambda$. Overdispersion is handled by quasi-Poisson $\operatorname{Var}(Y_h)=\phi n_h\lambda$ or negative-binomial $\operatorname{Var}(Y_h)=n_h\lambda(1+\kappa n_h\lambda)$. The proposed (uncalibrated) PI forms are $[l,u]=n^*\hat\lambda \pm z_{1-\alpha/2}\,\mathrm{SE}_{\text{pred}}$ with $\mathrm{SE}_{\text{pred}}$ adapted to the chosen overdispersion model (eqs. 13–14), and the bootstrap-calibrated versions replace $z$ by separately calibrated $q_l^{\text{calib}},q_u^{\text{calib}}$ (eqs. 15–16).","In Monte Carlo simulations targeting 95% two-sided coverage, classical Shewhart c/u chart limits achieved nominal coverage only when data were close to Poisson; with overdispersion, their coverage could drop dramatically (reported down to ~58% in some settings). Mean±2SD and Laney-style adjusted u-chart limits approached nominal coverage only with large historical sample sizes but did not achieve equal-tail probabilities under skewness. Bootstrap-calibrated quasi-Poisson and negative-binomial prediction intervals produced coverage probabilities close to nominal and markedly improved balance of lower vs upper tail coverages (approaching 97.5% per tail for 95% two-sided limits). For one-sided (upper) limits with varying offsets, calibrated limits remained close to 95% and were fairly robust to model misspecification, though negative-binomial GLM fits frequently failed to converge when $\lambda$ was very small and $H\le10$ (high zero inflation).","The authors note that prediction limits are pointwise and rely on the strong exchangeability assumption between historical and current data-generating processes. They also state that using limits to evaluate an entire current trial would require simultaneous prediction intervals accounting for within-study correlation/multiplicity, and that (to their knowledge) such methodology is not available. They report practical convergence issues for negative-binomial GLMs (MASS::glm.nb), especially with many zeros and small $H$.","The bootstrap calibration is parametric and inherits any misspecification from the fitted quasi-Poisson/negative-binomial model and the assumed offset structure; robustness to zero-inflation or structural changes is limited. The method focuses on coverage calibration rather than standard SPC run-length behavior (ARL/ATS), so practitioners interested in ongoing sequential monitoring may need additional design/tuning guidance. Computational cost can be high (e.g., B=10,000 bootstrap samples per limit), which may be burdensome for routine, real-time deployment without optimization.",They highlight the need for methodology to compute simultaneous prediction intervals for assessing whole current trials while accounting for within-trial correlation and multiple testing. They also emphasize assay-/community-specific work to define what magnitudes of between-cluster overdispersion are tolerable and how to manage controllable vs uncontrollable sources of variability.,"Extend the approach to zero-inflated and hurdle count models common in medical event data, with corresponding calibrated limits. Develop sequential-monitoring performance characterizations (e.g., steady-state ARL/ATS) for the calibrated limits to better align with SPC design objectives. Provide faster implementations (e.g., variance-reduced/bootstrap-approximation or analytic calibration) and broader software tooling (diagnostics for exchangeability, drift, and model fit) to support routine operational use.",2404.05282v1,local_papers/arxiv/2404.05282v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:58:11Z
TRUE,Univariate|Other,CUSUM,Phase II,Energy/utilities,TRUE,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"Not discussed (training uses multiple years of 10-minute SCADA data; example dataset spans 2017-09-25 onward with 846,968 filtered records across 6 turbines; one turbine example uses 130,849 train and 32,713 test intervals).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a wind-farm condition monitoring system based on probabilistic normal-behaviour modelling using a probabilistic multi-layer perceptron (PMLP) that outputs a heteroscedastic Gaussian predictive distribution for turbine power, plus a transfer-learning variant (LPMLP) that pre-trains on all turbines and fine-tunes to a specific turbine. For monitoring, it constructs standardized residuals from the predictive mean and standard deviation and applies a two-sided tabular CUSUM control chart with empirically tuned decision interval to balance precision/recall. The approach is evaluated on real 10-minute SCADA data from six turbines (Kelmarsh wind farm, UK), showing improved predictive accuracy and calibration (coverage) versus sparse Gaussian processes and a Bayesian neural network baseline. A case study demonstrates early warning: the CUSUM signals out-of-control ~2 days before a recorded converter-error forced outage. The paper emphasizes practical issues such as large datasets and missing data, addressed by pooling turbines and transfer learning.","The probabilistic model assumes power output $y\mid x \sim \mathcal N(\mu(x),\sigma^2(x))$, where $\mu(x)$ and $\sigma(x)$ are produced by an MLP with branching heads (Softplus on the $\sigma$ head). Training minimizes the negative log-likelihood $L_1=-\sum_i \log\mathcal N(y_i\mid \mu(x_i),\sigma^2(x_i))$ (and for all turbines $L_2=-\sum_{j}\sum_i \log\mathcal N(y_{ij}\mid \mu(x_{ij}),\sigma^2(x_{ij}))$). Monitoring uses standardized residuals $v_t=(y_t-\mu(x_t))/\sigma(x_t)$ and a two-sided tabular CUSUM: $S_H(t)=\max\{0, v_t-k+S_H(t-1)\}$ and $S_L(t)=\max\{0,-k-v_t+S_L(t-1)\}$; signal if either exceeds decision interval $I$.","On real SCADA data, the best model (LPMLP A1) achieved RMSE 27.38 kW and MAE 14.81 kW (NRMSE 1.34%, NMAE 0.72% of rated power 2050 kW), outperforming sparse GP (RMSE 49.57, MAE 33.87) and a Bayesian neural network (RMSE 40.28, MAE 22.54). Calibration improved substantially for LPMLP A1 with MCE 0.93% and empirical coverage of 94.56% (nominal 95%) and 98.40% (nominal 99%). In an early-warning example, the CUSUM first signals out-of-control at 2020-06-30 04:50, preceding a forced outage alarm at 2020-07-02 13:35. A decision-interval tuning exercise suggests $I=15$ yields precision 0.68, recall 0.77, and mean notice time 15.49 hours (SD 21.76) over 22 fault/22 normal 72-hour windows.","A decision-interval ($I$) selection analysis using historical anomalies is limited because detailed anomaly labels are unavailable: out of ~163K recordings, only 22 faults are reported. The authors note that this prevents fully data-driven tuning of $I$ for their dataset beyond a small balanced exercise.","The CUSUM design assumes the standardized residuals $v_t$ are i.i.d. $\mathcal N(0,1)$ under normal operation; with 10-minute SCADA data, residual autocorrelation/seasonality could inflate false alarms if not modeled. The chart thresholds are chosen via heuristic/empirical tuning (precision/recall) rather than ARL/ATS calibration, so false-alarm rates are not controlled in the classical SPC sense. The monitoring is effectively univariate (power residuals) and may miss faults that do not manifest as power deviations until late, limiting sensitivity for certain failure modes.","The authors propose extending the approach to identify which turbine component/operational characteristic caused the anomaly by monitoring additional SCADA variables (e.g., temperatures/pressures) and building a collection of CUSUM charts for different outputs. They also suggest replacing fine-tuning via parameter initialization with a Bayesian transfer-learning approach that uses pre-trained parameters to construct informative priors for a Bayesian neural network.","Calibrating CUSUM decision limits to target in-control ARL/false-alarm rates (potentially under autocorrelation) would strengthen the SPC validity and make settings transferable across farms. Extending the monitoring to multivariate charts (e.g., vector residuals across multiple outputs/sensors) could improve diagnosis and detection power. Providing open-source implementation and a reproducible benchmark (including preprocessing/filtering rules) would improve practical uptake and facilitate fair comparisons.",2404.16496v2,local_papers/arxiv/2404.16496v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:58:48Z
TRUE,Univariate|Other,Shewhart|CUSUM|Change-point,Phase II,Environmental monitoring|Other,NA,FALSE,FALSE,Simulation study,Detection probability|False alarm rate|Other,Not discussed,TRUE,Python,Not provided,NA,"The paper proposes a real-time pipe-burst localization approach for water distribution networks (WDNs) that relies on change-point detection (CPD) applied to streaming nodal pressure readings from sensors. It evaluates two SPC/CPD methods—CUSUM and a Shewhart-type control-chart rule—to detect abrupt pressure changes and then localizes the burst by identifying the node with the earliest/large-amplitude transient response and inferring the likely burst link via graph-based logic. Burst events and pressure time series are generated via transient simulations using TSNet (built on WNTR/EPANET models) to emulate real-time data availability. Performance is assessed primarily by burst-link localization accuracy across multiple sensor capture intervals (0.2, 2, 5, 10 seconds), showing that pairing the localization logic with CUSUM yields better average accuracy (70%) than Shewhart (65%), though Shewhart is best at the finest capture interval (96% vs 92% at 0.2 s). The work contributes an SPC-inspired, low-data-requirement alternative to ML-heavy localization methods, emphasizing rapid localization (about one second in their best setting) without historical labeled burst data.","CUSUM is defined using two one-sided cumulative sums on successive differences: $C_i^+ = \max(0, C_{i-1}^+ + (x_i-x_{i-1})-\text{drift})$ and $C_i^- = \max(0, C_{i-1}^- - (x_i-x_{i-1})-\text{drift})$; a change is signaled when either exceeds a threshold. The Shewhart-style limits are given as $\text{UCL}=\mu+s\sigma$ and $\text{LCL}=\mu-s\sigma$ (with $\mu,\sigma$ computed from the window/data), and an alarm occurs when observations exceed limits (in their implementation, also exceeding an amplitude threshold). Localization accuracy is computed as $\text{Acc}=\frac{P_c}{P_t}\times 100\%$, where $P_c$ is the number of correctly localized burst pipes and $P_t$ is the total number of pipes.","Across capture intervals of 0.2, 2, 5, and 10 seconds, reported localization accuracies (Table V) are: CUSUM = 92%, 72%, 52%, 64% (average 70%); Shewhart = 96%, 72%, 32%, 60% (average 65%). The best-performing setting for both methods is capture interval 0.2 s with localization interval 5, yielding 96% (Shewhart) and 92% (CUSUM). For the tested WDN (25 pipes), the counts of correctly located bursts at 0.2 s are 23/25 for CUSUM and 24/25 for Shewhart (Tables III–IV), with performance degrading as capture interval increases, especially for Shewhart at 5 s (8 correct, 32%).",The authors state that further experiments are needed on different WDNs to assess performance and accuracy on more complex network models and on real-world WDN data. They also note that parameter optimization for the approach should be explored to improve accuracy and overall performance.,"Results are based on simulated transient data from a single relatively small WDN (15 junctions, 25 pipes), so generalization to larger/looped/complex networks, different sensor placements, and real SCADA noise/latency is uncertain. Thresholds and localization intervals are tuned by trial-and-error, which can bias comparisons and may not transfer across networks or operating regimes without a principled design/optimization. The study focuses on localization accuracy but does not report standard SPC metrics (e.g., in-control ARL/false-alarm rate under no-burst conditions or detection delay distributions), limiting interpretability from an SPC perspective.",They propose conducting further experiments on different WDNs and using real-world WDN data to evaluate robustness and accuracy. They also suggest exploring parameter-optimization approaches to improve the model’s accuracy and performance.,"Develop a principled calibration method (e.g., ARL/false-alarm constrained design, Bayesian or robust thresholding) that adapts thresholds to operating conditions and sampling rates without trial-and-error. Extend the method to explicitly handle autocorrelated/seasonal pressure patterns, sensor noise, missing/irregular readings, and communication delays typical in SCADA/IoT deployments. Provide open-source implementation and benchmark comparisons against other CPD/localization approaches (e.g., GLR, EWMA, wavelet-based detectors) on multiple public WDN models and real datasets, including runtime and detection-delay metrics.",2407.09074v1,local_papers/arxiv/2407.09074v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T03:59:24Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Both,Manufacturing (general)|Transportation/logistics|Energy/utilities|Other,NA,FALSE,NA,Simulation study|Case study (real dataset),Detection probability|False alarm rate|Other,Not discussed.,TRUE,None / Not applicable,Not provided,https://csegroups.case.edu/bearingdatacenter/pages/welcome-case-western-reserve-university-bearing-data-center-website.,"The paper proposes a multivariate statistical process control (MSPC) fault-detection framework for rolling-element bearings that converts univariate vibration time series into multivariate batch features via windowed Fourier-transform component extraction. A PCA model is trained on healthy (in-control) data (offline modeling), then Hotelling’s $T^2$ and Squared Prediction Error (SPE, denoted SPEx) charts are used for online monitoring of new batches against calibrated control limits. The method includes optimization of batch length, number of retained Fourier components, selected feature subset, and number of PCs using a genetic algorithm to reduce false alarms and improve detection. Validation on the CWRU bearing dataset across fault types (ball/inner/outer race), fault sizes, sensor locations (Drive End/Fan End), and loads shows faults exceeding control limits reliably; individual-load models report 100% fault detection with 100% normal identification, while combined-load models maintain 100% detection but introduce ~0.001% false alarms. The contribution is an MSPC-oriented, batch-based FT feature pipeline that enables multivariate control-chart monitoring without needing faulty training data, positioned as a practical alternative to supervised/deep-learning approaches for condition monitoring.","The method computes a DFT for each batch (window) of length $N$: $FT_k=\sum_{n=0}^{N-1} x_n e^{-i2\pi (k/N)n}$ and reconstructs components via inverse DFT. Each batch $t$ is decomposed into dominant Fourier components plus residual: $x_t=\sum_{i=1}^{I} FT_{i,t}+r_t$, and features (e.g., magnitude, frequency, standard deviation) are assembled into a multivariate matrix $X$ and standardized $Z=(X-\bar X)/\sigma_X$. PCA is fit as $Z=T P^\top+E$, then monitoring statistics are Hotelling’s $T^2=\sum_{j=1}^{a} t_j^2/\lambda_j$ and $SPEx=\sum_i (z_i-\hat z_i)^2$ (with $\hat Z=T P^\top$), compared to limits; the paper also uses heuristic warning/alarm limits $\bar T^2+2\sigma_{T^2}$ (95%) and $\bar T^2+3\sigma_{T^2}$ (99.8%).","On the CWRU dataset, the proposed batch-based FT+PCA MSPC charts flag all tested bearing faults (inner/outer race and ball) as out-of-control across studied fault diameters (0.007–0.021 in) and sensor locations (DE, FE), with faults clearly exceeding both $T^2$ and SPEx control limits. Individual motor-load PCA models are reported to achieve 100% failure detection and 100% accuracy for normal-operation identification (no false alarms). A combined-load (0–3 hp) model retains 100% fault detection but produces a small false-alarm rate reported as <0.001% of observations. Example optimized hyperparameters reported include an optimal batch length of 5180 timestamps in one setting, and for a combined-load model an optimum of 4 PCs, 3 FT components, and batch length 6411; after variable selection for a 0 hp model, a minimum set of 6 variables is reported (FT1 freq, FT2 freq, FT3 mag/var/freq, and residual variance) with PCs reduced to 4.",The study is limited to evaluation on the CWRU bearing dataset. The authors note that further research should examine suitability under installation effects during normal operation and scenarios with multiple simultaneous faults.,"Control limits are set using mean±(2,3)SD heuristics for $T^2$ (and implicitly for SPEx), rather than standard distribution-based limits for PCA MSPC (e.g., $F$-based $T^2$ limits and Jackson/Mudholkar-type SPE limits), which may affect nominal false-alarm control and comparability across settings. The approach assumes batch independence/approximate stationarity within windows and does not explicitly address serial correlation typical in vibration features, which can inflate false alarms. Computational details (implementation language, reproducibility artifacts, and exact GA settings/search space) are not provided, limiting repeatability and practical adoption.",The authors suggest extending validation beyond CWRU by studying the method under installation effects in normal operation and by considering multiple simultaneous faults when assessing suitability for fault detection.,"Develop principled control-limit calibration for both $T^2$ and SPE (including small-sample Phase I effects) and evaluate in-control ARL/ATS to benchmark against established MSPC designs. Extend the framework to explicitly handle autocorrelated batch features (e.g., dynamic PCA, residual whitening, or state-space modeling) and assess robustness to varying speeds/loads via adaptive or covariate-adjusted models. Provide open-source implementation and standardized benchmarks on additional real industrial datasets to validate generalizability and deployment readiness.",2407.17236v2,local_papers/arxiv/2407.17236v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:00:01Z
TRUE,Univariate|Nonparametric|Image-based monitoring|Other,Shewhart|EWMA|Other,Phase II|Both,Environmental monitoring|Manufacturing (general)|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate|Other,"No explicit Phase I sample-size requirement; SOP charts are designed to be distribution-free and ""do not require prior Phase-I analysis."" Examples/simulations use grids such as (m,n) ∈ {(10,10),(15,15),(25,25),(40,25)} and note that m=n=1 is possible but performs poorly (large detection delays).",TRUE,Julia,Public repository (GitHub/GitLab),https://github.com/AdaemmerP/OrdinalPatterns.jl,"The paper proposes nonparametric control charts for monitoring spatial dependence in streams of regular rectangular (lattice) datasets, filling a gap where existing nonparametric spatial monitoring largely targets mean shifts. The method uses 2×2 spatial ordinal patterns (SOPs) and their three “types” to build four distribution-free charts based on statistics (τb, κb, τe, κe), implemented as both memoryless Shewhart-type charts and EWMA-smoothed versions. It also develops higher-order dependence monitoring via delayed SOPs and a new Box–Pierce-style aggregated SOP statistic that sums squared deviations across multiple spatial lags. Performance is evaluated primarily via extensive Monte Carlo simulations of zero-state ARL under many out-of-control scenarios (linear/nonlinear, unilateral/bilateral, contaminated with outliers, continuous and discrete with jittering), with parametric spatial-ACF charts as competitors; results show τe-EWMA is generally the most robust and effective, especially under nonlinear/bilateral dependence or outliers. The charts are illustrated on real datasets for heavy rainfall (Germany), war-related fires (Ukraine), and textile defect images, and the authors provide a Julia package implementing the methods.","From each rectangular dataset at time t, compute SOP type frequency vector \(\hat p_t\) (or EWMA-smoothed \(\hat p_t^{(\lambda)}=\lambda \hat p_t+(1-\lambda)\hat p_{t-1}^{(\lambda)}\)). The four charting statistics are functions of type frequencies: \(\hat\tau_b=\hat p_1-1/3\), \(\hat\kappa_b=\hat p_2-\hat p_3\), \(\hat\tau_e=\hat p_3-1/3\), \(\hat\kappa_e=\hat p_1-\hat p_2\); signal when \(|\cdot|\) exceeds two-sided limits \(l_{\cdot,\lambda}\). For higher-order dependence, define delayed SOP charts \(\hat\tau_{e,t}^{(\lambda,d)}=\hat p_{t,3}^{(\lambda,d)}-1/3\) and a Box–Pierce-type aggregate \(\tau_{e,t}^{BP(\lambda,w)}=\sum_{d_1,d_2=1}^w(\hat\tau_{e,t}^{(\lambda,d)}-\tau_{e,0}^{(d)})^2\) (with \(\tau_{e,0}^{(d)}=0\) under IC-iid).","Control limits are calibrated by simulation to achieve target in-control ARL0 (illustrated with ARL0≈370); e.g., for (m,n)=(10,10) and λ=0.10, simulated limits include \(l_{\tau_e,\lambda}=0.03174\) and for the ACF competitor \(l_{\rho,\lambda}=0.05313\). In simulations, the ACF-based chart is best for well-specified linear unilateral SAR dependence, but SOP charts (especially τe-EWMA) outperform under additive outliers, nonlinear (quadratic MA-type) dependence, and bilateral dependence; the paper reports cases where τe can be much faster (e.g., for nonlinear SQMA scenarios on (10,10), up to ~22× smaller OOC-ARL than the ACF chart). The parametric ACF chart’s IC-ARL is sensitive to marginal distribution misspecification (e.g., for (10,10) using N(0,1) limits with non-normal IC data, IC-ARLs ranged roughly from ~349 to ~591), whereas SOP charts remain distribution-free under continuous margins. In the rainfall case study, a τe-EWMA chart (λ=0.1) signals at hour 138 and continues alarming through the heavy-rain period; BP aggregation (w=3) does not improve detection because first-order dependence is already strong. For textile images, an IC-iid design fails, and a bootstrap/resampling-based calibration detects defects in patches 96–98 using τe-EWMA.","For discrete data, the proposed tie-handling via adding uniform noise (“jittering”) introduces randomness so computed SOP statistics depend on the particular noise realization, and ties may occur frequently for count data. The authors also note that very small grids (m=n=1) have low information content; although charts can be defined (with λ<1), OOC-ARLs decrease only slowly, implying large detection delays. They further highlight that ACF-based competitor charts are parametric and require correct in-control model specification, otherwise IC-ARL can deviate substantially from target.","The main in-control assumption for the distribution-free guarantees is that each rectangular dataset is i.i.d. within the lattice (spatial independence) and independent across times t; many real spatial processes exhibit baseline dependence, temporal correlation, or nonstationarity that would require explicit modeling or alternative IC definitions. Control limits are obtained by heavy Monte Carlo calibration (up to 10^6 runs for design), which may be computationally burdensome for practitioners without access to HPC. The methods focus on 2×2 SOP types (a coarse 3-category reduction), which may lose power for certain structured dependence changes or anisotropies not well captured by the type partition.","The authors propose developing “refined types” (finer partitions of SOPs beyond the 3-type scheme) to reduce information loss and potentially improve detection for some OOC scenarios. They also suggest alternatives to jittering for discrete data where ties may carry information, e.g., extending generalized ordinal patterns to a feasible notion of “generalized SOPs.” Finally, they recommend exploring Box–Pierce-type SOP aggregations in other ordinal-pattern testing/monitoring contexts beyond the spatial setting.","Extending the charts to handle temporal dependence between successive rectangular datasets (autocorrelated charting statistics) would broaden applicability to spatio-temporal streams. Developing principled Phase-I procedures for estimating baseline (non-iid) spatial dependence and then monitoring for changes relative to that baseline (robust Phase I/Phase II transition) could address common real-image/textured-surface IC structures. Providing lightweight approximations (e.g., asymptotic/Markov-chain/integral-equation approximations) or adaptive/sequential limit estimation could reduce reliance on massive simulation for calibration.",2408.17022v2,local_papers/arxiv/2408.17022v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:00:57Z
TRUE,Univariate,CUSUM|Other,Phase II,Food/agriculture|Manufacturing (general)|Healthcare/medical|Pharmaceutical|Environmental monitoring|Network/cybersecurity|Finance/economics|Energy/utilities|Transportation/logistics|Other,TRUE,FALSE,NA,Approximation methods|Economic design|Other,ARL (Average Run Length)|False alarm rate|Other,"In the numerical example, the decision variables are constrained to n ∈ {2,…,20}, h ∈ [0.01, 2] hours, and H ∈ [0.0001, 5]. The statistical constraints used are ARL0 ≥ 200 and ARLδ ≤ 14.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a multi-objective economic–statistical design for a two-sided CUSUM chart to monitor shifts in the process mean while balancing economic and statistical performance. The two objectives are minimizing expected cost per cycle (CE) from the Lorenzen–Vance (1986) cost model and minimizing the out-of-control average run length (ARLδ), subject to constraints on in-control and out-of-control ARL. Average run lengths are computed using Siegmund (1985) approximations (recommended by Woodall and Adams, 1993) rather than Markov-chain evaluation. The multi-objective optimization over design variables (n, sampling interval h, and decision interval H; with reference value K fixed at δ/2) is solved via NSGA-II to generate Pareto-optimal solutions. In a numerical example adapted from Lee (2011), the proposed approach achieves a minimum CE of $9.50 with (n=2, h=0.36 hours, H=4.19), yielding about a 43% reduction in CE compared with Lee’s single-objective design, and sensitivity analyses examine effects of cost/time parameters and shift size δ.","The CUSUM statistics are defined by $C_i^- = \max\{0,(\mu_0-K)-x_i+C_{i-1}^-\}$ and $C_i^+ = \max\{0,x_i-(\mu_0+K)+C_{i-1}^+\}$ with $C_0^-=C_0^+=0$. ARLs are computed using Siegmund’s approximation: for one-sided CUSUM, $\mathrm{ARL}=\frac{e^{-2\Delta b}+2\Delta b-1}{2\Delta^2}$ (for $\Delta\neq0$) where $b=H+1.166$, $\Delta=\delta-K$ (upper) or $\Delta=-\delta-K$ (lower), and $K=\delta/2$; two-sided ARL uses $1/\mathrm{ARL}=1/\mathrm{ARL}^-+1/\mathrm{ARL}^+$. The expected cost per cycle $C_E$ follows the Lorenzen–Vance cost model (given as Eq. (6)) incorporating sampling, false-alarm, and assignable-cause search/repair costs, with $\tau=\frac{1}{\lambda}-\frac{h}{e^{\lambda h}-1}$ and $S=\frac{1}{e^{\lambda h}-1}$.","Using Lee (2011)’s hypothetical parameter set with constraints $\mathrm{ARL}_0\ge 200$ and $\mathrm{ARL}_\delta\le 14$, NSGA-II yields 82 non-dominated solutions; the minimum reported $C_E$ is $9.50 with (n=2, h=0.36\,\text{hours}, H=4.19)$ and corresponding $\mathrm{ARL}_\delta\approx 8.72$ (Table 1). Lee (2011) reported (under normality) $C_E=16.78$, $n=2$, $h=0.85$ hours, $H=1.69$; the proposed approach reduces $C_E$ by 43.39%. Sensitivity results show that as shift size increases from $\delta=1.0$ to $2.5$, both $C_E$ and $\mathrm{ARL}_\delta$ decrease, while $n$ remains 2; generally $h$ decreases and $H$ increases with larger shifts (Table 2). In one-factor-at-a-time parameter changes, $C_1$ most strongly affects $C_E$, while sampling-cost parameter $d$ strongly affects $\mathrm{ARL}_\delta$ (Tables 3–4).","The authors note that they cannot compare out-of-control ARL (ARLδ) performance against prior work because of the “non-availability of references” for multi-objective CUSUM designs. They also motivate using Siegmund’s ARL approximation mainly for simplicity, implying reliance on an approximation rather than exact ARL evaluation.","The design assumes independent normal data with known in-control parameters (\mu and \sigma) and does not address parameter estimation error (Phase I) or robustness to non-normality/autocorrelation, despite citing work on correlated data. ARL computations rely on Siegmund’s approximation, and the impact of approximation error on the Pareto front and chosen designs is not quantified (e.g., vs. Markov-chain/integral-equation ARLs). The optimization/algorithmic settings for NSGA-II (population size, generations, crossover/mutation rates, constraint handling) and computational budget are not fully documented here, limiting reproducibility without shared code.","The paper indicates a “scope for future work” in the conclusion, suggesting that with adequately estimated cost and time parameters, the provided algorithm/pseudocode can be used for effective industrial implementation; however, specific methodological extensions are not detailed beyond this general statement.","Extend the framework to handle unknown parameters via Phase I estimation and assess the effect on in-control ARL and economic performance (e.g., self-starting or plug-in designs). Develop versions for autocorrelated/non-normal data (e.g., using ARMA residual charts or nonparametric CUSUM) and compare Pareto fronts under realistic departures. Provide rigorous benchmarking using exact/Markov-chain/integral-equation ARLs and include more competitor designs (single-objective and multi-objective) to validate gains beyond cost reduction. Release reproducible software (e.g., Python/R package) with NSGA-II settings, constraint handling, and plotting/decision support for selecting a Pareto-optimal design.",2409.04673v3,local_papers/arxiv/2409.04673v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:01:42Z
TRUE,Univariate|Self-starting|Bayesian,CUSUM,Phase II,Theoretical/simulation only,TRUE,FALSE,NA,Simulation study|Other,ARL (Average Run Length)|Conditional expected delay,"Not discussed (no Phase I). Performance is evaluated for change-point locations τ = 11, 21, ..., 101, corresponding to having 10–100 in-control observations available before the change; small shifts (δ = 0.5) may require substantially more IC points for stable/efficient detection.",TRUE,None / Not applicable,Not provided,NA,"The paper comparatively evaluates two parametric self-starting CUSUM-type control charts for detecting persistent location (mean) shifts in univariate Normal data with unknown mean and variance: the frequentist self-starting CUSUM (SSC; Hawkins & Olwell, 1998) and the Bayesian predictive ratio CUSUM (PRC; Bourazas et al., 2023). Both charts are tuned to have in-control ARL0 = 370 (two-sided) by numerically selecting decision limits using Regula Falsi based on 10,000 simulated standard-Normal sequences. Out-of-control performance is assessed via Conditional Expected Delay (CED) across shift sizes δ ∈ {0.5, 1, 1.5, 2} and change-point locations τ ∈ {11, 21, ..., 101}, with 10,000 replications per scenario. A prior sensitivity analysis is conducted for PRC using a non-informative reference NIG prior versus a weakly informative NIG prior equivalent to about four IC observations. Results indicate faster detection for larger shifts and improved performance as τ increases; informative priors notably reduce early-stage detection delay for PRC, while PRC (non-informative) and SSC are broadly similar with small differences depending on design parameter choices.","SSC standardizes the next observation using current estimates: $T_{n+1}=(X_{n+1}-\bar x_n)/s_n$, then transforms to an exact standard Normal $U_{n+1}=\Phi^{-1}\{F_{n-1}(\sqrt{n/(n+1)}\,T_{n+1})\}$ and applies one-/two-sided CUSUM recursions $C^+_{n+1}=\max\{0,C_n+U_{n+1}-k_{\mathrm{SSC}}\}$ and $C^-_{n+1}=\min\{0,S_n+U_{n+1}+k_{\mathrm{SSC}}\}$ with signal when exceeding control limit $h_{\mathrm{SSC}}$. PRC uses a Normal-Inverse-Gamma prior, derives the posterior predictive $f(X_{n+1}|X_n)$ and a shifted (OOC) predictive $f'(X_{n+1}|X_n)$, and charts the log predictive ratio $\log L_{n+1}=\log\{f'(X_{n+1}|X_n)/f(X_{n+1}|X_n)\}$ with CUSUM recursions $S^+_{n+1}=\max\{0,S_n+\log L_{n+1}\}$ and $S^-_{n+1}=\min\{0,S_n-\log L_{n+1}\}$; signal when exceeding $h_{\mathrm{PRC}}$. The OOC metric is $\mathrm{CED}(\tau)=\mathbb E_\tau[(T-\tau+1)^+]/\mathbb P(T\ge \tau)$.","Decision limits for both methods are calibrated to achieve $\mathrm{ARL}_0=370$ (two-sided) using 10,000 in-control simulations and a Regula Falsi search. For δ = 0.5 and very early changes (e.g., τ = 11), detection is poor across methods, with reported CEDs often near hundreds (e.g., at $k_{\mathrm{PRC}}=0.5$, CED: PRCi 265.6, PRCn 310.3, SSC 289.3), indicating near-in-control performance. For moderate/large shifts (δ ≥ 1), delays drop quickly as τ increases and stabilize; e.g., for δ = 2 and τ = 101, CEDs are around 3.77–4.95 depending on k/method (e.g., at $k_{\mathrm{PRC}}=1$, PRCi 3.791, PRCn 3.811, SSC 3.770). The informative prior (PRCi) consistently reduces early-stage CED versus PRCn, with the advantage diminishing as τ grows (more IC data). PRCn and SSC are generally similar overall; SSC is slightly better for smaller k and early τ, while PRCn can be slightly better for larger k.",None stated.,"The study is limited to i.i.d. Normal data with mean shifts only; robustness to non-normality, heavy tails, or autocorrelation is not evaluated. Results focus on one performance metric (CED) for long runs with ARL0 fixed at 370, so conclusions may differ under alternative economic criteria, steady-state metrics, or different ARL0 targets. No real-data case study or implementation guidance/software is provided, which may limit practical adoption and reproducibility.",None stated.,"Extend the comparison to non-normal distributions and robust/self-starting variants (e.g., t-based or rank-based CUSUMs) to assess sensitivity to outliers and heavy tails. Evaluate performance under autocorrelated processes and incorporate modeling approaches (e.g., residual-based charts or state-space corrections). Provide open-source code and explore design optimization (k, h) under alternative criteria such as steady-state ARL/ATS or economic design, and validate on real industrial/healthcare monitoring datasets.",2410.12736v1,local_papers/arxiv/2410.12736v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:02:20Z
TRUE,Profile monitoring|Functional data analysis|Other,GLR (Generalized Likelihood Ratio)|Change-point|Other,Both,Manufacturing (general)|Transportation/logistics|Theoretical/simulation only|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,Not discussed (the paper uses a training/tuning split in Phase I; simulation uses 400 training and 1000 tuning observations per cluster; case study splits 1802 IC observations into 901 training and 901 tuning).,TRUE,R,Not provided,NA,"The paper proposes the Functional Mixture Regression Control Chart (FMRCC) for multimode profile monitoring when a functional quality characteristic (response curve) is influenced by covariates and the in-control population is heterogeneous. The in-control behavior is modeled as a finite mixture of functional linear models (FLMs), estimated via basis representations and multivariate functional PCA, reducing the problem to a Gaussian mixture regression on truncated score vectors fitted by EM with BIC-based selection of the number of clusters and covariance parameterization. For monitoring, it uses a likelihood-ratio-test approach that asymptotically yields a monitoring statistic based on the negative log in-control mixture density of the new observation’s scores; control limits are set nonparametrically using a Phase I tuning set, and a studentized version adjusts prediction-error covariance for parameter-estimation uncertainty. A Monte Carlo study compares FMRCC to FRCC, an FPCA-based functional control chart, and a clustering-then-chart approach, showing FMRCC is especially superior when modes differ in regression structures (slope functions). A resistance spot welding case study (dynamic resistance curves with scalar covariates wear and dressing) finds two in-control modes and detects 32/37 known out-of-control profiles (TDR 0.864), outperforming the competing approaches.","The method models cluster-specific function-on-function FLMs: $Y(t)=\beta_{0k}(t)+\int_S \beta_k(s,t)^T X(s)\,ds+\varepsilon(t)$ and expands $X,Y$ in (multivariate) FPCA bases, truncating to $L,M$ components to obtain a finite mixture regression on scores: $\xi^Y_{i,M}\mid \xi^X_{i,L} \sim \sum_{k=1}^K \pi_k\,\mathcal N\big((B_{LMk})^T\xi^X_{i,L},\Sigma_k\big)$. The Phase II monitoring statistic is based on the likelihood ratio simplification $\Lambda \approx f(\xi^Y_{*,M}\mid \xi^X_{*,L};\hat\Psi)$ so $W=-\log \Lambda$; a studentized version replaces $\Sigma_k$ with the prediction-error covariance $\hat\Sigma_k^*$ that adds a term involving $\mathrm{Cov}(\hat B_{LMk})$. The upper control limit $W^{\alpha}_{\text{lim}}$ is set as the empirical $(1-\alpha)$ quantile of $W$ over a Phase I tuning set.","In simulation, performance is summarized by mean false alarm rate (FAR, at severity 0) and true detection rate (TDR) across severity levels for linear and quadratic mean shifts; FMRCC is stable under increasing cluster dissimilarity and clearly outperforms alternatives when modes differ only in regression coefficient functions (i.e., heterogeneous slopes with common intercept structure). In the resistance spot welding case study, BIC selects $K=2$ mixture components; using 1802 IC profiles split into 901 training and 901 tuning, and retaining 5 response PCs (≥95% FVE), FMRCC detects 32 of 37 OC curves for an estimated TDR of 0.864 with bootstrap 95% CI [0.756, 0.960]. Competing methods achieve lower TDRs on the same split: FRCC 0.486 [0.310, 0.581], FCC 0.486 [0.323, 0.608], and CLUST 0.621 [0.472, 0.729].",None stated.,"The monitoring statistic relies on an asymptotic approximation $\Lambda\approx f(\cdot;\hat\Psi)$ and a Gaussian mixture regression assumption on truncated FPCA scores; performance and Type I error control may degrade when sample sizes are small, tails are heavy, or the mixture model is misspecified. The control limit is calibrated via a tuning split rather than an explicit in-control ARL design, so practitioners must choose and justify the split and may lose Phase I efficiency. The approach assumes independent profiles (no serial dependence) and does not address autocorrelation, concept drift, or time-varying mixture proportions, which are common in industrial monitoring.",The authors propose extending the framework to nonlinear functional models and combining it with profile registration techniques to handle profiles observed on different domains. They also suggest developing time-weighted monitoring strategies (CUSUM/EWMA) within the framework to improve sensitivity to small and persistent shifts.,"Develop robust/nonparametric or heavy-tailed mixture versions (or robust FPCA) to reduce sensitivity to non-Gaussian errors and outliers in Phase I. Extend the FMRCC to autocorrelated/streaming settings with time-varying mixture weights or state-space formulations, and study steady-state/conditional signaling properties. Provide open-source implementation and guidance for practical design choices (e.g., selecting $L,M$, training/tuning split size, and computational shortcuts) and evaluate performance on additional real industrial datasets.",2410.20138v1,local_papers/arxiv/2410.20138v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:02:58Z
TRUE,Univariate|Other,CUSUM|Change-point|GLR (Generalized Likelihood Ratio),Phase II,Theoretical/simulation only,FALSE,FALSE,NA,Approximation methods|Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),Not discussed,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a CUSUM-type sequential test/control chart with observation-adjusted (random, data-dependent) control limits, termed CUSUM-OAL, to improve sensitivity for distributional change detection. The statistic is based on (mixture) log-likelihood ratios and targets parameter changes in extremely heavy-tailed distributions, with a focus on detecting shifts in the tail index parameter $\alpha$ (where mean/variance may be infinite). The authors derive large-control-limit asymptotic approximations for in-control and out-of-control average run lengths (ARL) under regimes determined by the sign of $E_v(Z_1)$ (exponential/square/linear scaling), and they compare these to classical CUSUM ARL approximations. Monte Carlo simulations for Pareto-type heavy-tailed data show substantially smaller out-of-control ARLs for CUSUM-OAL than for conventional CUSUM at matched in-control ARL targets (e.g., ARL0 = 300 or 500), though with larger in-control run-length variability. The work contributes an adaptive control-limit mechanism aimed at faster detection in heavy-tailed change-point monitoring settings where traditional moment-based charts are unsuitable.","Classical upper one-sided CUSUM stopping time: $T_C(c)=\min\{n\ge 0: \max_{1\le k\le n}\sum_{i=n-k+1}^n Z_i \ge c\}$ with $Z_i=\log\{p_{v1}(X_i)/p_{v0}(X_i)\}$. Proposed CUSUM-OAL replaces constant limit by an observation-adjusted limit: $T_C(cg)=\min\{n\ge 0: \max_{1\le k\le n}\sum_{i=n-k+1}^n Z_i \ge c\,g(\hat Z_n)\}$, using either $\hat Z_n=n^{-1}\sum_{i=1}^n Z_i$ or a sliding average $\hat Z_n(ac)$. For unknown post-change, they use a mixture LR: $Z_k=\log\{P_{v1}(X_k)/P_{v0}(X_k)\}$ with $P_{v1}(x)=\int_V P_v(x)\,Q(dv)$.","Simulations for Pareto-type heavy tails with $F(x)=1-1/x^\alpha$ ($0<\alpha<1$), monitoring a change from $\alpha_0=0.90$ to smaller $\alpha_1$, report much smaller out-of-control ARLs for CUSUM-OAL than classical CUSUM at matched ARL0. For ARL0=300 and $\alpha_1=0.50$, classical CUSUM has ARL1 \approx 15.28 (SDRL 10.65) while CUSUM-OAL yields \approx 9.88 (10.99) for $u=0.5$ and \approx 5.75 (9.41) for $u=5$; for $\alpha_1=0.30$, classical CUSUM \approx 6.16 (3.63) vs CUSUM-OAL \approx 3.20 (3.21) ($u=0.5$) and \approx 2.07 (2.29) ($u=5$). For ARL0=500 and $\alpha_1=0.50$, classical CUSUM \approx 17.64 (11.93) vs CUSUM-OAL \approx 10.99 (12.52) ($u=0.5$) and \approx 5.97 (10.06) ($u=5$). Theoretical asymptotics (Theorem 2.1) indicate ARL scaling can be exponential, quadratic, or linear in the control limit depending on whether $E_v(Z_1)<0$, $=0$, or $>0$, respectively, and Theorem 2.4 shows bounded ARL when the adjusted limit can become negative (i.e., when $g(\mu)<0$ with $\mu=E_v(Z_1)>0$).","The authors note that while CUSUM-OAL improves detection (smaller out-of-control ARLs), it tends to have larger standard deviations of run length in the in-control state compared with the conventional CUSUM. They also frame their theory largely in large-control-limit (asymptotic) approximations and illustrate performance primarily via numerical simulations for the heavy-tailed setting.","The method relies on specifying a reference post-change mixture distribution (choice of parameter space $V$ and prior/mixing measure $Q$) and a tuning function $g(\cdot)$ (e.g., slope parameter $u$), but practical guidance for selecting these to control false alarms robustly across scenarios is limited. Results appear focused on i.i.d. observations; robustness to serial dependence, model misspecification of tail behavior, and nonstationarity beyond a single change-point is not established. Evaluation is mainly simulation-based with one heavy-tailed family and a limited set of design choices; broader benchmarking against alternative heavy-tail change detectors or robust/nonparametric SPC charts is not shown. No implementation details/software are provided, which may hinder adoption and reproducibility.",None stated,"Develop data-driven or optimal selection rules for $g(\cdot)$ (and tuning parameters like $u$, $a$) to guarantee ARL0 control uniformly over $v\in V$ while minimizing worst-case detection delay. Extend the approach to autocorrelated/heavy-tailed time series and to multivariate heavy-tailed monitoring (e.g., tail-index changes in $\mathbb{R}^p$). Provide finite-sample calibration methods (e.g., bootstrap or importance sampling) for control limits under heavy tails where asymptotics may be inaccurate at moderate ARL0 targets. Release reproducible code and add comparisons to modern robust change-point methods (rank-based CUSUM, self-normalized procedures, or extreme-value based detectors).",2411.14706v1,local_papers/arxiv/2411.14706v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:03:39Z
TRUE,Multivariate|Other,Hotelling T-squared,Phase II,Manufacturing (general)|Other,TRUE,TRUE,NA,Exact distribution theory|Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate|Other,"Assumes Phase I provides a “sufficiently large” dataset to estimate VAR(p) parameters (no fixed m given for the general method). Numerical studies use subgroup sizes n = 3, 7, 15; steel example uses n = 4; chemical example groups m = 100 individual observations into 20 samples of size n = 5 for Phase I and uses n = 5 in Phase II illustration.",TRUE,R,Not provided,NA,"The paper studies Phase II performance of the Hotelling’s T² chart for monitoring the mean vector when within-sample observations are multivariate time-series data with autocorrelation and cross-correlation modeled by a VAR(p) process. It derives closed-form covariance matrices for the sample mean under VAR(1) and general VAR(p) (via companion-form VAR(1) representation), enabling direct T² charting on original observations rather than residuals. Using the chi-square (and noncentral chi-square) distribution of the resulting T² statistic, it computes ARL0/ARL1 and compares a direct-observation T² chart against a residual-based T² chart for VAR models. Extensive numerical results show that ignoring dependence inflates false alarms, and that the proposed direct-observation approach yields uniformly smaller out-of-control ARLs than the residual-based approach, especially for small shifts. Two illustrations are provided: a steel sheet rolling thickness process (VAR(1), v=3) and a chemical process viscosity/temperature dataset fitted with VAR(3), including a head-to-head “first-to-signal” comparison criterion.","Observations follow a VAR(p): $X_t-\mu_X=\sum_{i=1}^p \Phi_i(X_{t-i}-\mu_X)+\varepsilon_t$, with $\varepsilon_t\sim MN_v(0,\Sigma_\varepsilon)$. The proposed Phase II chart uses $T_t^2=(\bar X_t-\mu_{0X})^\top \Sigma_{\bar X}^{-1}(\bar X_t-\mu_{0X})$, where $\Sigma_{\bar X}$ is the closed-form covariance of the sample-mean vector under VAR(1) (Eq. 15) or VAR(p) via companion VAR(1) (Eqs. 18–21). In control, $T_t^2\sim \chi_v^2$ and UCL is $\chi^2_{v}(\alpha)$; out of control, $T_t^2\sim \chi^2_{v}(d)$ with noncentrality $d=\delta^\top \Sigma_{\bar X}^{-1}\delta$, giving $ARL_1=1/(1-F_{\chi^2(v,d)}(UCL))$ (Eq. 23).","For i.i.d.-designed T² limits applied to autocorrelated/cross-correlated VAR(1) data, the achieved ARL0 drops below the nominal 370 as dependence increases (Figure 1), implying many extra false alarms if dependence is ignored. In head-to-head comparisons with equal ARL0=370, the direct-observation T² chart has substantially smaller ARL1 than the residual-based T² chart for small-to-moderate shifts across multiple parameter cases and sample sizes (Figure 2); e.g., Case IV at $\delta=1.0$: proposed vs residual ARL1 = 95.4 vs 227.9 (n=3), 64.7 vs 121.2 (n=7), 33.8 vs 49.4 (n=15). Using the “first-to-signal” criterion estimated via 10,000 simulations, the proposed chart signals first more often than the residual-based chart across examined cases (Figure 3); e.g., Case IV with $\delta=1.0$, n=3: $p_1=0.677$ (proposed first), $p_2=0.274$ (residual first), $p_3=0.049$ (simultaneous). Example applications show Phase II detection in a steel rolling VAR(1) example (signals at 2nd and 17th OOC samples after shift) and in a chemical VAR(3) example (signals at inspections #7 and #10 after an induced mean shift).","The method is presented for Phase II monitoring assuming a perfect VAR(p) model fit and accurate parameter estimation from Phase I data. The authors note that performance under Phase I estimation error and model misspecification is not evaluated and may affect both the proposed and residual-based approaches. They also state that only statistical (ARL-based) design is considered, not economic-statistical design with sampling/inspection costs.","The approach assumes multivariate normality and (approximate) independence between successive subgroups/samples in Phase II, which may not hold in many high-frequency sensing settings. Computing $\Sigma_{\bar X}$ for VAR(p) via the companion-form Kronecker inverse can be computationally heavy for large v or p, and numerical stability/implementation details are not fully addressed. The comparison residual-based chart is treated under “perfect model fit,” but the proposed chart also relies on correct model specification; robustness to incorrect p/order selection or nonstationarity is not thoroughly quantified.","The authors suggest developing Phase I monitoring techniques for VAR(p) processes to help achieve reliable model fitting and parameter estimation prior to Phase II use. They also propose studying the effect of estimation error and model misspecification in Phase I on Phase II chart performance. Finally, they suggest extending the work to an economic-statistical design of the T² chart for VAR(p) processes that accounts for sampling/inspection costs.","Extend the direct-observation methodology to nonnormal/heavy-tailed and robust covariance estimation settings (e.g., elliptical models, M-estimators) and to missing/irregularly spaced sensor data. Develop computationally scalable implementations for higher-dimensional VAR(p) (large v) using sparse/structured VAR estimation and efficient Lyapunov/Stein solvers. Provide diagnostic tools to separate shifts in mean from changes in correlation dynamics (e.g., changes in $\Phi_i$ or $\Sigma_\varepsilon$) and evaluate steady-state and conditional run-length properties under dependent sampling.",2501.11649v1,local_papers/arxiv/2501.11649v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:04:29Z
TRUE,Univariate|Bayesian|Other,Shewhart|Other,Both,Energy/utilities|Environmental monitoring|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Other|Case study (real dataset),ARL (Average Run Length),"Phase II illustration uses the last 100 accidents for monitoring and the remaining 2,689 records for Phase I parameter estimation; performance comparison mentions generating 5,000 TBEs for parameter estimation. No general Phase I sample size guidance is provided beyond these choices.",TRUE,R,Supplementary material (Journal/Publisher),https://www.kaggle.com/datasets/usdot/pipeline-accidents,"The paper proposes a dynamic, risk-adjusted time-between-events (TBE) control chart built on a risk-adjusted non-homogeneous Poisson process (NHPP) to monitor the ratio Cost/TBE, interpreted as average cost per unit time (AC). The monitoring statistic is $W_i=Y_i/X_i$ (total cost divided by TBE), with stepwise, event-specific probability control limits computed from the conditional distribution of $W_i$ given the previous event time and current covariates. Risk adjustment is incorporated via a Cox proportional hazards-type model for the NHPP intensity, and dependence between TBE and cost is modeled with a copula (focused on Gumbel in the study). Performance is evaluated mainly by Monte Carlo estimated ARL (targeting ARL0=200), showing ARL-unbiasedness and sensitivity patterns with respect to NHPP parameters, cost distribution parameter, regression coefficient sign, and dependence strength. A real case study monitors U.S. oil pipeline accidents (PHMSA data), estimating a power-law RANHPP and applying the chart in Phase II to detect changes in AC, with diagnostics suggesting shifts driven primarily by the cost component.","Risk-adjusted NHPP intensity: $\lambda(t\mid z_t)=\lambda(t)\exp(\beta' z_t)$ with baseline power-law $\lambda(t)=\gamma\eta t^{\eta-1}$ or log-linear $\lambda(t)=\exp(\gamma+\eta t)$. Conditional TBE CDF given prior failure time $t_{i-1}$: $P(X_i\le x\mid t_{i-1},z_i)=1-\exp\{-\Lambda(t_{i-1}+x\mid z_i)+\Lambda(t_{i-1}\mid z_i)\}$. Monitoring statistic: $W_i=Y_i/X_i$; two-sided probability limits: $\mathrm{LCL}_i=F^{-1}_{W_i}(\alpha/2\mid t_{i-1},z_i,\theta_0)$ and $\mathrm{UCL}_i=F^{-1}_{W_i}(1-\alpha/2\mid t_{i-1},z_i,\theta_0)$, computed numerically from $F_{W_i}(w)=\int_0^\infty\int_0^{xw} f_{X_i,Y_i}(x,y)\,dy\,dx$ (with a copula-based joint density).","Simulations are run with nominal ARL0=200 and show the proposed chart is ARL-unbiased across studied shift scenarios (ARL0 > ARL1). The study reports that stronger positive dependence between TBE and cost (e.g., Kendall’s $\tau$ increasing from 0.3 to 0.8) generally lowers ARL1, improving detection. Risk-factor effects matter: when the risk coefficient is negative (e.g., $\beta=-2$) the chart tends to signal sooner than the $\beta=0$ case, while positive $\beta$ delays signals; an example given for log-linear intensity with $\tau=0.3$, $(\delta_\gamma,\delta_\eta,\delta_\mu)=(0.25,2.0,1.0)$ shows ARL1 increasing from 36.0 (β=-2) up to 128.9 (β=2). In a comparison to Ali (2021) (NHPP chart ignoring risk factors), the alternative can have much lower actual ARL0 than 200 (e.g., 28.65 when β=2), indicating excessive false alarms. In the pipeline case study, Phase II monitoring (last 100 observations) with α=0.005 flags 9 out-of-control signals (mostly below LCL), and separate charts suggest the AC shift is mainly driven by a downward shift in total cost rather than TBE.","The authors note that while the AC (Cost/TBE) chart signals changes in the ratio, it does not identify whether the shift is due to TBE, cost, or both; they suggest using separate charts for $X_i$ and $Y_i$ to diagnose the source. They also state that in simulations they focus on monitoring shifts in $(\gamma,\eta,\mu)$ while keeping the risk-model coefficients $\beta$ and copula parameter fixed.","The method requires specifying parametric models for the NHPP intensity, the cost marginal distribution, and the copula; misspecification could materially affect false-alarm control and detection power, but robustness checks are limited. Control limits are obtained via numerical inversion/Monte Carlo quantiles at each event (stepwise limits), which may be computationally heavy in real-time, high-frequency settings or with many covariates. The independence assumptions used for geometric RL arguments rely on NHPP independent increments and may be violated by practical features like reporting delays, seasonality, or serial dependence in covariates. The paper provides limited benchmarking against other modern TBE/cost monitoring methods (e.g., CUSUM/EWMA-type TBEA charts or GLR approaches under NHPP), so comparative performance breadth is narrow.",None stated.,"Develop robust or semi-parametric variants (e.g., nonparametric margins or flexible baseline intensity) to reduce sensitivity to distribution/copula misspecification, and study their in-control calibration. Extend the framework to autocorrelated/seasonal event processes or clustered failures, and evaluate steady-state (rather than zero-state) performance. Provide efficient, deployable software (streaming implementation and faster limit computation) and broader empirical validation across multiple real TBE/cost domains beyond pipelines. Explore memory-type versions (CUSUM/EWMA on $W_i$ or likelihood ratios) for improved sensitivity to small persistent shifts in AC.",2501.11809v1,local_papers/arxiv/2501.11809v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:05:12Z
TRUE,Univariate|Other,Shewhart,Both,Manufacturing (general)|Environmental monitoring|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|False alarm rate|Detection probability|Other,"Phase I sample size m is studied extensively. The paper concludes very large Phase I samples are needed (at least ~2000) to reduce variability of the conditional in-control ARL; sizes <100 are not recommended, and large samples (e.g., m>500; even m>=5000 for SDARL about 5–10% of nominal ARL0) may be required depending on the criterion.",TRUE,R,Upon request,https://www.R-project.org/|https://CRAN.R-project.org/package=VGAM,"The paper studies a two-sided Shewhart chart for individual observations to monitor continuous proportions in (0,1) modeled by the Kumaraswamy distribution, focusing on the impact of estimating in-control parameters from a Phase I sample. With unknown parameters, plug-in probability limits induce practitioner-to-practitioner variability, so the in-control performance is evaluated via the conditional false-alarm probability and the conditional in-control ARL (CARL0) distribution, estimated by Monte Carlo simulation. The study shows that plug-in limits can yield substantial variability and a large fraction of practitioners with CARL0 below the nominal ARL0, motivating guidance on Phase I sample size. Two control-limit adjustment strategies are proposed by modifying the nominal false alarm rate: (A) an unconditional adjustment targeting average in-control ARL close to nominal, and (B) an exceedance-probability adjustment that bounds the fraction of charts with CARL0 below a target (optionally with tolerance). Out-of-control performance is compared for shifts in each Kumaraswamy shape parameter, showing that stricter in-control guarantees (adjustment B) widen limits and can materially degrade detection of small-to-moderate shifts; two numerical examples (one simulated manufacturing proportion and one real relative-humidity dataset) illustrate implementation.","Kumaraswamy cdf: $F_K(y;\theta_1,\theta_2)=1-(1-y^{\theta_1})^{\theta_2}$ and inverse cdf: $F_K^{-1}(u;\theta_1,\theta_2)=[1-(1-u)^{1/\theta_2}]^{1/\theta_1}$. Two-sided probability limits satisfy $P(Y\le LCL)=\alpha/2$ and $P(Y>UCL)=\alpha/2$, giving $LCL=F_K^{-1}(\alpha/2;\theta_{01},\theta_{02})$ and $UCL=F_K^{-1}(1-\alpha/2;\theta_{01},\theta_{02})$; with estimated parameters, plug-in limits use $(\hat\theta_1,\hat\theta_2)$. Conditional false alarm probability is $\alpha_{\hat\theta|\theta_0}=1-F_K(\widehat{UCL};\theta_0)+F_K(\widehat{LCL};\theta_0)$, so $CARL_0=(\alpha_{\hat\theta|\theta_0})^{-1}$; for out-of-control parameters $\theta_1$, $ARL_1=1/(1-F_K(UCL;\theta_1)+F_K(LCL;\theta_1))$.","For nominal $\alpha=0.0027$ (nominal $ARL_0\approx370.4$), simulations with $N=25{,}000$ show plug-in limits produce large variability in $CARL_0$, especially for small Phase I sizes (e.g., SDARL on the order of hundreds to thousands for $m\le100$ in the reported scenarios). Even for very large $m$ (e.g., $m=5000$), roughly half of practitioners still have $CARL_0<ARL_0$, though the distribution concentrates near 370.4 and the 25th percentile is about 346 (≈7% below nominal) in the shown scenarios. Adjustment A requires increasing FAR above nominal (e.g., for $m=100$, $\alpha'\approx0.00291$) to bring the average in-control ARL within 5% of nominal, but it does not reduce the fraction with $CARL_0<ARL_0$ much (often still around or above ~50%). Adjustment B (exceedance control) requires substantially smaller FAR (e.g., for $m=100$, $\alpha''\approx0.00049$–0.00055 for 5% exceedance), greatly widening limits and inflating out-of-control ARLs for small-to-moderate shifts, demonstrating the trade-off between guaranteed in-control performance and detection speed.","The paper notes that there is no single “optimal” way to set limits with estimated parameters because different adjustments yield different in-control guarantees and out-of-control detection performance, so practitioners must choose based on needs. It also observes that the SHK chart with equal-tail limits struggles to detect increasing shifts in $\theta_{02}$ in the examined range, suggesting design changes may be needed. Code is not included in the paper; it is stated to be available from the author upon request.","The proposed chart and adjustments are tailored to the Kumaraswamy model; robustness to model misspecification (e.g., beta-like alternatives, contamination, or rounding at 0/1) is not assessed. The study focuses on independent observations; performance under autocorrelation (common in environmental time series like yearly humidity) is not developed. Comparisons are largely among variants of the same Shewhart/probability-limit design; broader benchmarking against alternative charts for bounded data (e.g., beta-regression charts, transformation-based charts, nonparametric approaches) is limited.","The authors suggest studying designs when rational subgroups of size $n\ge2$ are available (instead of individuals), for both known and unknown parameter cases. They also propose investigating simultaneous shifts in both parameters and developing follow-up procedures to identify which parameter(s) changed. Additionally, they mention that an ARL-unbiased design might address poor detection of certain increasing shifts and leave this as future work.","Develop robust or semi-parametric versions that maintain conditional guarantees under distributional misspecification or data near the boundaries (0/1), and extend the framework to autocorrelated/irregularly sampled data (e.g., humidity series) via time-series residual charts or state-space modeling. Provide computationally efficient design tools (and public software) for selecting $m$ and adjusted FARs across a range of in-control parameter values, possibly with interpolation/meta-modeling to avoid extensive Monte Carlo for each case. Explore adaptive/self-starting schemes for bounded distributions and compare against CUSUM/EWMA-style memory charts for improved small-shift detection while retaining conditional performance guarantees.",2502.02296v1,local_papers/arxiv/2502.02296v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:06:00Z
TRUE,Univariate|Nonparametric|Other,Shewhart|Machine learning-based|Other,Phase II,Manufacturing (general)|Theoretical/simulation only,FALSE,FALSE,FALSE,Approximation methods|Simulation study,ARL (Average Run Length)|False alarm rate,Not discussed (the paper illustrates designs and DL fitting for subgroup sizes n = 20 and n = 50; control limits C are computed separately for each n at 99.73% confidence).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Shewhart-type signed-rank control chart (SS-RCC) based on the Wilcoxon signed-rank statistic to monitor a location/shift parameter when the process distribution is unknown and when measurements can contain tied observations due to rounding/resolution limits. For untied observations (UOs), it approximates the Wilcoxon signed-rank statistic’s distribution with a (standard) normal approximation to set symmetric control limits and compute Type I/II errors and run-length properties. For tied observations (TOs), it models ties via an additional zero outcome in the sign variable, removes zeros, treats the effective sample size as random (binomial), derives moments/skewness/kurtosis, and introduces a discretized Scaled-Normal Distribution (SND) with three adjustment parameters (a,b,c) controlling height/location/width. A deep learning regression model with 6 fully connected hidden layers is trained to predict (a,b,c) from derived distributional features, enabling approximate control-limit performance calculations under ties and shifts. Performance is assessed using ARL under multiple symmetric Johnson-type benchmark distributions, showing that ties can materially affect sensitivity and that the proposed approach yields workable limits and ARL behavior under rounding-induced ties.","For UOs, the charting statistic is the Wilcoxon signed-rank sum $SR_t=\sum_{k=1}^n k\,S_{t,k}$ with $S_{t,k}=\operatorname{sign}(X_{t,k}-\nu_0)\in\{-1,+1\}$, and its CDF is approximated by $F_{SR_t}(s)\approx \Phi\big((s+0.5-m_1)/\sqrt{\mu_2}\big)$, where $m_1=\frac{n(n+1)}{2}(2p-1)$ and $\mu_2=\frac{2np(1-p)(n+1)(2n+1)}{3}$. Symmetric Shewhart limits are $(LCL,UCL)=(-C,C)$, with $C$ chosen to satisfy a target confidence (e.g., 99.73%) via a normal-approximation equation. For TOs, signs become $S'_{t,k}\in\{-1,0,+1\}$ with probabilities $(p_{-1},p_0,p_{+1})$ determined by standardized resolution $\tau=\eta/\sigma$ and shift $\delta$ (Eq. 3.3), a random effective count $N\sim\text{Binomial}(n,1-p_0)$, and the $SR_{t,N}$ distribution is approximated by a discretized SND with adjustment parameters $(a,b,c)$ estimated via a DL regression model.","At a 99.73% in-control confidence level, the paper reports control-limit constants $C$ for subgroup sizes $n=20$ and $n=50$: for UOs, $C=162$ (n=20) and $C=623$ (n=50); for TOs, $C=64$ (n=20) and $C=231$ (n=50). The DL model to estimate SND parameters $(a,b,c)$ is trained on 288 samples (11 inputs, 3 outputs) with an 0.85/0.135/0.015 train/validation/test split, SGD optimizer (10,000 epochs, learning rate 0.02, momentum 0.9) and MSE loss. Reported fit metrics include RMSE = 0.0761; validation MAE: $\text{MAE}_a=0.0183$, $\text{MAE}_b=0.0280$, $\text{MAE}_c=0.0136$; and test-set $R^2$: $R_a^2=0.9997$, $R_b^2=0.9965$, $R_c^2=0.9963$. Simulation comparisons use 1,000,000 repetitions for density approximations and ARL calculations across six symmetric Johnson-type benchmark distributions; qualitatively, the tables show smaller $ARL_1$ (faster detection) in tied-observation settings for small shifts (e.g., $\delta=\pm0.1$) relative to untied settings.","The authors note that the DL model is constructed only for two subgroup sizes ($n=20$ and $n=50$) and for out-of-control tied-observation scenarios; extending to other sample sizes or conditions requires adding corresponding scenarios to the training dataset. They also indicate that, for in-control conditions, they use ordinary normal approximations (so DL is not used for IC), implying the learned model is scoped to the TO/OOC SND-parameter estimation problem.","The work relies heavily on normal-approximation accuracy for the signed-rank statistic (UO) and on the proposed SND approximation (TO), but does not provide a systematic error analysis of approximation quality across $n$, tie severity, and distributional shapes beyond the chosen Johnson benchmarks. The “manual” selection of $(a,b,c)$ to create the training labels risks subjectivity and potential leakage of the intended approximation into the model, and may limit reproducibility without shared code/data. The chart is essentially a Shewhart-type chart (memoryless), so sensitivity to very small persistent shifts may still lag CUSUM/EWMA-type nonparametric competitors, which are not directly compared. Practical implementation details (Phase I estimation from real IC data, robustness to parameter-estimation error, and diagnostics for assignable causes) are not developed.","The authors state that to expand the DL model beyond the studied conditions (two sample sizes and TOs under OOC), additional scenario-specific values must be incorporated into the main dataset so the network can learn those new regimes; this implies future work on extending the trained model to other $n$ and settings.","A natural extension is a self-starting/estimated-parameter Phase I–Phase II framework that learns IC parameters (including tie rates and resolution effects) from historical data and propagates estimation uncertainty into control limits. Another promising direction is to build memory-type nonparametric charts (CUSUM/EWMA variants) for tied signed-rank statistics and compare against established nonparametric small-shift detectors under ties. The DL component could be replaced or augmented with more interpretable surrogate models (e.g., monotone regression, Gaussian processes) and accompanied by open-source code and benchmark datasets to enable reproducible evaluation. Finally, explicit handling of autocorrelation/irregular sampling and robustness to nonstationary tie mechanisms (changing resolution or rounding behavior) would broaden real-world applicability.",2503.20131v1,local_papers/arxiv/2503.20131v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:06:50Z
TRUE,Multivariate|Profile monitoring|Functional data analysis|Nonparametric,Hotelling T-squared|Other,Phase II,Manufacturing (general)|Transportation/logistics|Other,NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"Simulation: Phase I sample nIC = 2000 observations (split equally into training and tuning sets); 500 out-of-control observations per run. Case study: Phase I sample nIC = 708 (split equally into training/tuning), Phase II = 552 observations.",TRUE,R,Package registry (CRAN/PyPI)|Supplementary material (Journal/Publisher),https://doi.org/10.1080/00401706.2025.2491369,"The paper proposes an Adaptive Multivariate Functional Control Chart (AMFCC) for Phase II monitoring and diagnosis of multivariate functional quality characteristics (multichannel profiles). The method smooths noisy discrete signals via roughness-penalized splines, performs multivariate functional PCA (MFPCA), and forms multiple Hotelling’s $T^2$-type statistics across grids of smoothing ($\lambda$) and truncation ($L$) parameters. AMFCC adaptively combines the resulting partial-test p-values using nonparametric combination functions (notably Fisher’s omnibus and Tippett) to improve power under unknown out-of-control (OC) conditions, and provides post-signal component diagnostics via similarly combined contribution measures. A large Monte Carlo study compares AMFCC to existing multivariate functional charting approaches and non-functional baselines, showing improved detection while maintaining nominal false alarm. A real case study on resistance spot welding (dynamic resistance curves) further demonstrates higher true detection and better diagnostic localization than competing methods, with implementation available in the R package funcharts.","Smoothing is posed as penalized (weighted) least squares with roughness penalty, e.g. minimizing residual sum plus $\sum_{k=1}^p \lambda_k\int (f_k^{(m)}(t))^2dt$, with shared scalar smoothing via $\lambda_k=\lambda\,w_k/\sum_i w_i$. After MFPCA, the partial monitoring statistic is a functional Hotelling-type form $T^2_{i;\lambda,L}$ (Eq. 10) using the truncated inverse covariance kernel $\hat K^*_{k_1k_2;\lambda,L}(s,t)=\sum_{l=1}^L \hat\eta_{l,\lambda}^{-1}\hat\psi_{lk_1,\lambda}(s)\hat\psi_{lk_2,\lambda}(t)$ (Eq. 11). Partial-test p-values $p_t$ are estimated empirically from Phase I (Eq. 17) and combined into an overall statistic via $T_i^2=\Theta(p_1,\dots,p_T)$ (Eq. 14), with Fisher $-2\sum_t\log p_t$ or Tippett based on $\min_t p_t$ (Eqs. 15–16).","Simulation: across multiple dependence structures and four OC mean-shift shapes/severities, AMFCC (both Fisher and Tippett variants) achieves higher true detection rates (TDR) than MFCC variants that pick $L$ by explained variance (70/80/90%), and also outperforms non-functional baselines (MCC, DCC), while targeting FAR ≈ $\alpha=0.05$. Case study (automotive resistance spot welding): AMFCCF attains TDR 0.788 (bootstrap mean 0.787; 95% CI [0.759, 0.817]) and AMFCCT attains TDR 0.779 (0.777; [0.746, 0.806]), exceeding MFCC07 (0.712; [0.671, 0.746]) and other competitors. Diagnostic performance in the case study improves as well: cTDR 0.392 (AMFCCF; CI [0.373, 0.407]) and 0.445 (AMFCCT; CI [0.423, 0.462]) vs MFCC07 0.318 (CI [0.299, 0.333]).",None stated.,"The method’s validity depends on an empirically estimated in-control p-value distribution from the Phase I/tuning sample; with small or contaminated Phase I data, p-value calibration and control-limit accuracy may degrade. The approach assumes independent profiles over time (no explicit autocorrelation/stream dependence modeling) and uses a fixed grid of $(\lambda,L)$ choices, so performance and computational cost can be sensitive to grid design and scaling with many channels/components. While the partial statistic is Hotelling $T^2$-type, distributional robustness under heavy tails/non-Gaussian noise is not fully characterized beyond empirical calibration.","The authors suggest extending the adaptive-combination idea to functional real-time monitoring settings and to cases where additional covariate information is available (e.g., functional regression control charts). They also propose integrating prior knowledge about likely OC conditions into the AMFCC to further improve monitoring performance.","Develop formal guarantees for in-control error control under finite-sample Phase I estimation (including contaminated Phase I) and study robustness to non-Gaussian, heavy-tailed, or heteroscedastic errors. Extend AMFCC to explicitly handle autocorrelated/streaming functional data (e.g., ARMA errors or dynamic FPCA) and irregularly sampled/missing observations. Provide guidance/automation for selecting the $(\lambda,L)$ grids (or adaptive grid refinement) and deliver scalable implementations for high-dimensional multichannel profiles.",2504.09684v1,local_papers/arxiv/2504.09684v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:07:33Z
TRUE,Univariate|Nonparametric|Other,EWMA|Change-point|Other,Both,Environmental monitoring,NA,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Other,Phase I examples: SPEI-12 used 517 observations (Jul 1946–Jul 1989) yielding 47 events (SPEI≤-1) to estimate in-control medians/means; SPEI-24 used 623 observations (Jun 1947–Apr 1999) yielding 40 events. No general minimum sample size guidance stated beyond these case-study choices.,TRUE,R,Not provided,https://spei.csic.es/database.html,"The paper proposes using distribution-free Time Between Events and Amplitude (TBEA) monitoring to detect changes in drought characteristics (frequency/duration via time-between-events T and severity via amplitude X), using SPEI drought indices as the data source. It considers (i) an upper-sided distribution-free EWMA TBEA chart (from Wu et al., 2021) based on sign statistics and a “continuousify” mixture-normal transform, and (ii) nonparametric sequential change-point (CP) control charts based on Mann–Whitney and Kolmogorov–Smirnov statistics applied to a TBEA ratio statistic built from normalized T and X. A Bologna (Italy) case study is conducted for SPEI-12 and SPEI-24, with Phase I used to estimate in-control medians/means and Phase II monitoring configured to a common in-control ARL0=370 (α≈0.0027). Simulation studies assess in-control false-alarm behavior of CP charts in this TBEA setting and out-of-control performance (ARL1/SDRL and missed-alarm rates) under mean shifts in the underlying SPEI series. Results show coherent indications of worsening drought conditions over time and broadly comparable detection performance across EWMA and CP approaches, with CP charts additionally providing estimated change-point times.","TBEA EWMA uses sign statistics $S_T=\mathrm{sign}(T_i-\theta_T)$ and $S_X=\mathrm{sign}(X_i-\theta_X)$ and combines them as $S_i=(S_X-S_T)/2\in\{-1,0,1\}$. A continuousified statistic $S_i^*$ is formed by mapping $S_i\in\{-1,0,1\}$ to a mixture of normals with means $-1,0,1$ (sd $\sigma$), then the upper-sided EWMA recursion is $Z_i^*=\max\{0,\lambda S_i^*+(1-\lambda)Z_{i-1}^*\}$ with $Z_0^*=0$ and $\mathrm{UCL}=K\sqrt{\lambda(\sigma^2+0.5)/(2-\lambda)}$. For CP charts, the monitored statistic is the normalized ratio $Z_R=X'/T'$ with $T'=T/\mu_{T0}$ and $X'=X/\mu_{X0}$; sequential MW and KS two-sample statistics are computed across candidate split points $k$ and an alarm occurs when the maximized standardized statistic exceeds a simulated threshold.","All charts were tuned to ARL0=370 (α=0.0027). In a simulation mimicking Phase I settings, empirical false-alarm rates for CP charts were $\hat\alpha_0=0.00222$ (KS) and $0.00182$ (MW), not exceeding the nominal 0.0027. In out-of-control simulations (n=1500 total, change at m with SPEI mean shift $\delta\in\{-0.5,-1,-1.5\}$), ARL1 values were comparable: for $\delta=-0.5$, ARL1≈23.38 (EWMA), 24.53 (KS), 26.19 (MW) with missed-alarm rates 0.31%, 1.64%, 1.45% respectively; for $\delta=-1$, ARL1≈9.85 (EWMA), 9.19 (KS), 9.05 (MW); for $\delta=-1.5$, ARL1≈7.51 (EWMA), 6.64 (KS), 6.43 (MW. Case-study results for Bologna showed increases in the TBEA statistic beginning around Aug 2003 (KS/MW) and Aug 2007 (EWMA) for SPEI-12, and deterioration beginning around Oct 2007 for SPEI-24 (KS), with EWMA detecting subsequent drought periods.","The authors note that the analysis is non-exhaustive, considering only two nonparametric CP charts (MW and KS) and one distribution-free EWMA TBEA chart for comparison. They also acknowledge the CP charts are not specifically designed for TBEA monitoring, raising concern that their false-alarm rates may differ from theoretical values (addressed via simulation). The study uses data from a single geographical location (Bologna), limiting generality.","The CP approach monitors the ratio $Z_R=X'/T'$; ratio statistics can be unstable when $T'$ is small and may confound changes in T vs. X (limited diagnostic separation of which component drove a signal). The modeling and simulations largely assume independent observations; SPEI time series and derived event sequences can exhibit serial dependence, seasonality, and regime persistence that may affect run-length properties. Implementation details for threshold calibration (e.g., KS thresholds for target ARL0 in this specific sequential setting) are not fully reproducible without shared code and exact settings.","The authors propose extending the analysis to other geographical areas beyond Bologna. They also suggest considering change-point control charts based on other nonparametric statistics, specifically Lepage and Cramér–von Mises charts (as available in the cpm framework).","Develop TBEA-specific nonparametric CP charts that jointly handle T and X without collapsing to a single ratio (e.g., multivariate/rank-based two-sample tests) and provide component-wise diagnostics. Study robustness under autocorrelation/seasonality typical of climate indices and incorporate declustering or time-series SPC adjustments. Provide an open-source, fully reproducible workflow (data extraction, event definition, threshold calibration, and plotting) to facilitate adoption by climate analysts and water-resource agencies.",2506.21970v1,local_papers/arxiv/2506.21970v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:08:15Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Phase II,Energy/utilities,FALSE,NA,FALSE,Simulation study,Other,Slow learning: each new operating condition uses a one-week input-output dataset collected at sampling time τ = 5 minutes to train each new model M[n]. Fast learning GP: uses kmin = 25 (minimum) and kmax = 300 (maximum) most recent samples for online training (discarding older samples).,TRUE,Python|MATLAB,Not provided,https://readingraphics.com/book-summary-thinking-fast-and-slow|https://vbn.aau.dk/en/publications/heat-roadmap-europe-4-quantifying-the-impact-of-low-carbon-heatin,"The paper proposes a two-fold (“slow” and “fast”) architecture for lifelong adaptation of data-based dynamical models under out-of-domain and in-domain uncertainty. The slow component is an offline-trained ensemble whose model weights depend on statistical proximity (Mahalanobis distance) between the current input and each model’s training inputs; it includes an SPC-based monitoring scheme that uses empirical Hotelling’s T² control charts to detect when the ensemble becomes unreliable and to trigger training/integration of a new model for a newly encountered operating condition. The fast component is an online Gaussian process (GP) model that learns and compensates the residual error of the slow ensemble in real time using a sliding window of recent samples. The approach is demonstrated in simulation on a district heating system digital twin, showing that the SPC monitoring detects operating-condition changes and that combining slow+fast learning substantially improves predictive fit compared with single models, simple ensemble averaging, or a standalone online GP. Overall, SPC is used as a core mechanism for monitoring and event-triggering model updates in a continual-learning setting rather than for traditional manufacturing quality control.","The monitoring statistic is the Mahalanobis distance (Hotelling’s T²): $T^2(\mathbf z_e,\mathbf z)=\{(z_e(k)-\mu_z)^\top\Sigma_z^{-1}(z_e(k)-\mu_z)\}_{k\in I_e}$ with sample mean $\mu_z$ and covariance $\Sigma_z$. Empirical Hotelling T² chart limits are set by percentile: $\mathrm{LCL}=0$ and $\mathrm{UCL}=p_j$ such that $P_e(T^2\le p_j)=j/100$ (typ. $j=99.73$). Slow-ensemble weights are based on inverse Mahalanobis distance to each model’s training inputs: $\lambda^{[i]}(u)=\frac{w^{[i]}(u)}{\sum_{r=1}^n w^{[r]}(u)}$ with $w^{[i]}(u)=1/T^2(u,u^{[i]})$, and the ensemble output is $y_s(k)=\sum_{i=1}^n \lambda^{[i]}(u(k))\,y^{[i]}(k)$.","In the district heating system simulation, the proposed slow ensemble $M_s$ (Mahalanobis-proximity weighting) achieved FIT = 69.5% on a two-day test, outperforming single models trained on individual operating conditions (48.6% and 42.0%) and simple arithmetic averaging of models (54.1%). Adding the online GP fast-learning correction improved FIT from 69.5% (slow only) to 94.2% (combined model $M$). A standalone online GP model trained directly on inputs achieved FIT = 61.4%, worse than the proposed two-fold approach. The paper also reports example empirical control limits (e.g., UCLe = 187.5 then 1027.2 after adding a second model; input UCLs such as 59.1 and 37.3) illustrating the control-chart-based detection of new operating conditions.",None stated.,"The SPC procedure is used as an event trigger for model adaptation, but the paper does not provide formal guarantees on false-alarm rate, detection delay, or run-length behavior of the empirical T² charts under the non-Gaussian, autocorrelated time-series data typical of dynamical systems. The monitoring assumes availability of representative reference/test splits for each operating condition and may be sensitive to choices of window/batch sizes and percentile $j$ without a robustness study. The approach is evaluated on a simulated digital twin of a single energy system; broader real-world validation and sensitivity analyses (e.g., to noise, drift, missing data, and changing disturbance statistics) are not shown.","Future work will establish theoretical properties of the proposed architecture when integrated into a model predictive control framework, including addressing the exploration–exploitation trade-off with safety guarantees.","Developing principled design methods for the empirical T² chart thresholds (e.g., targeting a specified in-control ARL under dependence) and analyzing detection delay under gradual vs abrupt operating-region changes would strengthen the SPC component. Extending the monitoring to explicitly handle autocorrelation (e.g., residual modeling/whitening, dynamic T²) and missing/irregularly sampled data would improve practical applicability. Providing open-source implementations (MATLAB/Python) and benchmarking against alternative change-detection methods (CUSUM/EWMA/GLR) on multiple real industrial datasets would improve reproducibility and generalizability.",2507.12187v1,local_papers/arxiv/2507.12187v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:08:52Z
FALSE,Other,CUSUM|Machine learning-based|Other,Phase II,Transportation/logistics|Theoretical/simulation only|Other,NA,NA,NA,Simulation study|Other,False alarm rate|Detection probability|Expected detection delay|Other,"Not discussed (dataset sizes reported: train 720 trajectories / 1,906,320 sliding-window samples; val 120 / 317,720; val2 120 / 317,720; test 240 / 635,440; test2 900 / 929,900; window length 100, step 10).",TRUE,Python,Not provided,https://github.com/tsfresh/tsfresh|https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/|https://www.sciencedirect.com/science/article/pii/S0196890410005273|https://www.sciencedirect.com/science/article/pii/S1568494621006724|http://jmlr.org/papers/v9/vandermaaten08a.html,"The paper proposes a configurable onboard fault detection and diagnosis (FDD) pipeline for the electrical valve actuator system of a reusable space launcher, emphasizing confidence estimation, out-of-distribution (OOD) detection, and false-alarm control. Raw multivariate sensor time series are segmented into sliding windows, scaled, and encoded by a temporal convolutional autoencoder (TCAE) trained on nominal data; features from the latent space and reconstruction residuals feed calibrated histogram-based gradient boosting tree (HGBT) classifiers for binary fault detection and multiclass diagnosis. OOD detection is implemented via inductive conformal anomaly detection using TCAE reconstruction errors, with an additional trajectory-level criterion to reduce spurious alerts. To limit false alarms, the authors post-process the calibrated fault probability stream with a CUSUM scheme and tune thresholds; they also use threshold moving to address class imbalance in fault detection. Evaluation is performed on simulated trajectories from a physical model of an electric motor-controlled valve; results show good diagnosis and OOD detection performance, while fault detection suffers under prevalence shift and class overlap, motivating further validation on real test-bench data.","Fault-probability post-processing uses a one-sided CUSUM on the calibrated failure probability stream: $C_i=\max\{0,\, C_{i-1}+(x_i-T_{fp}-k)\}$ with $C_0=0$, where $x_i$ is the classifier’s calibrated fault probability, $T_{fp}$ a probability threshold, and $k$ a slack term; signal when $C_i>T_{cs}$. OOD thresholding is calibrated via inductive conformal anomaly detection using reconstruction error $e$: $thr_{ood}=\text{Quantile}_{\lceil (n+1)(1-\alpha)\rceil/n}(e)$ (quantile computed on calibration errors), with significance level $\alpha$. The TCAE block depth is guided by a receptive-field coverage formula: $L=\lceil\log_b(\frac{(T-1)(b-1)}{2(k-1)}+1)\rceil$ for dilation base $b$, window length $T$, and kernel size $k$.","Using the final configuration on trajectory-level evaluation, fault detection achieved Accuracy 0.83 (test) and 0.80 (test2) with FPR 0.03→0.06 and Recall 0.69→0.66 (FNR about 0.31–0.34), showing degradation under prevalence shift. Diagnosis performance was strong: Accuracy 0.92 (test) and 0.94 (test2), Recall 0.93–0.94. OOD detection met the window/trajectory false-positive target with FPR 0.01 (test) and 0.00 (test2) and Recall 1.00 on their synthetic OOD set. For the selected TCAE(1) (window length 100, step 10), reported inference time was ~2.9 ms per window and memory footprint ~1.27 MB; HGBT hyperparameters were tuned via Optuna, and fault CUSUM thresholds were set to $T_{fp}=0.75$, $T_{cs}=4$, $k=2$.","The study is evaluated only on simulated data generated from a physical model; the authors state that testing on real data is necessary to reach operational maturity. Significant class overlap means only a subset of fault classes (0, 16, 128, 511) is effectively separable, limiting the scope of diagnosis. The binary fault detector’s probabilities become strongly miscalibrated under prevalence shift between development and final validation sets, and adapting thresholds would require estimating deployment prevalence, which the authors note is difficult. Conformal prediction guarantees are only marginal and rely on assumptions (e.g., exchangeability/independence) that are violated by overlapping time-series windows.","CUSUM is used as a post-processing heuristic on classifier probabilities rather than modeling the underlying time-series dependence; performance may be sensitive to window/step choices and to temporal correlation not explicitly accounted for. Comparisons to established SPC/anomaly detection baselines (e.g., EWMA/Shewhart on engineered residuals, Bayesian/GLR change detection, or state-space/AR models) are limited, so it is unclear how much benefit comes specifically from the ML components versus simpler monitoring. The OOD evaluation uses synthetically perturbed trajectories that appear well separated from ID data; this may overestimate OOD performance relative to realistic unknown faults or sensor degradations closer to nominal behavior. Reproducibility is limited because implementation details are given but no code or trained models are shared, and onboard compute/memory constraints beyond a single GPU training setup are not fully characterized for an embedded ECU deployment.","The authors plan to transition from simulated data to real data collected on an integrated test bench with electric motors operating real valves to validate the approach under realistic conditions. They also propose investigating conformal prediction (beyond post-hoc calibration) for confidence estimation in FDD because it may be more resilient to distribution shifts. Additionally, they mention exploring how to leverage both simulated and real data jointly in future development.","Develop a principled approach to handle prevalence and distribution shift online (e.g., test-time recalibration, prior probability shift correction, or drift detection) so probability thresholds and CUSUM parameters adapt safely without requiring true prevalence. Extend the monitoring layer with SPC-style design criteria (e.g., ARL/ATS targets) and evaluate detection delay/false alarm trade-offs across a wider range of shifts and autocorrelation structures. Provide embedded-friendly reference implementations and perform hardware-in-the-loop timing/robustness tests on representative ECUs. Strengthen OOD validation with more realistic fault modes and sensor anomalies (drift, saturation, dropouts) and evaluate robustness to missing data and irregular sampling.",2507.13022v1,local_papers/arxiv/2507.13022v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:09:38Z
FALSE,Other,Shewhart|Other,Phase II,Healthcare/medical|Theoretical/simulation only|Other,NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Other,Not discussed.,TRUE,R|Other,Public repository (GitHub/GitLab),NA,"The paper proposes deep learning survival models (LSTM and CNN-LSTM) with learnable copula-based activation functions (Clayton, Gumbel, hybrid, and Clayton–ReLU) to model dependence in correlated, right-censored multivariate survival outcomes. Control charts are used only as a model monitoring/evaluation tool: residuals (observed minus predicted survival times) are plotted on Shewhart charts with 2σ limits and summarized using average run length (ARL) across simulations and cross-validation folds. Performance is studied via a synthetic Weibull-based simulation with induced inter-response dependence and right censoring, and via a real breast-cancer dataset (METABRIC) downloaded from Kaggle. Results reported in tables compare mean/SD of residuals and mean/SD of ARL across activation variants, with Clayton–ReLU often described as improving residual stability for binary/categorical responses. Overall, this is primarily a machine learning/survival-analysis paper; SPC is not the methodological focus and no new control-charting procedure is developed beyond using standard Shewhart limits for residual diagnostics.","Residuals are defined as $R = Y - \hat{Y}$. A Shewhart residual chart is applied with 2-sigma limits: $\mathrm{UCL}=\bar{R}+2\sigma_R$ and $\mathrm{LCL}=\bar{R}-2\sigma_R$. Sensitivity is summarized by $\mathrm{ARL}=1/P(\text{signal})$, where a signal occurs when a residual exceeds the control limits. Copula-based activations map $x\in\mathbb{R}$ to $u=\Phi(x)\in[0,1]$ and then apply, e.g., Clayton: $C_\theta(u,v)=(u^{-\theta}+v^{-\theta}-1)^{-1/\theta}$ and $g_{\text{Clayton}}(x,\theta)=(u^{-\theta}-1)^{-1/\theta}$; Gumbel: $C_\theta(u,v)=\exp(-(({-\log u})^\theta+({-\log v})^\theta)^{1/\theta})$ and $g_{\text{Gumbel}}(x,\theta)=\exp(-(-\log u)^\theta)$; hybrid averages Clayton and Gumbel.","In simulated data (Table 1), mean ARLs range roughly from ~22 to ~95 depending on model/response; for example, CNN–LSTM ReLU and CNN–LSTM Sigmoid report mean ARL = 95.1 for Response 2, while CNN–LSTM Clayton–ReLU reports mean ARL ≈ 50.05 for Response 2. The authors state that despite high ARL for some ReLU/Sigmoid cases, the residual charts show instability for binary/categorical responses, and they recommend CNN–LSTM with Clayton–ReLU for multivariate mixed-type responses. On METABRIC (Table 3), mean ARL for Response 2 is reported as ~34.09 (CNN–LSTM Clayton), ~72.41 (CNN–LSTM Clayton–ReLU), ~102.33 (CNN–LSTM ReLU), and ~101.08 (CNN–LSTM Gumbel), with corresponding residual means near 0 for Response 2. For Response 1 on METABRIC, mean residuals are large (e.g., ~45.5 for CNN–LSTM Clayton, ~126.7 for CNN–LSTM Sigmoid/Gumbel), and the discussion notes Clayton–ReLU as yielding better residual stability for Response 2.","The paper notes the current copula-activation approach is ""a functional surrogate for a more formal theory"" and calls for theoretical guarantees (e.g., universal approximation, Lipschitz continuity, training stability). It also states that the current framework focuses on the bivariate case (only two survival responses per instance for the copula layer) and restricts attention to bivariate copulas due to computational constraints, suggesting higher-dimensional extensions via vine copulas.","The control-chart component is purely diagnostic (standard Shewhart with ad hoc 2σ limits) and is not calibrated to provide a specified in-control ARL under a defined residual distribution; thus ARL comparisons may conflate model error distribution changes with chart design choices. Residuals for censored outcomes are not straightforwardly defined (since $Y$ may be censored), and the paper’s approach may yield biased residual diagnostics without careful treatment of censoring. Autocorrelation/serial dependence in residuals (common in time-indexed predictions) is not modeled, which can invalidate Shewhart assumptions and ARL interpretation. The METABRIC summary shown indicates event status has only one value (all censored), suggesting potential data or preprocessing issues that would undermine survival-model validation.","The authors propose developing formal theoretical guarantees for copula-based activation functions (e.g., universal approximation, Lipschitz continuity, and training stability). They also suggest extending from the bivariate setting to higher-dimensional dependence using pair-copula constructions (vine copulas such as C-vines/D-vines). In the conclusion, they state future work will focus on competing-risks modeling for clustered survival data within the CNN–LSTM copula-activation framework.","Provide principled SPC calibration for the residual charts (e.g., choose limits to target a nominal in-control ARL and account for estimation uncertainty), and evaluate robustness when residuals are non-normal/heavy-tailed. Develop censoring-aware residual definitions (e.g., martingale/deviance residuals or IPCW-based residuals) and study their run-length properties under censoring. Extend monitoring to autocorrelated residual streams (e.g., EWMA/CUSUM on residuals or score processes) to improve small-drift detection with controlled false-alarm behavior. Release a reproducible software package/workflow (with fixed seeds, data preprocessing scripts, and charting utilities) and validate on additional real-world multivariate survival datasets with nontrivial event rates.",2507.14641v1,local_papers/arxiv/2507.14641v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:10:29Z
TRUE,Multivariate|Profile monitoring|Other,EWMA|Hotelling T-squared|MEWMA|Machine learning-based|Other,Both,Healthcare/medical|Finance/economics|Theoretical/simulation only|Other,NA,FALSE,FALSE,Simulation study|Other,False alarm rate|Detection probability|Other,"Not discussed (but notes that Zhang et al. (2023) requires very large holdout sets for small FAR, e.g., for FAR=0.001 the holdout size should be much larger than 1,000 and often at least 10,000; this paper’s bootstrap CL is aimed at smaller n).",TRUE,Python|Other,Not provided,NA,"The paper addresses score-based concept drift monitoring, where changes in a supervised learning model’s predictive relationship are detected by monitoring the mean of Fisher score vectors with a MEWMA chart (as in Zhang et al., 2023). Its main contribution is a nested bootstrap procedure for calibrating the MEWMA Hotelling’s T^2 control limits so the full initial dataset can be used to fit the baseline model, eliminating the need for a large holdout set. The authors show a naïve nested bootstrap mis-calibrates variability of the MEWMA statistic and propose a 0.632-like variance inflation correction, yielding time-varying control limits that better control pointwise false-alarm probability, especially for small training samples and stringent FAR (e.g., α=0.001). Numerical examples (ridge regression on a mixed-linear population; neural-network regression on a nonlinear oscillator system) demonstrate improved pointwise false-alarm control versus the two-sample/holdout quantile method of Zhang et al. (2023) while retaining the same detection statistic. The work situates concept-drift monitoring within SPC/profile-monitoring ideas, emphasizing practical calibration of control limits under finite-sample model-estimation uncertainty (Phase I) and future observation variability (Phase II).","Score vector for observation i is s(\hat\theta;x_i,y_i)=\nabla_\theta \log P(y_i\mid x_i;\theta)\rvert_{\theta=\hat\theta}; for the ridge/Gaussian regression example it is s(\hat\theta;x_i,y_i)=(y_i-x_i^\top\hat\theta)x_i-(\gamma/n)\hat\theta. The MEWMA recursion is z_t=\lambda s_t+(1-\lambda)z_{t-1} with z_0=0, and monitoring uses Hotelling’s statistic T_t^2=(z_t-\bar s)^\top \hat\Sigma^{-1}(z_t-\bar s). Control limits are calibrated by nested bootstrap with a variance correction z_{b,j,i}/\sqrt{k(\lambda,i,n)}, where k(\lambda,i,n) is the derived covariance-inflation factor (Eq. 3.3), and CL_i is the empirical (1-\alpha) quantile of the corrected bootstrap T_{b,j,i}^2 (Eq. 3.5).","In the mixed-linear example (n=2000, \lambda=0.01, \alpha=0.001, B_O=100, B_I=200), the proposed bootstrap control limits keep the empirical pointwise false-alarm rate close to 0.001, whereas the Zhang et al. (2023) two-sample control limit yields severely inflated PFAR (reported as exceeding 0.06). In the nonlinear oscillator example (n=3000, MLP with 3361 parameters but monitoring only the last-layer 33 parameters), the method detects a parameter shift introduced at i=201 at about i=215 (low noise) and i=221 (high noise) in typical runs, while maintaining pre-change PFAR near 0.001 in both regimes. A large Monte Carlo validation (R=8000 streams) reports PFAR curves close to the nominal \alpha=0.001 during the in-control period, and comparisons in Appendix B indicate the two-sample method has inflated/unstable PFAR under both low- and high-noise conditions. The method emphasizes pointwise Type I error control rather than ARL due to computational cost of nested bootstrapping at each time step.","The paper notes that targeting in-control ARL is not pursued because the monitoring statistic depends on finite-sample variability of the fitted model and Monte Carlo ARL estimation would be computationally prohibitive due to the nested bootstrap required at each time step. It also states the approach assumes a differentiable likelihood/optimization objective and is not directly applicable to non-differentiable/nonparametric tree methods (e.g., Random Forests, XGBoost). Regularization/hyperparameters are treated as fixed during bootstrap refits for tractability, rather than re-tuned within each bootstrap replicate.","The proposed calibration controls a pointwise false-alarm probability, which does not directly translate to a desired in-control ARL or to global false-alarm probability over a monitoring horizon; practitioners may still need guidance on mapping \alpha to operational false-alarm frequency. The method assumes i.i.d. training and monitoring observations; performance under serial dependence, seasonality, or covariate shift in X (common in streaming settings) is not addressed. Nested bootstrap with repeated model refits can be expensive for large deep networks and high-dimensional monitored parameter vectors; scalability and memory/runtime tradeoffs (and sensitivity to B_O, B_I) could be further characterized. The approach monitors score mean shifts; if drift manifests primarily as changes in higher moments or distributional shape with little score-mean change for the chosen parameterization, detectability may degrade.",None stated.,"Extend the control-limit calibration to explicitly handle autocorrelated/temporally dependent streams (e.g., block bootstrap or model-based resampling) and irregular sampling. Provide methods to translate pointwise \alpha to run-length or horizon-wise error guarantees (e.g., controlling overall false alarm probability over T times or steady-state ARL approximations). Develop robust/nonparametric variants for misspecified likelihoods and for models without clean likelihoods (e.g., score surrogates for tree ensembles), and release an implementation package to facilitate adoption and reproducibility.",2507.16749v2,local_papers/arxiv/2507.16749v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:11:15Z
TRUE,Univariate|Other,Shewhart|Other,Both,Environmental monitoring,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate,"Phase I uses k reference subgroups of size n each (m=n×k observations). Simulation settings use n=10, k=20; applications use n=10, k=3 (RH) and n=5, k=4 (flood levels).",TRUE,R,Not provided,https://github.com/RfastOfficial/Rfast|http://www.met.wau.nl/,The paper proposes a Studentized Bootstrap Truncated-Beta Control Chart (SBTBC) to monitor a chosen percentile (quantile) of proportion-type environmental data modeled by a truncated beta (Tbeta) distribution when in-control parameters are unknown. Phase I estimates Tbeta shape parameters via MLE (numerical optimization) and constructs LCL/UCL for the target percentile using a studentized parametric bootstrap with a specified false alarm rate. Phase II monitors successive subgroups by re-estimating the percentile from each subgroup and signaling when it falls outside the bootstrap limits. Performance is evaluated via Monte Carlo simulations (B=5000 bootstrap resamples; 5000 run-length replications) reporting in-control ARL/SDRL and out-of-control ARL under shifts in Tbeta parameters. Real environmental applications (relative humidity and river flood levels) illustrate practical use and show the proposed chart signals earlier/more often than a conventional beta-percentile bootstrap chart when truncation is present.,"The truncated beta density is $f(x\mid\theta_1,\theta_2)=\frac{x^{\theta_1-1}(1-x)^{\theta_2-1}}{I_b(\theta_1,\theta_2)-I_a(\theta_1,\theta_2)}$ for $a\le x\le b$, with percentile $\xi_p=F^{-1}(p;\theta_1,\theta_2)$ and plug-in estimate $\hat\xi_p=F^{-1}(p;\hat\theta_1,\hat\theta_2)$. Bootstrap percentiles $\hat\xi_{p}^{*}$ are computed from bootstrap MLEs $(\hat\theta_1^*,\hat\theta_2^*)$ and studentized as $t_i^*=(\hat\xi_{ip}^*-\hat\xi_p)/\mathrm{SE}(\hat\xi_p^*)$. Control limits are obtained by inverting the empirical $\nu/2$ and $1-\nu/2$ quantiles of $t^*$: $\mathrm{LCL}=\bar\xi_p^*+t_{Lp}^*\,\mathrm{SE}(\hat\xi_p^*)$ and $\mathrm{UCL}=\bar\xi_p^*+t_{Up}^*\,\mathrm{SE}(\hat\xi_p^*)$.","In simulations with $\theta_1=2,\theta_2=15$, truncation intervals $(a,b)\in\{(0,0.5),(0,0.6),(0.1,0.6)\}$, $n=10$, $k=20$, and $\nu\in\{0.005,0.0027,0.002\}$, the in-control ARL values closely match nominal values $1/\nu$ (about 200, 370, and 500) across percentiles $p\in\{0.1,0.25,0.5,0.75,0.9\}$. Out-of-control ARL decreases sharply for both upward and downward shifts in $(\theta_1,\theta_2)$; e.g., for $a=0,b=0.5,\nu=0.0027$, a 4% decrease in $\theta_1$ (with $\theta_2$ in-control) reduces the median’s ARL1 by about 22.85%, while a 4% increase reduces it by about 10.05%. In the RH application (May 2007 baseline, support (0.3,1), $n=10,k=3$, $p=0.9$, $\nu=0.0027$), the SBTBC limits are UCL=0.976, CL=0.926, LCL=0.805 and a 20% decrease in $\theta_1$ yields multiple signals (3/20 subgroups below LCL). Compared to a studentized bootstrap beta-percentile chart, SBTBC signals earlier (first at subgroup 2 vs 7 in one comparison) and is claimed to detect shifts faster overall.",None stated.,"The method relies on correct specification of a truncated beta model and fixed truncation bounds (a,b); misspecification could distort control limits and ARL. It assumes independent subgroups/observations and does not develop adjustments for serial correlation common in environmental time series. Implementation requires repeated MLE optimization inside a large bootstrap loop (e.g., B=5000), which may be computationally heavy and sensitive to numerical convergence, especially for small n or extreme truncation.",None stated.,"Develop robust or semi-/nonparametric variants that reduce sensitivity to Tbeta model misspecification and truncation-bound uncertainty. Extend the chart to autocorrelated environmental processes (e.g., via residual charts or time-series modeling) and to adaptive/self-starting settings with limited Phase I data. Provide open-source software (R package) and guidance on selecting (a,b), B, and Phase I sample sizes under practical constraints.",2507.23732v1,local_papers/arxiv/2507.23732v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:11:59Z
TRUE,Univariate,Shewhart,Both,Manufacturing (general)|Semiconductor/electronics|Theoretical/simulation only,FALSE,FALSE,NA,Simulation study|Integral equation|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Detection probability|False alarm rate|Other,"Phase I sample size is denoted by m; simulation examines m ∈ {30, 50, 100, 200, 500, 800, 1000, 2000, 5000, 8000}. The paper notes IC performance is poor for small m and that for m > 5000 the ACARL is close to the nominal IC ARL (370.4) with reduced variability.",TRUE,None / Not applicable,Not provided,NA,"The paper develops a two-sided Shewhart-type Weibull time-between-events (TBE) control chart aimed at detecting increases or decreases in the Weibull scale parameter (with the shape parameter treated as fixed/known). It contrasts the known-parameter case (Case I), where probability limits yield an in-control ARL determined solely by the false alarm rate, with the practical case (Case II) where the scale parameter is unknown and estimated from Phase I using an MLE, producing plug-in (conditional) control limits. Recognizing that estimated limits distort nominal in-control performance, the paper proposes adjusting limits to achieve desired in-control behavior using criteria based on the conditional ARL distribution: the average of conditional ARL (AARL/ACARL) and the standard deviation of conditional ARL (SDARL/SDCARL). Analytical expressions are given for conditional signal probability and conditional ARL, and the distribution of the scale estimator is used to evaluate (via integration) unconditional measures. A simulation study quantifies how Phase I sample size m affects IC performance and variability, showing convergence toward nominal ARL as m grows and substantial extra false-alarm tendency for small m.","Weibull TBE model: $f(x;\eta,\beta)=\frac{\eta}{\beta}(x/\beta)^{\eta-1}\exp\{-(x/\beta)^\eta\}$ and $F^{-1}(u)=\beta[-\ln(1-u)]^{1/\eta}$. For known parameters (Case I), probability limits set $\text{LCL}_K=\beta_0 A_1$ and $\text{UCL}_K=\beta_0 A_2$ with $A_1=[-\ln(1-\alpha_0/2)]^{1/\eta_0}$ and $A_2=[-\ln(\alpha_0/2)]^{1/\eta_0}$; the signal probability is $PS=P(X<\text{LCL}_K\text{ or }X>\text{UCL}_K)$ and $ARL=1/PS$ (geometric RL). For unknown scale (Case II), $\hat\beta=\left(\frac{1}{m}\sum_{i=1}^m x_i^{\eta_0}\right)^{1/\eta_0}$ is plugged into limits, yielding conditional signal probability (CPS) and conditional ARL (CARL) given the Phase I estimate; the distribution of $W=(\hat\beta/\beta_0)^{\eta_0}$ is Gamma (and $Y=2mW\sim\chi^2_{2m}$), enabling evaluation of $E(CARL)$ and $SD(CARL)$ by integration.","Using $\alpha_0=0.0027$ (nominal IC ARL 370.4), simulations for m ∈ {30,…,8000} show ACARL is substantially below nominal for small m (e.g., ≈338.9 at m=30) and approaches 370.4 as m increases (≈370.15 at m=5000 and ≈370.31 at m=8000 in the tabulated example). Variability in IC performance decreases markedly with m (e.g., SDCARL drops from ≈135.8 at m=30 to ≈14.7 at m=5000 and ≈11.7 at m=8000 for one parameter setting). The exceedance probability that CARL is less than 370.4 is around 50% across m, implying frequent IC underperformance (more false alarms) when relying on plug-in limits, particularly at small m. The paper concludes large Phase I samples are needed for estimated-limit charts to deliver near-nominal IC behavior.",None stated.,"The work is restricted to a Shewhart (memoryless) chart and primarily targets large, abrupt shifts; it does not develop or compare against more sensitive TBE schemes (e.g., CUSUM/EWMA Weibull charts) for small shifts. The model assumes a fixed/known Weibull shape parameter and independent TBEs; misspecification of shape or serial dependence (common in reliability/repairable systems) could materially change false-alarm and detection properties. The paper reports simulations and conditional-performance criteria but does not document software, random seeds, or reproducibility details, and the scope of out-of-control comparisons/benchmarks versus existing Weibull charts appears limited from the provided text.",None stated.,"Extend the estimated-parameter approach to the more realistic case where both Weibull shape and scale are unknown (joint estimation and its impact on conditional/unconditional RL). Develop and compare estimated-parameter Weibull CUSUM/EWMA charts for improved sensitivity to small scale changes, including steady-state ARL analysis. Study robustness to autocorrelation/repair effects and to shape-parameter misspecification, and provide practitioner guidance via software (e.g., R package) implementing limit adjustment for targeted unconditional IC properties.",2508.05790v1,local_papers/arxiv/2508.05790v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:12:26Z
FALSE,Univariate|Other,CUSUM|Change-point|Machine learning-based,Phase II,Other,NA,FALSE,NA,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Expected detection delay|Other,Not discussed,TRUE,None / Not applicable,Not provided,https://www.mathmodels.org/Problems/2024/MCM-C/index.html,"The paper proposes a sports-analytics framework to quantify and predict “momentum” in tennis singles using point-by-point match data (2023 Wimbledon Men’s Singles). Momentum existence is tested using contingency tables and Pearson’s Chi-squared independence test on streak-length vs. next-point outcome, rejecting independence with a highly significant result (e.g., χ²=111.497, p=9.51×10⁻¹⁸ over 31 matches). A momentum index M_t is constructed via an entropy weight method on selected performance features, producing a time series per match. The authors then apply a CUSUM control chart to the momentum series to flag change points (positive/negative shifts) and define a relative-distance intensity measure V_t based on inter-change-point durations. Finally, they build a BP neural network optimized with particle swarm optimization (PSO) and show that adding M_t, change-point labels, and V_t improves point-outcome prediction (AUC up to 0.7443) and is competitive versus Random Forest, SVM, and logistic regression, with SHAP identifying unforced errors and winners as most influential predictors.","Momentum is defined as a weighted composite of standardized features: $M_t=\sum_{i=1}^m w_i z_{it}$, where weights come from entropy values $e_i=-\frac{1}{\ln T}\sum_{t=1}^T p_{it}\ln(p_{it}+\epsilon)$ and $w_i=\frac{1-e_i}{\sum_{i=1}^m(1-e_i)}$. Change-point detection uses a (one-sided/absolute) CUSUM recursion $C_t=M_t-\mu+C_{t-1}-d$ with thresholding at $\pm h$ to define $CP_t\in\{-1,0,+1\}$. Momentum-shift intensity at detected change points is $V_{t_i}=CP_{t_i}\times (D_{\max}/D_i)$ with linear interpolation between change points.","Across 31 matches (n=3595 winning-streak instances), Pearson’s Chi-squared test for independence of streak length and next-point result gives $\chi^2=111.497$ with p-value $9.51\times 10^{-18}$, supporting momentum effects. In the Wimbledon final example, the entropy-weighted momentum metric uses weights (z3,z4,z6,z7,z9,z10) = (0.3959, 0.1122, 0.0987, 0.1162, 0.1136, 0.1633). CUSUM-based monitoring of $M_t$ in the final detects 40 change points (20 positive, 20 negative). Prediction improves as momentum features are added: BP+PSO AUC increases from 0.7125 (Base) to 0.7253 (Base+M), 0.7315 (Base+M+CP), and 0.7443 (Base+M+CP+V); under Base+M+CP+V, BP+PSO outperforms Random Forest (AUC 0.6550), SVM (0.6383), and logistic regression (0.7310).",None stated,"CUSUM is used as a change-point tool on a constructed momentum index rather than for traditional process monitoring; control-limit design, in-control ARL/false-alarm calibration, and sensitivity to serial dependence are not analyzed, so signaling behavior may be hard to interpret statistically. The CUSUM threshold $h$ is tuned to hit a target number of change points (rather than a specified false-alarm rate), which can bias comparisons and limits reproducibility across matches. Dependence among points within games/sets (and contextual covariates like server effects) could induce autocorrelation in $M_t$, potentially inflating detected change points. Implementation details (software, hyperparameters, random seeds) and code are not provided, limiting replicability.","The authors propose refining momentum measurement via improved feature selection spanning psychological, technical, and physical dimensions combined with statistical methods. They also suggest combining multiple machine learning algorithms to improve robustness and predictive performance.","Calibrate CUSUM parameters ($d,h$) to explicit statistical guarantees (e.g., specified in-control ARL or false-alarm probability) and report run-length metrics to make the monitoring component comparable to SPC literature. Extend the approach to explicitly model serial dependence (e.g., ARMA errors, state-space models, or dependent CUSUM) and assess robustness under varying match contexts (serve/return, score leverage). Provide an open, reproducible software implementation with standardized benchmarks across tournaments and women’s matches, and evaluate alternative change-point detectors (e.g., GLR/CUSUM variants, Bayesian online change-point detection) on the same momentum series.",2509.01243v1,local_papers/arxiv/2509.01243v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:13:09Z
TRUE,Univariate|Other,Shewhart|EWMA,Phase II,Healthcare/medical|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),Not discussed (design examples use n=1 and n=10; real-data Phase I uses first 150 observations and Phase II uses the remainder).,TRUE,R|None / Not applicable,Not provided,https://EconPapers.repec.org/RePEc:ste:nystbu:94-10|https://books.google.com.pk/books?id=vQUNprFZKHsC|https://rdocumentation.org/packages/AER/versions/1.2-10|https://cran.r-project.org/web/packages/pscl/index.html,"The paper proposes Phase II Shewhart-type and EWMA control charts for monitoring count processes modeled by a zero-inflated negative binomial (ZINB) distribution, motivated by data with excess zeros and over-dispersion. The ZINB-EWMA statistic is defined on the sample mean and corresponding control limits are derived using the in-control mean and variance of the ZINB distribution. Performance is assessed primarily via extensive Monte Carlo simulation, reporting in-control and out-of-control ARL and SDRL under shifts in ZINB parameters (notably the success probability p, and also shifts in other parameters in additional experiments). Across many scenarios (various k, θ, and smoothing parameters λ), the ZINB-EWMA generally achieves the target ARL0 more reliably than the ZINB-Shewhart chart and tends to detect small/moderate shifts faster (smaller ARL1), with smaller λ typically performing better. A real-data illustration using the Owls dataset (glmmTMB) shows more out-of-control signals for the EWMA chart than the Shewhart chart, consistent with higher sensitivity.","ZINB pmf: $P(Y=0)=\theta+(1-\theta)p^k$ and for $y\ge1$, $P(Y=y)=(1-\theta)\binom{y+k-1}{y}p^k(1-p)^y$. EWMA statistic (for sample mean) $Z_i=\lambda\bar{Y}_i+(1-\lambda)Z_{i-1}$ with IC mean $E(Z_i)=k(1-\theta)(1-p)/p$ and IC variance $\mathrm{Var}(Z_i)=\frac{\lambda}{n(2-\lambda)}\left(\frac{k(1-\theta)(1-p)[1+(1-p)\theta k]}{p^2}\right)$. Control limits: $\mathrm{UCL}=\mu+L\sqrt{\mathrm{Var}(Z_i)}$, $\mathrm{CL}=\mu$, and $\mathrm{LCL}=\max\{0,\mu-L\sqrt{\mathrm{Var}(Z_i)}\}$ (often using only UCL since the statistic is nonnegative).","Monte Carlo evaluation uses 10,000 replications per scenario and calibrates L to target ARL0 (commonly 500; also 370 in some comparisons). Example (n=1,k=1,θ=0.85): ZINB-EWMA with λ=0.05 and L=3.105 yields ARL0≈500.81 (SDRL0≈501.87), whereas the ZINB-Shewhart (λ=1) with L=5.067 gives ARL0≈396.01 (SDRL0≈394.49), failing to match the target ARL0. For the Owls data illustration, Phase I uses first 150 observations and Shewhart signals point 212 as OOC, while EWMA signals points 195–200 and 212. Simulations also indicate smaller λ generally leads to smaller ARL1 (better detection) for shifts in p.","The authors note that ZINB-Shewhart charts may not achieve the pre-specified in-control ARL0 reliably; small changes in L can produce large changes in ARL0, making ARL1 comparisons less trustworthy when ARL0 is not matched. They also highlight that their study is Phase II focused.","The chart limits rely on moment-based variance expressions and a normal-approximation-style limit form ($\mu\pm L\sigma$), which may be inaccurate for highly discrete/zero-inflated data, especially for small n and extreme parameter settings. Autocorrelation/seasonality and other common count-process features (e.g., clustering, time-varying covariates) are not modeled, though these can materially affect false-alarm rates. The paper appears to rely on simulation calibration for L rather than providing a general exact/Markov-chain ARL computation for the proposed ZINB-EWMA, which may limit reproducibility/implementation for practitioners without code.","The authors suggest extending the work to CUSUM and adaptive control charts, and developing variable sample size (VSS) and variable sampling interval (VSI) schemes. They also mention bivariate extensions of the ZINB chart and proposing an ARL-unbiased design.","Provide exact/efficient ARL/SDRL computation methods (e.g., Markov chain or integral-equation approximations) for the ZINB-EWMA to avoid heavy simulation calibration in routine use. Develop robust/self-starting Phase II procedures that account for parameter estimation error from Phase I and quantify its impact on ARL. Extend to regression/ZINB-GLM monitoring with covariates (risk-adjusted charts) and to autocorrelated count time series (e.g., INGARCH/State-space ZINB) to better match real surveillance data.",2509.03304v1,local_papers/arxiv/2509.03304v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:13:47Z
TRUE,Image-based monitoring|High-dimensional|Other,Hotelling T-squared|Machine learning-based|Other,Phase II,Manufacturing (general)|Other,FALSE,FALSE,FALSE,Simulation study|Case study (real dataset),Detection probability|False alarm rate|Other,"Not discussed. (The paper specifies window/group sizes used for correlation estimation, e.g., n=25 in simulation and n=8 in the case study, but does not give general Phase I sample size guidance.)",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a deep learning in-situ monitoring framework that learns nonlinear, quality-oriented representations by maximizing canonical correlation between high-dimensional process signatures (e.g., optical emission spectra) and offline quality data (e.g., CT images) using Deep Canonical Correlation Analysis (DCCA). For online monitoring, since quality data are unavailable in real time, the method pairs each incoming process-signal window with the nearest neighbor reference quality sample from normal historical data and computes a canonical-correlation score as the monitoring statistic; a signal is triggered when the score falls below a threshold set to control Type-I error on validation data. This design is motivated by avoiding the normality assumption behind traditional $T^2$-chart monitoring of extracted features, which can fail under non-normal learned representations. The authors provide learning-theoretic guarantees and a finite-sample (non-asymptotic) error bound for the canonical-correlation estimate used in training/monitoring. Simulation experiments and a Direct Metal Deposition (additive manufacturing) case study show improved defect/shift discrimination versus baselines built on PCA/PLS features with $T^2$ statistics, especially when noise or non-normality degrades $T^2$ methods.","Offline training minimizes negative estimated DCCA correlation: $L(f,g)=-(1/N_0)\sum_{(X,Y)\in D_0} H_{n,p}(f(X;\theta_1),g(Y;\theta_2))$, where $H_{n,p}(U,V)=\|\hat\Sigma_{11}^{-1/2}\hat\Sigma_{12}\hat\Sigma_{22}^{-1/2}\|_*$. Online monitoring constructs a surrogate $Y^*$ by nearest-neighbor matching in normal process-signal space and computes $\rho^*(X^*)=H_{n,p}(f(X^*),g(Y^*))$, then signals via $\eta(X^*;\tau)=\mathbb{I}[\rho^*(X^*)<\tau]$ with $\tau$ chosen to control Type-I error.","In the case study (DMD additive manufacturing), the proposed DCCA+nearest-neighbor correlation score achieved test-set performance around FPR 9.5% and FNR 14.0% (F1 87.98%) using a validation-chosen threshold, while PCA+$T^2$ and PLS+$T^2$ exhibited very high test FPRs (63.5% and 74.0%) despite low FNRs (4.0% and 5.5%), indicating severe false-alarm behavior due to violated normality assumptions. When thresholds were adjusted to enforce FPR=10% across methods, DCCA+NN maintained FNR 14.0% (F1 87.76%), whereas PCA+$T^2$ and PLS+$T^2$ had much larger FNRs (83.5% and 77.5%) and low F1 (26.09 and 33.97). In simulation, performance advantages grew with increasing additive noise on process signals: the proposed approach improved Type-II error/F1 relative to PCA/PLS+$T^2$ and a classifier trained with limited defect samples. The paper also derives a non-asymptotic bound for correlation estimation error that decays as $O(n^{-1/2})$ in the window size $n$, and a generalization bound with $O(N_0^{-1/2})$ dependence on training sample size.","The authors note that direct online evaluation of correlation between process signals and true quality data is infeasible because quality measurements (e.g., CT scans) are not available during production, motivating their nearest-neighbor surrogate quality matching heuristic. They also note that several method parameters (e.g., correlation dimension selection tolerances and related hyperparameters) vary by process and are typically determined via empirical domain knowledge or iterative refinement. The case-study dataset cannot be made publicly available due to licensing restrictions (available upon request).","The online monitoring performance depends on the quality of the nearest-neighbor matching step; if the in-control process-signal space is multi-modal or drifts, nearest-neighbor pairing may yield unstable scores or increased false alarms without an explicit adaptation mechanism. Threshold selection is done by empirical Type-I error control on validation windows, but the method’s operating characteristics are not summarized in standard SPC terms (e.g., ARL/ATS), making comparisons to classical charts less direct. The approach assumes i.i.d. samples/windows and does not explicitly model serial dependence that is common in sensor streams, which can affect false-alarm properties. Reproducibility is limited because code is not provided and key implementation choices (distance metric details, architecture/training specifics beyond appendices) can materially affect results.","The authors propose future work to leverage the learned correlated features to reconstruct quality-related images directly from process signature signals, potentially via a jointly developed generative model, to improve interpretability and enable more direct visualization/analysis of emerging defects.","A valuable extension would be to incorporate autocorrelation/streaming dependence explicitly (e.g., block bootstrapping, state-space modeling, or sequential score control) and report ARL/ATS under dependence. Developing adaptive or drift-aware reference set management for the nearest-neighbor quality surrogate (e.g., forgetting factors or clustering-based libraries) could improve robustness in long-running production. Providing an open-source implementation and a sensitivity study over distance metrics, window size $n$, and feature dimension $p$ would strengthen practical adoption. Extending the approach to multivariate/multi-stream monitoring with principled run-rule or sequential decision calibration (e.g., CUSUM/EWMA on correlation scores) could connect it more directly to SPC practice.",2509.19652v1,local_papers/arxiv/2509.19652v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:14:31Z
TRUE,Univariate|Multivariate|Nonparametric|High-dimensional|Image-based monitoring|Other,EWMA|MEWMA|Other,Phase II,Semiconductor/electronics|Theoretical/simulation only|Other,TRUE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Detection probability,"For the manifold-fitting framework, the authors note a theoretical Phase I requirement of about m = O(σ^{-(d+3)}) points for manifold fitting error guarantees (via Yao et al., 2023). For chart setup they recommend at least 10 in-control deviations and a window size w ≥ 5; in experiments they often use 700 points for manifold fit/learn, 400 for AR model fitting, and 100 for chart calibration (and in an extreme TE case: 200 for fit, 60 for AR fitting, 20 for chart setup).",TRUE,Python|MATLAB,Public repository (GitHub/GitLab),https://github.com/iburaktas/Manifold-SPC,"The paper proposes two Phase II SPC frameworks for high-dimensional, dynamic processes assuming observations lie near a nonlinear low-dimensional manifold. The first framework fits the manifold directly in the ambient space using a state-of-the-art manifold fitting procedure, then monitors scalar deviations (distance to the fitted manifold) with a new distribution-free EWMA-style control chart (UDFM) designed to control Type I error (and hence in-control ARL). The second framework learns an explicit low-dimensional embedding (e.g., LPP/NPE), filters serial dependence in the embedded coordinates with AR models, and then applies a distribution-free multivariate EWMA chart (DFEWMA) to obtain controllable in-control performance. Both approaches explicitly address serial correlation by AR prewhitening (of deviations or embedded coordinates) to support valid false-alarm control. Extensive Monte Carlo studies on a synthetic sphere process and a replicated Tennessee Eastman chemical process, plus an image anomaly example (Kolektor commutator surfaces), show the manifold-fitting approach is competitive and sometimes superior, while manifold-learning methods can be strong for small shifts but may miss faults outside the retained subspace without additional reconstruction-error monitoring.","Process model: $Y_t=X_t+E_t$ (in-control) and $Y_t=X_t+\Delta+E_t$ (out-of-control), with $E_t\sim N(0,\sigma^2 I_D)$ and latent $X_t\in\mathcal M\subset\mathbb R^D$. Manifold-fitting monitoring statistic uses the estimated projection $\hat\pi(\cdot)$ and distance/deviation $d_t=\|Y_t-\hat\pi(Y_t)\|_2$ (or squared). The UDFM chart is a distribution-free EWMA built from rank-based scores $Z_{j,n}=\frac{1}{m+n}\max\{0, R_{j,n}-(m+n+1)/2\}$ over a rolling window $w$, with charting statistic $T_{n,w,\lambda}=\frac{\sum_{j=n-w+1}^n(1-\lambda)^{n-j}(Z_{j,n}-\mathbb E[Z_{j,n}])}{\sqrt{\operatorname{Var}(\sum_{j=n-w+1}^n(1-\lambda)^{n-j}Z_{j,n})}}$ and control limit $c_n(\alpha)$ chosen to satisfy a conditional false-alarm probability $\alpha$ at each step; run length is geometric under $H_0$.","In the synthetic 2-sphere example (D=6) with nominal ARLin≈20, the manifold-fitting (MF) method achieved ARLout 7.17 (SDRL 7.09) for a 3σ mean shift in dimension 1 and 2.74 (1.85) for a 10σ shift, while manifold-learning methods (NPE/LPP/PCA) had much larger ARLs (≈9–18) and were unable to detect shifts applied in an orthogonal ambient dimension (dimension 4), where MF achieved ARLs as low as 1.99 (0.67) for a 10σ shift. In replicated Tennessee Eastman simulations (300 variables) with nominal ARLin≈20, for fault amplitudes 0.05 the NPE/LPP approaches typically had smaller ARLout (e.g., fault 3: 3.34/3.44) than MF (6.50), while for amplitude 0.1 MF was comparable or best (e.g., faults 3 and 4: MF 2.09 vs NPE 2.20–2.21, LPP 2.30–2.32); PCA largely failed to detect these nonlinear-manifold faults (ARLout≈20). In an extreme high-dimensional case (m=280<D=300) MF maintained ARLin≈21.33 and showed degraded but still meaningful detection (e.g., fault 3 amplitude 0.1: ARLout 2.65; amplitude 0.05: 9.89). On the Kolektor surface-defect image dataset, with nominal ARLin=200 (w=5, λ=0.05), the MF/UDFM procedure signaled by the 5th defective image.","The authors note that UDFM is formulated assuming i.i.d. deviations, even though estimated deviations may exhibit mild autocorrelation; they address this pragmatically by AR prewhitening and state deviations can be prewhitened if necessary. They also note their practical noise estimator (Algorithm 3) ignores manifold curvature effects and is intended as a practical estimate rather than an unbiased estimator. For manifold-learning approaches (LPP/NPE), they state the generalized eigenproblem becomes ill-posed when m < D, restricting applicability in extreme high-dimensional settings.","The manifold-fitting method depends on multiple tuning constants (e.g., C0, C1, C2; neighborhood radii; AR order choices), and performance/false-alarm control may be sensitive to these in practice, especially when process conditions drift. The theoretical Type I control for the chart relies on permutation/conditional calibration assumptions that may be computationally heavy online and may not strictly hold after AR filtering (residuals only approximately i.i.d.). The paper focuses mainly on sustained mean shifts; broader fault classes (variance/covariance changes, transient faults, manifold topology changes) are not systematically evaluated with controlled benchmarks, and diagnostic capability (which variables caused the signal) is only briefly suggested rather than developed.",The authors suggest extending the manifold-fitting approach beyond mean shifts to detect shape changes in the underlying manifold (analogous to covariance changes). They also propose using the projected points on the manifold (not just deviation magnitudes) to help identify which variables contributed to a signal (fault diagnosis).,"Develop a fully self-starting version that updates the manifold fit/embedding online while preserving guaranteed in-control performance under concept drift. Add principled, data-driven selection of manifold-fitting radii/graph parameters and AR model structure, with robustness studies to non-Gaussian noise and nonstationary autocorrelation. Provide efficient implementations/approximations for real-time control-limit calibration (e.g., sequential Monte Carlo or analytical bounds) and extend to missing/irregularly sampled streams common in sensor networks.",2509.19820v1,local_papers/arxiv/2509.19820v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:15:27Z
TRUE,Univariate|Other,Shewhart|Other,Phase I|Phase II,Manufacturing (general)|Healthcare/medical|Energy/utilities|Theoretical/simulation only|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|Detection probability|False alarm rate|Other,"Simulations and illustrative examples use subgroup sizes n = 5, 10, and 15. For a demonstration dataset, they generate 50 subgroups (first 30 in-control, remaining 20 out-of-control with shift δ = 1).",TRUE,R,Not provided,NA,"The paper proposes two improved Shewhart-type process-mean control charts under repetitive sampling and one auxiliary variable X: an Mrep chart based on a ratio–product exponential-type estimator and an Mrwp chart based on a difference-cum-exponential-type estimator with weights. These charts extend the existing Mr chart (regression-estimator-based chart using known auxiliary mean μx) for monitoring shifts in the process mean of a study variable Y under a bivariate normal (Y, X) assumption. Performance is evaluated primarily via Monte Carlo simulation, comparing average run length (ARL) curves of baseline T0 (\bar{y}), existing T1 (Mr), and proposed T2 (Mrep) and T3 (Mrwp) across shifts, correlations (e.g., ρxy = 0.30, 0.60, 0.90), and subgroup sizes n = 5, 10, 15. The proposed charts (especially T3/Mrwp) show lower out-of-control ARL (faster detection) than the existing charts for small-to-moderate mean shifts while maintaining specified in-control ARL settings (e.g., 200, 371, 500). An illustrative charting example with 50 subgroups (30 IC, 20 OOC) shows earlier and more frequent OOC signals for the proposed structures than for T0/T1.","Existing regression-type estimator for the charting statistic: $M_r=\bar{y}+b(\mu_x-\bar{x})$ with $b=r_{xy}(s_y/s_x)$. The proposed ratio–product exponential estimator used for Mrep is $\hat{\bar{Y}}_{S,RP}=\bar{y}\left[\alpha\exp\left(\frac{\bar{X}-\bar{x}}{\bar{X}+\bar{x}}\right)+(1-\alpha)\exp\left(\frac{\bar{x}-\bar{X}}{\bar{x}+\bar{X}}\right)\right]$ with $\alpha_{opt}=\tfrac12+\rho_{xy}C_y/C_x$. The Mrwp statistic is $\hat{\bar{Y}}^{*}_P=\left(\hat{\bar{Y}}_{S,RP}+w_1(\bar{X}-\bar{x})+w_2\bar{y}\right)\exp\left(\frac{\bar{X}-\bar{x}}{\bar{X}+\bar{x}}\right)$ with given $w_1^{opt},w_2^{opt}$; Shewhart-style limits are described via probability limits and the usual 3-sigma form $\mathrm{LCL}=\bar{M}_r-3\sigma_{M_r}$, $\mathrm{CL}=\bar{M}_r$, $\mathrm{UCL}=\bar{M}_r+3\sigma_{M_r}$.","Across simulated settings (bivariate normal; n = 5, 10, 15; correlations ρxy = 0.30, 0.60, 0.90; in-control ARL targets 200, 371, 500), the ARL curves indicate that the proposed T2 (Mrep) and especially T3 (Mrwp) detect mean shifts faster (smaller out-of-control ARL) than baseline T0 and existing T1 (Mr). In the illustrative example with 50 subgroups (shift after subgroup 30), T2 and T3 signaled immediately after the shift (first signal at subgroup 31), whereas the usual structure’s first signal occurred later (reported at subgroup 43). For one illustrated case (Figure 1(i)), T0 and T1 detected 5 and 16 out-of-control signals, while T2 and T3 detected 17 and 20 out-of-control signals, respectively, after the shift point. The paper also reports additional performance measures (EQL, RARL, PCI) and states T3 attains the smallest EQL among T0–T3 in their evaluations.",None stated.,"The methods and performance claims rely heavily on the assumption of bivariate normality and (implicitly) independence of samples; robustness to non-normality, heavy tails, and autocorrelation is not analyzed. Control-limit calibration under repetitive sampling and with estimated parameters is not fully developed theoretically (e.g., exact in-control false-alarm properties/ARL under the proposed estimators), and results are largely simulation-based. Real industrial/medical case studies are suggested conceptually, but the demonstrated “illustrative example” appears to be generated data rather than an external real dataset, limiting practical validation.","The authors suggest extending the work to other distributions/settings (they mention Bayesian and multivariate distributions) and broadening scope by incorporating more auxiliary variables. They also propose adding run rules, investigating varying sampling strategies, and extending the proposed structures to memory-type schemes such as EWMA and CUSUM frameworks.","Develop exact or numerically accurate in-control run-length/false-alarm calibration (e.g., via Markov-chain/integral-equation approaches) for the proposed repetitive-sampling charts to ensure nominal ARL0 under parameter estimation. Study robustness to autocorrelation and non-normality (skew/heavy tails) and provide robust or nonparametric variants using auxiliary information. Provide open-source implementations (e.g., an R package) and validate on multiple real industrial/healthcare datasets with guidance on selecting design parameters (e.g., α, weights, repetitive-sampling limits) in practice.",2510.05086v1,local_papers/arxiv/2510.05086v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:16:08Z
TRUE,Nonparametric|Image-based monitoring|Other,Other,Phase II,Manufacturing (general)|Other,NA,FALSE,NA,Simulation study,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),In-control (Phase I/reference) set size should be at least m0 = 200 parts to achieve nominal type I error (α=0.05) and geometric-like in-control RL behavior; smaller m0 (≤150) inflates ARL/SDRL.,TRUE,R|Python,Supplementary material (Journal/Publisher),https://drive.google.com/drive/folders/1yz9N3H_M7iiEAk2uMXwYWD4qmdg7qk-1,"The paper proposes a new Statistical Process Control (SPC) framework for monitoring 3D lattice structures in additive manufacturing using Topological Data Analysis (TDA), specifically persistent homology (PH) features in dimensions 0 and 1. It defines a nonparametric, permutation-test-based monitoring scheme that compares a new part’s persistence diagram to a Phase I reference set and signals when the within-reference dispersion statistic is unusually small under label permutations. Distances between persistence diagrams are discussed (Wasserstein/bottleneck via TDA and a faster duration-based distance via TDAstats), and the proposed SPC implementation uses PH computed by TDA’s alphaComplexDiag with distances computed using TDAstats::phom.dist for speed. Extensive Monte Carlo run-length studies on simulated lattice parts (cubes, layered cubes, 4-cube lattice, and a complex “egg” lattice) report in-control ARL/SDRL behavior and out-of-control detection speed for defects such as layer shifts, collapsed edges, and missing struts. The method is compared to spectral SPC based on Laplace–Beltrami spectra, showing strong performance for moderate-to-large defect severities (often near-immediate detection) while spectral methods can be better for very small collapse defects under some conditions.","Distances between persistence diagrams include the p-Wasserstein-type metric $d_p(X,Y)=\left(\inf_{\phi:X\to Y}\sum_{x\in X}\|x-\phi(x)\|_p^p\right)^{1/p}$ and bottleneck distance $d_\infty(X,Y)=\inf_{\phi:X\to Y}\sup_{x\in X}\|x-\phi(x)\|_\infty$. The implemented distance is the TDAstats duration-based metric: compute feature lifetimes (death−birth), pad the shorter vector with zeros, sort, then $d(X,Y)=\sum_{i=1}^{n_Y}|x_{(i)}-y_{(i)}|$. The permutation-test statistic for monitoring is the within-reference total pairwise distance $d_{\text{in-group}}=\sum_{1\le i<j\le m_0} d(X_i,X_j)$; the null distribution is formed by the $m_0+1$ label permutations that assign each part once as the singleton test group.","In-control RL simulations (10,000 replications, α=0.05) show ARL/SDRL approach the nominal geometric values (~20 and 19.49) as the Phase I size increases, and the authors recommend m0 ≥ 200; with m0=100, in-control ARLs are ~25 for multiple parts. For cube-type defects with severity δ=0.05 and m0=200, 1D PH monitoring detects almost immediately (e.g., ARL ≈ 1.002 for a collapsed-edge cube; ARL ≈ 1.247 for a shifted-layer cube), while 0D features are much slower (ARL ≈ 15–16). Joint monitoring of 0D and 1D approximately halves in-control ARL (≈11) versus single-feature charts, consistent with near-independence. For the complex “egg” part, collapse defects parameterized by kσ show decreasing ARL with increasing severity; for example at k=1/5, ARL is ≈16.545 (1D) and ≈8.668 (combined), while for large severities detection is essentially immediate (ARL=1). In comparisons without skeletonization (5,000 replications), TDA-based monitoring is often faster than spectral SPC for moderate-to-large severities, though spectral can be better for the smallest collapse severity tested (k=1/5) in that setup.","The authors note that Wasserstein/bottleneck distances require solving an assignment problem and are computationally expensive, motivating use of the faster but less faithful duration-based distance (phom.dist), which can underestimate differences when birth/death locations differ but durations match. They also mention a potential bug/limitation in TDA’s alphaComplexDiag: it computes all (n−1) lower-dimensional features regardless of the specified maxdimension parameter, though they state this is not a serious concern given speed. They acknowledge that only 0D and 1D features are monitored (not 2D), largely due to practical considerations.","The Phase II procedure uses a fixed reference set and repeatedly re-permutes with each new part; computational cost may still be substantial for real-time deployment on high-throughput lines, especially if PH must be computed on dense meshes/point clouds without downsampling/skeletonization. The method’s in-control RL theory relies on independence of sequential tests/new parts; in additive manufacturing, temporal correlation (machine drift, thermal effects) could invalidate this and alter false-alarm behavior. The choice of distance (duration-only) may reduce sensitivity to certain defect types that change birth/death geometry rather than lifetimes, and there is limited guidance on tuning α or adapting for multiple-feature monitoring beyond observing ARL inflation/deflation. Results are predominantly simulation-based; generalizability to real scanned data with complex noise, occlusions, and measurement artifacts (and to different lattice families/materials) remains uncertain without more real-case validation.","They propose validating the Phase II scheme on a sequence of real-world additively manufactured lattice parts to demonstrate practical applicability and robustness. They suggest extending monitoring to 2-dimensional topological features to capture a broader class of defects. They also propose extending the method from mesh/point-cloud data to voxel data from industrial tomography, analogous to prior LB-spectrum work for voxels.","Developing a more accurate yet computationally tractable persistence-diagram distance (or kernel/embedding) tailored for online SPC could improve sensitivity while maintaining speed. Incorporating dependence/streaming adaptations (e.g., block permutation, time-series-aware false-alarm control, or self-starting updates of the reference set) would better match real AM processes with drift. Creating diagnostic tools to localize which regions/struts contribute to a topological alarm (not just signal/no-signal) would increase actionability for practitioners. Providing a packaged, reproducible software implementation (e.g., an R/Python package) and benchmarking on shared real AM datasets would aid adoption and fair comparison across methods.",2510.11740v1,local_papers/arxiv/2510.11740v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:16:59Z
TRUE,Univariate|Nonparametric|Other,Shewhart|Change-point|Other,Phase I,Manufacturing (general)|Theoretical/simulation only,FALSE,FALSE,NA,Simulation study|Markov chain|Case study (real dataset),Detection probability|False alarm rate|Other,Not discussed (examples and simulations use Phase I sample sizes n=50 and n=100; application uses n=40 sample means).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Phase I distribution-free control charts for individual observations to monitor an unknown target/location parameter without specifying the underlying in-control distribution. Observations are thresholded into a Bernoulli sequence via $X_i=\mathbf{1}(Y_i\ge c)$, and monitoring is performed using runs- and patterns-type statistics, illustrated with (i) the number of success runs $R_n$ (R-1 chart) and (ii) the scan statistic $S_n(r)$ (R-2 chart). Control limits are set to achieve a prescribed in-control signal probability by using exact conditional null distributions given the number of successes $N_1$, leveraging random permutation/conditioning arguments. These conditional distributions are computed exactly using the finite Markov chain imbedding (FMCI) technique, and randomized tests are used when discreteness prevents exact attainment of the nominal level. Simulation studies across normal, exponential, and heavy-tailed $t(3)$ distributions show the proposed charts control the in-control signal probability and have competitive out-of-control detection compared with Phase I nonparametric competitors (MW, KS, ELR), with particularly strong performance for heavy-tailed cases; an application to piston-ring data demonstrates detection and localization of instability near the end of the series.","Data are binarized by $X_i=\mathbf{1}(Y_i\ge c)$ with baseline proportion $p_0=\Pr(Y\ge c)$ and $N_1=\sum_{i=1}^n X_i$. For the runs chart, the exact conditional distribution is $\Pr(R_n=r\mid N_1=n_1)=\xi_0\left(\prod_{t=1}^{n_2} M_t^{(1)}\right)e_r$, where the nonhomogeneous transition matrices $M_t^{(1)}$ come from sequential zero-insertion in a random permutation with $n_1$ ones and $n_2=n-n_1$ zeros. For the scan chart $S_n(r)=\max_{1\le t\le n-r+1}\sum_{i=t}^{t+r-1}X_i$, the conditional tail probability is computed via FMCI using a pattern-waiting-time duality: $\Pr(S_n(r)<s\mid N_1=n_1)=\xi_0\left(\prod_{t=1}^{n} N_t^{(2)}\right)\mathbf{1}$, where $N_t^{(2)}$ is the transient submatrix of the imbedded Markov chain associated with avoiding the compound pattern set $\Lambda_{r,s}$.","Simulations target an in-control signal probability of 0.005 and show achieved IC signal probabilities close to nominal across distributions; for example (Table 2) with $n=50$: $\Pr(\text{signal})$ ranges roughly 0.0039–0.0057 across charts/statistics and distributions, and similarly for $n=100$ about 0.0042–0.0059. Out-of-control performance is reported as OC signal probability (“power”) under a change-point mean-shift model with multiple change locations and shift sizes; proposed charts are competitive with MW/KS/ELR in many settings and tend to be stronger under heavy-tailed $t(3)$ while weaker under Exp(1). The paper provides recommended baseline proportions $p_0$ by scenario and scan window size (Table 1), generally favoring moderate $p_0$ (e.g., 0.2–0.7) and smaller $p_0$ for right-skewed distributions. In the piston-ring case study (n=40 sample means), at $\alpha=0.05$ the charts signal with observed values $R_{40}=4$, $S_{40}(6)=5$, and $S_{40}(10)=7$, localizing the likely instability region around indices 31–40 (e.g., window {31,…,40} and run {37,…,40}).","The authors note that because runs- and patterns-type statistics are discrete, controlling the in-control signal probability at an arbitrary prescribed level may not always be feasible; they therefore use a randomized test to attain the desired level. They also indicate that the choice of threshold (baseline proportion $p_0$) materially affects performance and that extreme $p_0$ can make the binary sequence too sparse/dense, reducing resolution and discriminative power.","The approach requires selecting a threshold $c$ (or $p_0$) and (for scan charts) a window size $r$, and practical guidance is heuristic and scenario-dependent; mis-specification can substantially reduce power. The FMCI-based exact computations may become computationally heavy as $n$, $n_1$, and the pattern set size grow (state space can expand quickly for scan statistics), but computational scaling and implementation details are not documented here. The method assumes i.i.d. observations in Phase I; robustness to serial dependence, clustering not well captured by a single threshold, or heteroscedasticity is not analyzed.","The authors state that the framework is flexible and can be extended to other run/pattern statistics beyond $R_n$ and $S_n(r)$, allowing practitioners to tailor charts to detect other shift types (e.g., variance changes) and to construct corresponding distribution-free Phase I charts. They also note the one-sided formulation can be extended to two-sided charts with both lower and upper limits to detect both downward and upward shifts.","Develop scalable software and complexity-reduction techniques (e.g., state aggregation, dynamic programming, or approximations) for larger n and richer pattern sets to facilitate routine use. Extend the methodology to handle autocorrelated Phase I data (e.g., via permutation conditionality under dependence models, block permutations, or model-based residual charts) and to irregular/missing sampling. Provide data-driven selection procedures for $(p_0,r)$ (and for general patterns) that optimize power under classes of alternatives while controlling the conditional false-alarm probability.",2511.13672v1,local_papers/arxiv/2511.13672v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:17:43Z
FALSE,Other,Other,NA,Theoretical/simulation only|Other,NA,NA,NA,Simulation study|Other,Other,Not discussed,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/immortal5655/ChartAnchor,"This paper introduces ChartAnchor, a benchmark for chart grounding that evaluates multimodal large language models (MLLMs) on bidirectional alignment between chart images and structured semantics. The dataset contains 8,068 chart–table–code triples spanning 30 chart types, sourced from real-world web data plus augmented instances from existing chart datasets, and rendered using multiple plotting libraries (e.g., matplotlib and plotly). ChartAnchor defines two tasks: chart-to-code generation (produce executable Python plotting code to recreate a chart) and controlled chart-to-table reconstruction (recover tabular data given predefined headers), enabling cross-validation of structural and numerical fidelity. The evaluation framework combines functional validity (execution/parsing pass rate), visual-structure consistency (text, color, type, layout), semantic data fidelity via tuple-based matching with tolerance levels, and perceptual similarity using CLIPScore. Experiments across 14 MLLMs reveal persistent weaknesses in precise numerical recovery and color fidelity even when code executes and visual similarity is high.","Key components include a tuple-based semantic fidelity representation and matching scheme. For chart-to-code, generated figures are parsed and converted to type-specific normalized tuples (e.g., line: $(n,x_i,y_i)$; candlestick: $(n,x,low,high,open,close)$), forming a set $L_T=\{\tau_1,\ldots,\tau_n\}$. Tuple fields are compared using edit distance $J$ for strings and relative error $e$ for numerics under tolerance levels (strict/slight/high). Performance includes $P_{tol}=|M|/|Pred|$, $R_{tol}=|M|/|GT|$, $F1_{tol}=2PR/(P+R)$, and $IoU_{tol}=|M|/(|Pred|+|GT|-|M|)$ where $|M|$ is the number of matched tuples.","ChartAnchor contains 8,068 validated samples across 30 chart types; average image resolution is 3,346×2,266 px, and mean code length is 627.67 tokens (Table 1). On chart-to-code, GPT-4o achieves the best reported overall score (63.02) with pass rate 91.88 and CLIPScore 86.88, while Claude-3-7-Sonnet attains a higher CLIPScore (94.11) but lower overall (61.18) (Table 4). On controlled chart-to-table, Claude-3-7-Sonnet has the highest F1 under high tolerance (38.62) among proprietary models, while Qwen2.5-VL-32B-Instruct reaches the highest reported F1-high overall (41.04) with pass rate 97.83 (Table 5). The paper highlights large gaps between strict and relaxed tolerances, indicating poor exact numerical precision despite reasonable structural outputs.","The authors note that ChartAnchor focuses on static chart understanding and does not cover interactive or dynamic visualizations (e.g., drill-down interactions, animated transitions, multi-view dashboards). They state that such charts add complexity through temporal structure, interaction states, and evolving visual contexts that are not evaluated in the current benchmark.","Because ChartAnchor is a benchmark for chart/code/table grounding rather than process monitoring, it does not assess SPC-specific requirements such as stable in-control modeling, ARL/false-alarm control, or sensitivity to small mean/variance shifts. The evaluation emphasizes executability/visual similarity and tuple-matching tolerances, which may not capture domain-critical correctness for industrial decision-making when small numeric deviations are consequential. Reported results also depend on the chosen parsing/extraction rules for each chart type and the tolerance thresholds, which can affect fairness and comparability across models and chart families.",The authors propose extending ChartAnchor to incorporate interactive and time-varying visualizations. They suggest designing benchmarks that simulate interaction flows and temporal data changes to evaluate model capabilities under dynamic semantics and multi-state rendering.,"A useful extension would be to incorporate robustness tests for real-world degradations (compression artifacts, low resolution, occlusions) and to evaluate calibration/uncertainty for recovered numerical values and code parameters. It would also be valuable to broaden library coverage and enforce stronger semantic equivalence checks (e.g., verifying recovered tables against downstream analytical queries) to reduce the gap between perceptual similarity and data correctness. Finally, providing standardized, open-source evaluation tooling for all chart types (parsers/extractors and metrics) would improve reproducibility and reduce metric-implementation variance.",2512.01017v2,local_papers/arxiv/2512.01017v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:18:25Z
TRUE,Univariate|Multivariate|Nonparametric|High-dimensional|Other,Shewhart|Machine learning-based|Other,Both,Manufacturing (general)|Theoretical/simulation only|Other,FALSE,NA,NA,Simulation study|Other,False alarm rate|Detection probability|Other,"Not discussed (only describes collecting an initial in-control calibration dataset of size n, without numeric guidance).",TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/christopherburger/ConformalSPC,"The paper proposes a hybrid Statistical Process Control (SPC) framework that integrates Conformal Prediction (CP) to obtain distribution-free, model-agnostic monitoring guarantees under exchangeability rather than normality. It introduces (i) Conformal-Enhanced Control Charts that replace parametric control limits with a conformal threshold derived from calibration nonconformity scores, and optionally plot conformal prediction intervals over time to visualize uncertainty and trigger “uncertainty spike” signals. It also proposes (ii) Conformal-Enhanced Process Monitoring that reframes multivariate SPC as online anomaly detection and monitors a single conformal p-value over time via a p-value chart with a control limit at significance level α. Nonconformity scores are defined using robust center measures (e.g., median-based absolute deviations) or model-based residuals (possibly normalized by an estimated local scale), allowing use with arbitrary predictive/anomaly-detection models (e.g., SVMs, autoencoders). The paper provides simulated chart comparisons for normal and exponential data and argues improved robustness and sensitivity relative to traditional Shewhart-style charts while maintaining interpretability.","For a calibration set $X_1,\dots,X_n$, define a nonconformity score such as $s(X_i)=|X_i-\mathrm{median}(X_1,\dots,X_n)|$ (or subgroup/variability analogs). Choose a false-alarm level $\alpha$ and set the conformal control limit $q$ as the $(1-\alpha)$ quantile of calibration scores, specifically the $\lceil (1-\alpha)(n+1)\rceil$-th smallest score. For model-based monitoring, use $s(x,y)=|y-\hat y(x)|$ (or normalized $s(x,y)=|y-\hat y(x)|/\hat\sigma(x)$), yielding conformal intervals $[\hat y(x_{new})-q,\hat y(x_{new})+q]$; signal if $s(X_{new})>q$, and in the multivariate setting monitor conformal p-values against a horizontal limit at $\alpha$.","The paper presents simulation-based visual comparisons of traditional Shewhart charts versus conformal-enhanced charts for normally distributed data and for exponential (non-normal) data with an induced mean shift, reporting that the conformal-enhanced chart flags shifts with greater sensitivity in these examples. It also proposes an “uncertainty spike” signal when normalized residual-based conformal interval widths increase abruptly, intended as a leading indicator even before the quality characteristic shifts. No numerical ARL/ATS values or tabulated performance comparisons are reported in the provided text.","The authors note that conformal guarantees depend on the quality and representativeness of the initial in-control calibration dataset; if it is not truly reflective of in-control behavior, guarantees are compromised. They also state that the proposed nonconformity scores are simple/robust but largely heuristic, and that producing tight, informative intervals depends on better score design and domain knowledge.","The framework relies on an exchangeability assumption that may be violated in many processes with autocorrelation, drift, seasonality, or feedback control, but the paper does not provide methods (e.g., Mondrian/online CP variants) to address dependence. The multivariate “conformal anomaly detection” performance depends strongly on the chosen unsupervised model and score calibration; without clear guidance, benchmarking, or diagnostics, practitioner results may vary widely. The paper does not provide formal run-length (ARL/ATS) analysis or systematic comparisons to established nonparametric/multivariate SPC methods (e.g., CUSUM/EWMA, rank-based charts, robust T²) under multiple shift types (mean, variance, correlation, mixed).","The authors suggest developing more sophisticated, adaptive nonconformity scores that learn from process data to improve efficiency/tightness of intervals. They also emphasize validating the proposed ideas in real manufacturing settings to demonstrate practical applicability.","Extend the approach to explicitly handle serial dependence (e.g., block/weighted conformal methods, conformalized forecasting residuals, or change-point-aware calibration) and study robustness when exchangeability fails. Provide a full SPC-style performance evaluation (in-control/out-of-control ARL, ATS, steady-state behavior) and compare against modern MSPC baselines across multiple shift scenarios, dimensions, and contamination levels. Develop clear practitioner guidance for selecting/validating anomaly models and nonconformity scores, plus diagnostic tools for root-cause attribution when a p-value chart signals; release a reproducible software implementation (e.g., an R/Python package) with examples.",2512.23602v1,local_papers/arxiv/2512.23602v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:18:59Z
TRUE,Univariate|Nonparametric|Other,EWMA|Other,Phase II,Manufacturing (general)|Healthcare/medical|Network/cybersecurity|Theoretical/simulation only,FALSE,FALSE,NA,Exact distribution theory|Simulation study,Other,Not discussed,TRUE,R,In text/Appendix,NA,"The paper derives exact, time-varying mean and variance expressions for an EWMA statistic used to monitor binomial proportions in multiple independent data streams, avoiding early-phase inaccuracies from asymptotic variance approximations. The method constructs a cumulative binomial count across k streams and time t, standardizes it, and then applies an EWMA recursion to form the CSB-EWMA statistic with adaptive (time-varying) control limits. Closed-form results include $E[r_t]=(1-\lambda)^t r_0$ and an exact finite-sample variance expression based on the covariance structure of the standardized cumulative-count statistics. Theoretical properties are validated via Monte Carlo simulation (10,000 replications) showing close agreement between simulated and theoretical moments and convergence of the variance to 1 as $t\to\infty$. The work positions the chart as distribution-free for binary outcomes derived from a median-based indicator and is motivated by monitoring across parallel streams in SPC settings.","Binary recoding per stream/time uses $x_{ij}=I(y_{ij}>\mu_{0i})$, so at time $j$ the stream-total count is $C_j=\sum_{i=1}^k x_{ij}\sim \mathrm{Bin}(k,p_0)$ (with $p_0=0.5$ under the in-control median assumption). The cumulative count is $Q_t=\sum_{j=1}^t C_j\sim \mathrm{Bin}(kt,p_0)$, and the standardized statistic is $W_t=(Q_t-\mu t)/\sqrt{t\sigma^2}$ with $\mu=kp_0$ and $\sigma^2=kp_0(1-p_0)$. The EWMA recursion is $r_t=\lambda W_t+(1-\lambda)r_{t-1}$, yielding $E[r_t]=(1-\lambda)^t r_0$ and an exact finite-$t$ variance based on $\mathrm{Cov}(W_i,W_j)=\sqrt{\min(i,j)/\max(i,j)}$; adaptive limits are $\mathrm{UCL}_t=(1-\lambda)^t r_0 + L\sqrt{\mathrm{Var}(r_t)}$ and $\mathrm{LCL}_t=(1-\lambda)^t r_0 - L\sqrt{\mathrm{Var}(r_t)}$ (typically $L=3$).","Monte Carlo validation uses $N_{sim}=10{,}000$, $k=10$, $p_0=0.5$, and $\lambda=0.2$ (R 4.3.2). Theoretical vs simulated variances at selected times show close agreement: at $t=10$, 0.6369 vs 0.6385 (0.26% relative bias); at $t=50$, 0.9512 vs 0.9359 (-1.61%); at $t=100$, 0.9768 vs 0.9758 (-0.10%); at $t=500$, 0.9955 vs 0.9967 (0.12%); at $t=1000$, 0.9978 vs 0.9996 (0.19%). The mean is theoretically 0 for the in-control case (with $r_0=0$) and simulated means are near 0 with reported RMS bias $2.89\times 10^{-4}$ and maximum absolute bias $9.00\times 10^{-4}$. The variance converges rapidly to its asymptotic value 1, reaching 99% by $t=227$ (as reported).",None stated,"The work focuses on moment derivations and validation rather than full chart performance (e.g., ARL/ATS under shifts), so practical detection capability relative to competing binomial EWMA/CUSUM charts is not quantified. Assumptions of independence across streams and within-time-point Bernoulli sampling may be unrealistic in many MSP settings (e.g., shared common-cause variation or serial dependence), and no robustness analysis is provided. The “distribution-free” claim relies on median-based dichotomization with in-control $p_0=0.5$; in practice the in-control median (and thus $p_0$) is rarely known exactly, and Phase I estimation effects are not addressed.",None stated,"Extend the results to correlated streams and/or autocorrelated observations (e.g., shared latent factors, ARMA counts) and quantify the impact on covariance and control limits. Develop Phase I procedures to estimate the in-control median (or $p_0$) and propagate estimation uncertainty into control limits and false-alarm properties. Provide full performance studies under out-of-control scenarios (ARL/ATS, steady-state ARL) and comparisons against existing binomial EWMA/CUSUM and GLR-style MSP monitors; package the method in an R package for routine use.",2601.09968v1,local_papers/arxiv/2601.09968v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:19:33Z
TRUE,Univariate|Functional data analysis|Nonparametric|Other,Hotelling T-squared|Other,Phase II,Transportation/logistics|Other,FALSE,NA,TRUE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Detection probability|False alarm rate|Other,"Uses an initial training segment of N0 = 30 PDF samples (e.g., first 30 days) to build the FPCA model, and a tuning segment of m = 30 for the rank-based chart (default m = 30). Control limits are selected to achieve a target in-control ARL (e.g., IC ARL = 500 via Zhou et al. (2009) Table 1).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a warping function-based control-chart framework to detect distributional changes (including complex shape deformations) in damage-sensitive features (DSFs) for structural health monitoring. DSF data are subgrouped and summarized as a sequence of estimated PDFs; changes are monitored via warping functions that map each PDF to a reference distribution, so both location shifts and shape changes appear as changes in the warping functions. Warping functions are transformed (SRSF and tangent-space mapping), reduced by FPCA, and monitored using two features: Hotelling’s T^2 (in the retained principal subspace) and SPE (residual subspace). A rank-based nonparametric change-point-model control chart (based on standardized Mann–Whitney ranks with a weighted moving average and Y-max statistic) provides online signaling, robustness to outliers, and change-point estimation. Simulations show higher detection power than competing PDF-FPCA control charts and a Bayes-space change-point method, and a real case study on cable tension ratio distributions from a cable-stayed bridge demonstrates practical change detection and localization.","Key elements are: (i) warping transformation between PDFs f and g via a warping function \(\gamma\): \(g(x)=f(\gamma(x))\,\dot\gamma(x)\) and equivalently for CDFs \(G(x)=F(\gamma(x))\). (ii) Warping computation by quantiles: \(\gamma(x)=F^{-1}(G(x))\), using a reference CDF built from the mean quantile of training data. (iii) Monitoring features from FPCA on transformed warping functions: Hotelling \(T_k^2=\sum_{i=1}^d \xi_{k,i}^2/\rho_i\) and SPE \(\|v_k-\hat v_k\|^2\). Charting uses a rank-based Y-max statistic built from standardized Mann–Whitney statistics and a weighted moving average recursion.","In Simulation Study I, the proposed method shows much higher empirical detection power than PDF-FPCA-CC and Bayes-CPD for small-to-moderate changes; e.g., Scenario I (N=130, change at 100) at \(\delta=0.10\): 0.60 (proposed) vs 0.02 (PDF-FPCA-CC) vs 0.04 (Bayes-CPD), and at \(\delta=0.20\): 0.99 vs 0.27 vs 0.04. In Scenario II (N=200), at \(\delta=0.07\): 0.83 vs 0.11 vs 0.09, and at \(\delta=0.10\): 0.99 vs 0.16 vs 0.10. In a robustness simulation with injected outlying PDFs, the proposed rank-based charts avoid false alarms and estimate the true change point correctly (\(\hat\tau=200\)), while direct charting with quantile-based limits signals falsely at outlier locations. In the bridge case study (170 daily PDFs per cable-pair, 30-day training), detected change points closely match manually identified ones (reported true change indices: 53, 61, 101, 119; estimates shown near these values).",None stated.,"The method relies on several practitioner-chosen tuning parameters (subgrouping/windowing, kernel density bandwidth choice, PDF preprocessing mixing coefficient \(\epsilon\), FPCA variance threshold, and rank-chart smoothing \(\lambda\)), and performance may be sensitive to these settings across applications. It assumes a stable pre-change training period (first \(N_0\) PDFs) and does not explicitly address serial dependence in the PDF/warping-function sequence, which is common in SHM time series and can affect false-alarm properties. The paper does not provide implementation code/software, which may hinder reproducibility and adoption for complex steps (density estimation on bounded support, warping computation, tangent-space mapping, FPCA, and rank-based limits).",None stated.,"Develop guidance and theory for dependent (autocorrelated) distributional sequences, including control-limit calibration under serial dependence and steady-state performance. Provide software (e.g., an R/Python package) with default parameter selection, diagnostics, and automated subgrouping/bandwidth selection to improve reproducibility and practical uptake. Extend the framework to multivariate DSFs (multiple correlated features) and to adaptive/self-starting settings where the in-control reference distribution may drift slowly over time.",2601.12221v1,local_papers/arxiv/2601.12221v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-28T04:20:11Z
