is_spc_paper,chart_family,chart_statistic,phase,application_domain,assumes_normality,handles_autocorrelation,handles_missing_data,evaluation_type,performance_metrics,sample_size_requirements,code_used,software_platform,code_availability_source,software_urls,summary,key_equations,key_results,limitations_stated,limitations_unstated,future_work_stated,future_work_unstated,id,pdf_path,llm_provider,llm_model,repeat_id,extracted_at
TRUE,Multivariate|Bayesian|Other,EWMA|Other,Both,Finance/economics|Manufacturing (general)|Other,TRUE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Other,Phase I uses a historical dataset of length $N^*$ (examples: $N^*=150$ for LME differenced series; $N^*=180$ for the production process). No general minimum sample-size guidance beyond using “enough data” in Phase I for convergence/stable estimates.,TRUE,None / Not applicable,Not provided,http://www.lme.co.uk,"The paper proposes a new multivariate SPC scheme for autocorrelated/serially correlated vector processes using a Bayesian state-space (multivariate local level) model fit via discount weighted regression (DWR). At each time point it computes a Bayes factor comparing the predictive error density from the fitted model to a specified in-control (target) error density, and charts the log Bayes factor with a univariate modified EWMA designed for correlated observations. The approach is intended to simultaneously monitor shifts in the mean vector, the covariance (dispersion) matrix, or more generally deviations of the entire predictive distribution from a target distribution. Phase I is used to fit/tune the DWR model and identify an AR/ARMA dependence model for the log Bayes factor series to set modified-EWMA limits for a desired in-control ARL; Phase II applies the fixed design online. The method is illustrated with two real examples: (i) London Metal Exchange spot-price data (via first differences to focus on volatility/dispersion) and (ii) a 5-variable production temperature time series, where the chart flags substantial out-of-control behavior in Phase II similar to prior residual/state-space charting work.","The process is modeled with a Bayesian multivariate local level state-space model: $y_t=\mu_t+\varepsilon_t$, $\mu_t=\mu_{t-1}+\omega_t$, with $\varepsilon_t\sim N_p(0,\Sigma)$ and $\omega_t\sim N_p(0,\Omega_t\Sigma)$ using a discount factor $\delta$. The one-step forecast error density is $e_{t+1}\mid \Sigma=S_t,\,\mathbf y_t\sim N_p\!","A theorem proves $S_t\xrightarrow{P}\Sigma$ (and unbiasedness) and $P_t\to P=(\sqrt{\delta^2+4}-\delta)/2$, justifying stable long-run use of the DWR forecast variance in chart construction. A simulation with 1000 bivariate observations illustrates that the in-control distribution of $LBF(t)$ is non-normal but roughly symmetric, motivating EWMA robustness for small $\lambda$ values (e.g., 0.05–0.1). In the LME example, Phase I uses $t=1$–150 and Phase II $t=151$–210; with AR(1) dependence estimated as $\phi=0.1$ and target ARL 370.4, the design uses (for $\lambda=0.05,\phi=0.1$) $c=2.469$, and signals occur at $t=172$ for larger $\lambda$ (0.2 and 0.5) but not for $\lambda=0.05$ or 0.1. In the production example, Phase I uses $t=1$–180 and Phase II $t=181$–276; an AR(1) for $LBF(t)$ is fitted as $LBF(t)=-4.624+0.062LBF(t-1)+\nu_t$ and, after centering, the modified EWMA charts show the process largely out of control in Phase II, broadly agreeing with Pan & Jarrett (2004).","The paper notes that the $LBF(t)$ series is both autocorrelated and non-normal, making chart design (dependence modeling and robustness) nontrivial. It also states that meaningful comparison with standard multivariate charts (e.g., Hotelling $T^2$, MEWMA) is difficult because the proposed method is model-based and comparisons must control for time-series model misspecification; the author suggests such a comparison would require a full separate paper.","The procedure relies heavily on correct specification/adequacy of the DWR local-level model and on specifying an appropriate target distribution (often normal with covariance $V$); deviations may reflect model mismatch rather than true process changes. The univariate reduction via $LBF(t)$ can make diagnosis (which variable/parameter changed) difficult without additional decomposition tools, and the paper does not provide a full diagnostic method. Control-limit design depends on fitting a simple AR/ARMA model for $LBF(t)$ and using tabulated/approximate ARL results; performance under more complex dependence, heavy tails, or structural breaks in $LBF(t)$ is not fully explored. The approach presumes availability of analytic predictive/target densities; extensions to non-Gaussian targets/predictives are suggested but not developed or validated.","The author suggests extending the approach beyond normal predictive/target densities to other analytically available distributions. The paper also notes that other chart types (e.g., modified CUSUM and nonparametric charts) could be applied to the $LBF(t)$ sequence, and highlights the design of charts for an autocorrelated, non-normal Bayes-factor process as an area motivating further research.","Develop formal methods to localize and diagnose whether signals are driven by mean shifts, covariance changes, or higher-order distributional changes (e.g., contributions by variable or by eigen-directions). Study robustness and calibration when Phase I parameter estimation uncertainty is substantial, including small $N^*$ and contaminated Phase I data, and provide principled Phase I cleaning procedures. Extend to missing/irregularly sampled multivariate time series and to high-dimensional settings where covariance estimation is unstable. Provide open-source software and systematic benchmarks against residual charts, $T^2$/MEWMA/MCUSUM under matched time-series models, including steady-state ARL and detection-delay evaluations.",0802.0218v1,local_papers/arxiv/0802.0218v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:40:58Z
TRUE,Univariate|Other,Shewhart|Other,Both,Food/agriculture,TRUE,TRUE,FALSE,Simulation study|Other,Other,Not discussed.,TRUE,Other,Not provided,NA,"The paper proposes an integration framework between Automated/Engineering Process Control (CAP/APC) and Statistical Process Control (MSP/SPC) for a continuous flour-gluten blending process. The incoming raw-flour protein content is modeled as an AR(1) process and measurement error is added; SPC is applied via a residual individuals & moving range chart (X, RM) to check normality/independence assumptions and to detect assignable causes at the process input. On the APC side, the authors compare the feedback adjustment rule from Fearn & Maris (1990) with a newly proposed first-order control-rule family parameterized by two tuning constants (\(\lambda_1,\lambda_2\)). Using simulation (30 replications), they show the proposed rule improves centering and reduces output variability relative to the literature rule. The work positions the residual control chart as a monitoring layer alongside feedback regulation to jointly stabilize the process and detect when the model/regulator becomes invalid.","Process/output and measurement models: \(Y_i = X_i + u_{i-1}\) (after simplification) and \(Z_i = Y_i + \varepsilon_i\) with \(\varepsilon_i\sim N(0,\sigma_\varepsilon^2)\). Input dynamics: \(X_i-\mu = \phi(X_{i-1}-\mu)+\nu_i\) with \(\nu_i\sim N(0,\sigma_\nu^2)\) (AR(1)). Feedback rules: Fearn & Maris: \(u_i = u_{i-1}-(Z_i-\tau)\); proposed rule: \(u_i-u_0 = \lambda_1(u_{i-1}-u_0)-\lambda_2(Z_i-\tau)\) (with \(\lambda_1=\lambda_2=1\) recovering the Fearn rule). Residuals for SPC at input: \(\hat x_i = X_i-\hat\phi X_{i-1}-\hat C\), monitored with an individuals/moving-range (X, RM) chart.","Simulations were run with parameters \(X_0=\mu=10\), \(u_0=6\), \(\phi=0.7\), \(\tau=16\), \(\sigma_\nu^2=0.5\), \(\sigma_\varepsilon^2=0.5\), using 30 replications per rule. For the Fearn & Maris (1990) rule, the reported output mean and variance are \(E(Z_i)=16.23\) and \(V(Z_i)=1.11\), indicating poorer centering and higher variability. For the proposed rule with \(\lambda_1=\lambda_2=0.5\), the reported \(E(Z_i)=16.01\) and \(V(Z_i)=0.87\), showing improved centering and reduced variability. The residual (X, RM) chart for AR(1) input residuals shows at least one outlier point, interpreted as evidence of assignable causes affecting variability.",None stated.,"The SPC component is limited to a univariate residual individuals/moving-range chart and does not provide explicit control-limit formulas, ARL/false-alarm calibration under estimation error, or guidance on how limits should be adjusted for small Phase I samples. The comparison of control rules is based on a single simulated scenario and a limited performance summary (mean/variance), without standard SPC detection metrics (e.g., ARL/ATS) or robustness checks to model misspecification (non-normality, different \(\phi\), larger measurement error). Practical implementation aspects (e.g., actuator constraints, adjustment costs, sampling/measurement delay) are not modeled, though they can strongly affect APC-SPC integration outcomes.","The authors propose (i) finding the optimal tuning of the proposed control rule, i.e., determining the best coefficients \(\lambda_1\) and \(\lambda_2\), and (ii) validating the integration structure for more general input models beyond AR(1), such as ARMA/ARIMA-type models.","Extend the monitoring layer to explicit joint SPC/APC performance criteria using run-length metrics (ARL/ATS) and steady-state behavior under feedback, including the effect of estimating AR(1) parameters in Phase I. Evaluate robustness to autocorrelation/normality violations and incorporate measurement delay, actuator saturation, and economic costs to enable realistic integrated design. Provide implementable software (e.g., R/Python) and a clear procedure for setting control limits and tuning parameters in practice, potentially including adaptive/self-starting variants.",0804.4325v1,local_papers/arxiv/0804.4325v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:41:26Z
TRUE,Univariate|Nonparametric,Shewhart|Other,Phase II,Manufacturing (general)|Theoretical/simulation only,FALSE,FALSE,NA,Exact distribution theory|Approximation methods|Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|Detection probability|False alarm rate|Conditional expected delay|Other,"Not discussed (uses Phase I reference sample of size m and Phase II test sample size n; examples/tables use m from 50 up to 2000 and n in {5,10,25}).",TRUE,Other,Personal website,http://www.win.tue.nl/~markvdw,"The paper proposes a Phase II, two-sided Shewhart-type nonparametric (distribution-free) control chart for monitoring process location using the Mann–Whitney (Wilcoxon rank-sum) statistic computed between each new test sample of size n and a fixed Phase I reference sample of size m. Because repeated use of the same reference sample induces dependence across successive charting statistics, the authors derive the unconditional in-control ARL by conditioning on the reference sample, showing the in-control run-length behavior is distribution-free for continuous distributions (can be reduced to U[0,1]). Control limits for a target ARL0 are obtained via efficient computation/approximation of the conditional signal probability using probability generating functions (exact) and tail approximations, especially the Lugannani–Rice saddlepoint approximation; Monte Carlo integration is used to approximate ARL and conditional-ARL percentiles. Simulation comparisons indicate substantially better in-control performance than the Shewhart X̄ chart with estimated parameters (notably higher 5th percentile of the conditional in-control ARL distribution for small/moderate m), while out-of-control performance is nearly as good under normality and better under heavy-tailed (Laplace) or skewed (Gamma) distributions. A piston-ring manufacturing dataset illustrates the chart and demonstrates practical implementation with computed limits and signaling behavior.","For reference sample $X_1,\ldots,X_m$ and test sample $Y_1,\ldots,Y_n$, the charting statistic is the Mann–Whitney count $M_{XY}=\sum_{i=1}^m\sum_{j=1}^n I(X_i<Y_j)$, taking values in $[0,mn]$. A two-sided chart signals on sample $h$ if $M^{(h)}_{XY}<L_{mn}$ or $M^{(h)}_{XY}>U_{mn}$ with symmetry-motivated choice $L_{mn}=mn-U_{mn}$. The unconditional ARL is derived by conditioning on the reference sample: $\mathrm{ARL}=E\{1/p_G(X)\}$ where $p_G(x)=P_G(M_{xY}<mn-U_{mn})+P_G(M_{xY}>U_{mn})$; in control $F=G$ implies distribution-free ARL0 via transformation to $U[0,1]$. Conditional tail probabilities $P(M_{xY}>U_{mn})$ are computed exactly from the pgf $H_2(z)=(\sum_{l=0}^m a_l z^l)^n$ or approximated via the Lugannani–Rice saddlepoint formula.","Tables of control limits are provided for selected $(m,n)$ achieving target $\mathrm{ARL}_0\in\{370,500\}$ (e.g., for $m=100,n=5,\mathrm{ARL}_0=500$: $L_{mn}=65, U_{mn}=435$). In a piston-ring example with $m=125,n=5$ and target $\mathrm{ARL}_0=400$, computed limits are $U_{mn}=540$ and $L_{mn}=85$, flagging test groups 12–14 as out of control. In-control simulations (Normal(0,1), $n=5$, target $\mathrm{ARL}_0=500$) show the MW chart has markedly larger 5th percentile of conditional in-control ARL than the Shewhart X̄ chart for small/moderate m (e.g., at $m=50$: MW 5th perc 97 vs Shewhart 49; at $m=150$: 251 vs 154), indicating fewer short false-alarm runs when parameters are estimated. Out-of-control comparisons show MW is nearly as effective as Shewhart X̄ under normal shifts but more effective for Laplace (heavy-tailed) and Gamma(2,2) (skewed) shift alternatives, both in unconditional ARL and in the 95th percentile of conditional out-of-control ARL.",None stated.,"The procedure assumes independent test samples over time and independence between Phase I reference data and Phase II samples; performance under autocorrelation or drift is not addressed. Distribution-free guarantees rely on continuity (no ties); practical handling of ties (e.g., discrete measurements/rounding) is not developed. Control-limit computation depends on Monte Carlo plus saddlepoint/Edgeworth approximations and can still be time-consuming for some $(m,n)$; implementation is provided in Mathematica/website rather than as a widely used open-source package, which may limit adoption/reproducibility.","The authors propose future work on (i) a detailed treatment of nonparametric individuals charts (n=1), and (ii) monitoring dispersion in a location-scale setting using a nonparametric test for scale alongside the location chart. They also note alternative chart design criteria based on conditional ARL percentiles or probabilities of short in-control runs, which their software supports and could be further explored.","Extending the MW chart to explicitly handle serial dependence (e.g., ARMA errors) or streaming data with overlapping subgroups would broaden applicability. Robust tie-handling methods (midranks, randomized tests) and evaluation under discretization/rounding would strengthen practical guidance. Additional benchmarking against modern distribution-free CUSUM/EWMA-type rank charts, adaptive control limits, and self-starting variants could clarify tradeoffs for small-shift detection and reduce reliance on large Phase I samples; providing an R/Python implementation would improve accessibility and reproducibility.",0805.2292v1,local_papers/arxiv/0805.2292v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:42:04Z
TRUE,Multivariate|High-dimensional|Other,Shewhart|CUSUM|EWMA|Hotelling T-squared|MEWMA|MCUSUM|Other,Both,Manufacturing (general)|Other,TRUE,NA,NA,Case study (real dataset)|Other,False alarm rate|Other,Not discussed,NA,None / Not applicable,Not applicable (No code used),NA,"This paper is a short overview of Multivariate Statistical Process Control (MSPC) with an emphasis on the interpretation problem: identifying which variable(s) drive an out-of-control signal after a multivariate chart alarms. It reviews core multivariate charting approaches, including Hotelling’s $T^2$ (Mahalanobis-distance) Shewhart-type charts for the mean vector, as well as MCUSUM and MEWMA schemes for improved sensitivity to small/moderate shifts. It also discusses monitoring multivariate dispersion via generalized variance (determinant of the sample covariance matrix) and the trace of the covariance matrix. For diagnosis/interpretation, it summarizes methods such as Bonferroni/simultaneous intervals, $T^2$ decompositions, and PCA/PLS-based approaches (including contribution plots) and other visualization techniques. A real industrial chemical-process example (three variables) illustrates Phase I checks (correlation and multivariate normality), Phase II $T^2$ monitoring, and graphical diagnostic techniques that identify the third variable as responsible for an alarm.","Shewhart-type MSPC for the mean is based on the squared Mahalanobis distance/Hotelling’s statistic, e.g., $T_i^2 = n(\bar{\mathbf X}_i-\boldsymbol\mu)^\top\boldsymbol\Sigma^{-1}(\bar{\mathbf X}_i-\boldsymbol\mu)$ when $\boldsymbol\mu,\boldsymbol\Sigma$ are known, or $T_i^2 = n(\bar{\mathbf X}_i-\bar{\mathbf X})^\top \mathbf S^{-1}(\bar{\mathbf X}_i-\bar{\mathbf X})$ using pooled estimates. Under independence and multivariate normality, $T_i^2$ is referenced to a $\chi^2_p$ distribution (known parameters) or a scaled $F$ distribution (estimated parameters) to set probability control limits. For monitoring multivariate dispersion, summary statistics such as $|\mathbf S_i|$ (generalized variance) or $\mathrm{tr}(\mathbf S_i)$ (sum of variances) are suggested.","In the three-variable chemical-process case study, Phase I analysis indicates strong pairwise correlations (reported as $r(X_1,X_2)=0.5929$, $r(X_1,X_3)=0.7287$, $r(X_2,X_3)=0.8170$), motivating a multivariate chart. A Phase I $T^2$ chart shows the process in control through time point 100, and Phase II monitoring signals an out-of-control condition at time point 101 (with the plotted $T^2$ exceeding the shown UCL=14.16). Two graphical diagnostic procedures (cited to Maravelakis et al. and Maravelakis & Bersimis) both indicate the third variable as the main contributor to the out-of-control signal at point 101. No ARL/ATS comparisons or run-length tables are reported, consistent with the paper’s overview-and-applications focus.","As discussed in the paper, a key practical limitation of traditional multivariate Shewhart, MCUSUM, and MEWMA schemes is interpretability: when an overall multivariate statistic signals, identifying the responsible variable(s) is not straightforward. The paper also notes that classical multivariate methods can be impractical for high-dimensional systems with collinearity, motivating projection methods like PCA/PLS. It further notes that principal components may lack clear physical interpretation, which can hinder diagnosing assignable causes from PCA-based monitoring.","Because the article is primarily an overview with one illustrative application, it does not provide systematic performance evaluation (e.g., ARL/ATS/SDRL) across shift sizes, correlation structures, and dimensionalities, limiting evidence for method selection. The Phase I “in-control” assumption underlying PCA/PLS model building and parameter estimation is treated as given; robustness to Phase I contamination/outliers and to estimation error is not analyzed. Autocorrelation and dynamic process behavior (common in process industries) are not explicitly modeled, which can inflate false alarms if present. Implementation details for the diagnostic graphics (algorithmic steps and reproducible computation) are not fully specified, which may hinder practitioners trying to replicate the example.","The paper highlights several areas for further research: robust design of multivariate control charts, development of nonparametric multivariate control charts, and further work on multivariate attributes charts. It also emphasizes that interpreting an out-of-control signal in MSPC remains an open problem needing additional investigation.","Useful extensions would include comprehensive comparative studies reporting ARL/ATS (including steady-state and conditional measures) for competing MSPC charts and diagnostic methods under realistic estimation (Phase I) uncertainty. Developing methods that jointly address high-dimensionality and interpretability—e.g., sparse/regularized $T^2$/MEWMA/MCUSUM or graphical diagnostics with variable-selection guarantees—would improve practicality in modern sensor-rich environments. Explicit treatment of autocorrelated or dynamic multivariate processes (e.g., state-space/VAR modeling combined with residual-based MSPC) would broaden applicability. Finally, providing open-source implementations (e.g., R/Python) for the interpretation/visualization procedures would facilitate adoption and reproducibility.",0901.2880v1,local_papers/arxiv/0901.2880v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:42:25Z
TRUE,Univariate|Nonparametric,CUSUM,Phase II,Manufacturing (general)|Theoretical/simulation only|Other,FALSE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length),"Phase I (in-control) data are required; simulations fix Phase I size at m = 1000 and note results are “quite satisfactory” around m ≈ 100 (performance improves with larger m). In the aluminum smeltering example, the first 50 observations are used as Phase I data.",TRUE,R,Not provided,http://www.world-aluminium.org/About+Aluminium/Production/Smelting/index.html,"The paper proposes a distribution-free Phase II univariate CUSUM scheme for detecting upward shifts in location when both in-control and out-of-control distributions are unknown. Instead of a single fixed control limit, it uses a sequence of control limits indexed by the “sprint length” $T_n$ (time since the CUSUM last reset to zero), choosing limits from the conditional distribution of $C_n\mid T_n=j$. These conditional distributions (and hence limits) are estimated from Phase I in-control data using a smoothed bootstrap (kernel density estimate with cross-validated bandwidth), followed by an iterative calibration step to match a nominal in-control ARL (e.g., ARL0 = 200). Simulation studies show the conventional normal-theory CUSUM can have severely distorted in-control ARL under skewed/heavy-tailed distributions, while the proposed bootstrap CUSUM maintains ARL0 near nominal and provides competitive detection delays; it also outperforms a classical rank-based nonparametric CUSUM in many scenarios. A real-data illustration on aluminum smeltering chemistry variables (after pre-whitening autocorrelation via AR models) demonstrates time-varying limits and flags shifts at specific time points.","The CUSUM is $C_0=0$ and $C_n=\max(C_{n-1}+X_n-k,0)$, signaling when $C_n$ exceeds a control limit. The sprint length is $T_n=0$ if $C_n=0$, otherwise $T_n=j$ if $C_n,\ldots,C_{n-j+1}\neq 0$ and $C_{n-j}=0$; limits are set as a sequence $\{h_j:1\le j\le j_{\max}\}$ plus $h^*$ for $T_n>j_{\max}$, signaling if $C_n>h_{T_n}$ (or $C_n>h^*$ when $T_n>j_{\max}$). Preliminary bootstrap limits use a tail probability level $\alpha_b=(p_b/(2\,ARL_0))^{-1}$ (with $p_b$ the proportion of Phase I data exceeding $k$), and final limits are calibrated via iterative simulation and linear interpolation so the estimated in-control run length matches $ARL_0$.","When $F$ is skewed right/left (cases II/III) with nominal $ARL_0=200$, the classical normal-theory CUSUM’s actual in-control ARL can be far from nominal (e.g., about 669.7 in case II and 119.8 in case III), whereas the proposed method keeps in-control ARL close to 200 (e.g., about 207.1 in case II and 194.8 in case III in a representative setting). In broader simulations with Phase I size $m=1000$ and $ARL_0=200$, bootstrap CUSUM variants (with $j_{\max}\ge 30$) typically achieve in-control ARLs within about one standard error of 200 across normal and skewed cases, with reasonable out-of-control ARLs for shifts $\delta=0.5$ and $1$. The rank-based nonparametric CUSUM configurations (NP1/NP2) often show poor calibration (too low or too high in-control ARL depending on skew) and comparatively large detection delays for larger shifts. In the aluminum smeltering example (after AR pre-whitening), the bootstrap CUSUM signals potential shifts at the 61st, 83rd, and 62nd time points for the three residual series.","The authors note the method is not tailored to any specific out-of-control distribution $G$, so it can be less sensitive than a correctly specified conventional CUSUM when $F$ and $G$ are normal with known parameters. Performance depends on how well the Phase I-based estimate $\hat F$ approximates $F$ (they use kernel density estimation for simplicity). The approach requires moderate Phase I sample size and “considerable computation” to set up (though monitoring is routine once limits are computed).","The procedure assumes independent observations in Phase II; while they pre-whiten in the application, the method itself does not provide a principled treatment of residual dependence or model misspecification in the pre-whitening step. The chart is designed primarily for upward location shifts; if variance/shape changes occur, detection behavior and interpretability may be unclear (and diagnosis after a signal is not built in). Guidance for selecting tuning parameters ($j_{\max}$ and how to set $E[T_n]$ / $k$) remains largely empirical, and performance can be unstable for small $j_{\max}$.","They call for research-based guidelines for choosing $j_{\max}$ and $k$ (via $E[T_n]$), since the dependence of $(C_n,T_n)$ on these parameters is not well understood. They suggest studying detection of shifts in variance or other aspects of $F$ beyond the mean, and developing post-signal methods to diagnose whether changes are in mean, variance, or other features. They also mention extending the approach to multidimensional (multivariate) settings and studying combined procedures (e.g., pairing with Shewhart charts) theoretically and numerically.","Develop a self-starting/online version that updates $\hat F$ (or uses a rolling window) while controlling false alarm rates under drift, which would reduce reliance on a fixed Phase I dataset. Provide robustness studies to bandwidth selection and alternative nonparametric density estimators (e.g., log-concave, mixture models) and quantify their effect on ARL calibration. Extend to dependent data directly (e.g., block bootstrap, model-based bootstrap, or state-space residual monitoring) to avoid separate pre-whitening and potential model error. Release reference software (R package) with default parameter-selection heuristics and computational accelerations to improve practical adoption.",0906.1421v1,local_papers/arxiv/0906.1421v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:43:02Z
TRUE,Univariate|Other,Other|Change-point,Phase II,Finance/economics|Theoretical/simulation only,NA,TRUE,FALSE,Exact distribution theory|Simulation study|Other,ARL (Average Run Length)|Other,"Monitoring uses a fixed time horizon $T$ and starts signaling after $k=\lfloor T\kappa\rfloor$ observations (simulation examples use $T=250$, with $k=50$ for $S_T$ and $k=75$ for the t-type chart). Bandwidth is chosen relative to $T$ via $T/h_T\to \zeta$; no general Phase I sample-size guidance is given beyond choosing $k$ and $T$.",TRUE,None / Not applicable,Not provided,NA,"The paper develops sequential (kernel-)weighted Dickey–Fuller processes for monitoring a time series to detect a change from a unit-root (random walk) behavior to stationarity, framing the procedure explicitly as a “weighted Dickey–Fuller control chart.” The proposed charting statistic is a sequentially updated, kernel-weighted version of the DF statistic, yielding stopping times that signal when evidence for stationarity becomes strong. Under weak conditions allowing dependence and conditional heteroskedasticity in innovations, the author proves functional CLTs for the DF process under the unit-root null and under local-to-unity alternatives (with Ornstein–Uhlenbeck limits). For dependent innovations, the limit law depends on a nuisance parameter (long-run variance ratio), and the paper provides two remedies: (i) estimate the nuisance parameter (Newey–West) and use corresponding estimated asymptotic control limits, or (ii) apply a Phillips-type transformation to obtain nuisance-parameter-invariant limits. Finite-sample simulations (ARMA(1,1) errors) compare estimated-limit vs transformed charts and also t-type versions, showing the estimated-limit approach is generally more accurate in the reported settings.","The main charting statistic is the sequential kernel-weighted DF process
\[D_T(s)=\frac{\lfloor Ts\rfloor^{-1}\sum_{t=1}^{\lfloor Ts\rfloor} Y_{t-1}\Delta Y_t\,K\big((\lfloor Ts\rfloor-t)/h\big)}{\lfloor Ts\rfloor^{-2}\sum_{t=1}^{\lfloor Ts\rfloor} Y_{t-1}^2},\quad s\in[0,1],\]
with stopping time \(S_T=\inf\{k\le t\le T: D_T(t/T)<c\}\). Under the random-walk null, \(D_T(\cdot)\Rightarrow D_\vartheta(\cdot)\) where the limit depends on the nuisance parameter \(\vartheta=\eta/\sigma\) (long-run SD over innovation SD). A transformed statistic
\[E_T(s)=D_T(s)+\frac{\hat\sigma^2_{\lfloor Ts\rfloor}-\hat\eta^2_{\lfloor Ts\rfloor}}{2\lfloor Ts\rfloor}\,\frac{\sum_{t=1}^{\lfloor Ts\rfloor}K((\lfloor Ts\rfloor-t)/h)}{\lfloor Ts\rfloor^{-2}\sum_{t=1}^{\lfloor Ts\rfloor}Y_{t-1}^2}\]
has a nuisance-invariant limit (the \(\vartheta=1\) limit).","Asymptotically, the stopping rule satisfies \(S_T/T\Rightarrow \inf\{s\in[\kappa,1]:D_\vartheta(s)<c\}\) under the unit-root null, with \(D_\vartheta\) depending on \(\vartheta\) unless innovations are uncorrelated (\(\vartheta=1\)). With Newey–West estimation (lag truncation \(m=o(T^{1/2})\)), the estimated-control-limit chart is proved consistent for size control: \(\mathbb P(S_T\le T)\to\alpha\). The Phillips-type transformation yields a chart whose asymptotic null distribution is invariant to \(\vartheta\), enabling fixed control limits. Simulation study (ARMA(1,1), $T=250$, Gaussian kernel $h=25$, $\alpha=5\%$) reports empirical rejection rates and delay summaries (ARL/CARL); the DF chart with estimated control limits performs well overall, while the t-type version is reported as disappointing in these settings, and the estimated-limit approach is more accurate than the transformed approach in their experiments.",None stated.,"The work is largely asymptotic; practical implementation requires obtaining control limits from nonstandard limit processes (often via simulation), and the paper does not provide software or ready-to-use tables for practitioners. The monitoring design fixes a finite horizon $T$ and start time $k=\lfloor T\kappa\rfloor$, which may be less natural for open-ended industrial monitoring and makes performance sensitive to choosing $k$, $h$, and kernel form. Although dependence/heteroskedasticity is allowed via invariance principles and a nuisance parameter, the finite-sample robustness of Newey–West estimation and bandwidth/lag-truncation choices is only explored in a limited ARMA setting. The procedures target unit-root to stationary changes; they do not directly address other common SPC shifts (mean/variance changes) or provide diagnostics for the nature/timing of a detected change beyond the stopping time.",None stated.,"Developing practical calibration tools (tables or software) for control limits for common kernels and $\zeta$ values would improve usability. Extending the framework to open-ended monitoring (no fixed $T$), to multivariate/unit-root cointegration settings, or to settings with structural breaks and regime switching would broaden applicability. More systematic finite-sample guidance on selecting $h$, $k$, and Newey–West lag truncation (including data-driven selection rules) and robustness checks under heavy tails would strengthen practitioner adoption. Real-data case studies in econometrics/finance (or other streaming domains) would help validate performance beyond simulations.",1001.1833v1,local_papers/arxiv/1001.1833v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:43:31Z
TRUE,Univariate|Nonparametric|Image-based monitoring|Other,Shewhart|Other,Phase II,Manufacturing (general)|Finance/economics|Theoretical/simulation only|Other,FALSE,TRUE,NA,Exact distribution theory|Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),"Uses a moving-window buffer of length M and assumes historical in-control pre-run data are available to fill the initial buffer (Z_{-1},…,Z_{-M}). Also discusses choosing (M,k) to achieve target in-control ARL (e.g., ARL0≈370, 435, or 840); no fixed Phase I subgroup count is prescribed.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a modified binary (attribute) control chart for detecting small permanent mean shifts in a process by thresholding observations into Bernoulli indicators and counting the number of recent “successes” in a moving window of length M. Unlike the classical nonparametric Np-chart using non-overlapping samples, the proposed chart updates at every time point and signals when the moving-window count J_n crosses binomial-based control limits, improving sensitivity to very small shifts. The method is distribution-free under symmetry of the in-control error distribution (allowing heavy tails such as Cauchy) and is extended theoretically to dependent data via a martingale difference array assumption. A functional central limit theorem is derived for the centered/scaled moving-window process, yielding Brownian-motion limits and explaining why using fewer (recent) observations can improve detection right after a small change-point under local alternatives. Extensive Monte Carlo simulations (30,000 runs) show the chart can outperform several established schemes (CUSUM, optimal EWMA, Shewhart-EWMA, GEWMA, GLR) in out-of-control ARL for very small shifts (≈0.1–0.25σ), but is slower for larger shifts, motivating parallel use with other charts.","Observations follow a change-point model $Y_n=Y+m\,\mathbf{1}(n-q)+\varepsilon_n$ with symmetric in-control error cdf $F$; define binary indicators $Z_n=\mathbf{1}(Y_n\ge 0)$ (or sign-thresholding). The moving-window statistic is $J_n=\sum_{i=n-M}^{n-1} Z_i$, which is compared to binomial-style limits $\mathrm{UCL}=Mp_0+k\sqrt{Mp_0(1-p_0)}$ and $\mathrm{LCL}=Mp_0-k\sqrt{Mp_0(1-p_0)}$ (with $p_0=1/2$ under symmetry). For asymptotics, $J_N(t)=N^{-1/2}\sum_{i=\lfloor Nt\rfloor-M}^{\lfloor Nt\rfloor-1}(Z_i-p_0)$ converges to $\eta_0\{B(t)-B(t-M(t))\}$, leading to a stopping-time representation for signaling.","For in-control ARL tuned near 435 with Gaussian errors, the chart with (M=150, k=1.8) achieves out-of-control ARL ≈243.5 for a 0.1σ shift and ≈97.6 for a 0.25σ shift, with corresponding run-length SD ≈172.6 and ≈58.7. The paper reports competing charts from Han & Tsung (2004) have ARLs ≈295–324 (for 0.1σ) and ≈105–110 (for 0.25σ), so the proposed chart is faster for very small shifts. For larger shifts (>0.5σ), the proposed binary chart is substantially slower than CUSUM/EWMA-type methods. Robustness simulations show advantages persist under Laplace errors (e.g., M=40,k=2.22: ARL≈191.4 at 0.1, ARL≈59.5 at 0.25) and it remains workable under Cauchy errors (M=28,k=2.28: ARL≈334.8 at 0.1, ARL≈167.3 at 0.25).","The authors note the modified binary chart is designed for (and mainly advantageous for) very small mean shifts; when the jump is larger than about 0.5σ it is “much slower” than alternatives, so they recommend running several charts simultaneously to cover different shift sizes. They also point out that because $J_n$ is integer-valued, it may be impossible to tune the in-control ARL to match a desired target exactly (only approximately).","The approach relies on a fixed threshold at 0 (effectively using sign information), which can discard magnitude information and may reduce efficiency when distributional assumptions are partly known or when shifts affect variance/shape rather than location. The method assumes in-control symmetry (implying $p_0=1/2$); in practice, asymmetry, drift, or mis-centering would bias $p_0$ and can inflate false alarms unless additional centering/estimation is done. Although a martingale-difference framework allows certain dependence structures, many autocorrelated processes violate the martingale-difference property (e.g., ARMA with nonzero conditional mean), so applicability to general autocorrelation is not universal. Phase I estimation/robust calibration of the threshold and limits under unknown $p_0$ (beyond assuming symmetry) is not fully developed, and no implementation/software is provided to reproduce tuning results.",None stated.,"Develop self-starting or Phase I/Phase II integrated procedures that estimate/track $p_0$ (or the threshold/median) online under symmetry departures and provide guaranteed in-control ARL under estimation error. Extend the moving-window binary idea to multivariate or image-wide monitoring (e.g., combining multiple pixel streams with spatial aggregation) with diagnostic tools to localize changes. Provide systematic design rules (or optimization) for choosing M and k under specified shift distributions and dependence models, including steady-state ARL/delay analyses. Release reproducible software (R/Python) and benchmark studies against modern nonparametric and robust change-detection methods under broader dependence and contamination scenarios.",1001.1841v1,local_papers/arxiv/1001.1841v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:44:10Z
TRUE,Univariate|Other,Other|Change-point,Phase II,Finance/economics|Manufacturing (general)|Theoretical/simulation only|Other,NA,TRUE,FALSE,Exact distribution theory|Simulation study,ARL (Average Run Length)|False alarm rate|Other,"Not discussed (monitoring starts after a fraction of data: $k=\lfloor T\kappa\rfloor$ with $\kappa\in(0,1)$; residuals require at least $p+1$ observations and calculations start at $g_T$ with $p+1\le g_T<\lfloor T\kappa\rfloor$).",TRUE,None / Not applicable,Not provided,NA,"The paper develops a sequential monitoring (control-chart) procedure to detect when regression errors in a polynomial trend model change from a random-walk (unit-root, integrated) behavior to stationarity. At each time point it recomputes least-squares residuals using all data up to that time (“sequentially updated residuals”) and applies a kernel-weighted variance-ratio statistic related to the KPSS/Breitung unit-root testing framework. The authors derive functional central limit theorems for the residual process and for the induced variance-ratio monitoring process, leading to an asymptotic distribution for the stopping time (control chart signal time). Under the null (random-walk errors) the limiting law is nuisance-parameter free (depends on kernel and bandwidth ratio but not on long-run variance), enabling control limits chosen to achieve a specified asymptotic type-I error. They also provide asymptotic results under a change-point model where the errors become stationary after an unknown fraction of the sample, and they study finite-sample behavior via Monte Carlo simulation.","Observations follow $Y_t=\beta_0+\beta_1 t+\cdots+\beta_p t^p+\varepsilon_t$ with monitoring aimed at testing $H_0:\rho_t\equiv 1$ (random-walk errors) vs change to $|\rho|<1$. Sequentially updated residuals $\hat\varepsilon_j(t)$ are recomputed at each time $t$ from least squares on $(Y_1,\dots,Y_t)$. The charting statistic is the kernel-weighted variance ratio
$$U_t=\frac{t^{-4}\sum_{i=1}^t\Big(\sum_{j=1}^i \hat\varepsilon_j(t)\Big)^2 K((i-t)/h)}{t^{-2}\sum_{j=1}^t \hat\varepsilon_j^2(t)},$$
and the control chart stopping time is $R_T=\inf\{k\le t\le T:U_t\le c_R\}$ (finite horizon $T$; monitoring starts at $k=\lfloor T\kappa\rfloor$).","The paper derives FCLTs for (i) the sequentially updated residual process and (ii) the induced kernel-weighted variance-ratio process $V_T(s)$, and proves $R_T/T\Rightarrow R$ where $R=\inf\{s\in[\kappa,1]:V(s)\le c_R\}$. A key result is that the null limit of $V_T$ (and thus of $R_T$) is asymptotically distribution-free with respect to nuisance dependence/scale parameters (it depends on the kernel $K$, the limit $\zeta=T/h_T$, and Brownian motion, but not on $\eta$). In simulations with $T=500$ using a Gaussian kernel and $h\in\{25,50\}$, empirical no-change signal probabilities were about 0.03–0.10 depending on correlation settings. For an early change-point (e.g., $q=25$), empirical rejection rates were high (roughly 0.44–0.97 depending on parameters), while late changes were much harder to detect.","The paper notes that monitoring is analyzed with a finite time horizon $T$ (though extension to infinite horizon is said to be straightforward) and that late changes are difficult to detect in finite samples (e.g., with $T=500$), which is a general limitation for such problems.","The method relies on repeatedly recomputing full least-squares residuals at each time point, which can be computationally heavy for large $T$ unless optimized, and no implementation guidance/software is provided. Although weak dependence and FCLTs are allowed, practical calibration still depends on selecting kernel/bandwidth and on asymptotics; finite-sample performance and robustness to strong autocorrelation/long-memory or conditional heteroskedasticity beyond the assumed conditions may be uneven. The work is focused on univariate error monitoring in polynomial regression; it does not address multivariate regressions, missing/irregular sampling, or diagnostic tools to localize the change-point once a signal occurs.",None stated (beyond noting that extending results from a finite time horizon to an infinite monitoring horizon is straightforward and briefly discussed).,"Provide open-source software (e.g., R/Python) for computing sequentially updated residual charts efficiently, including practical control-limit calibration via simulation of the limiting process. Study robustness and calibration under long-memory, stronger serial dependence, and heteroskedastic/volatility models, and develop self-starting or adaptive bandwidth versions. Extend the framework to multivariate regression settings and offer post-signal diagnostics/estimators for the change-point time and for the post-change stationary model.",1001.1845v1,local_papers/arxiv/1001.1845v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:44:37Z
TRUE,Univariate|Other,EWMA|Other,Phase II,Network/cybersecurity,TRUE,TRUE,NA,Simulation study|Case study (real dataset)|Other,Other,"Not discussed (window size guidance only: model fitting/prediction uses a moving window of past m observations; prediction performance reported as robust for m ≥ 10, and in many scenarios works even with m < 10).",TRUE,None / Not applicable,Not provided,NA,"The paper develops a network-wide probabilistic model for computer traffic that links unobserved link loads to observed links via the routing equation and a Gaussian flow model, enabling kriging-style prediction of unmeasured link traffic. It introduces a network-specific mean model for flow traffic using a low-rank PCA factor structure learned offline from expensive NetFlow measurements, and a mean–variance power-law model (with exponent γ≈3/4) to parameterize flow variances (diagonal ΣX). Parameters are estimated online from observed links using an iterated generalized least squares (iGLS) scheme, yielding plug-in kriging predictors and estimated prediction-error variances. For anomaly detection (when all links are observed), the paper proposes using a modified EWMA control chart on prediction residuals and derives an adjusted EWMA variance formula for long-range dependent (fractional Gaussian noise) data to reduce false alarms. Empirical results on Internet2 traffic and simulations show the network-specific model substantially improves prediction accuracy over ordinary kriging in well-posed scenarios, and the LRD-adjusted EWMA chart better controls false positives compared with the standard i.i.d.-based EWMA limits.","Traffic routing: $Y(t)=AX(t)$ with $X(t)\sim N(\mu_X(t),\Sigma_X(t))$ implies $Y(t)\sim N(A\mu_X(t),A\Sigma_X(t)A^t)$. Network-specific model: $\mu_X(t)=F\beta(t)$ (PCA factors) and $\Sigma_X=\sigma^2\,\mathrm{diag}(|F\beta|^{2\gamma})$, leading to $Y(t)=AF\beta+\sigma A\mathrm{diag}(|F\beta|^{\gamma})Z(t)$ with $Z\sim N(0,I)$. Anomaly detection uses an EWMA on residuals, with an LRD-adjusted EWMA variance for fGn given by Eq. (5.3) (numerical integration form).","On Internet2 data (12 scenarios, 5 days), the network-specific model (with $p=2$ PCA factors) reduces relative prediction error versus ordinary kriging in scenarios where observed links share sufficient flows with the target link (scenarios 1–9), with reported average improvement of 0.1887 ReMSE points (18.87%) and improvements ranging from about 0.8% to 78%. In hard scenarios with fewer shared flows (scenarios 10–12), neither method performs well; ordinary kriging can outperform the network-specific model in some cases, but both have large ReMSEs. The learned PCA mean structure $F$ is shown to be robust over time: a matrix learned from one day’s NetFlow data (Feb 19, 2009) remains effective for predictions on other days up to about a month later. For detection, the standard EWMA (i.i.d.-based limits) produces many false positives on long-range dependent traffic, while the LRD-adjusted EWMA limits make anomaly onset more discernible in the provided examples.","The authors note that direct flow-level measurements are expensive (NetFlow mapping is computationally expensive and impractical to run repeatedly online), so their approach treats NetFlow-derived flow estimates as auxiliary offline data to learn the model. They also state that the study of heavy-traffic/congested scenarios (where correlations between forward/reverse flows may become non-negligible) is outside the scope of the work, motivating their diagonal (uncorrelated) flow covariance assumption. They acknowledge that prediction accuracy has inherent limits in scenarios with insufficient shared flows due to network routing, which neither ordinary kriging nor their model can overcome.","The anomaly detection component is only a small part of the paper and does not provide standard SPC performance characteristics (e.g., in-control ARL calibration, out-of-control detection delay) for the proposed LRD-adjusted EWMA limits. The Gaussian modeling assumptions and the diagonal flow-covariance structure may be fragile under real network events (congestion, routing changes, diurnal shifts) and could affect both prediction intervals and control limits. The control-chart design choices (e.g., EWMA smoothing, thresholds) are not presented as an optimized design with false-alarm guarantees under dependence, so practical deployment would require additional calibration/validation.","The authors indicate that their model must be updated if structural changes in the network occur (e.g., changes in traffic structure or routing), implying future work around model updating/maintenance. They also point out that studying heavy-traffic (congested) regimes beyond the operating characteristics of the network is interesting but outside the scope of the paper.","A natural extension is to formally design and calibrate the LRD-adjusted EWMA chart in SPC terms (set control limits to achieve a target in-control ARL/false-alarm rate under long-range dependence, and quantify detection delays for specified shifts). Another direction is to relax the Gaussian/diagonal-covariance assumptions by incorporating correlated flows, heavy tails, or robust/nonparametric residual monitoring to improve reliability during congestion or anomalies. Providing open-source software and a reproducible benchmark (Internet2 scenarios, NetFlow mapping pipeline, and charting code) would also materially improve practical adoption and allow broader comparison with alternative change-point/anomaly detection methods used in network monitoring.",1005.4641v1,local_papers/arxiv/1005.4641v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:45:11Z
TRUE,Univariate,Shewhart|Other,Phase II,Theoretical/simulation only,TRUE,FALSE,FALSE,Exact distribution theory,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|Detection probability|False alarm rate|Other,Not discussed,NA,None / Not applicable,Not applicable (No code used),NA,"The paper proposes a modified runs-rules scheme for Shewhart-type \(\bar{X}\) control charts, called the modified \(r/m\) (denoted \(M\text{–}r/m\)) chart, aimed at improving sensitivity to small and moderate mean shifts. The method signals when \(r\) points fall beyond the UCL (or below the LCL), allowing up to \(m-r\) intervening points that must lie between the center line and the corresponding control limit, making it more sensitive than the classical \(r/m\) rule. Assuming i.i.d. normal observations with known \(\sigma=1\), the authors compute exact run length distributions and provide exact ARL and run-length SD values for many \(r,m\) combinations with control limits calibrated to match a specified in-control ARL (notably 370.4). Results show that for mean shifts from 0 to 2.6\(\sigma\), modified \(M\text{–}r/5\) schemes (\(r=2,3,4\)) provide the best overall ARL performance among those studied, while for larger shifts the standard Shewhart (1/1) can be slightly better. The paper also reports run-length percentiles for the best-performing modified schemes and compares them with a Shewhart chart using Western Electric rules, showing comparable ARL performance at a matched in-control ARL level.","Standardized charting observations are assumed i.i.d. \(X_t\sim N(\mu,1)\) with center line at 0 and symmetric limits \(\pm h\) (UCL/LCL), where \(h\) is chosen to achieve a target in-control ARL. In the classical \(r/m\) rule, a signal occurs if in the last \(m\) points at least \(r\) are above UCL (or at least \(r\) below LCL). In the proposed modified \(M\text{–}r/m\) rule, a signal occurs if \(r\) points exceed UCL (or LCL) with at most \(m-r\) intervening points, and any intervening points must lie between the center line and that same limit (i.e., in \((0,\mathrm{UCL})\) for an upper-side signal, or \((\mathrm{LCL},0)\) for a lower-side signal). Run length distribution/ARL values are then computed exactly (via state-based probability calculations) for given \(r,m,h\).","All schemes are calibrated to have in-control ARL 370.4 (matching the standard 3-sigma Shewhart chart), yielding different control limits (e.g., 1/1 uses \(\pm3\), while examples include \(M\text{–}2/5\) with \(\pm1.91\) and \(M\text{–}4/5\) with \(\pm0.949\)). For mean shifts in \([0,2.6]\sigma\), the modified \(M\text{–}r/5\) schemes (\(r=2,3,4\)) achieve the lowest ARLs overall among the considered schemes; for example at shift 1.0, ARL is 18.26 (\(M\text{–}2/5\)) and 15.46 (\(M\text{–}3/5\)) vs 43.90 for standard 1/1. The modified schemes uniformly improve upon their corresponding classical \(r/m\) schemes for each shift (as stated by the authors and reflected in Table 1). The paper reports run-length percentiles; e.g., for \(M\text{–}2/5\) in-control (shift 0), the 5th/50th/95th percentiles are 21/257/1105, highlighting strong right-skew. At matched in-control ARL 94.75, the proposed \(M\text{–}r/5\) charts have ARLs very close to the Western Electric rules chart (C1234) across small-to-moderate shifts (Table 5).",None stated,"The development assumes i.i.d. normal observations with known and constant \(\sigma\) (set to 1), so robustness to non-normality, parameter estimation error (Phase I uncertainty), and autocorrelation is not addressed. The work is largely tabular and design-oriented; it does not provide a general algorithm/software for practitioners to compute limits/ARLs for arbitrary \(r,m\) or to handle real-world complications (e.g., rational subgrouping, varying subgroup sizes). The method focuses on mean shifts only; performance for variance shifts, drifts, or multivariate settings is not studied. Practical guidance on selecting \(r,m\) beyond the explored grids (and beyond ARL-based criteria) is limited, and no real data case study is provided.",None stated,"Extend the modified \(M\text{–}r/m\) idea to settings with unknown parameters (Phase I/Phase II with estimation) and provide adjusted limits that maintain nominal in-control ARL under estimation error. Study robustness to autocorrelation and non-normality (e.g., heavy tails) and develop distribution-free or robust variants. Generalize to multivariate mean monitoring (e.g., combine with Hotelling \(T^2\)/MEWMA) or to joint mean-variance monitoring. Provide open-source software to compute control limits, exact/approximate ARL/percentiles, and to aid practitioners in choosing \((r,m)\) under design constraints (e.g., ATS, economic criteria).",1007.3225v1,local_papers/arxiv/1007.3225v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:45:49Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Other,FALSE,FALSE,NA,Case study (real dataset),Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes using Statistical Process Control (SPC) control charts to monitor software reliability based on inter-failure time data modeled by an NHPP with an exponential mean value function (Goel–Okumoto form). It develops a Modified Maximum Likelihood Estimation (MMLE) approach that replaces iterative MLE solving with analytical/approximate closed-form estimators for the NHPP parameters a and b. Using the fitted mean value function m(t), the authors construct a mean value (inter-failure) control chart by plotting successive differences of m(t) against failure number and applying probability-based control limits (via chosen tail probabilities) mapped back to m(t) and then to time/control thresholds. A published dataset (Xie et al., 2002) is analyzed, and the chart flags out-of-control signals at the 10th and 25th failures (points falling below the LCL), while other points remain within limits. The contribution is primarily an SPC-based monitoring scheme for software failure processes coupled with an MMLE parameter-estimation shortcut intended to simplify implementation relative to iterative MLE while yielding similar signaling behavior.","The NHPP exponential (Goel–Okumoto) mean value function is $m(t)=a(1-e^{-bt})$ with intensity $\lambda(t)=\frac{dm(t)}{dt}=b(a-m(t))=abe^{-bt}$. The likelihood for failure times $s_1,\ldots,s_n$ is $L=\exp(-m(s_n))\prod_{k=1}^n \lambda(s_k)$, leading to likelihood equations for MLEs of $a,b$; the paper then approximates terms like $\frac{e^{-bs_k}}{1-e^{-bs_k}}\approx m_k b + c$ to obtain an analytical MMLE for $b$ and then $\hat a=\frac{n}{1-e^{-\hat b s_n}}$. Control limits are set by equating $F(t)=1-e^{-bt}$ (or equivalently the pdf/probability levels used in the paper: 0.00135, 0.5, 0.99865) and converting to corresponding $m(t)$ limits to form UCL/CL/LCL on the mean-value chart.","For the Xie et al. (2002) inter-failure dataset (n=30), the reported MMLE parameter estimates are $\hat a=33.396342$ and $\hat b=0.003962$. Using probability levels 0.00135, 0.5, and 0.99865, the paper reports mean-value chart limits (converted to $m(t)$) approximately LCL $\approx 0.0451$, CL $\approx 16.6982$, and UCL $\approx 33.3513$ (as shown near Fig. 1). The mean-value (inter-failure) control chart indicates out-of-control signals at the 10th and 25th failures where the plotted successive differences of $m(t)$ fall below the LCL. All remaining plotted points are within the control limits and are interpreted as stable/in-control.",None stated.,"The control-limit construction is nonstandard for NHPP mean-value monitoring (using fixed probability levels like 0.00135/0.99865 without calibrating in-control false-alarm properties such as ARL), so the false alarm rate and detection performance are not quantified. The approach assumes the exponential NHPP (Goel–Okumoto) form is correctly specified and does not assess robustness to model misspecification, overdispersion, or autocorrelated/clustered failures common in software defect discovery. Only a single real dataset is used and there is no comparative study against established software reliability monitoring charts or change-detection methods, limiting generalizability.",None stated.,"Calibrate control limits to achieve a target in-control ARL (or ATS) for NHPP-based monitoring and report out-of-control detection delays for various shift scenarios. Extend the method beyond the exponential NHPP to other common software reliability growth models (e.g., S-shaped, Weibull) and evaluate robustness under misspecification. Provide simulation studies and benchmark comparisons versus CUSUM/EWMA or likelihood-ratio/change-point methods tailored to NHPP processes, and release implementable software (e.g., an R/Python package) for practitioners.",1111.1826v1,local_papers/arxiv/1111.1826v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:46:19Z
TRUE,Univariate|Profile monitoring|Nonparametric|Other,Shewhart|CUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Other,Both,Healthcare/medical|Finance/economics|Theoretical/simulation only|Other,NA,FALSE,NA,Exact distribution theory|Simulation study|Markov chain|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|ATS (Average Time to Signal)|ANOS (Average Number of Observations to Signal)|Detection probability|False alarm rate|Expected detection delay|Conditional expected delay|Other,"Uses Phase I samples of past in-control observations; simulations consider n = 50, 100, 500, 1000, 10000 (and mention n = 1000 still has substantial estimation impact). No general minimum sample size rule is given.",TRUE,R,Not provided,NA,"The paper develops a general bootstrap-based adjustment for control charts whose in-control distribution/parameters are estimated from Phase I data, addressing the resulting degradation in conditional in-control performance (e.g., false alarm probability and conditional ARL). Rather than targeting unconditional (average-over-estimation) performance, it constructs one-sided bootstrap confidence bounds for a chart-performance functional q(P; \hat{\xi}) (e.g., ARL, hit probability, or the threshold c needed to attain a target ARL/hit rate), thereby guaranteeing with probability 1-\alpha that the conditional in-control performance meets the desired target given the estimated in-control state. The approach applies broadly to Shewhart and CUSUM charts, including likelihood-ratio CUSUMs, and extends to risk-adjusted/regression-based and survival-model-based monitoring; nonparametric bootstrap variants provide robustness to model misspecification. Large-sample validity is established via an extended functional delta-method/Hadamard differentiability theorem showing asymptotically correct coverage for the constructed bounds. Simulation studies (implemented in R) using Markov chain approximations for CUSUM run-length properties show the method prevents overly short conditional in-control ARLs (reducing false alarms) with only modest increases in out-of-control delay, and illustrate that nonparametric bootstrap can outperform parametric bootstrap under misspecification.","For a mean-shift CUSUM with estimated in-control mean, the chart recursion is $S_t=\max\{0, S_{t-1}+X_t-\hat\mu-\Delta/2\}$ (scaled form: $S_t=\max\{0, S_{t-1}+(X_t-\mu-\Delta/2)/\sigma\}$). A likelihood-ratio CUSUM is $S_t=\max\{0, S_{t-1}+\log(f_1(X_t,\theta)/f_0(X_t,\theta))\}$. The proposed guarantee uses a bootstrap one-sided bound: with $p^*_\alpha$ the $(1-\alpha)$ empirical quantile of $q(\hat P^*;\hat\xi^*)-q(\hat P;\hat\xi^*)$, the adjusted design/estimate is $q(\hat P;\hat\xi)-p^*_\alpha$, yielding an approximate $(1-\alpha)$ lower bound for $q(P;\hat\xi)$ (or upper bound depending on q).","For normal-data CUSUM simulations (B=1000 bootstrap reps; 1000 Monte Carlo replications) nominal 90% one-sided intervals achieved coverage near 0.89–0.93 for transformed criteria and larger n, while untransformed ARL could be conservative/poor for small n (e.g., ARL coverage 1.000 for n=50, 0.929 for n=500 in the parametric case). When calibrating thresholds to guarantee (with 90% probability) an in-control ARL of at least 100 using the log(cARL) adjustment, the adjusted thresholds eliminated many cases with conditional in-control ARL far below 100 that occur with unadjusted (and unconditional-bias-calibrated) thresholds. The increase in out-of-control ARL from the adjustment was reported as modest in the illustrated examples. Under model misspecification (e.g., Exponential or scaled chi-square updates with variance 1), the parametric-bootstrap adjustment assuming normality failed to attain the desired in-control behavior, whereas the nonparametric-bootstrap adjustment performed well especially for larger Phase I samples (e.g., n=500).","The authors note that the paper does not fully verify some technical conditions needed for the main asymptotic theorem in all settings; specifically, they state that verifying condition (d) (continuity of the limiting distribution of the functional derivative applied to the limit process) is outside the scope of the paper. They also caution that Shewhart charts are sensitive to tail behavior, making nonparametric methods (and simple nonparametric bootstrap) problematic for small samples, motivating a preference for parametric bootstrap in that case. For regression/survival extensions, they present conjectures and illustrative setups rather than complete theoretical proofs in full generality.","The method requires repeatedly computing q(·) (e.g., ARL or threshold) inside a bootstrap loop; for many charts this needs heavy simulation/Markov-chain/integral-equation approximations, so computational cost and numerical error can materially affect the estimated quantile $p^*_\alpha$ and thus the guarantee. The guarantee is only approximate in finite samples and depends on the adequacy of the bootstrap scheme (parametric correctness or nonparametric representativeness), which can be weak when Phase I data are small, contaminated, autocorrelated, or nonstationary—issues not treated in depth. The approach focuses on ensuring in-control conditional performance and does not provide a systematic framework for diagnosing or estimating the out-of-control distribution/shift size (e.g., \Delta) uncertainty, which influences CUSUM design and effectiveness. Comparisons are largely internal (adjusted vs unadjusted) and do not benchmark against alternative parameter-estimation-aware chart designs (e.g., self-starting CUSUMs) in a comprehensive way.","They suggest the approach should extend to many other control charts and settings beyond those illustrated, including regression-model variants, autocorrelated data, and multivariate monitoring, and they conjecture the theorem’s conditions will hold for many commonly used charts. They also mention that the bootstrap machinery can be adapted to perform bias adjustments (though they do not strongly advocate it) in addition to guaranteed-performance adjustments. They state the methods are implemented in a flexible R package intended to be made available on CRAN.","Developing efficient, reproducible implementations (with error control) for computing $q(P;\hat\xi)$ and its bootstrap distribution—especially for complex charts, high-dimensional settings, and survival models—would improve practical adoption and the reliability of the guarantees. Extending the framework to explicitly handle autocorrelation/streaming dependence (e.g., ARMA residual charts, block bootstrap, dependent wild bootstrap) would address a major real-world SPC gap. Robust Phase I procedures (outlier/contamination handling) integrated with the bootstrap guarantee, and studying the impact of Phase I screening on the conditional guarantees, would make the method more usable in practice. Finally, formal finite-sample calibration studies comparing this guaranteed-conditional approach to self-starting and Bayesian/empirical-Bayes designs across standardized benchmarks would clarify when each method is preferable.",1111.4180v1,local_papers/arxiv/1111.4180v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:47:00Z
TRUE,Univariate|Other,Shewhart|Other,Both,Theoretical/simulation only,FALSE,FALSE,NA,Exact distribution theory,False alarm rate|Other,Not discussed,FALSE,R,Not applicable (No code used),http://www.r-project.org,"This note reviews and clarifies how to compute exact (discrete) control limits for attribute/count-data control charts without relying on historical graphs and tables. It focuses on fiducial (exact) confidence limits for the parameters of the binomial, Poisson, and geometric distributions when the true parameters are unknown, noting their use as control limits for charts such as p, u, c, and g charts. For each distribution, the paper provides closed-form expressions for lower/upper fiducial limits in terms of standard distribution quantiles (F and gamma), obtained by inverting equal-tail probability statements. The authors emphasize that modern statistical software can compute these quantiles directly, eliminating interpolation error and allowing arbitrary confidence levels. The contribution is primarily expository: it consolidates formulas and connects them to practical computation for exact control limits in SPC for discrete data.","For the binomial with observed count x out of n, the equal-tail inversion yields Clopper–Pearson-type limits expressible via F-quantiles: $p_L=\left(1+\frac{n-x+1}{x\,F^{(\alpha/2)}_{2x,2(n-x+1)}}\right)^{-1}$ and $p_U=\left(1+\frac{n-x}{(x+1)\,F^{(1-\alpha/2)}_{2(x+1),2(n-x)}}\right)^{-1}$ (with boundary cases $p_L=0$ if $x=0$, $p_U=1$ if $x=n$). For the Poisson with $Y=\sum_{j=1}^n X_j=y$, Garwood limits are $\lambda_L=\frac{1}{2n}\Gamma^{(\alpha/2)}_{y,2}$ and $\lambda_U=\frac{1}{2n}\Gamma^{(1-\alpha/2)}_{y+1,2}$ (with $\lambda_L=0$ if $y=0$). For the geometric, with $Y=\sum X_j=y$, limits for p are given via F-quantiles as in equations (7)–(8).","No Monte Carlo or empirical performance tables are reported; the paper’s main quantitative content is the exact fiducial-limit formulas themselves. It highlights that exact control limits can be computed at any desired confidence level $1-\alpha$ using standard quantile functions rather than reading/interpolating from published tables/graphs. Boundary-case behaviors are stated explicitly (e.g., binomial $p_L=0$ when $x=0$, Poisson $\lambda_L=0$ when $y=0$, geometric $p_U=1$ when $y=0$).",None stated,"The note does not translate the fiducial limits into full chart design guidance (e.g., how to map a chosen $\alpha$ to in-control ARL for discrete charts or how discreteness affects two-sided false-alarm allocation). It also does not address common SPC complications such as overdispersion, parameter drift, autocorrelation, or pooled/estimated parameters across varying subgroup sizes (e.g., varying n for p-charts). Finally, it provides no comparative study of detection performance versus approximate limits (normal approximations) or alternative exact/conditional methods.",None stated,"Extend the exposition into practical control-chart design by explicitly linking fiducial-limit choices to in-control ARL/false-alarm properties under discreteness, including optimal tail allocation for asymmetric discrete distributions. Evaluate robustness and detection performance (ARL/ATS) of fiducial-limit-based charts versus common approximations under small shifts and under overdispersion (e.g., beta-binomial, negative binomial). Provide implementable software (e.g., an R function/package) that outputs limits for p/u/c/g charts with varying subgroup sizes and optional corrections for discreteness and overdispersion.",1203.3882v1,local_papers/arxiv/1203.3882v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:47:20Z
TRUE,Profile monitoring|Nonparametric|Functional data analysis,Other,Both,Manufacturing (general)|Theoretical/simulation only,FALSE,TRUE,NA,Approximation methods|Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"Not discussed (authors note that when there are insufficient Phase I profiles, corrections may be needed; also that sparse/irregularly spaced profiles may not provide enough Phase I profiles to stably estimate screening thresholds).",TRUE,None / Not applicable,Not provided,NA,"The paper develops Phase I/Phase II profile control charts using a nonparametric location–scale model estimated via robust L-1 (least absolute deviation/median) regression. Phase I estimates a reference profile function μ(x) (conditional median) and a reference deviation function s(x) (conditional MAD) using kernel smoothing with jackknife bias correction, allowing a general stationary dependence structure within each profile. Phase II screens a new profile using three metrics: a standardized vertical (location) deviation D based on profile medians, and two shape-deviation statistics computed from standardized residuals—T^(1) (maximum absolute residual) to detect local distortions and T^(2) (sum of absolute residuals) to detect overall shape deviations. Control limits for D, T^(1), and T^(2) are set empirically (or by bootstrap) from Phase I profiles, with a procedure to choose a per-metric significance level α to control an overall false detection rate α0. The method is illustrated on engineered wood-board vertical density profiles (314 points per profile) and is further evaluated via Monte Carlo simulations under Gaussian and heavy-tailed (t3) correlated errors, showing increasing detection rates as contamination amplitude increases while maintaining near-nominal Type I error.","Phase I model: $Y_{i,j}=\delta_i+\mu(x_{i,j})+s(x_{i,j})e_{i,j}$, with $\delta_i=\mathrm{median}(Y_i)$, $\mu(x)$ the conditional median reference profile, and $s(x)$ the conditional MAD reference deviation. Phase II standardized residuals: $\hat e_l=(Y_l-\delta^*-\tilde\mu_{b_n}(x_l))/\tilde s_{h_n}(x_l)$, with shape statistics $T^{(1)}=\max_l|\hat e_l|$ and $T^{(2)}=\sum_l|\hat e_l|$; location statistic $D=|\delta^*-\hat\mu_\delta|/\hat s_\delta$ where $(\hat\mu_\delta,\hat s_\delta)$ are the Phase I median and MAD of $\delta_i$. Control limits are the empirical/bootstrap $(1-\alpha)$ quantiles of the corresponding Phase I scores ($d_i,T_{i,1},T_{i,2}$).","On the 24-profile vertical density profile case study, using per-metric significance level $\alpha=0.03$ (overall $\alpha_0\approx0.12\approx3/24$), the resulting screening thresholds are 2.99 for $D$, 7.94 for $T^{(1)}$, and 663.6 for $T^{(2)}$; the most extreme profiles differ by low overall level (A6), strong local turbulence (A3), and an overall ‘too flat’ shape (B6). In simulations with overall significance 0.05, the observed false discovery rates under the true model are about 0.05 (±0.02) for Gaussian errors and 0.08 (±0.02) for t3 errors. Detection rates increase with distortion magnitude: for model (a) sine-shape distortion, Gaussian errors yield 45%, 80%, 98% detection for A=0.75,1.0,1.25 (t3: 21%, 62%, 100%); for model (b) local spike distortion, Gaussian errors yield 36%, 82%, 100% for B=0.02,0.03,0.04 (t3: 12%, 44%, 95%).","The authors note that control limits are estimated from data and thus subject to parameter-estimation effects; with insufficient Phase I profiles, corrections may be needed to maintain good chart properties. They also state that their empirical reference distributions for shape scores implicitly require a sufficient number of Phase I profiles measured at similar locations; for sparse, irregularly spaced profiles there may be too few Phase I profiles to stably estimate screening thresholds, requiring further research.","The approach is primarily a screening procedure using empirical/quantile thresholds rather than a classic sequential control chart design with run-length guarantees; ARL/ATS properties under sustained monitoring are not developed. Because control limits are derived from Phase I empirical distributions, performance can be sensitive to Phase I contamination, the chosen overall error allocation across the three statistics, and bandwidth-selection variability; robustness of the thresholding step itself is not deeply analyzed. Practical implementation choices (kernel, bandwidths, bootstrap details) may materially affect results but no software or reproducible workflow is provided.","The authors suggest exploring alternative deviation scores (e.g., using a high percentile of residuals instead of the maximum) and studying their properties. They note that focusing on one metric when a specific deviation type is of primary concern could improve power. They also propose incorporating application-specific structure (e.g., symmetry constraints) for more efficient estimation, generalizing the stationary-error assumption to more general error sequences (e.g., linear combinations of stationary sequences), and developing corrections for small Phase I sample sizes and for sparse/irregularly spaced profiles where threshold estimation is unstable.","Developing Phase II run-length/ATS theory (including steady-state behavior) and guidelines to set limits for desired in-control ARL would make the charts more directly usable in ongoing monitoring. Extending the method to multivariate profiles (multiple responses) and high-dimensional functional streams, with computationally efficient implementations, would broaden applicability. Providing open-source software (e.g., an R/Python package) with principled bandwidth and bootstrap defaults, plus diagnostic tools to attribute alarms to location vs. local vs. global shape, would improve adoption and reproducibility.",1203.4661v1,local_papers/arxiv/1203.4661v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:47:59Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Other,FALSE,FALSE,FALSE,Case study (real dataset),Other,"Suggests grouping inter-failure times into non-overlapping subgroups of size 4 or 5 (more generally size r, with k = n/r subgroups). Example given: n=100 inter-failure times grouped into 20 subgroups of size 5.",TRUE,C/C++,Not provided,NA,"The paper proposes an SPC-based monitoring procedure for software reliability using a non-homogeneous Poisson process (NHPP) framework combined with an ordered-statistics grouping of inter-failure times. Inter-failure time data are partitioned into non-overlapping subgroups of size r (illustrated with r=4 and r=5), and a mean value function based on an exponential distribution/NHPP mean value form is used to compute sequential points for a mean value control chart. Model parameters are estimated via maximum likelihood, solved numerically using the Newton–Raphson method, and control limits are obtained by mapping selected CDF levels (0.00135, 0.5, 0.99865) through the mean value function to produce LCL/CL/UCL. The method is illustrated on Musa (1975) software failure inter-failure time data, producing mean value charts that show out-of-control signals (points below LCL) interpreted as early detection of process deterioration. The contribution is a reliability-monitoring control-chart construction tailored to grouped/ordered inter-failure data when failures are frequent and tracking each inter-failure time is tedious.","The NHPP/Goel–Okumoto-style mean value function is used (with exponential form), e.g., $m(t)=a(1-e^{-bt})$ with failure intensity $\lambda(t)=m'(t)$. For grouped data (subgroup size $r$), the ordered-statistics-based mean value expression is written as a function of $r$ (e.g., involving $[a(1-e^{-bt})]^r$) and is used in an MLE likelihood for grouped/ordered inter-failure observations; parameters $(a,b)$ are solved via Newton–Raphson. Control limits are set by choosing CDF levels $F(t)$ equal to 0.00135, 0.5, and 0.99865 (analogous to 3-sigma), converting them to corresponding times and then to $m(t)$ values to obtain LCL/CL/UCL lines for the mean value chart of successive differences in $m(t)$.","Using Musa (1975) inter-failure data grouped into ordered-statistics subgroups, the paper reports parameter estimates for r=4: $\hat a=2.415117$, $\hat b=0.000099$, and for r=5: $\hat a=1.933309$, $\hat b=0.000114$. Control limits are constructed from CDF probabilities 0.00135, 0.5, and 0.99865 and plotted as horizontal lines on mean value charts of successive differences of $m(t)$. The resulting charts (for both 4th- and 5th-order groupings) show points below the LCL, which the authors interpret as out-of-control signals indicating assignable causes and early detection of failure situations. No standard ARL/ATS values are reported; performance is presented via the case-study charts and qualitative interpretation of signals.",None stated.,"The work does not report standard chart performance metrics (e.g., in-control/out-of-control ARL, false alarm rate, detection delay), so it is hard to calibrate and compare the proposed chart to Shewhart/CUSUM/EWMA designs. The approach relies on a specific NHPP/exponential mean value model and independence of inter-failure times; model misspecification, trend changes, or autocorrelation could materially affect signaling behavior. The evaluation is limited to a single historical dataset (Musa 1975) with no broader simulation study or benchmarking against alternative software reliability control-chart methods. Details needed for reproducible implementation (exact control-limit computation steps, numerical settings, and the C code) are not provided.",None stated.,"A natural extension is to provide ARL/ATS (including steady-state and conditional measures) via simulation/Markov-chain/integral-equation methods to tune limits and compare fairly against EWMA/CUSUM/GLR charts for small and moderate shifts. Robust or nonparametric variants could reduce sensitivity to NHPP/exponential model misspecification and accommodate overdispersion or nonstationary debugging phases. The method could be extended to explicitly handle autocorrelation/irregular testing effort and to incorporate covariates (e.g., test coverage) or multivariate reliability signals. Publishing reproducible software (e.g., an R/Python package) and additional real case studies would improve adoption and validation.",1205.6440v1,local_papers/arxiv/1205.6440v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:48:31Z
TRUE,Univariate|Other,CUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Other,Phase II,Finance/economics|Manufacturing (general)|Theoretical/simulation only,TRUE,TRUE,FALSE,Simulation study|Markov chain,ARL (Average Run Length)|ATS (Average Time to Signal)|Expected detection delay|Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper develops several sequential control charts to detect increases in the variance (scale) of a Gaussian time series under a change-point model, focusing on direct monitoring rather than only residual charts. It derives variance-monitoring schemes using the likelihood ratio (LR) approach, generalized likelihood ratio (GLR), sequential probability ratio test (SPRT), Shiryaev–Roberts (SR), and generalized versions (generalized SPRT and generalized SR), with recursive implementations provided for AR(1) (and also discussed for AR(2)). The methods are aimed at autocorrelated processes (Gaussian ARMA family via innovations/likelihood factorization) and are evaluated primarily by in-control and out-of-control average run length (ARL) and average delay (expected detection delay) in extensive Monte Carlo simulation. Simulations calibrated to a common in-control ARL (500) show that LR and SPRT charts perform best for medium/large variance increases when the reference value matches (or is not far below) the true change, while the generalized SR chart can outperform even optimally tuned reference-value charts for small changes. The study also highlights that applying iid-variance charts (including SR for iid) directly to correlated data can degrade performance as autocorrelation increases, supporting the need for time-series-aware designs.","The change model is $X_t=Y_t$ for $t<\tau$ and $X_t=\mu+\Delta(Y_t-\mu)$ for $t\ge\tau$ with $\Delta>1$. For Gaussian processes, the likelihood is written via innovations: $L_n\propto\prod_{j=1}^n v_{j-1}^{-1/2}\exp\{-\tfrac12\sum_{j=1}^n (X_j-\hat X_j)^2/v_{j-1}\}$. The LR-based CUSUM statistic compares $f_0$ vs $\max_{1\le i\le n} f_i$ and yields a stopping time $N_{LR}(c;\Delta^*)=\inf\{n: \max_{1\le i\le n}[\cdots]>c\}$; for AR(1) a recursion is provided for $A_n^+(\Delta^*)$ (eqs. (11)–(12)). The SPRT-based chart reduces to a residual/innovation CUSUM: $N_{SPRT}(c;\Delta^*)=\inf\{n: \max_{0\le i\le n}(S_n(\Delta^*)-S_i(\Delta^*))>c\}$ with $S_n(\Delta)=\sum_{j=1}^n (X_j-\hat X_j)^2/v_{j-1}-nK(\Delta)$ and $K(\Delta)=\log(\Delta^2)/(1-1/\Delta^2)$.","All charts are calibrated to have in-control ARL 500, and out-of-control ARL and average delay are estimated by simulation (typically $10^6$ replications; $10^5$ for GLR due to computation). The minimum out-of-control ARL for LR and SPRT charts (with reference value $\Delta^*$) is attained when $\Delta^*=\Delta$ (true change), and these two procedures have very similar performance. For small shifts (reported as $\Delta\le 1.3$), the generalized Shiryaev–Roberts (GSR) chart yields the smallest ARL and can even beat reference-value charts tuned optimally; e.g., at $\phi_1=0.4$, $\Delta=1.3$, ARLs are about 32.5 (LR/SPRT), 39.4 (GLR), 49.2 (GSPRT), and 32.7 (GSR) (Table 4). For larger changes (e.g., $\Delta=2.0$ at $\phi_1=0.4$), LR/SPRT achieve ARLs around 7.5/6.8, while GSR is around 11.4 and GLR about 9.4 (Table 4), but GSR can have smaller delay when changes occur later.","The paper notes that for time-dependent processes explicit formulas for ARL and average delay are generally not available, so control limits and performance comparisons must be obtained by simulation. It also notes that GLR charts are more computationally demanding because no recursive presentation is given, leading to fewer simulation repetitions for GLR performance estimation. The authors emphasize the practical drawback of LR/SPRT/SR charts that require choosing a reference value $\Delta^*$ when the change magnitude is unknown, motivating the generalized procedures.","The methodology is developed under Gaussianity and a specific multiplicative scale-change model; robustness to non-Gaussian heavy tails (common in finance) or other variance dynamics (e.g., GARCH-type conditional heteroskedasticity) is not assessed. The study focuses on one-sided detection of variance increases and primarily evaluates an AR(1) target process in simulations, so performance/generalizability to broader ARMA/long-memory settings (despite theoretical coverage claims) is not extensively empirically validated. Phase I issues (estimation error in AR parameters/innovations variance and its effect on false alarms) are not treated; the charts largely presume known in-control model parameters or correctly specified innovations. No software/code is provided, which may hinder reproducibility and adoption.",None stated.,"Extend the proposed schemes to accommodate parameter uncertainty (self-starting or Phase I estimation effects) and model misspecification, including robust/nonparametric variants for non-Gaussian or heavy-tailed time series. Evaluate performance on conditional variance models (e.g., GARCH) and under long-memory dependence, and develop computationally efficient recursive implementations for GLR-type procedures to reduce runtime. Provide diagnostic tools for post-signal attribution (variance vs. model-change) and release an implementation package (e.g., R/Python) with calibration utilities for practitioners.",1209.4678v1,local_papers/arxiv/1209.4678v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:49:13Z
TRUE,Univariate|Multivariate|Nonparametric,Shewhart|EWMA|Hotelling T-squared|MEWMA|Other,Both,Theoretical/simulation only,TRUE,FALSE,FALSE,Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),"Phase I uses 80 in-control rational subgroups (univariate) to set limits; Phase II uses 10,000 subgroups to estimate ARL. For multivariate bootstrapping they draw 100 in-control subgroups and resample 1,000 subgroups with replacement; subgroup size is typically 20 (also recommend around 20 to avoid unstable ARL).",TRUE,R|MATLAB,Not provided,http://sas.uwaterloo.ca/stats_navigation/techreports/07WorkingPapers/2007-07.pdf,"The paper proposes robust alternatives to classical Shewhart $\bar{X}$–$S$ charts by replacing the subgroup mean with the $100\alpha\%$ trimmed mean and replacing dispersion with a winsorized standard deviation/variance, yielding a robust $\bar{X}_t$–$S_t$ chart. It extends the same robustness idea to EWMA monitoring by using trimmed subgroup means and winsorized dispersion within EWMA-style updating. For multivariate monitoring, it proposes depth-based trimming (spatial/L1, Tukey, Liu/simplicial, and Oja depths) to construct a trimmed mean vector and winsorized covariance, producing robust analogues of Hotelling’s $T^2$ chart and MEWMA; because the null distributions are complex, control limits are obtained via bootstrap empirical quantiles. Performance is evaluated mainly by simulation under no shift, small mean shifts, and outlier contamination, reporting ARL and SDRL. The results indicate the proposed charts are comparable to standard charts without outliers and substantially more robust (higher in-control ARL / fewer false alarms) under outlier contamination while maintaining good detection under mean shifts, and they compare favorably to Liu’s rank-based chart and an MCD-based robust method in the scenarios studied.","Univariate $100\alpha\%$ trimmed mean: $\bar{X}_t=\frac{\sum_{i=t+1}^{n-t}x_{(i)}}{n(1-2\alpha)}$ with $t=\lfloor n\alpha+0.4\rfloor$; its standard error uses winsorized SD $s_w$: $se_t=s_w/\sqrt{n(1-2\alpha)}$. EWMA analogue: $Z_i=\lambda\bar{X}_{it}+(1-\lambda)Z_{i-1}$ (trimmed mean substituted). Robust multivariate Hotelling analogue: $\tau^2=(\bar{X}_t-\mu)^T S_w^{-1}(\bar{X}_t-\mu)$ where $\bar{X}_t$ and $S_w$ are depth-trimmed/winsorized. Robust MEWMA analogue: $Z_{it}=\lambda\bar{X}_{it}+(1-\lambda)Z_{i-1,t}$ and chart statistic $\psi^2=(Z_{it}-\hat\mu_z)^T S_{zw}^{-1}(Z_{it}-\hat\mu_z)$; UCLs for $\tau^2,\psi^2$ taken from bootstrap quantiles (e.g., 90th percentile) with LCL=0.","Univariate simulations (Phase I: 80 subgroups; Phase II: 10,000) show that under outliers the proposed trimmed/winsorized charts keep much larger in-control ARL than the mean/SD-based charts (e.g., Table 1 reports outlier-case ARLs often near 100–180 for trimmed charts versus much smaller values for the mean-based comparator, depending on subgroup size and trimming). For EWMA (subgroup size 20, L=3, $\lambda\in\{0.2,0.25,0.4\}$), the outlier scenario shows trimmed EWMA yields ARL 116.7–151.8 while the mean-based EWMA signals almost immediately (ARL about 1.19–2.69), indicating strong robustness (Table 2). In multivariate $T^2$ simulations (bivariate normal, subgroup size 20), depth-based trimmed charts have similar no-shift ARLs (~131–158) to the mean-based method (161) but dramatically higher ARL under outliers compared with mean-based $T^2$ (e.g., one-outlier with mean (5,5): mean-based ARL 17.26 vs depth-based ARLs 86–147; Table 3). Comparing methods (Table 5), for one outlier from $N(3,3)$ the proposed method has ARL 104.78 versus 16.68 for Liu’s rank method and 94.33 for MCD; for a 0.25 shift, Liu’s method is fastest (ARL 13.4) but the proposed method is intermediate (24.9) and MCD is slow (60.04).","They note that the distributions of the proposed multivariate statistics ($\tau^2$ and $\psi^2$) are hard to fit well with standard parametric families; gamma fits are inadequate in the upper tail, so they resort to bootstrapping, which they describe as computationally difficult. They also caution that performance depends on subgroup size and depth choice: some depths (e.g., Tukey depth) are unreliable for small subgroup sizes and can cause excessive data loss, and a poor trimming cutvalue can lead to unstable UCL/LCL and unreliable charts.","The work is largely simulation-based with no real industrial/healthcare case study, so practical effectiveness (data quirks, measurement systems, implementation burden) is not validated. The methods assume rational subgroups and (in the classical-limit discussions) normality/independence; robustness to autocorrelation, nonstationarity beyond outliers, and variance/covariance shifts is not systematically explored. Bootstrap-based limit setting may be challenging for online/real-time monitoring and for higher-dimensional settings where computing some depths (especially Tukey/Oja) can be expensive; scalability is not quantified. Phase I robustness to contaminated Phase I samples (beyond the designed outlier insertion rules) and effects of estimating $\mu,\Sigma$ with limited data are not fully characterized theoretically (e.g., guaranteed in-control ARL under parameter estimation).","They suggest that although bootstrapping is used due to lack of good distributional fits, practitioners could approximate cut-offs using a gamma distribution to save computation time, implying future refinement of parametric approximations to the empirical distributions of $\tau^2$ and $\psi^2$. They also discuss the need for careful selection/estimation of depth cutvalues and note depth-function limitations across dimensions, pointing to further work on choosing depths/cutvalues reliably across settings.","Develop self-starting or adaptive versions that update trimming/cutvalues and winsorized dispersion sequentially without a large Phase I sample, with theoretical guarantees on in-control ARL. Extend the approach to explicitly handle autocorrelated processes (e.g., residual charts or state-space models) and to detect variance/covariance shifts, not just mean shifts/outliers. Provide scalable algorithms and complexity analysis for depth computations in higher dimensions and compare against more modern robust multivariate charts (e.g., robust PCA/HD monitoring) on benchmark and real industrial datasets. Release reproducible software (R package) to standardize implementation and facilitate practitioner adoption.",1211.4262v1,local_papers/arxiv/1211.4262v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:49:58Z
TRUE,Univariate|Nonparametric,CUSUM|Change-point|Other,Phase II,Theoretical/simulation only,FALSE,FALSE,NA,Simulation study|Other,ARL (Average Run Length),Assumes an available in-control reference sample of size m (stated examples use m=10 and m=50); authors state the historical sample size is assumed to be larger than 10. Future monitoring considers sequences up to length 500 (n up to 490 in tables).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a new univariate nonparametric upper-sided CUSUM control chart for detecting small shifts in a process location (mean) using a change-point framing and the standardized Mann–Whitney (Wilcoxon rank-sum) statistic computed relative to an in-control reference sample. The charting statistic is a one-sided CUSUM recursion that accumulates standardized Mann–Whitney scores, aiming to improve sensitivity to small mean shifts under completely distribution-free conditions. Because analytical calibration is difficult, control limits (decision values) are estimated via large-scale Monte Carlo simulation (one million sequences of length 500 from a standard normal distribution) for several nominal in-control ARLs and reference sample sizes (e.g., m=10 and m=50). Performance is compared (in ARL terms) to a parametric change-point chart (Hawkins et al., 2003), showing faster detection for small shifts while the parametric change-point approach can be faster for larger shifts. The authors also provide qualitative comparisons arguing their Mann–Whitney-based CUSUM uses richer rank information than several prior nonparametric CUSUM schemes (e.g., sequential ranks, within-group signed ranks, exceedance counts).","For a split point t in a sequence of length l, the Mann–Whitney statistic is $MW_{t,l}=\sum_{i=1}^{t}\sum_{j=t+1}^{l} I(x_j<x_i)$ with in-control mean $E_0(MW_{t,l})=\tfrac{t(l-t)}{2}$ and variance $\mathrm{Var}_0(MW_{t,l})=\tfrac{t(l-t)(l+1)}{12}$. The standardized score is $SMW_{t,l}=(MW_{t,l}-E_0(MW_{t,l}))/\sqrt{\mathrm{Var}_0(MW_{t,l})}$. The proposed upper-sided CUSUM is $S_j(m,n)=\max\{0, S_{j-1}(m,n)+SMW_{j,(m+n)}-k\}$ with $k=\Delta/2$ (set to $1/2$ in their tables) and signal rule $\max_j S_j(m,n)>h_{m,n}$, where $h_{m,n}$ is calibrated to achieve a desired in-control ARL.","Simulated decision values $h_{m,n}(\alpha)$ are tabulated for in-control ARLs 100, 200, 370, and 500 (e.g., for m=10 and ARL0=200, $h_{m,n}$ rises from about 1.185 at n=1 to about 2.41 around n=240 and stabilizes). In ARL comparisons for $N(0,1)$ with m=10 and $\alpha=0.005$ (ARL0=200), the proposed CUSUM is faster than the Hawkins et al. change-point chart for small shifts, e.g., at $\delta=0.05$ and $\tau=50$: ARL 39.9 (CUSUM) vs 66.4 (C-PC); at $\delta=0.025$ and $\tau=100$: 96.4 vs 130.7. For larger shifts the change-point chart can be faster, e.g., at $\delta=2.0$ and $\tau=100$: 4.3 (CUSUM) vs 3.2 (C-PC). The authors also note the control limit sequence increases initially in n and then stabilizes.",The authors state that computation of the proposed CUSUM statistic and especially obtaining decision values/control limits is “somewhat difficult” and requires computer programming/simulation rather than analytic solutions. They also point out that nonparametric control-chart work remains largely theoretical and that development of practical applications is highly desired.,"Control-limit calibration is done via Monte Carlo using standard normal sequences even though the method is advertised as distribution-free; finite-sample accuracy of the achieved ARL under strongly non-normal distributions is not fully demonstrated. The proposed scheme relies on an in-control reference sample (m) and does not address robustness to contamination/miscalibration in Phase I or provide guidance for estimating/handling an unknown in-control location beyond having a clean reference set. The paper focuses on independence and does not treat autocorrelation, which can strongly affect rank-based sequential statistics in practice. Comparisons are limited (mostly ARL and one parametric competitor), with little discussion of steady-state performance, false alarm variability (SDRL), or real-data case studies.","They explicitly suggest that practical applications of nonparametric control charts are needed, noting the literature is largely theoretical and that development toward real-world use is highly desired.","Provide distribution-free (or at least distribution-robust) calibration methods for $h_{m,n}$ that avoid reliance on normal-based simulation, and study achieved ARL across a wider range of distributions. Extend the approach to handle autocorrelated/streaming data (e.g., via block ranks, prewhitening, or model-based residual charts) and to two-sided monitoring for both upward and downward shifts. Develop Phase I procedures for selecting/cleaning the reference sample and quantify the impact of reference-sample contamination. Release software (e.g., an R package) to compute $SMW$ efficiently and implement the chart with recommended default choices for k and control limits.",1305.4318v1,local_papers/arxiv/1305.4318v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:50:33Z
TRUE,Univariate|Bayesian|Other,Shewhart|Other,Both,Manufacturing (general)|Food/agriculture|Other,FALSE,FALSE,NA,Case study (real dataset)|Other,False alarm rate|Other,Designed for very small samples; examples use n=4 per sampling period (wood specimens) and n=2 (concrete). Phase I examples use first 10 samples as in-control for each process in the wood case; concrete example notes typical Phase I is 40–50 points but here reduced to about 18 in-control points after a tuning period (first 22 samples shown).,NA,None / Not applicable,Not applicable (No code used),NA,"The paper proposes a Bayesian control chart for comparing two processes by monitoring the ratio of their Weibull percentiles (e.g., a lower-tail strength percentile). Both in-control and out-of-control parameters are treated as unknown, and the chart updates posterior distributions sequentially using all accumulated samples, producing cumulative (not single-sample) estimates and continually updated control limits. Percentile control limits are obtained by transforming the conditional posterior of a Weibull percentile into a standard Gamma variable, while ratio limits for $u=x_R/y_R$ are obtained via a transformation to an Inverted Beta distribution. The approach avoids data transformations to normality and is positioned for very small sample sizes (including potential use with individual observations). Two real-data illustrations are provided: comparing modulus of rupture percentiles for two wood specimen sizes and comparing compressive strength percentiles from two concrete production lines, showing the ratio chart can support homogeneity assessments and detect a simulated mean shift.","Weibull percentile parameterization uses $x_R=\delta[\ln(1/R)]^{1/\beta}$ and $F(x;x_R,\beta)=1-\exp\{-\ln(1/R)(x/x_R)^\beta\}$. The conditional posterior for $x_R$ given $\beta_k$ leads to the transformation $z=x_R^{-\beta_k}A(k)$ with $z\sim\text{Gamma}(k\,n+1,1)$, yielding control limits via $x_R=z^{-1/\beta_k}A(k)^{1/\beta_k}$. For the ratio $u=x_R/y_R$, a transformation $v=u^{\beta_k}C(k)$ produces an Inverted Beta distribution used to compute updated ratio control limits.","In both real-data examples (wood MOR and concrete compressive strength), the ratio control charts do not signal out-of-control during the presented in-control periods, supporting homogeneity of the paired processes. In a simulated scenario created by multiplying the last 15 observations of the second wood process by 1.15 (about a 15% mean increase after Phase I), the ratio chart signals promptly at the 12th sample after the shift. A Shewhart-type false alarm risk of $\alpha=0.27\%$ is used to set control limits in the examples, with limits recalculated each sampling period during updating. The concrete example highlights operation with very small subgroup size (n=2) and a reduced effective Phase I (about 18 in-control points after an initial tuning period).","The authors state that a complete understanding of the chart’s performance requires further investigation, specifically suggesting a large Monte Carlo study, which they defer to a future dedicated paper. They also characterize the paper as an introductory note giving a “first picture” of the chart’s features via two applications, implying limited performance characterization beyond these illustrations.","The method relies on correct Weibull modeling and on the assumption that the two percentiles $x_R$ and $y_R$ are independent and share a common shape parameter $\beta$; violations (e.g., differing shapes, dependence due to shared environment or measurement system) could distort limits and signaling behavior. Performance is not benchmarked systematically against alternative ratio-monitoring approaches (e.g., bootstrap/nonparametric charts, GLR/CUSUM/EWMA variants), and operating characteristics (ARL/ATS) are not reported. Implementation requires repeated numerical integration to estimate $\hat\beta$ (and repeated recalculation of limits), which may be nontrivial in practice without provided software, especially for online use with many streams.","The paper explicitly suggests that further investigation is needed to fully understand chart performance, including conducting a large Monte Carlo study, and notes this will be addressed in a future dedicated paper.","A natural extension is a systematic ARL/ATS study (in-control and out-of-control) across shift types (changes in percentile via scale vs. changes in shape) and across subgroup sizes, with comparisons to existing percentile charts and ratio charts. Robust variants could relax the shared-$\beta$ and independence assumptions (e.g., hierarchical Bayes for separate shapes or copula dependence) and assess sensitivity to Weibull misspecification. Providing an open-source implementation (e.g., R/Python) and practical guidance for selecting priors/hyperparameters and for diagnosing which process drives a ratio signal would improve adoption.",1305.5962v3,local_papers/arxiv/1305.5962v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:51:02Z
TRUE,Univariate|Bayesian,Shewhart|Other,Both,Manufacturing (general)|Semiconductor/electronics|Healthcare/medical|Finance/economics|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Other,Phase I: recommends about 50 training data (roughly 50 in-control observations) as a compromise; examples include m=10 samples of size n=5 (50 observations) and m=25 samples of size n=2 or n=1 with m×n=50. They note too much Phase I data can make the posterior overly strong and reduce responsiveness in Phase II.,TRUE,Other,Not provided,NA,"The paper proposes a Bayesian (semi-empirical) Shewhart-type control chart to monitor a specified Weibull percentile (e.g., 0.99-quantile) when both in-control and out-of-control Weibull parameters are unknown, targeting reliability/rare-event settings with very small sample sizes. Unlike many Weibull charts that transform data to approximate normality, it models Weibull data directly and updates a replicated prior/posterior sequentially using all accumulated samples (a memory-type Bayesian learning-by-sampling scheme). Control limits are probability-based credible limits derived from the conditional posterior of the percentile and are adaptively refined during Phase I, then fixed for Phase II monitoring; an optional companion chart monitors the Weibull shape parameter β for stability. Performance is demonstrated on a carbon-fiber breaking-strength dataset and via Monte Carlo studies (e.g., N=1000 runs), reporting ARL/SDRL and showing improved out-of-control detection compared with Padgett & Spurrier (1990) and an earlier Bayesian Weibull chart (Erto & Pallotta, 2007) under comparable scenarios. The authors recommend using roughly 50 Phase I observations to balance tighter limits against loss of Phase II responsiveness due to an overly informative posterior.","Weibull CDF is re-parameterized in terms of percentile x_R and shape β: F(x;x_R,β)=1-\exp\{-[\ln(1/R)](x/x_R)^β\}. A uniform prior is assumed for β on (β1,β2) and an inverse-Weibull prior for x_R; combining with the Weibull likelihood yields a replicated joint posterior pdf for (x_R,β) after k samples (size n) that depends on all k·n observations. The plotted statistic is the posterior mean \hat{x}_{R,k}=E[x_R\mid x^k,\beta_k] with a closed form involving Gamma functions (Eq. 16), and control limits are obtained by transforming x_R into a standard Gamma variable z (Eq. 18–20) and taking z-quantiles corresponding to false-alarm risk α.","In the Padgett & Spurrier (1990) comparison scenarios (n=5, m=25), the proposed chart reduces OOC ARL relative to P&S; e.g., for R=0.90 with β shifting 3→2 (δ=1), OOC ARL is 54.9 (SDRL 25.0) vs 84.82 for P&S (and 66.1 for Erto & Pallotta 2007). In the carbon-fiber example (R=0.99, n=5), the percentile chart signaled 7 samples after the simulated shift; with a longer Phase I/resampling strategy it signaled by the 3rd sample after shift, and the β-stability chart detected shape change by the 4th sample after shift. Monte Carlo studies (N=1000) report ARL/SDRL across multiple OOC scenarios and show that the total number of observations needed to signal is roughly similar across subgroup sizes due to cumulative Bayesian updating (e.g., m×n fixed at 50). The chart is reported robust to poor priors (e.g., ±25% or ±50% prior perturbations) with similar signaling behavior and control-limit convergence.",None stated.,"The method relies on correct Weibull model specification; robustness to model misspecification (e.g., lognormal/gamma mixtures) is not analyzed. The approach assumes independent observations within and across samples; performance under autocorrelation or nonstationarity beyond Weibull-parameter shifts is not evaluated. Control limits come from posterior quantiles (via a Gamma transform) but the mapping between Bayesian credible limits and frequentist false-alarm properties under parameter estimation uncertainty is not fully characterized beyond simulation; practitioner guidance on choosing α and priors in regulated environments could be stronger.",They note that the Bayesian framework enables computation of a posterior predictive density useful for process capability analysis (suggesting extension in that direction).,"Develop robust or semi-parametric variants that remain effective under Weibull misspecification, censoring, or contamination/outliers common in reliability data. Extend the chart to handle autocorrelated/conditionally dependent failure processes and incorporate covariates (e.g., stress factors) for accelerated life testing. Provide open-source software (e.g., R/Python) and practical prior-elicitation templates, and study steady-state/conditional ARL and diagnostic tools for estimating change type (scale vs shape vs both).",1308.0691v1,local_papers/arxiv/1308.0691v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:51:37Z
TRUE,Univariate|Other,GLR (Generalized Likelihood Ratio)|Change-point|Other,Phase I,Theoretical/simulation only,TRUE,FALSE,FALSE,Approximation methods|Simulation study|Other,False alarm rate|Detection probability|Other,"Uses a Phase I batch size of m = 200 individual observations, motivated by guidance that 20–25 samples of size 3–5 are desirable for a trial control chart; thresholds/expected values are computed for m = 200 via simulation.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a likelihood ratio test (LRT)-based retrospective (Phase I) model to detect and estimate multiple step shifts (multiple change points) in the mean of a univariate Gaussian process from individual observations. The method evaluates all candidate split points, signals out-of-control when the maximum LRT statistic exceeds a threshold, and estimates the change location by the maximizer; multiple change points are found via binary segmentation. Because the raw LRT statistic’s in-control expectation varies with segment size, the authors use a normalized statistic dividing by its in-control expected value (estimated via simulation) and then set thresholds to achieve desired false-alarm probabilities. Performance is assessed with Monte Carlo experiments (m=200) over shift magnitudes δ from 0.5 to 5.0 and multiple-change scenarios (up to 7 tests/change points), reporting accuracy (bias/standard error of estimated change locations) and precision (coverage of intervals around true change points). The authors conclude the method yields approximately unbiased estimates for single changes and acceptable accuracy/precision for multiple changes, but practical use is constrained by the need to know the correct data distribution (normality).","For a candidate split at m1 (with m2=m−m1), the LRT statistic is based on the difference between the maximized log-likelihood under one normal mean (IC) versus two normal means (one change): $\text{lrt}(m_1,m_2)=-2\,(\ell_0-\ell_a)$, which is asymptotically $\chi^2$ with 2 degrees of freedom. The paper also defines a normalized statistic to remove dependence on segment size: $\text{Nlrt}(m_1,m_2)=\text{lrt}(m_1,m_2)\,/\,\mathbb{E}[\text{lrt}(m_1,m_2)]$ where the expectation is computed under IC via simulation, and signals if the maximum over $m_1$ exceeds an upper threshold.","Monte Carlo studies use m=200, σ=1, and step-shift sizes δ∈{0.5,1,2,3,4,5} with uniformly spaced multiple shifts alternating between two means. The method shows near-unbiased change-point estimates for a single change, and small but patterned biases for intermediate change points under multiple shifts (e.g., for R=3 and δ=2, true change points 50/100/150 are estimated as 51.3/98.7/151.6; for R=4 and δ=3, true 40/80/120/160 estimated as 39.3/79.2/122/160.3). Thresholds are calibrated by simulation to target false-alarm probabilities, with example thresholds reported (e.g., 7.0089, 7.5745, 7.3684, 8.3876, 8.1206, 8.0292, 7.9153 for a sequence of tests). Precision results indicate that even for δ=0.5, 31% of estimates hit the exact change point and about 50% are within 2 units, and across shift sizes the method identifies locations within 15 units of truth about 90% of the time.","The authors state the method is restricted by distributional assumptions: forming the LRT requires knowing the exact distribution of Phase I data, which is rarely true in real problems. They also note some situations where heterogeneous segments may follow different (non-identical) distributions, which degrades practicability and calls for a more generalized approach (e.g., GLRT or nonparametric methods).","The approach assumes independent observations and (within segments) a common variance, so performance may degrade with autocorrelation or variance shifts typical in Phase I data. Binary segmentation can be suboptimal for multiple change-point estimation (it may miss closely spaced changes and can propagate early mistakes), and the paper’s calibration appears tuned to m=200 and a fixed maximum number of tests, limiting transferability. The evaluation focuses on simulated Gaussian mixture/step patterns with uniformly spaced changes; broader, more realistic scenarios (random spacing, heavy tails, outliers) and comparisons against modern multiple change-point methods are limited.","The authors propose extending the approach to more general distribution families (e.g., exponential family/normal family beyond the simple Gaussian mean-shift setup), detecting other shift types (e.g., linear trend or sporadic changes), and simultaneously detecting shifts in more than one moment (e.g., mean and variance). They also suggest developing a more generalized form flexible to cases where segments may follow different distributional forms (e.g., Johnson family or exponential-family-based LRT/GLRT).","Developing robust/self-starting Phase I procedures that jointly estimate nuisance parameters (especially variance) under multiple shifts/outliers would improve practical deployment. Extending the method to handle autocorrelation (e.g., ARMA residual monitoring or state-space modeling) and to provide principled multiple-testing control (family-wise error/FDR) across recursive segmentations would strengthen false-alarm guarantees. Providing open-source software and benchmarking against state-of-the-art multiple change-point algorithms (e.g., PELT, Wild Binary Segmentation, fused lasso) on shared datasets would improve adoption and evidentiary support.",1403.0668v1,local_papers/arxiv/1403.0668v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:52:14Z
FALSE,Univariate|Other,Other,Both,Other,NA,NA,NA,Case study (real dataset),Other,Not discussed,TRUE,Other,Not provided,http://www.jmp.com/software/pro/,"The paper applies educational data mining to four years (2010–2014) of Dibrugarh University BA/BSc examination data to compare student performance by major subject, gender, and caste/category. Analyses and visualizations are produced in SAS JMP, including scatterplots/3D scatterplots, bubble plots, Fit Y by X (one-way analysis), run charts, and control charts (including an Individuals–Moving Range style display) of pass percentage. The work is descriptive/analytic rather than methodological: it does not propose new SPC charts or study run-length properties, but uses control-chart-style graphics as exploratory tools to visualize variation across subjects and groups. Reported conclusions include higher pass percentages for female students than male students, higher performance for General category than other categories, and identification of consistently high-performing majors (e.g., Bengali/Statistics in Arts; Geology/Electronics/Home Science/Statistics in Science). The authors suggest future work to build predictive models for student performance using additional variables.",Not applicable,"The study reports descriptive findings from real examination records: female candidates show higher pass percentages than male candidates; General category students outperform other caste/category groups; and certain majors have consistently high pass percentages (e.g., 100% for Bengali Major and Statistics Major in Arts in the discussed results). Control-chart visuals shown for pass percentage include an Individuals chart with centerline around ~87.48 (BA majors) and ~88.46 (BSc majors) with displayed UCL/LCL values on the plots, but no SPC detection-performance metrics (e.g., ARL) are evaluated.",None stated,"Although control charts are displayed, the paper does not frame the problem as process monitoring (Phase I/II) and provides no discussion of rational subgrouping, time ordering, or the assumptions required for Individuals/Moving Range charts. There is no evaluation of false alarm behavior or detection performance (e.g., ARL/ATS), so the control-chart graphics function only as descriptive plots. The analysis may also be sensitive to differing cohort sizes across subjects/categories (unequal denominators) and potential dependence across years, but these issues are not addressed.",The authors state that they plan to develop a predictive model for BA and BSc students based on various variables.,"A useful extension would be to formalize the monitoring objective (e.g., year-to-year shifts in pass rate) and use appropriate attribute or proportion charts (e.g., p/u charts with varying sample sizes) rather than applying Individuals charts to percentages without modeling denominators. The study could also incorporate methods that handle temporal dependence and changes in exam difficulty or cohort composition, and validate any predictive models using cross-validation and out-of-sample testing with clearly defined performance metrics.",1411.2081v1,local_papers/arxiv/1411.2081v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:52:35Z
TRUE,Univariate|Other,CUSUM|Change-point,Phase II,Transportation/logistics|Other,TRUE,TRUE,NA,Simulation study|Approximation methods|Case study (real dataset)|Other,False alarm rate|Detection probability|Other,Not discussed,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a modified CUSUM-based control-chart algorithm to automatically detect steady-state intervals in short time series from pedestrian bottleneck experiments, using density and speed (via the Voronoi method) as low-noise proxies for flow. The classic two-sided CUSUM is simplified into a single bounded statistic with a step-function update applied to z-scored observations, improving sensitivity and making the statistic return to steady state faster. The decision threshold is calibrated via a Neyman–Pearson-style quantile of the chart statistic under an in-control model, where the in-control process is modeled as a Gaussian AR(1) process to account for autocorrelation; the threshold is obtained via simulation or a numerical approximation to a stationary Markov-chain balance equation. Robustness is assessed by varying the manually selected reference segment, showing detected steady states are nearly unchanged for reasonable choices. In application to three experimental datasets, the method yields reproducible steady-state detection and supports an empirical finding that the discrepancy between “all-states” flow and steady-state flow mainly depends on the ratio N/b, with a critical value around 115 persons/m above which the two flows are nearly identical.","The modified one-sided bounded CUSUM statistic is updated as $s_i=\min\{\max\{0, s_{i-1}+F(\tilde x_i)\}, s_{\max}\}$ with initialization $s_0=s_{\max}$, where $F(\tilde x_i)=1$ if $|\tilde x_i|>q(\alpha)$ and $F(\tilde x_i)=-1$ otherwise, and $\tilde x_i=(x_i-\mu)/\sigma$ is a standard-score transform based on the reference (in-control) segment. The detection threshold $\theta$ is set as an upper $\gamma$-quantile of the in-control $s_i$ distribution, with the in-control $\tilde x_i$ modeled as a Gaussian AR(1): $y_i=c y_{i-1}+\sqrt{1-c^2}\,\varepsilon_i$. Reaction-time corrections are $t_{\text{reaching}}=(s_{\max}-\theta)/f$ and $t_{\text{leaving}}=\theta/f$ (frames per second $f$).","The paper fixes key calibration settings at $\alpha=0.99$, $\gamma=0.99$, and $s_{\max}=100$, and calibrates $\theta$ using an AR(1) model fitted to the reference segment’s lag-1 autocorrelation. Robustness tests using three non-overlapping manually selected reference segments on the same series yield almost identical detected steady-state intervals, indicating reproducibility with respect to reasonable reference choices. On three groups of bottleneck experiments, flow in steady state is approximately linear in bottleneck width and supports the prior conclusion that flow per unit width is roughly constant. The absolute difference between flow computed over all states and over steady state decreases as $N/b$ increases, with an empirical critical ratio of approximately 115 persons/m beyond which all-states flow is nearly identical to steady-state flow.",None stated,"The method relies on a manually selected reference segment to estimate $\mu$, $\sigma$, and autocorrelation $c$, so it is not fully self-starting and may be sensitive when no clear steady-state reference exists. The in-control calibration assumes a Gaussian AR(1) model for z-scored observations; deviations from Gaussianity or higher-order/long-memory dependence could miscalibrate $\theta$ and thus the false transition rate. Performance is not reported in standard SPC terms (e.g., in-control/out-of-control ARL/ATS) across a range of shifts, making it harder to compare to established CUSUM design benchmarks. Code and implementation details are not provided, which may hinder exact reproduction of the numerical scheme for the analytical calibration.","The authors suggest that future studies should use steady-state flow (rather than all-states flow) when combining results across different bottleneck experiments. For experiments where steady state is difficult or impossible to detect, they propose using the ratio $N/b$ to estimate the likely difference between all-states flow and steady-state flow.","Develop a self-starting version that does not require a manually chosen in-control reference segment, or provide guidance/automation for selecting it. Extend the calibration beyond Gaussian AR(1) to more realistic dependence structures (e.g., ARMA, state-space, or nonparametric bootstrap for dependent data) and quantify robustness to mis-specification. Report standard SPC performance measures (e.g., ARL/ATS under specified change scenarios) to enable fair comparison with alternative change-point and control-chart methods. Provide open-source software to implement the detection and calibration pipeline and facilitate adoption in other domains with short, autocorrelated series.",1506.02433v1,local_papers/arxiv/1506.02433v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:53:05Z
TRUE,Univariate|Bayesian|Other,Other,Both,Manufacturing (general)|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Other,"Phase I uses m training samples of size n (examples: m=10,20,30 with n=4; simulation study uses m=20 with n=5). In the conclusion the author recommends exploiting about 80–100 training data (observations) as a compromise to set both the control interval and the last joint posterior for Phase II.",TRUE,None / Not applicable,Not provided,NA,"The paper studies the performance of a Bayesian control chart designed to compare two processes by monitoring the ratio of two Weibull percentiles (e.g., $u=x_R/y_R$) at a specified reliability level $R$. It assumes both processes follow Weibull distributions with a common (stable) shape parameter $\beta$ and treats the two percentiles as independent random variables, using a practical Bayesian updating approach for Weibull data. By transforming the ratio statistic to an Inverted-Beta distributed variable, the chart’s control limits are obtained from Inverted-Beta percentiles under a chosen false-alarm risk $\alpha$. The paper investigates (i) how the number of Phase I training samples affects control limit width and Phase II responsiveness, (ii) robustness to poor priors (±50% shifts in prior $x_R$, $y_R$, and $\beta$), and (iii) out-of-control detection via Monte Carlo estimates of run-length properties across multiple shift scenarios. Results show narrower limits with more Phase I data but potentially slower detection due to an overly strong posterior, while the chart remains fairly robust to poor priors and detects moderate percentile shifts with acceptable ARL in most scenarios.","Weibull model: $F(x;\delta,\beta)=1-\exp\{-(x/\delta)^\beta\}$ and percentile $x_R=\delta[\ln(1/R)]^{1/\beta}$. The monitored statistic is the percentile ratio $u=x_R/y_R$ with a derived posterior density for $u$ given Phase I samples and $\beta$ (Eq. 2). A one-to-one transformation $v=u^{\beta_k}C(k)$ yields $v\sim$ Inverted-Beta with $\text{pdf}(v)=\frac{\Gamma(2(kn+1))}{\Gamma(kn+1)^2}\frac{v^{kn}}{(1+v)^{2(kn+1)}}$ (Eq. 10), so $\text{LCL},\text{UCL}$ are obtained by mapping Inverted-Beta quantiles back through the inverse transform at false-alarm risk $\alpha$.","In the illustrative lumber-strength example (percentile ratio monitoring at $\alpha=0.27\%$), using m=10 Phase I samples yields a Phase II run length RL=12 for a simulated shift, while extending Phase I to m=20 or m=30 narrows the interval (UCL–LCL ≈ 0.12 and 0.10 vs 0.15) but can fail to signal within 15 Phase II samples unless the Phase II prior is weakened (then RL improves to 10 for m=20 and 5 for m=30). Robustness checks with priors perturbed by ±50% show signals occurring between 11 and 21 samples, with control-limit widths remaining close to the baseline case. In Monte Carlo testing (N=1000 per scenario) with in-control ARL ≈ 370 (R=0.95, n=5, m=20), out-of-control ARLs ranged from about 4.3–29.9 depending on the combination of percentile shifts; worst cases were moderate one-sided changes such as $x_R^{out}/y_R^{out}=0.5/0.8, 0.8/1.2, 0.5/1.0$ (and inverses).",None stated.,"The approach relies on key modeling assumptions—independence between the two percentiles and a common, stable Weibull shape parameter across processes—which may not hold in practice and could materially affect false-alarm and detection performance. Performance evaluation is largely based on a single illustrative dataset plus a Monte Carlo study with fixed design choices (e.g., R=0.95, specific priors and shift patterns), so sensitivity to other settings (different R, sampling schemes, or prior elicitation errors beyond ±50%) is not fully characterized. No implementation details (software, numerical integration accuracy, computational cost) are provided, which may hinder reproducibility and practical adoption.",None stated.,"Extend the chart to handle unequal or time-varying shape parameters (monitoring $\beta_x$ and $\beta_y$ jointly) and assess robustness when the common-$\beta$ assumption is violated. Develop versions that explicitly address dependence between processes (correlated sampling or common-cause effects) and study resulting impacts on in-control ARL calibration. Provide open-source software and systematic guidance for selecting Phase I windowing/forgetting strategies (e.g., using only the most recent training samples) to balance tight limits with Phase II responsiveness.",1507.01044v1,local_papers/arxiv/1507.01044v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:53:40Z
FALSE,Multivariate|Nonparametric|Other,Other,Phase II,Theoretical/simulation only|Manufacturing (general),NA,FALSE,FALSE,Exact distribution theory|Simulation study,ARL (Average Run Length)|False alarm rate,Not discussed (examples use historical/reference sample size n=500 and then 5000 Phase II observations in simulations).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a refined empirical half-space (Tukey) depth estimator $R_n$ that remains positive and informative outside the convex hull of the data, addressing the degeneracy of the empirical half-space depth $D_n$ in the tails. The refinement uses extreme value theory to estimate tail half-space probabilities along projection directions, yielding a depth that equals $D_n$ in the central region but extrapolates smoothly in the extremes. The authors establish uniform ratio consistency of $R_n/D$ over regions where the true depth $D(x)$ can be as small as $\delta_n$ with $n\delta_n\to 0$, under univariate max-domain-of-attraction conditions (plus second-order conditions) and multivariate regular variation assumptions. Simulation studies across light- and heavy-tailed distributions show $R_n(x)/D(x)$ remains well-centered near 1 at very small depth levels where $D_n(x)$ is often zero. Applications to nonparametric multivariate SPC and DD-plot classification demonstrate that replacing $D_n$ by $R_n$ can correct severely inflated false alarm rates and improve tail/outside-hull classification performance.","Half-space depth is $D(x)=\inf_{\|u\|=1}P(u^TX\ge u^Tx)$ with empirical version $D_n(x)=\inf_{\|u\|=1}\frac1n\#\{i:u^TX_i\ge u^Tx\}$. In 1D the tail is refined via EVT: for $x\ge X_{n-k:n}$, $p^r_n(x)=\frac{k}{n}\Big(\max\{0,1+\hat\gamma\frac{x-\hat b}{\hat a}\}\Big)^{-1/\hat\gamma}$ (and similarly for the left tail), and $R_n$ equals $D_n$ in the central region and uses these tail estimators beyond. In $d\ge2$ with regular variation and $\gamma>0$, along each direction $u$ the right-tail estimator simplifies to $p_{n,u}(w)=\frac{k}{n}\left(\frac{w}{W_{n-k:n}}\right)^{-\hat\alpha}$ with $W_i=u^TX_i$, and $R_n(x)=\inf_{\|u\|=1}\{1-\hat F_u(u^Tx-)\}$ where $\hat F_u$ combines empirical and tail-extrapolated parts.","In SPC simulations with $n=500$ reference points and nominal false alarm rate $\alpha=0.0027$, the $D_n$-based depth chart yields a much higher achieved false alarm rate (due to many future points falling outside the convex hull and getting depth 0), while the $R_n$-based procedure achieves false alarm rates near the nominal level under both bivariate normal and a heavy-tailed bivariate elliptical distribution. For detection performance, average run length (ARL) boxplots under bivariate normal shifts (location, scale, and both) show the $R_n$-based procedure performs comparably to the ideal (infeasible) procedure using the true depth $D$, and close to the parametric Hotelling-$T^2$ chart when normality holds. Under the bivariate elliptical setting (non-normal), the parametric procedure fails to maintain the nominal false alarm rate, while the $R_n$-based nonparametric procedure remains well-calibrated and exhibits ARLs similar to the $D$-based benchmark.","The authors note that performance depends on the threshold/tuning parameter $k$ (an intermediate sequence), and choosing an “optimal” $k$ is a known difficult problem in extreme value statistics; they use a heuristic stability-plot approach and acknowledge it may be suboptimal. They also point out that their approximate affine invariance for $R_n$ holds only approximately (via the asymptotic ratio result), not exactly as for the true half-space depth.","The SPC use is as a depth-based outlier signal rule rather than a fully developed control-chart design (e.g., no guidance for setting control limits under parameter estimation uncertainty beyond simulations, and no treatment of serial dependence typical in process data). The method relies on extreme-value modeling assumptions (univariate domain of attraction; multivariate regular variation) that may be hard to validate in routine SPC deployments, especially for light-tailed distributions or bounded-support processes; calibration could degrade if assumptions fail. Computationally, $R_n$ still depends on minimizing over directions/half-spaces and may be expensive in higher dimensions without careful algorithms/approximations; scalability is not analyzed.","The authors suggest developing a more principled (optimal) procedure for selecting $k$ for $R_n$. They also propose investigating whether their extreme-value-theory refinement approach can be modified to refine other depth functions that suffer from empirical-count tail degeneracy, such as simplicial depth, noting such extensions may be nontrivial.","For SPC specifically, it would be valuable to extend the $R_n$-based monitoring rule to autocorrelated/streaming settings (e.g., ARMA residual monitoring) and to provide steady-state ARL or ATS results under dependence. Additional work could study robustness of $R_n$ under model misspecification of tail behavior and provide diagnostic tools for checking regular variation/domain-of-attraction assumptions in Phase I. Packaging the method in open-source software with efficient directional search/optimization and providing standard control-limit selection recipes would improve practical adoption, especially in moderate/high dimensions.",1510.08694v1,local_papers/arxiv/1510.08694v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:54:18Z
FALSE,Other,CUSUM|Other,Both,Network/cybersecurity|Other,TRUE,FALSE,NA,Simulation study|Other,Other,Not discussed,TRUE,None / Not applicable,Not provided,http://www.gs1.org/gsmp/kc/epcglobal/uhfc1g2/uhfc1g2%201%200%209-standard-20050126.pdf|http://www.rfidjournal.com/article/articleview/1080/1/1|http://www.rfidjournal.com/article/articleview/1385|http://www.rfidjournal.com/article/articleview/3211/1/1,"The paper proposes a generic framework for estimating RFID tag population in both static and dynamic settings using an extended Kalman filter (EKF), with frame size set to the current population estimate in framed-slotted ALOHA. For dynamic systems, it augments EKF with a CUSUM change-detection test on the EKF innovation to detect population changes and adapt a tuning parameter that controls filter responsiveness. The authors provide a stochastic stability analysis via Lyapunov drift, deriving closed-form sufficient conditions under which the estimation error is exponentially bounded in mean square and the relative error tends to zero as population size grows. Monte Carlo simulations for large-scale scenarios (e.g., initial populations 10^4 and 10^5) demonstrate fast convergence for static cases and stable tracking under various dynamic change patterns, attributing improved accuracy at larger scale to reduced measurement variance from longer frames. Although it uses a CUSUM control chart component, the work is primarily about state estimation for RFID systems rather than SPC/control-chart design for process quality monitoring.","The state/measurement model is $z_{k+1}=z_k$ (static) or $z_{k+1}=z_k+w_k$ (dynamic) with measurement $y_k=N_k/L_k=p(z_k)+u_k$, where $p(z_k)\approx e^{-z_k/L_k}$. The EKF update uses innovation $v_k=y_k-p(\hat z_{k|k-1})$ and Kalman gain $K_k=\frac{P_{k|k-1}C_k}{P_{k|k-1}C_k^2+R_k}$ with $R_k=\phi_k P_{k|k-1}C_k^2$. Change detection uses normalized innovation $\Phi_k=\frac{v_k}{\sqrt{(P_{k|k-1}+Q_{k-1})C_k^2+\mathrm{Var}[u_k]}}$ and CUSUM recursions $g_k^+=\max\{0,g_{k-1}^+ + \Phi_k-\Upsilon\}$, $g_k^-=\min\{0,g_{k-1}^- + \Phi_k+\Upsilon\}$, signaling when $g_k^+>\theta$ or $g_k^-<-\theta$.","Theoretical results give sufficient conditions (bounds on tunable parameters such as $Q_k$ and $\phi_k$, and initialization constraints) ensuring the estimation error is exponentially bounded in mean square and bounded with probability one in both static (Theorem 1) and dynamic cases (Theorem 2, assuming bounds on $\mathbb{E}[w_k]$ and $\mathbb{E}[w_k^2]$). Simulations are run for static and dynamic systems with $z_0=10^4$ and $10^5$ using parameters including $q=0.1$, $P_{0|0}=1$, $J=3$, $\theta=4$, $\Upsilon=0.5$, and $\phi$ values (e.g., $\phi=0.25$ and $\bar\phi=10$) and show rapid convergence in static cases even with large initial relative error (up to 0.9). For dynamic cases, the estimator remains stable for population variations ranging from on the order of $\sqrt{\hat z_{k|k-1}}$ up to large fractions (reported up to about $0.4\hat z_{k|k-1}$ for $10^4$ and $0.5\hat z_{k|k-1}$ for $10^5$) aided by CUSUM-triggered parameter adaptation. The paper also reports that larger-scale systems (e.g., $10^5$) yield more accurate/stable estimates due to reduced measurement variance from longer frames.",None stated.,"The CUSUM component is used as a heuristic change detector within an EKF tuning loop, but the paper does not frame the method as an SPC chart with in-control/out-of-control ARL design for a quality characteristic; thus standard SPC design comparisons and guidance are missing. The approach relies on approximations valid for large $L_k$ and $z_k$ (e.g., normal approximation for idle slots and $p(z)\approx e^{-z/L}$) and assumes independence/whiteness of the normalized innovation for CUSUM calibration, which may be violated in practice. No software or implementation details are provided, which limits reproducibility and practical adoption.",They plan to extend the theoretical framework to tag estimation problems with multiple readers with overlapping coverage areas.,"A useful extension would be a more formal sequential change-detection design for the CUSUM component (e.g., calibrating $\theta$ and $\Upsilon$ to target false-alarm/delay metrics under realistic innovation dependence) and robustness to model mismatch in the idle-slot probability approximation. Providing open-source implementations and benchmarking against modern Bayesian filters/particle filters or other adaptive estimators on real RFID traces would strengthen practical validation.",1511.08355v1,local_papers/arxiv/1511.08355v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:54:52Z
TRUE,Multivariate|Nonparametric|High-dimensional|Other,Machine learning-based|Other,Both,Energy/utilities|Theoretical/simulation only|Other,FALSE,NA,FALSE,Simulation study|Case study (real dataset)|Other,Expected detection delay|Other,"Phase I dataset size in the Tennessee Eastman example is 125,352 in-control observations; Phase II has 4,598,620 observations. For the sampling-based SVDD training, they recommend sample size as v+1 where v is the number of variables (e.g., v=41 so sample size=42).",TRUE,MATLAB|SAS,Not provided,http://depts.washington.edu/control/LARRY/TE/download.html|http://blogs.sas.com/content/subconsciousmusings/2015/10/09/multistage-modeling-delivers-the-roi-forinternet-of-things/--is-epub/,"The paper proposes a new SVDD-based control chart for high-frequency multivariate sensor data, called the $K_T$ chart, intended for non-normal/unknown distributions where Hotelling’s $T^2$ is inappropriate. Instead of monitoring each individual SVDD kernel-distance (the original K-chart), it summarizes each sliding window of observations by an SVDD center $a$ (central tendency) and threshold $R^2$ (dispersion), producing two companion charts: an $a$-chart and an $R^2$-chart. Because SVDD training can be computationally expensive for large windows, the authors use a sampling-based fast SVDD training approach to build window models efficiently and robustly. They illustrate the approach on the Tennessee Eastman chemical process data (simulated from published MATLAB code), showing that most injected faults are detected via either the $a$ or $R^2$ chart, with a few faults not detected. The contribution advances SPC for IoT/big-data monitoring by turning SVDD-based monitoring into interpretable, window-level charts for both location and spread with feasible computation.","SVDD uses a kernelized distance for scoring: $\mathrm{dist}^2(z)=K(z,z)-2\sum_i \alpha_i K(x_i,z)+\sum_{i,j}\alpha_i\alpha_j K(x_i,x_j)$, and flags outliers when $\mathrm{dist}^2(z)>R^2$. The proposed $K_T$ charts compute, for each sliding window, an SVDD center $a_q$ and threshold $R_q^2$; they then plot $R_q^2$ on an $R^2$ chart and plot $\mathrm{dist}^2(a_q)$ (distance of each window center from a Phase I reference center $a^*$) on an $a$ chart. Control limits given are: for the $a$ chart, LCL=0 and UCL=$R_a^2$; for the $R^2$ chart, UCL=$\bar{R^2}+3\sigma_{R^2}$, CL=$\bar{R^2}$, LCL=$\bar{R^2}-3\sigma_{R^2}$ (optional warning limits at $\pm 2\sigma_{R^2}$).","In the Tennessee Eastman study, Phase I used 125,352 normal observations and Phase II contained 4,598,620 observations with faults interleaved. Using window size 10,000 and overlap 3,000, Gaussian bandwidth $s=25.5$ (Peak criterion), outlier fraction $f=0.001$, and sampling size 42, the $K_T$ charts detected all faults except faults 14, 15, and 19 using either the $R^2$ chart or the $a$ chart. Reported detection delays are summarized in numbers of windows: delay 1 window for faults 4,5,7; delay 2 for fault 1; delay 3 for fault 6; delay 4 for fault 16; delay 5 for faults 2,3,10,11,12,13,18; delay 6 for faults 8,9,14,15,19,20. The authors also report “magnitude performance gains” from sampling-based training versus full-window SVDD, but do not provide detailed timing numbers in the excerpt.","The authors note that the original SVDD K-chart is an individual-value chart and provides no clear guidelines for interpreting kernel-distance plots to infer mean shifts or variation changes, which limits usability in high-frequency/big-data settings. They also emphasize that full SVDD training is computationally expensive for large/high-frequency windows, motivating their sampling-based approach. They report that in their TE evaluation, the method could not identify some faults (faults 14, 15, and 19).","The approach depends heavily on choices of window length/overlap, kernel bandwidth $s$, and outlier fraction $f$; while guidelines are discussed, there is no principled, end-to-end design guaranteeing in-control false alarm properties (e.g., target ARL/ATS). Control limits for the $R^2$ chart use a normal-theory style $\pm 3\sigma$ rule on window-level $R^2$ values, but the distribution of $R^2$ (and dependence induced by overlapping windows) is not characterized, so achieved false alarm rates may vary. The method’s “center” $a$ is defined in feature space via SVDD and may be less interpretable than a mean vector; diagnosing which variables drive signals is not addressed in the current work. Evaluation is limited to the Tennessee Eastman benchmark (simulated from existing code) and may not reflect real sensor noise, drift, missingness, or strong autocorrelation typical in IoT streams.",They plan to develop a methodology to identify the variables responsible for changes in process center and spread signaled by the $K_T$ charts. They also state they are researching a new process capability index based on $K_T$ statistics such as the SVDD threshold $R^2$ and center $a$.,"Developing calibrated control limits to meet specified in-control ARL/false-alarm rates under overlapping, autocorrelated windows would improve practical deployability. Extending the method to explicitly handle serial dependence (e.g., residual-based monitoring or block bootstrap calibration) and missing/irregular sampling common in sensor streams would broaden applicability. Providing open-source implementations (e.g., R/Python) and benchmarking computational cost/latency on edge hardware would support adoption. A robust, data-driven procedure for choosing kernel bandwidth and window parameters jointly (possibly adaptive over time) would strengthen performance and reduce reliance on expert tuning.",1607.07423v3,local_papers/arxiv/1607.07423v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:55:34Z
TRUE,Univariate|Multivariate|Bayesian,EWMA|CUSUM,Both,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,NA,Simulation study|Economic design,ARL (Average Run Length)|Other,Not discussed (uses benchmark instances; sampling interval h and sample size n are decision/design parameters but no general minimum Phase I guidance is provided).,TRUE,MATLAB,Personal website,https://www.dropbox.com/s/vva7yd3d8y0qqy2/SimulationCodeCostMEWMA.m?dl=0|https://www.dropbox.com/s/xdqz4z4mw4m7qta/AARL1MEWMA.m?dl=0|https://www.dropbox.com/s/kpr3nmert92xybv/ANFAMEWMA.m?dl=0,"The paper critiques the widely used Lorenzen–Vance (1986) long-run expected average cost formula for economic design and shows it is generally incorrect for memory-type control charts (e.g., EWMA/MEWMA, CUSUM, and Bayesian charts) because successive charting statistics are dependent. It provides a simulation-based method to estimate the long-run average cost directly via renewal-cycle simulation, avoiding reliance on only in-control and out-of-control ARLs. Through extensive numerical experiments for EWMA and MEWMA charts (including many benchmark instances), it demonstrates that even with very accurate ARL estimates, the Lorenzen–Vance formula can misestimate costs by up to about 20% and lead to substantially inferior economic designs, with additional costs reported up to roughly 45%. The authors derive a corrected (modified) cost formula for memory-type charts that replaces single ARL quantities with averages over conditional, history-dependent out-of-control run lengths and includes the average number of false alarms per cycle. They conclude the modified formula is not computationally convenient in practice and recommend simulation-based optimization, and they call for reappraisal of prior economic-design studies that used the flawed formula for memory-type charts.","The objective is the long-run expected average cost $F=\lim_{T\to\infty} \mathbb{E}\left(\frac{\int_0^T C(t)\,dt}{T}\right)$ and is estimated by cycle simulation as $\hat F=\frac{\sum_{i=1}^N C_i}{\sum_{i=1}^N T_i}$. The paper analyzes the Lorenzen–Vance economic-design formula that plugs in $ARL_0$ and $ARL_1$ for in- and out-of-control behavior (their Eq. (4)), and shows it fails for memory-type charts due to dependence across sampling epochs. A modified formula (their Eq. (11)) replaces $1/ARL_0$ and $ARL_1$ terms with quantities based on the average number of false alarms per cycle (ANFA) and an averaged out-of-control run length $AARL_1=\sum_{m\ge1} \Pr(A_m)\,ARL_{1|A_m}$, where $A_m$ denotes the assignable-cause occurrence between sampling epochs $m$ and $m+1$.","Across 36 benchmark instances for EWMA and MEWMA charts, the absolute percentage difference between costs computed using the Lorenzen–Vance formula and costs estimated via direct cycle simulation reaches about 20% for smaller smoothing parameters $r$ (e.g., $r=0.05$). When choosing economically optimal $r$ (with control limit fixed at $\sqrt{\chi^2_{q,\alpha}}$), using the Lorenzen–Vance formula can yield substantially different optima and increases in expected cost ranging from about 0.20% up to 45.64% (EWMA) and about 0.26% up to 44.55% (MEWMA). For the non-memory special case $r=1$ (equivalent to Shewhart/Hotelling-type charts), the formula aligns closely with simulation (reported maximum deviations around 0.35%), validating that the issue is specific to memory-type dependence. The modified formula (Eq. (11)) produces values much closer to simulation on selected hard instances (e.g., $M_4, M_5, M_9, M_{16}, M_{18}$) but is computationally heavy because it requires estimating multiple conditional run-length quantities.","The authors note that although they derive a corrected Lorenzen–Vance-style formula for memory-type charts, it “cannot be a basis for an efficient computational method” because it requires estimating many history-conditional out-of-control ARLs and related quantities (e.g., via simulation), which is time-consuming. They also state that, for brevity, they do not present analogous numerical demonstrations for CUSUM and Bayesian charts, though the same issue applies.","The work focuses on mean-shift scenarios under (multivariate) normality and an exponential time-to-failure model; performance and conclusions for other shift types (variance, distributional shape) or non-exponential failure mechanisms are not established. Practical implementation guidance for selecting truncation level $k$ (when approximating the infinite sum in the modified formula) is only sketched and may be nontrivial in real applications. The paper does not provide a full, efficient alternative to simulation (e.g., variance-reduced simulation, surrogate modeling, or optimization heuristics with guarantees) despite emphasizing computational burden.","The authors suggest that results in prior economic-design papers that relied on the Lorenzen–Vance formula for memory-type charts should be reappraised using accurate methods (e.g., their proposed simulation approach). They also identify an open research direction: developing a more efficient computational method than brute-force simulation for evaluating/optimizing economic designs of memory-type control charts.","Develop variance-reduction and efficient optimization frameworks (e.g., stochastic approximation, Bayesian optimization, or surrogate-assisted search) to reduce the simulation cost of economic design for memory-type charts. Extend the corrected economic-design approach to non-normal, autocorrelated, and non-stationary processes, and to other memory-type charts (CUSUM/Bayesian) with comprehensive benchmarks and real case studies. Provide implementable software (e.g., an R/Python package) and practical guidelines for choosing truncation levels and ensuring numerical stability when estimating ANFA and conditional ARLs.",1708.06160v1,local_papers/arxiv/1708.06160v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:56:14Z
TRUE,Univariate|Other,Shewhart,Both,Manufacturing (general)|Theoretical/simulation only|Other,TRUE,FALSE,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length)|False alarm rate,"For estimating in-control parameters/limits, the authors study m between 5 and 25 Phase I samples and report negligible bias for m ≥ 20 (relative bias ≤ 0.001) for k = 3,4,5. In the real-data illustration they use m = 25 samples to estimate limits, then 75 samples for Phase II monitoring.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes Shewhart-type control charts for monitoring a normal-process mean using neoteric ranked set sampling (NRSS), a ranked-set design that orders a single set of k^2 units and then measures k strategically spaced ranks to improve estimation efficiency. Control limits are centered at the in-control mean and use the variance of the NRSS sample mean, which under perfect ranking can be computed from order-statistic variances/covariances; under imperfect ranking the variance components are obtained via a large preliminary simulation. Chart performance is evaluated primarily by Monte Carlo average run length (ARL), with limits tuned (under perfect ranking) to match in-control ARL0 ≈ 370.5 (the standard 3-sigma Shewhart benchmark) and compared against SRS, RSS, ERSS, and MRSS charts. Across simulated shift sizes and sample sizes (k = 3,4,5), NRSS charts generally yield smaller out-of-control ARL than the competing single-cycle ranked-set designs, with performance degrading as ranking quality (correlation with an auxiliary variable) decreases. A concrete compressive-strength dataset illustrates Phase I estimation of limits and Phase II monitoring, showing more out-of-control signals for NRSS/RSS than SRS when an artificial mean shift is introduced.","NRSS chart uses the sample mean $\bar{Y}_{\mathrm{NRSS}}=\frac{1}{nk}\sum_{j=1}^n\sum_{i=1}^k Y_{[(i-1)k+l]j}$ with variance $\mathrm{Var}(\bar{Y}_{\mathrm{NRSS}})=\frac{1}{nk^2}\sum_{i=1}^k\mathrm{Var}(Y_{[(i-1)k+l]})+\frac{2}{nk^2}\sum_{1\le i<i'\le k}\mathrm{Cov}(Y_{[(i-1)k+l]},Y_{[(i'-1)k+l]})$. Control limits are Shewhart-style: $\mathrm{LCL}=\mu_0-A\sqrt{\mathrm{Var}(\bar{Y}_{\mathrm{NRSS}})}$, $\mathrm{CL}=\mu_0$, $\mathrm{UCL}=\mu_0+A\sqrt{\mathrm{Var}(\bar{Y}_{\mathrm{NRSS}})}$; with unknown parameters they replace $\mu_0$ by $\bar{\bar{Y}}_{\mathrm{NRSS}}$ and $\mathrm{Var}(\bar{Y}_{\mathrm{NRSS}})$ by an estimator computed from $m$ Phase I samples (Eqs. 10–14).","Under perfect ranking ($\rho=1$) and matched ARL0 ≈ 370.5, NRSS yields consistently smaller ARL1 than SRS/RSS/ERSS/MRSS; e.g., for k=5, δ=0.40: SRS 200.08 vs NRSS 60.14; for k=3, δ=0.80: SRS 71.55 vs NRSS 21.25; for k=5, δ=1.60: SRS 12.38 vs NRSS 1.46. Under imperfect ranking with A=3, ARL decreases (improves) as correlation increases; for k=3, δ=0.8: ARL 59.69 (ρ=0.50), 43.59 (ρ=0.75), 31.15 (ρ=0.90), 21.61 (ρ=1.00). Phase I variance-estimator bias was found negligible for m ≥ 20 (relative bias ≤ 0.001) for k=3–5. In the concrete-strength illustration (ρ≈0.50), for k=3 and an injected shift δ=1.2, NRSS produced 13 points beyond limits vs RSS 6 and SRS 2.","They note that under imperfect ranking, NRSS efficiency decreases as the correlation between the auxiliary and study variable decreases (ranking errors reduce gains). They also point out an operational drawback: ranking $k^2$ units in a single set (NRSS) may be complicating if ranking is done by visual judgment, though less problematic when using an auxiliary variable.","The proposed chart is essentially a Shewhart mean chart; it is not optimized for very small shifts compared with memory charts (CUSUM/EWMA), and no direct comparison to CUSUM/EWMA is provided for the normal setting. The imperfect-ranking variance needed for limits is obtained via a large preliminary simulation, which may be cumbersome in practice and depends on correctly specifying the joint model (e.g., bivariate normal and correlation). The paper focuses on mean shifts with (implicitly) independent observations; robustness to autocorrelation, heavy tails, and model misspecification is not analyzed.","They suggest, as future work, developing NRSS designs based on two or more ordering cycles (motivated by performance comparisons and the operational trade-offs with double-ranked set designs).",Develop self-starting/adaptive NRSS charts that avoid extensive pre-simulation for imperfect ranking and provide online variance estimation with guaranteed in-control performance. Extend NRSS-based monitoring to autocorrelated processes and to robust/nonparametric variants that reduce sensitivity to non-normality and outliers. Compare NRSS mean charts directly against NRSS-based CUSUM/EWMA designs (or develop them) to improve sensitivity to small sustained shifts and provide practical design guidance/software.,1709.04979v1,local_papers/arxiv/1709.04979v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:56:54Z
TRUE,Image-based monitoring|Other,EWMA|Change-point,Phase II,Network/cybersecurity|Service industry|Theoretical/simulation only|Other,NA,TRUE,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),"Not discussed (case study uses 30 initial weeks aggregated for initialization and weeks 31–60 as in-control reference, but no general sample size guidance is provided).",TRUE,MATLAB,Not provided,NA,"The paper proposes a statistical process control framework for quick detection of abrupt changes in dynamic streams of attributed networks. Each network snapshot is modeled via a generalized linear model (GLM) relating edge weights/probabilities to edge attributes, and network evolution is captured by a state-space model on the GLM coefficients with online estimation via an Extended Kalman Filter (EKF). Change detection is performed by computing one-step-ahead Pearson residuals between the observed and predicted adjacency matrices and monitoring the mean residual with an EWMA control chart. Performance is assessed via Monte Carlo simulations for both binary (Bernoulli-logit) and weighted (Poisson-log) networks, showing substantially reduced out-of-control ARL versus static GLM and sliding-window baselines, especially for small shifts. A case study on the Enron email network demonstrates detection of anomalous periods consistent with known events in Enron’s history.","Observation model: for each edge, $w_{ij,t}\sim f(\theta_{ij,t})$ with $\theta_t=g(X_t\beta_t)$ (GLM link $g$ for Bernoulli/Poisson, etc.). State evolution: $\beta_t=F\beta_{t-1}+\xi+\varepsilon_t$ with Gaussian process noise. One-step prediction $\hat w_{t|t-1}=g(X_t\beta_{t|t-1})$; Pearson residuals $r_{i,t}=(w_{i,t}-\hat w_{i,t|t-1})/\sqrt{\mathrm{var}(\hat w_{i,t|t-1})}$; monitored summary $\bar r_t=\frac{1}{m}\sum_i r_{i,t}$. EWMA: $z_t=\lambda\bar r_t+(1-\lambda)z_{t-1}$ with time-varying limits $\pm l\, s\sqrt{\frac{\lambda}{1-\lambda}(1-(1-\lambda)^{2t})}$.","Control limits were tuned by Monte Carlo to achieve $\mathrm{ARL}_0=200$; for example, the dynamic model used $(l,\lambda)=(2.44,0.1)$ for binary and $(2.53,0.1)$ for weighted networks (Table 1). In simulations (2000 replications), the dynamic method yielded much smaller $\mathrm{ARL}_1$ than static and sliding-window baselines for small changes; e.g., for a binary network global change with magnitude $\delta=1.5$, dynamic $\mathrm{ARL}_1=8.57$ (SE 0.144) versus static 113.27 (SE 3.869) and sliding 59.32 (SE 2.770). For larger shifts (about $\delta\ge 2.5$), methods became comparable, though the dynamic method often remained slightly faster. In the Enron case study, the EWMA chart signaled out-of-control between weeks 79 and 89, aligned with the period when the Enron scandal was revealed, and a focused CEO–CEO subgraph chart detected two jumps (weeks ~76 and ~89).","The authors note that detecting local changes in binary networks can be difficult due to limited data, leading to reduced detection power for all methods in that setting. They also assume in Phase II that the in-control model and initial parameters (e.g., $F$, $\xi$) are known or can be estimated from an in-control sequence.","The monitoring statistic uses the average Pearson residual across all edges, which may dilute sensitivity to sparse/local anomalies unless additional subgraph-focused monitoring is performed. The approach assumes conditional independence of edges given attributes and the state, and model misspecification (incorrect link/distribution, omitted attributes, dependence between edges) could affect false-alarm and detection performance. Parameter/design choices (e.g., how $F,Q$ are set/estimated, and Monte Carlo tuning for $l,\lambda$) may be nontrivial in practice and are not packaged into a fully automated procedure. No treatment is provided for missing edges/attributes or irregular sampling, which are common in real network streams.",They suggest combining community detection techniques with the proposed methodology to detect changes in community structures over time.,"Extend the framework to explicitly monitor localized changes (e.g., scan/subgraph EWMAs, multiscale monitoring) rather than relying primarily on global residual averages. Develop robust/nonparametric variants and investigate sensitivity to model misspecification and edge dependence. Provide systematic Phase I procedures for estimating $F,Q,\xi$ (including diagnostics for nonstationarity) and quantify the impact of estimation error on $\mathrm{ARL}_0/\mathrm{ARL}_1$. Release an implementation (e.g., R/Python) and benchmark computational scaling on large networks, including strategies for dimensionality reduction or sparsity exploitation.",1711.04441v1,local_papers/arxiv/1711.04441v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:57:32Z
TRUE,Univariate|Multivariate|Bayesian|Other,Other,Both,Theoretical/simulation only,NA,NA,NA,Other,Other,Not discussed,FALSE,None / Not applicable,Not applicable (No code used),NA,"This note critiques broad claims in the SPC literature that Bayesian control charts with a posterior-probability threshold (control-limit) policy are generally economically optimal. It argues that the optimality result of Girshick and Rubin (1952) holds only for a special discrete-time production setting with 100% inspection and should not be generalized to more general inspection/decision settings. The authors point to Taylor (1965) as providing an analytical counterexample showing non-optimality of a Bayesian threshold rule in the non-100% inspection case, contradicting later claims/citations (e.g., Calabrese 1995; Makis 2008, 2009). The paper further disputes assertions that posterior probabilities are sufficient to guarantee optimal threshold-type Bayesian control in the broader POMDP class, noting that cited sources do not provide the claimed proof in this context. It concludes that characterizing the class of optimal quality-control (Bayesian) policies for these settings remains open and that existing threshold-based Bayesian control charts cannot generally be optimal.",Not applicable,"No new control chart is proposed and no ARL/ATS or numerical comparisons are reported; the main result is a conceptual/methodological critique supported by citation analysis. The key substantive claim is that Taylor (1965) provides a counterexample establishing that a Bayesian control-limit policy is not optimal in the non-100% inspection version of the Girshick and Rubin (1952) model, undermining later statements that Bayesian charts are generally optimal. The paper also asserts that commonly cited POMDP references are not shown to imply optimality of threshold-type Bayesian control charts for the quality-control problems discussed.",None stated,"Because this is a short note, it does not formalize a general theorem delineating when Bayesian threshold policies are or are not optimal, nor does it reproduce/extend Taylor’s counterexample in the paper’s own notation to make the argument self-contained. It also does not provide empirical/simulation evidence showing practical performance consequences (e.g., cost or detection-delay impacts) of non-optimality under realistic parameterizations.","The authors state that an open area is to characterize the class of optimal quality-control charts/policies for the discrete-time or continuous-time production systems studied in Girshick and Rubin (1952), Calabrese (1995), Makis (2008), and related work. They also state that characterizing optimal controls for the broader class of partially observable Markov decision processes (POMDPs) covering these quality-control problems remains a future research direction.","Developing explicit sufficient conditions (on cost structure, inspection scheme, state dynamics, and observation model) under which a posterior-threshold policy is optimal would make the critique operational for practitioners. Constructing modern computational benchmarks (dynamic programming/POMDP solvers) comparing optimal policies against standard Bayesian control-limit charts across representative SPC scenarios would quantify the practical gap and guide approximate policy design.",1712.02860v2,local_papers/arxiv/1712.02860v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:57:48Z
TRUE,Univariate|Self-starting|Nonparametric|Bayesian,CUSUM|Other,Both,Manufacturing (general)|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|ATS (Average Time to Signal)|Other,"Self-starting scheme requires at least m \ge 2d-1 reference observations for Theorem 1 to hold exactly; simulations suggest m=20 works well for d \in {10,20,30,40}. A warm-up period of 20 observations is used before monitoring in simulations and the real-data example.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a distribution-free, nonparametric adaptive CUSUM control chart designed to detect arbitrary distributional changes (not limited to mean or variance shifts). It converts observations into categorical indicators using in-control quantile-based binning, then constructs two CUSUM statistics that preserve ordering information via cumulative unions: one targets left-to-right ordering (more sensitive to location changes) and the other targets center-outward ordering (more sensitive to scale changes). To avoid specifying an out-of-control distribution, it adaptively estimates multinomial cell probabilities using post-reset data and Dirichlet-prior smoothing, yielding a self-starting recursive scheme that updates required quantiles sequentially when reference data are limited. Control limits are obtained by Monte Carlo to match desired in-control ARL (ARL0), and the resulting chart is shown via simulation to be competitive or better overall than nonparametric change-point detection (CPD) charts while being computationally simpler. A real manufacturing dataset (aluminium electrolytic capacitor capacitance) illustrates detection and a built-in post-signal diagnostic that indicates which type of change (positive/negative location, scale up/down) triggered the alarm.","Data are binned into d categories using in-control quantiles to form Y_t^{(i)} and cumulative indicators Z_{t,j}^{(i)}=\sum_{\ell\le j}Y_{t,\ell}^{(i)} for i\in{1,2} (left-to-right vs center-outward binning). The (non-adaptive) CUSUM updates are S_t^{(i)}=\max\{0,S_{t-1}^{(i)}+\sum_{j=1}^{d-1}\frac{d^2}{j(d-j)}[Z_{t,j}^{(i)}\log(\frac{\sum_{\ell\le j}p_\ell^{(i)}}{j/d})+(1-Z_{t,j}^{(i)})\log(\frac{1-\sum_{\ell\le j}p_\ell^{(i)}}{1-j/d})]\}, and the combined chart uses S_t=\max(S_t^{(1)},S_t^{(2)}). The adaptive version plugs in \hat p_{t,\ell}^{(i)}=(\alpha_\ell+N_{t,\ell}^{(i)})/(\sum_{j=1}^d\alpha_j+N_t^{(i)}) with N counts accumulated since the last time the corresponding CUSUM reset to 0, and signals when \hat S_t=\max(\hat S_t^{(1+)},\hat S_t^{(1-)},\hat S_t^{(2+)},\hat S_t^{(2-)})>h.","Control limits h for target ARL0 are computed by Monte Carlo (10,000 replications) and reported for d=10,20,30,40 and ARL0 \in {200,370,500,1000} (e.g., for ARL0=370: h=105.941, 218.886, 333.933, 449.201 respectively). Self-starting ARL0 simulations under N(0,1), t(2.5), and LN(1,0.5) show ARL0 close to nominal for d=10 or 20 even with m=10, while for larger d (30–40) m needs to be larger; m=20 performs well across tested settings. Extensive 10,000-replication studies report out-of-control performance (ARL1/average detection time) for location shifts, scale shifts, and 8 general distribution-change scenarios; overall the proposed chart (recommended d=20) is competitive with or better than CvM and Lepage CPD charts across a broad range, particularly strong for scale decreases where Lepage is weaker. In a real aluminium electrolytic capacitor dataset (200 observations), with ARL0=500 and d=20, the chart signals at observation 188, and the diagnostic indicates the alarm is mainly driven by the negative-location component \hat S_t^{(1-)}.","The paper notes that when the available reference size m is smaller than 2d-1, the in-control distribution of the self-starting categorized variables is not exactly Multi(1;1/d,…,1/d) for early times (t < 2d - m), though the authors expect the impact on ARL0 to be negligible if 2d-m is not large. It also remarks that categorization can lose some rank/order information, which can make the approach slower than rank-based CPD methods for very large location shifts.","Control limits are obtained via simulation and depend on chosen design parameters (notably d and target ARL0), but no ready-to-use implementation or guidance for fast/accurate limit computation in practice is provided. The method assumes independent observations; there is no treatment of autocorrelation, which is common in many continuous-process monitoring settings and can severely distort ARL. The post-signal diagnostic is heuristic (based on which component exceeded h) and the paper does not quantify diagnostic accuracy (e.g., confusion rates) under mixed or subtle distributional changes.","The authors state they plan to further evaluate the performance of related derived charts obtained by monitoring subsets/maxima of the component statistics (e.g., only \max(\hat S_t^{(1+)},\hat S_t^{(1-)}) for location, or \max(\hat S_t^{(2+)},\hat S_t^{(2-)}) for scale, or excluding scale-decrease components when not of interest).","Extend the framework to handle serial dependence (e.g., via residual-based monitoring, block bootstrapping for limits, or explicit time-series modeling) and study robustness of ARL under autocorrelation. Develop principled, data-driven selection of d (or multi-resolution/adaptive binning) and quantify the tradeoff between sensitivity to small vs large changes. Provide software and computational accelerations for control-limit calibration and for real-time implementation, and evaluate diagnostic accuracy formally under composite changes (simultaneous location+scale+shape shifts).",1712.05072v1,local_papers/arxiv/1712.05072v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:58:36Z
TRUE,Univariate|Other,EWMA|Other,Both,Manufacturing (general),TRUE,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate|Other,Not discussed.,TRUE,Other,Not provided,NA,"The paper presents an SPC case study for monitoring a single manufacturing process that produces many different products whose measurement scales differ substantially. It proposes standardizing measurements within product (by product-specific center and spread), akin to short-run SPC, so that all products can be monitored on a single chart. Phase II monitoring is performed using Individuals–Moving Range (IR) charts and EWMA charts on the standardized values, and performance is assessed via simulation using Average Run Length (ARL) under in-control (ARL0) and out-of-control (ARL1) conditions. A simulation study compares multiple standardization choices (mean vs. median; SD vs. robust SD vs. IQR vs. MAD) and recommends median plus robust SD (M-estimator) as a compromise yielding realistic ARL0 and good sensitivity (lower ARL1). When signals occur, the paper adds a root-cause analysis step using a partition tree (decision tree) on process/context variables (e.g., tooling, raw materials, date) to suggest likely drivers of the detected deviation.",Standardization is defined as $y^{(s)}_{ij}=(y_{ij}-\bar y_i)/s_i$ or robustly $y^{(r)}_{ij}=(y_{ij}-\tilde y_i)/s_i^{(r)}$ (median for center and a robust spread estimator such as M-estimator SD). EWMA smoothing is $z_t=\lambda y_t+(1-\lambda)z_{t-1}$ with $z_0:=\bar y$ and the paper uses $\lambda=0.2$. IR-chart limits are based on moving ranges (formulas referenced to Montgomery).,"For IR charts under a stable process, simulated ARL0 varies widely by standardization; with no outlier contamination, median + robust SD gives ARL0 ≈ 391.8 (closest to the nominal ~370), while mean + robust SD gives ARL0 ≈ 463.6 and SD-based variants are much larger (≈ 676–778). With 1% outliers, ARL0 drops substantially (e.g., median + robust SD ≈ 151.2; mean + robust SD ≈ 157.7), and MAD/IQR produce too many false alarms (e.g., ARL0 ≈ 76–79). Under injected mean shifts affecting subsets of products (root causes A/B/C) with magnitudes 2–8$\sigma_i$, median + robust SD generally yields lower ARL1 than SD-based standardization (e.g., for root cause B at 4$\sigma$: ARL1 30.9 vs. 110.1 for SD). The chosen workflow is then demonstrated on a simulated example where the partition tree identifies the correct tooling factor (including the simulated problematic tool) as a key split.","The authors note an imprecision: after within-product standardization, the transformed observations are no longer independent and the induced correlation structure is not explicitly accounted for, especially when products have few lots. They state this has not led to undesired behavior in practice so far but remains open for justification or modification, particularly for small sample sizes per product.","The approach relies on normality and (aside from the acknowledged induced dependence) uses standard IR/EWMA limits that assume i.i.d. behavior; performance may degrade with strong time-series autocorrelation or nonstationarity beyond simple shifts. The root-cause analysis uses observational covariates and a single partition tree; without validation/regularization or alternative models, it may be unstable and susceptible to confounding, and variable-importance uncertainty is not quantified. The “joint chart” standardization may mask product-specific special causes or changes in within-product variance, and it does not explicitly model differing measurement error across products or varying lot counts when estimating per-product parameters.","The authors suggest investigating justification or modification to address the induced correlation from standardization, particularly for products with small numbers of production lots, and potentially incorporating more advanced short-run methods (citing Q-chart enhancements). They also note that many other control chart types could be applied to the standardized data and identify this as a potential area for future research.","Develop self-starting or hierarchical/Bayesian standardization that borrows strength across products while accounting for uncertainty in per-product center/spread estimates, especially for very small $n_i$. Extend the monitoring to explicitly handle autocorrelation and irregular production patterns (e.g., model-based residual charts or state-space approaches) and evaluate steady-state ARL/ATS. Provide a more rigorous causal/diagnostic layer (e.g., stability selection, permutation tests, or causal discovery/propensity adjustments) and release reproducible code and a de-identified benchmark dataset to support adoption and comparative studies.",1801.01660v1,local_papers/arxiv/1801.01660v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:59:07Z
TRUE,Univariate|Nonparametric|Other,Shewhart|Change-point|Other,Phase II,Manufacturing (general)|Theoretical/simulation only,FALSE,FALSE,NA,Simulation study|Markov chain|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Other,"Uses reference (historical) data size $m_0=20$ in simulations; piston rings example uses 25 Phase I samples (subgroups of size 5) to set up Phase II. Also reports sensitivity checks with $m_0=30,50,100$.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes exact distribution-free runs- and patterns-type control charts for monitoring an unknown target value (location parameter) for both continuous and discrete data. Observations are converted to Bernoulli indicators via a threshold, and monitoring is performed using runs-type statistics—studied in detail via the longest run and scan statistics—under a conditioning-on-the-total-number-of-ones framework that removes dependence on the underlying distribution. Time-varying, data-dependent control limits are computed exactly using the finite Markov chain imbedding (FMCI) technique to obtain conditional distributions, ensuring the in-control run length is geometric with parameter $\alpha$ and thus $\text{ARL}_0=1/\alpha$ exactly. Monte Carlo studies (normal, heavy-tailed t, and skewed gamma) show robustness of $\text{ARL}_0$ and illustrate how threshold/window choices affect detection performance ($\text{ARL}_1$). A piston rings case study (Phase II) demonstrates the longest-run chart can signal an upward shift around samples 9–14 (average signal time about 11.86 in repeated runs).","Data are thresholded to Bernoulli indicators, e.g., $X_n=\mathbf{1}(Y_n\ge c)$. The scan statistic is $S_n(r)=\max_{1\le t\le n-r+1}\sum_{i=t}^{t+r-1}X_i$, and the longest-run statistic is $L_n$ (max run of 1s). Control limits are data-dependent and chosen to satisfy conditional signaling constraints: for scan charts, $P(S_n(r)\ge c_n(\alpha)\mid N_n)\le \alpha$ (and a conditional version given past non-signal); for longest-run charts, $P(L_n\ge k_n(\alpha)\mid N_n)\le \alpha$ (and its conditional counterpart). Exact conditional probabilities are computed via FMCI; e.g., $P(S_n(r)<s\mid \sum_{i=1}^nX_i=m)=\xi_0\prod_{t=1}^n N_t(m)\,\mathbf{1}^\top$.","Simulations set $\alpha=0.005$ to target $\text{ARL}_0=200$ using 1000 sequences from standardized $N(0,1)$, $t(4)$, and $\text{Gamma}(1,1)$; empirical $\text{ARL}_0$ values remain near 200 across distributions and thresholds (e.g., for R-2 charts: around 193.7–226.0 depending on $c$ and distribution). For R-2 under $N(0,1)$, $\text{ARL}_1$ depends strongly on threshold: for mean shifts $\mu=1,2,3$, the best-performing $c$ among those tested is approximately $0,1,2$, respectively (e.g., at $\mu=2$, $\text{ARL}_1=7.39$ for $c=1$ vs 77.23 for $c=3$). For R-1 scan charts with $c=2$, larger window sizes help small shifts (e.g., $\mu=2$: $\text{ARL}_1=10.25$ for $r=8$ vs 14.61 for $r=6$), while smaller windows help larger shifts (e.g., $\mu=3$: $\text{ARL}_1=3.88$ for $r=6$ vs 4.39 for $r=8$). In the piston rings Phase II example with $\alpha=1/400$, the chart signals as early as $t=9$ (no later than $t=14$) and averages about $t\approx 11.86$ over 100 repetitions.","The authors note a drawback: if the chart fails to detect a signal early, performance can deteriorate because the procedure “adapts” to the shifted process after time $t$, requiring stronger evidence to signal later. They suggest mitigation by restarting monitoring, using a large reference sample, or using a lagged conditioning approach to reduce adaptation.","The approach hinges on i.i.d. sampling; there is no developed treatment of autocorrelation/common in SPC data streams (beyond noting related work that can handle Markov dependence). Control limits are data-dependent and require computing many conditional probabilities via FMCI; this may be computationally intensive for large $n$, large windows $r$, or more complex patterns, and practical implementation details/software are not provided. The method reduces data to binary indicators using a threshold, which can discard magnitude information and may be less efficient than rank- or score-based nonparametric charts when shifts are subtle or when selecting $c$ is difficult.","They propose pursuing a “lagged” version of the conditioning argument where statistics at time $t$ depend on time $t-k$ (for $1<k<t$) rather than $t-1$, to prevent the chart from adapting to a shifted process and thereby mitigate late-stage performance deterioration.","Develop explicit extensions for serial dependence (e.g., ARMA errors) or batch/overlapping subgroup structures common in industrial SPC, with guarantees for conditional false-alarm control. Provide guidance and algorithms for optimal/adaptive selection of $(c,r,k)$ (threshold, window, run parameter) under varying shift sizes, possibly using economic or steady-state criteria. Release implementation code (e.g., an R/Python package) and include computational complexity benchmarks to facilitate practitioner uptake and reproducibility.",1801.06532v2,local_papers/arxiv/1801.06532v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T22:59:47Z
TRUE,Univariate|Self-starting|Other,CUSUM|Change-point,Phase II,Network/cybersecurity|Service industry|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Expected detection delay,Not discussed.,TRUE,R,Not provided,https://dev.twitter.com/docs/streaming-apis|https://www.R-project.org/|https://cran.r-project.org/package=rtweet|https://CRAN.R-project.org/package=stringr|https://CRAN.R-project.org/package=glue|https://CRAN.R-project.org/package=tidyverse|http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html|https://CRAN.R-project.org/package=changepoint|http://dx.doi.org/10.1561/1500000011|http://dl.acm.org/citation.cfm?id=2390470.2390490|http://apiwiki.twitter.com/Streaming-API-Documentation|http://proceedings.mlr.press/v17/bifet11a.html|https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.2213|http://www.sciencedirect.com/science/article/pii/S0925231212007606,"The paper proposes a lightweight real-time methodology for detecting sentiment changes in Twitter streaming data by combining lexicon-based sentiment scoring with online statistical process monitoring. Tweets are streamed and immediately processed (cleaning, tokenization, lexicon lookup) to produce a bounded univariate sentiment-score time series, avoiding any offline training or storage of historical tweets. Change detection is performed with an online two-sided CUSUM chart based on the log-likelihood ratio under a Gaussian mean-shift model, using an adaptive threshold formulation and a reset/re-initialization after each signal to track evolving sentiment baselines. The threshold is tuned conceptually via in-control average run length (ARL0) and detection delay (ARL1), and in the case study they set a fixed threshold (e.g., h=20) and a fixed change magnitude. The approach is demonstrated on a real hashtag stream (15,491 English tweets for #theresamay, 2018-03-15 to 2018-03-24) and compared qualitatively against an offline multiple change-point method (R changepoint), showing broadly consistent change locations with small delays.","Sentiment observations $y_k$ are monitored using a Gaussian mean-shift CUSUM based on the log-likelihood ratio $s_i=\log\{p_{\theta_1}(y_i)/p_{\theta_0}(y_i)\}$ and cumulative sum $S_k=\sum_{i=1}^k s_i$. For the normal model with constant variance $\sigma^2$ and mean parameter $\theta=\mu$, the increment is $s_i=\frac{\theta_1-\theta_0}{\sigma^2}\left(y_i-\frac{\theta_0+\theta_1}{2}\right)$. The decision statistic uses an adaptive reference $m_k=\min_{1\le j\le k} S_j$ and signals when $g_k=S_k-m_k>h$, with stopping time $t_a=\min\{k:g_k>h\}$; two one-sided versions are run for two-sided detection with resets after signals.","A real-time case study streams 15,491 English tweets for hashtag #theresamay (2018-03-15 to 2018-03-24) and computes per-tweet sentiment using the bing lexicon. The two-sided CUSUM is initialized with $\theta_0=-0.5$ and change magnitude 0.5 (so $\theta_1^{pos}=0$ and $\theta_1^{neg}=-1$) and uses threshold $h=20$, producing multiple positive/negative change signals over the series. The detected change points visually align with shifts in the moving average (window size 200). An offline multiple change-point procedure (changepoint; penalty $2\log(n)$, maximum number of changes set to the number found by CUSUM) yields change locations that largely agree with CUSUM, with only minor delays between estimates.","The authors note that assuming known pre- and post-change parameters ($\theta_0,\theta_1$) is ""quite unrealistic"" in practical applications and that in general these would need experimental estimation using test data, which is not available in their streaming setting. They also state that selecting the CUSUM threshold $h$ is application/user dependent and thus subject to question. In future work they indicate a need to enhance the robustness of the change detection algorithm and to further test sentiment characterization methods.","The Gaussian/i.i.d. assumption for tweet-level sentiment scores is unlikely to hold for social streams, where sentiment can be autocorrelated, bursty, and heavy-tailed; this can distort false-alarm rates and detection delays. The design uses fixed, ad hoc parameter choices (e.g., $h$, change magnitude, baseline updates) without a principled calibration to achieve a target ARL0 under realistic data-generating conditions. Evaluation is primarily qualitative on one hashtag and does not report empirical ARL/ATS or false-alarm rates, limiting comparability to established SPC benchmarks. The reset strategy (baseline set to a short recent average) may be sensitive to outliers and could miss gradual drifts or lead to repeated signaling in volatile periods.",They intend to examine approaches that enhance the robustness of the change detection algorithm and to further test methodologies for sentiment characterization.,"A valuable extension would be to calibrate thresholds and reference values under non-normal, bounded discrete sentiment distributions (e.g., via bootstrap/Markov-chain approximations) to guarantee ARL0. Incorporating dependence handling (e.g., prewhitening, state-space models, or batch aggregation) could improve reliability on autocorrelated streams. Broader empirical validation across many hashtags/events with annotated ground-truth change times and reporting standard SPC metrics (ARL/ATS, false-alarm rate, detection delay distributions) would strengthen evidence. Providing an open-source R implementation (or package) and guidance for parameter selection (window for baseline update, robust estimators) would improve reproducibility and practitioner adoption.",1804.00482v1,local_papers/arxiv/1804.00482v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:00:28Z
TRUE,Univariate|Other,Shewhart|Other,Both,Manufacturing (general)|Environmental monitoring|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset),False alarm rate|Other,"For the initial implementation (Phase I), the paper suggests using 100 observations or more for the initial implementation of the CEV X and CEV S charts. In the worked example they use K=100 subgroups of size n=5 (i.e., 500 observations) to estimate in-control parameters.",TRUE,None / Not applicable,Not provided,NA,"The paper develops control-charting procedures for processes with highly left-censored measurements (values below a fixed detection limit C), where standard Shewhart X̄ and S charts can be biased and prone to false alarms. Assuming normality, it replaces each censored observation by its conditional expected value (CEV) given it falls below C, yielding a transformed dataset of “CEV weights” that can be summarized by subgroup mean and standard deviation. In-control parameters (μ, σ) are estimated via an iterative maximum-likelihood (EM-like) algorithm that alternates computing CEV replacements and re-estimating μ and σ until convergence. The authors propose two one-sided (upper-limit) charts—CEV X̄ and CEV S—to detect increases in the mean and/or dispersion, and provide standardized UCL coefficients as a function of subgroup size n and censoring proportion Pc. Control limits are obtained by simulation (≥1000 replications per censoring level) at α=0.0027 and demonstrated on a geotextile drainage-flow test where the instrument has a 50 ml/h detection limit, producing heavily censored data.","Censored values (t≤C) are replaced by a conditional expected value: $W_c=E(T\mid T\le C)=\mu-\sigma\,\frac{\phi(Z_c)}{\Phi(Z_c)}$, where $Z_c=(C-\mu)/\sigma$. The transformed data are $w_i=t$ if $t>C$ and $w_i=W_c$ if $t\le C$, and iterative ML estimates update $\hat\mu=\frac{1}{n}\sum_i w_i$ and $\hat\sigma$ using a correction term involving $\lambda(Z_c)$ (given in the paper). Final (one-sided) control limits are $\text{UCL}_{\bar X}=\mu+\sigma\,\text{UCLX}$ and $\text{UCL}_S=\sigma\,\text{UCLS}$, where UCLX and UCLS are standardized coefficients obtained via simulation for given (n, Pc).","Control-limit coefficients are simulated (≥1000 estimates per censoring level) for subgroup sizes n∈{3,5,10,20} and multiple censoring proportions, using α=0.0027; tables in the appendix report UCL coefficients (e.g., for n=5 and 1−%C=0.04, UCLX=1.42 and UCLS=2.09). In the geotextile example with detection limit C=50 ml/h and Phase I data of K=100 subgroups of size n=5, the ML/CEV procedure yields $\hat\mu=49.0279$ and $\hat\sigma=0.9915$ (versus naive censored summaries μ0=50.0846 and σ̂=0.2720). The computed CEV replacement is Wc=48.7330 and the implied theoretical censoring proportion is Pc≈0.843. Using UCLX=1.42 and UCLS=2.09 gives UCL for the mean chart ≈50.4358 and UCL for the S chart ≈2.0524; plotted Phase I points fall within limits in their example.","The authors note that the estimation algorithm can be imprecise and biased when censoring is high, because subgroups with all censored observations contain little information about parameter changes. They also state ML estimates work well for large samples and that the iterative ML method can require substantial computational effort when the censoring level is large. They emphasize that the method is mainly suited to detecting increases (mean/dispersion) and that decreases are difficult to detect with left censoring, especially when censoring exceeds about 50%.","The method relies on correct specification of an in-control normal distribution and a fixed detection limit C; departures from normality, mixture behavior near the detection limit, or time-varying detection limits could degrade performance. Control limits are simulation-derived under known in-control distribution; the paper does not fully quantify the impact of parameter-estimation uncertainty (Phase I estimation error) on Phase II false-alarm properties beyond the fixed α design. No explicit ARL/ATS curves are reported for various shift sizes, making it harder to compare detection speed versus common competitors (e.g., censored-data CUSUM/EWMA or likelihood-based charts). Practical guidance for handling autocorrelation, varying subgroup sizes, or intermittent missingness/irregular sampling is not addressed.","The paper notes that “many other practical censorship schemes should be investigated,” suggesting extensions beyond fixed-level left censoring and the specific scheme studied here.","Extending the approach to non-normal and robust/semi-parametric models (e.g., using transformation models or distribution-free imputation) would improve applicability when normality is questionable. Developing CUSUM/EWMA or GLR versions of the CEV-based statistic for faster detection of small shifts, and providing ARL/ATS performance comparisons, would better position the method relative to modern SPC. Incorporating autocorrelation modeling (e.g., ARMA residual charts with censoring-aware estimation) and quantifying Phase I estimation error effects on Phase II limits would make the procedure more reliable in practice. Public software (e.g., an R package) implementing the iterative ML/CEV estimation and simulated control-limit lookup/interpolation would greatly facilitate adoption.",1804.00760v2,local_papers/arxiv/1804.00760v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:01:11Z
TRUE,Univariate|Profile monitoring,Shewhart|Other,Phase II,Manufacturing (general)|Environmental monitoring,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length),"Not discussed (simulation considers n = 200, 300, 500, 1000 and real-data examples use n = 18 and n = 729, but no prescriptive Phase I sizing guidance is given).",TRUE,R,Not provided,https://www.R-project.org/,"The paper proposes a Beta Regression Control Chart (BRCC) for monitoring fraction/rate/proportion data in (0,1) when the response depends on control variables. It models both the beta mean $\mu_t$ and dispersion $\sigma_t$ via regression structures (beta regression with varying dispersion), and sets time-varying control limits using beta quantiles so limits stay within (0,1) and accommodate asymmetry/heteroscedasticity. Performance is evaluated via extensive Monte Carlo simulation comparing BRCC (varying dispersion) against a beta-regression chart with constant dispersion (BRCCC) and a standard normal-based regression control chart (RCC), using in-control and out-of-control ARL. Simulations indicate BRCC yields faster detection (lower ARL1) for mean shifts and dispersion increases, and avoids pathological behavior of RCC near boundaries where limits can exceed (0,1). Two real applications (tire manufacturing scrap proportion and Brasília relative humidity) illustrate that BRCC can flag atypical observations missed by BCC and RCC while providing covariate-adjusted, bounded control limits.","The chart assumes $y_t\sim\text{Beta}(\mu_t,\sigma_t)$ with $\mathrm{E}(y_t)=\mu_t$ and $\mathrm{Var}(y_t)=\mu_t(1-\mu_t)\sigma_t^2$. Mean and dispersion are linked to covariates by $g(\mu_t)=\sum_{i=1}^k x_{ti}\beta_i$ and $h(\sigma_t)=\sum_{j=1}^s z_{tj}\gamma_j$ (often logit links). For a target $\mathrm{ARL}_0$, set $\alpha=1/\mathrm{ARL}_0$ and define time-varying limits via beta quantiles: $\mathrm{LCL}_t = Q(\alpha/2;\mu_t,\sigma_t)$ and $\mathrm{UCL}_t = Q(1-\alpha/2;\mu_t,\sigma_t)$ (implemented using MLE-fitted $\hat\mu_t,\hat\sigma_t$).","Monte Carlo experiments use 50,000 replications, with sample sizes n = 200, 300, 500, 1000 and scenarios spanning different mean/dispersion regimes; all charts are tuned to $\mathrm{ARL}_0=200$ (i.e., $\alpha=0.005$). Across scenarios, BRCC shows consistently smaller ARL1 than BRCCC and especially RCC for mean shifts (implemented as an additive shift $\delta$ on the mean link, with $\delta\in[-0.15,0.15]$) and for dispersion increases (additive $\delta$ on the dispersion link, $\delta\in[0,0.15]$). The paper highlights cases where RCC performs poorly near boundaries: for n = 1000, reported examples include RCC having $\mathrm{ARL}_1\approx 237$ at $\delta=-0.02$ when $\mu\approx0.20$, $\mathrm{ARL}_1\approx 256$ when $\mu\approx0.80$, and $\mathrm{ARL}_1\approx 587$ when $\mu\approx0.08$, i.e., worse than in-control performance. In the tire-manufacturing case (ARL0=200), BRCC flags observation 6 as out-of-control while BCC and RCC do not; for Brasília RH (729 days), BRCC identifies five atypical high-humidity observations whereas RCC shows none and BCC flags only one.",None stated.,"The chart limits are computed using plug-in MLEs from the same data stream, but the impact of parameter-estimation uncertainty on the achieved in-control ARL (especially with small Phase I samples) is not fully characterized analytically. The approach assumes independent beta observations given covariates; no treatment of serial correlation is provided, which can be important for environmental time series (e.g., daily humidity) and can inflate false alarms. Implementation relies on fitting potentially complex mean/dispersion submodels and selecting covariates; guidance on model-misspecification robustness and practical model-selection risk for SPC deployment is limited.",None stated.,"Extend BRCC to explicitly handle autocorrelated beta regression errors (e.g., beta-ARMA or state-space beta models) and study its effect on in-control ARL. Develop Phase I/Phase II procedures (including robust estimation and re-estimation schemes) to ensure nominal ARL under parameter uncertainty, and provide sample-size guidance. Provide open-source implementations (e.g., an R package) with functions to compute limits, diagnose model fit, and support online monitoring, plus extensions to handle zero/one-inflated proportions.",1804.01454v1,local_papers/arxiv/1804.01454v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:01:46Z
TRUE,Multivariate,MEWMA,Phase II,Theoretical/simulation only,TRUE,FALSE,NA,Approximation methods|Simulation study|Markov chain|Integral equation,ARL (Average Run Length)|Steady-state ARL|Conditional expected delay,Not discussed (assumes known in-control parameters; no Phase I sample size guidance).,TRUE,R,Package registry (CRAN/PyPI),http://www.netlib.org,"The paper develops accurate approximations for the steady-state (quasi-stationary and cyclical stationary) behavior of the MEWMA statistic used for multivariate mean-shift monitoring under i.i.d. multivariate normal data with known covariance. It derives steady-state densities for the MEWMA Mahalanobis distance and shows these can be expressed as products of one-dimensional functions, enabling feasible computation even for larger dimensions. Using Fredholm integral equations (solved via Nyström methods with Gauss–Legendre quadrature) and comparisons to Markov-chain approaches, it computes steady-state and worst-case average run lengths (ARLs) more accurately than earlier algorithms. The work reports that for large dimension p the steady-state behavior differs from univariate intuition, affecting steady-state ARL and optimal smoothing-constant choices. It provides numerical studies (including very large Monte Carlo confirmation runs) and derives optimal MEWMA smoothing constants λ for zero-state, steady-state, and worst-case criteria.","The MEWMA recursion is $Z_0=z_0$, $Z_n=(1-\lambda)Z_{n-1}+\lambda X_n$ with $0<\lambda\le 1$, and monitoring is based on $T_n^2=(Z_n-\mu_0)^\top\Sigma_Z^{-1}(Z_n-\mu_0)$ where $\Sigma_Z=\frac{\lambda}{2-\lambda}\Sigma$; signal when $T_n^2>h_4$ (stopping time $N=\inf\{n\ge1:T_n^2>h_4\}$). In-control and out-of-control ARL/steady-state quantities are obtained from (single and double) Fredholm integral equations (e.g., Rigdon-type equations) for the ARL function $L(\cdot)$ and left eigenfunctions $\psi(\cdot)$; a key structural result shows the bivariate steady-state eigenfunction factors as $\psi(u,w)=d(w)\,\psi^\circ(u)$, with $d(w)\propto (1-w^2)^{(p-3)/2}$.","For in-control target $E_\infty(N)=200$, the in-control conditional steady-state ARL $D^\circ$ is below 200 and decreases with dimension; e.g., at $\lambda=0.1$, $D^\circ\approx192.6$ (p=2) down to $\approx186.4$ (p=50), with cyclical values $D^{\circ *}$ very close. For out-of-control performance at $\lambda=0.1$ and selected p, steady-state ARLs $D$ and cyclical $D^*$ closely match Monte Carlo results; e.g., p=4: at $\sqrt\delta=1$, $D\approx11.36$ and $D^*\approx11.38$ (with zero-state $L\approx12.15$). The proposed Nyström/Gauss–Legendre integral-equation approach achieves higher accuracy than earlier Markov-chain approximations while using much smaller linear systems (e.g., r=30 nodes leading to 30- and 900-dimensional systems). The paper also finds that optimal smoothing constants differ by criterion: minimizing steady-state ARL $D$ tends to yield smaller optimal $\lambda$ than minimizing zero-state $L$, while minimizing worst-case ARL $W$ yields substantially larger optimal $\lambda$.","The analysis assumes serial independence, multivariate normality, and known covariance matrix $\Sigma$ (“to avoid further complications, we assume that the covariance matrix $\Sigma$ is known”). The work focuses on mean-shift changes under a simple change-point model and does not treat additional complications beyond that setup.","The methods are presented for i.i.d. Gaussian vectors with known parameters, so robustness to covariance misspecification, heavy tails, or autocorrelation (common in practice) is not established. Although the paper notes implementation in an R package, it does not provide fully reproducible code snippets/workflows within the paper for all experiments, and the computational burden for very high p or very tight ARL tolerances is not fully characterized. Phase I estimation effects (finite-sample parameter estimation and its impact on steady-state ARL) are not addressed, limiting direct applicability when $\mu_0$ and $\Sigma$ must be estimated.","The paper notes that the decomposition approach (factorization of the steady-state eigenfunction) could be used to compute finite-change-point conditional delays $D_\tau=E_\tau(N-\tau+1\mid N\ge\tau)$ for $\tau=1,2,3,\ldots$ and to study convergence $D_\tau\to D$ to assess the validity of the steady-state measure $D$.","Extending the steady-state methodology to unknown-parameter settings (estimating $\Sigma$ and $\mu_0$ in Phase I) and quantifying the impact on steady-state/worst-case ARL would improve practical relevance. Robust or nonparametric adaptations (e.g., elliptical/heavy-tailed data) and explicit handling of autocorrelation (e.g., VAR or state-space residual monitoring) would address common real-world violations. Providing open, fully reproducible code and benchmarks (including runtime scaling with p and node counts) and extending comparisons to modern multivariate change-detection alternatives (e.g., GLR/scan, high-dimensional methods) would strengthen adoption.",1808.05069v1,local_papers/arxiv/1808.05069v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:02:27Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Both,Healthcare/medical,TRUE,TRUE,TRUE,Simulation study|Case study (real dataset),False alarm rate|Other,"Uses the first m = 19 days as Phase I for parameter estimation (both centers). Center A has n = 24 participants and center B has n = 12, with average daily measurements n̄ ≈ 20 (A) and n̄ ≈ 9 (B) used for UCL simulation.",TRUE,R,Not provided,https://dx.doi.org/10.1016/j.ijmedinf.2019.03.011,"The paper develops a multivariate data-quality monitoring procedure for telehealth vital-sign streams when observations are missing and the daily sample size varies randomly. It adapts the Hotelling’s T-squared chart by monitoring a daily mean vector of multiple vital signs and by adjusting the covariance of the mean vector using a weighting matrix derived from the number of paired observations available each day (to accommodate MAR missingness without imputation). Robust Phase I estimation of the in-control mean and covariance is performed using the OGK estimator, and the MYT decomposition is used to attribute signals to specific vital signs or subsets. Control limits are obtained via simulation to target a fixed false-alarm probability (α = 0.02), with additional dimension reduction and time-varying UCLs when some vital signs are completely missing on a day. The approach is validated retrospectively on a center with a known capped SBP measurement fault and prospectively implemented in a second center, where it detected additional device/data issues in near real time.","Daily subgroup mean vector: $\bar{\mathbf X}_i=(\bar X_{i1},\ldots,\bar X_{ip})^\top$, where $\bar X_{ij}$ averages over the $n_{ij}$ observed participants for vital sign $j$ on day $i$. Under MAR and independence across individuals, the mean-covariance relationship is $\Sigma_{\bar{\mathbf X}_i}=\mathbf W_i\circ\Sigma$ with Hadamard product, where $W_{i,jj'}=|U_{ij}\cap U_{ij'}|/(n_{ij}n_{ij'})$. The monitoring statistic is $T_i^2=(\bar{\mathbf X}_i-\hat\mu)^\top(\mathbf W_i\circ\hat\Sigma)^{-1}(\bar{\mathbf X}_i-\hat\mu)$, signaling when $T_i^2>\mathrm{UCL}$ (with dimension reduction and adjusted UCL if some components of $\bar{\mathbf X}_i$ are missing).","Control limits were simulated to achieve a false-alarm rate of $\alpha=0.02$ (about one false alarm every 50 days on average), yielding UCL = 17.31 for center A (with $m=19$, $\bar n=20$) and UCL = 18.59 for center B ($m=19$, $\bar n=9$). In center A, the chart detected the known SBP capping problem (maximum 136) with signals starting 20.02.2018, though detection was delayed by about four days; MYT decomposition attributed signals primarily to SBP (and also BT). In center B, a signal on 09.03.2018 was traced to DBP being set to a maximum level and was corrected; later signals (13.04.2018 and 29.05.2018) were attributed to an unusually high HR (single participant) and to unusual SBP high/SpO2 low, respectively. When only three vital signs were available (last two days in center A), a reduced-dimension UCL of 13.29 was used.","The authors note that ideally Phase I estimation would be robust simultaneously to outliers, missing data, and slight deviations from normality in subgrouped data, but to their knowledge such a covariance estimator does not yet exist. They therefore use OGK on “clubbed” complete cases and assume it remains unbiased/consistent under MAR, and state that it is difficult to verify this empirically. They also note their UCL simulation assumes complete data for convenience, though they checked several missing-data scenarios and found UCLs within about 5%.","The method monitors aggregated daily mean vectors, so it can miss within-day or individual-level anomalies and may dilute small or localized data-quality faults (detection delay is plausible, as seen with SBP capping). The approach relies on approximate independence over time after averaging; if autocorrelation persists (e.g., systematic day-of-week effects), in-control false-alarm properties may not hold. Control limits are obtained by simulation using estimated parameters and an approximate missingness model (using $\bar n$), but the paper does not provide sensitivity analysis across broader missingness mechanisms or stronger departures from MAR/individual independence. No reusable implementation is provided, which may hinder adoption and reproducibility.","The authors suggest comparing the proposed method with existing approaches for monitoring with missing observations, such as imputation-based methods. They also propose extending the approach to faster-detecting memory charts, noting that EWMA/CUSUM can detect failures more quickly than Hotelling’s $T^2$, and specifically suggest developing a multivariate CUSUM with dynamic probability control limits (building on Huang et al., 2016).","Develop a principled Phase I estimation procedure that is jointly robust to outliers and missingness (and accommodates changing covariance induced by varying sample sizes) to reduce reliance on complete-case OGK. Extend the chart to explicitly handle residual autocorrelation/seasonality (e.g., state-space modeling plus residual monitoring) and assess steady-state/conditional signaling properties under dependence. Provide diagnostic tools that distinguish device/data-quality faults from true population health shifts (e.g., hierarchical models or dual charts for system vs. subject effects). Release an R/Python implementation and benchmarking suite to facilitate replication and broader evaluation across different missing-data patterns and telehealth settings.",1809.03127v2,local_papers/arxiv/1809.03127v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:03:11Z
TRUE,Univariate|Nonparametric,Shewhart|Other,Both,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,FALSE,Simulation study|Other,ARL (Average Run Length)|False alarm rate|Other,"Not discussed (Phase I uses k rational subgroups of size n, but no specific recommended values are given).",TRUE,MATLAB,Not provided,NA,"The paper proposes constructing robust Shewhart S control charts by replacing conventional Phase I estimators (sample mean and sample standard deviation) with robust location and scale estimators. For subgroup scale estimation it studies MAD, Rousseeuw–Croux $Q_n$, and a logistic M-scale (MSLOG); for estimating the chart center/parameters across subgroups it studies M-Huber, Harrell–Davis, and Hodges–Lehmann location estimators. Phase I contamination is modeled via three outlier mechanisms (diffuse symmetric, diffuse asymmetric, and localized contamination), and Phase II performance is assessed via unconditional in-control and out-of-control average run length (ARL) after calibrating limits to target $ARL_0\approx 370.4$ under uncontaminated normal data. A Monte Carlo simulation study (implemented in MATLAB) compares estimator combinations in terms of Phase I MSE for $\sigma$ estimation and Phase II ARL behavior. Results indicate robust estimators better preserve $ARL_0$ and improve detection power under diffuse contaminations, while under localized contamination conventional subgroup S with a robust location estimator can be competitive or best.","The Shewhart S-chart limits are parameterized as $\mathrm{LCL}=L_n\hat\sigma$ and $\mathrm{UCL}=U_n\hat\sigma$, with $L_n,U_n$ chosen to achieve a target type-I error: $P(L_n\hat\sigma\le \hat\sigma_i\le U_n\hat\sigma)=1-\alpha$. Unconditional run length performance is evaluated via Monte Carlo, using $\mathrm{ARL}=E(E(\mathrm{RL}\mid\hat\sigma))\approx \frac{1}{M}\sum_{t=1}^M \frac{1}{p_t}$ where $p_t$ is the conditional signal probability. Robust scale estimators include $\mathrm{MAD}_n=b_n\,\mathrm{med}_i|x_i-\mathrm{med}_j x_j|$ and $Q_n=c_n\,\mathrm{med}_{i<j}|x_i-x_j|$, alongside an M-scale (MSLOG) defined implicitly through an M-estimation equation with logistic $\psi$; robust location estimators include Huber M-location (bounded \psi), Harrell–Davis quantile estimator, and Hodges–Lehmann (median of Walsh averages).","Under diffuse symmetric contamination (model 1) at high contamination (reported example with $a=4$), the Phase I MSE for conventional S with mean location is about 0.66 versus about 0.17 (MAD), 0.19 (MSLOG) and 0.22 ($Q_n$). For model 1, conventional methods show degraded in-control performance: $ARL_0$ is reported as 305.6 at low contamination and drops to 78.1 at high contamination, while robust combinations (notably MAD or MSLOG paired with robust location estimators) maintain much higher $ARL_0$. For model 1 out-of-control monitoring with $\phi=1.4$, at the highest contamination the conventional S+mean combination yields ARL about 480.9, while a robust combination (MSLOG+Harrell–Davis) reports ARL about 287.5. For diffuse asymmetric contamination (model 2), $Q_n$ is highlighted as best: at high contamination conventional methods have a false signal about every 38.5 samples ($ARL_0\approx 38.5$), whereas $ARL_0$ for $Q_n$ is reported about 364 (near the 370.4 design), and for $\phi=1.4$ $Q_n$ (often with Huber-M location) is most powerful across contamination levels. For localized contamination (model 3), conventional subgroup S is relatively efficient in Phase I, but using a robust location estimator is important; for $\phi=1.4$ the smallest reported ARL is about 66.2 using a robust location estimator (Harrell–Davis) with subgroup S, whereas S+mean can increase to ARL 114.9 (moderate) and 470.8 (high contamination).",None stated.,"The study calibrates control limits to achieve $ARL_0=370.4$ under uncontaminated normal data, but does not provide a general, practitioner-ready procedure for selecting limits when Phase I data are contaminated or when distributional form is unknown. The work assumes independence within and across rational subgroups and does not address autocorrelation, which commonly inflates false alarm rates for Shewhart charts. Comparisons are primarily simulation-based and do not include a real manufacturing case study demonstrating implementation details (e.g., subgroup size choice, estimator tuning such as MSLOG constants) and interpretability for practitioners. The focus is on S-charts (scale only); joint monitoring of mean and variance or extensions to multivariate/high-dimensional settings are not considered.",None stated.,"Extend the robust estimation approach to autocorrelated processes (e.g., via residual-based charts or time-series modeling) and assess robustness under dependence. Provide adaptive or data-driven selection of tuning constants (e.g., MSLOG $\kappa$ or Huber $c$) and a direct method to set limits under contamination without relying on uncontaminated-normal calibration. Validate the proposed robust S-chart designs on real industrial datasets and package the methods in reproducible software (e.g., MATLAB/R code) to support adoption. Consider integrated robust monitoring schemes for both location and scale (e.g., paired robust $\bar{X}$ and S charts) and potential extensions to multivariate dispersion monitoring.",1812.11132v1,local_papers/arxiv/1812.11132v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:03:50Z
TRUE,Univariate|Nonparametric|Other,Shewhart|CUSUM|EWMA|Other,Phase II,Theoretical/simulation only,FALSE,NA,FALSE,Simulation study,ARL (Average Run Length)|False alarm rate|Other,"Uses subgroup/sample size n = 5 in simulations (10,000 runs). No Phase I sample size guidance is provided.",TRUE,None / Not applicable,Not provided,NA,"The paper develops and studies robust adaptive control charts to reduce false alarms under violations of normality/independence while mitigating the detection-delay penalty typical of robust charts. It uses a robust CUSUM based on the sample median (denoted CUSUM \(\tilde{X}\)) and then applies a zone-type adaptive scheme that dynamically shrinks control limits based on which zone recent statistics fall into, yielding an adaptive robust chart (Ad-CUSUM \(\tilde{X}\)). Performance is evaluated via Monte Carlo simulation (10,000 runs, n=5) under both normal data \(N(\delta,1)\) and contaminated mixtures (94% \(N(\delta,1)\) + 6% \(N(\delta,6.25)\)). Results show the adaptive robust chart improves detection for small shifts while maintaining better robustness on contaminated data, summarized further via a relative-ARL index (RARLC) that normalizes ARLs to a common in-control level. The work contributes a practical adaptive mechanism layered on a robust (median-based) sequential scheme, aligning with SPC literature on robustness and adaptive chart design.","Robust median-based CUSUM: \(\tilde{C}^+_{n+1}=\max\{0,\tilde{C}^+_{n}-(\mu_0+\delta_0)+\tilde{X}_n\}\), \(\tilde{C}^-_{n+1}=\max\{0,\tilde{C}^-_{n}+(\mu_0+\delta_0)-\tilde{X}_n\}\), signal when \(\tilde{C}^+_n\ge L\) (analogously for \(\tilde{C}^-\)). Zone-adaptive control-limit update uses shrinkage \(L_n(LCL,s)=LCL+s\sigma_{\bar{X}}\) and \(L_n(UCL,s)=UCL-s\sigma_{\bar{X}}\), shrinking the relevant side depending on which zone \(Z_k\) the current statistic falls in. Relative ARL on contaminated data is defined as \(RARLC = k\,ARLC(\delta)\) where \(k=ARL(0)/ARLC(0)\).","Under normal data \(N(\delta,1)\), with designs set to achieve in-control ARL near 500, Ad-CUSUM \(\tilde{X}\) has ARL ≈ 124.9 at \(\delta=0.1\), better than CUSUM \(\bar{X}\) (130.0) and EWMA \(\bar{X}\) (136.3). Under contaminated data (94% \(N(\delta,1)\)+6% \(N(\delta,6.25)\)), classical Shewhart \(\bar{X}\) collapses in-control (ARL0 ≈ 87.1), while Ad-CUSUM \(\tilde{X}\) maintains ARL0 ≈ 466.2. Using the RARLC normalization, Ad-CUSUM \(\tilde{X}\) yields the smallest relative ARLs among the compared methods for shifts \(\delta\ge 0.1\) (e.g., RARLC at \(\delta=0.1\) is 130.3 vs 162.1 for CUSUM \(\tilde{X}\) and 183.5 for CUSUM \(\bar{X}\)). Overall, the adaptive robust scheme is most favorable for contaminated/heavy-tailed scenarios while remaining competitive for small shifts in the normal case.",None stated.,"The adaptive scheme is evaluated only via simulation with a single subgroup size (n=5) and a specific contamination model (6% variance inflation to 6.25), so robustness across other heavy-tailed, skewed, or outlier-generating mechanisms is unclear. Autocorrelation is discussed as a motivation, but the proposed method is not demonstrated on explicitly autocorrelated data nor paired with a concrete time-series modeling/whitening step. The paper does not provide guidance for practical parameter tuning of zone limits/shrinkage parameters beyond the specific simulation settings, which may limit deployability. No software/code is provided, which may hinder reproducibility and adoption.",None stated.,"Extend the study to explicitly autocorrelated processes (e.g., AR(1), ARMA) and assess whether the adaptive robust chart maintains nominal in-control performance without preprocessing. Develop data-driven or optimization-based tuning rules for zone boundaries and shrinkage parameters targeting specified ARL0 and minimizing out-of-control ARL across a range of shifts/contaminations. Provide self-starting or Phase I–robust estimation procedures so the method can be deployed with limited historical data and quantified estimation error. Release an implementation (e.g., an R/Python package) and include additional real-data case studies to validate practical performance and usability.",1901.03701v1,local_papers/arxiv/1901.03701v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:04:32Z
TRUE,Univariate|Other,Shewhart|Other,Both,Transportation/logistics|Service industry|Other,TRUE,TRUE,NA,Case study (real dataset)|Other,False alarm rate|Other,Uses weekly measurements from a construction project over the first 42 weeks to estimate/iterate control limits (remove out-of-control points and recompute limits until all remaining points are in-control). No general Phase I sample-size recommendation is given.,TRUE,Other,Not provided,NA,"The paper applies Shewhart Individuals (I) control charts to monitor project schedule and cost performance using Earned Duration Management (EDM) indices: EDI, DPI, and CPI. Because project-performance index time series can be non-normal and autocorrelated, it proposes a two-step adjustment framework: (1) test normality (Anderson–Darling) and, if needed, apply Johnson transformation to obtain approximately normal measurements; (2) remove autocorrelation by checking stationarity, applying differencing, and fitting ARIMA models to extract residuals. Control charts are then built on the ARIMA residuals to reduce false alarms caused by serial dependence. A construction-project case study with 42 weekly observations illustrates implementation; the method flags special-cause weeks (e.g., out-of-control points for DPI and EDI) that are not obvious from raw index values alone. The authors conclude that combining EDM indices with adjusted control charts improves timely detection of cost/schedule problems and supports corrective action.","EDM indices include $\mathrm{EDI}=\mathrm{TED}/\mathrm{TPD}$ and $\mathrm{DPI}=\mathrm{ED}(t)/\mathrm{AD}$ (with CPI as $\mathrm{EV}/\mathrm{AC}$). The Shewhart Individuals chart limits are given as $\mathrm{UCL}=\bar{Y}+3(\sigma/\sqrt{n})$, $\mathrm{CL}=\bar{Y}$, and $\mathrm{LCL}=\bar{Y}-3(\sigma/\sqrt{n})$, where $Y_t$ is an index measurement at time $t$ and limits are applied to ARIMA residuals after normality/autocorrelation adjustments. Autocorrelation is addressed by differencing to achieve stationarity and fitting ARIMA models (reported as ARIMA(4,1,1) for CPI, ARIMA(1,1,1) for DPI, and ARIMA(4,1,1) for EDI) to obtain residuals for charting.","In the case study (42 weekly observations), Anderson–Darling normality testing indicates EDI is approximately normal (reported p-value 0.222 at significance level 0.20), while DPI and CPI are non-normal and are normalized using Johnson transformation (example shown for DPI with transformed p-value 0.5777). All three series exhibit autocorrelation; after one-lag differencing for stationarity and ARIMA fitting, Individuals charts on residuals identify special-cause points (e.g., out-of-control observations for DPI around week 3 and for EDI around week 9, and a CPI point above UCL around week 3). The charts suggest schedule-performance variability is larger early in the project (roughly weeks 1–23) and becomes smoother later (after ~week 23 for DPI and ~week 29 for EDI). The authors link detected special-cause weeks to procurement/material delivery delays in the case study narrative.",None stated.,"The chart-limit formulas presented use $\sigma/\sqrt{n}$ (typical of $\bar{X}$ charts) rather than standard Individuals-chart estimation via moving range, and the paper does not clearly justify this choice for I-charts on residuals. The empirical validation is limited to a single construction-project case study (42 weeks) with no systematic simulation/ARL study comparing false-alarm and detection performance versus alternative approaches (e.g., EWMA/CUSUM on residuals or dedicated charts for autocorrelated data). The workflow relies on commercial software (Minitab) and multiple modeling choices (transformation type, differencing order, ARIMA order selection) that may be sensitive and are not fully standardized for practitioners.","The authors suggest extending the approach by utilizing fuzzy time series methods to provide more practical prediction under project uncertainty, especially when prior performance shows no clear trend.","A natural extension is to evaluate in-control and out-of-control run-length properties (ARL/ATS) via simulation for the proposed two-step framework and compare against residual-based EWMA/CUSUM or specialized charts for autocorrelated/non-normal data. Developing open-source implementations (e.g., R/Python) with automated model selection and diagnostics (transformation choice, stationarity checks, ARIMA identification) would improve reproducibility and adoption. The method could also be generalized to multivariate monitoring of (CPI, EDI, DPI) jointly (e.g., MEWMA/Hotelling $T^2$ on residual vectors) to capture correlated shifts across indices.",1902.02270v1,local_papers/arxiv/1902.02270v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:05:01Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Healthcare/medical,TRUE,FALSE,TRUE,Simulation study|Markov chain|Economic design|Case study (real dataset),False alarm rate|Other,Uses sample size n = 1 (single observation per sampling/visit); no Phase I sample size guidance is provided.,TRUE,R,Not provided,https://www.ncbi.nlm.nih.gov/books/NBK279318/,"The paper develops a Markov chain-based economic design framework for univariate X control charts tailored to healthcare/patient monitoring, where key assumptions of classical economic SPC are relaxed. It allows (i) random shift sizes (modeled via a Poisson process for event times with exponentially distributed shift magnitudes leading to a Poisson–gamma/Erlang mixture), (ii) imperfect repair/treatment effects (modeled via a Beta-distributed proportion of remaining deviation from target), and (iii) random/failed sampling due to non-compliance (modeled via logistic or state-dependent beta-based sampling probabilities). The long-run expected cost is computed from the stationary distribution of a discretized finite-state Markov chain, and the optimization can target a weighted objective combining mean cost and the standard deviation of cost. The authors implement the approach in R and study parameter sensitivity; incorporating cost variability can substantially reduce cost standard deviation with only a small increase in expected cost. A real-data-informed application to LDL cholesterol monitoring for high cardiovascular-risk patients in Hungary suggests cost-optimal visit intervals and alarm thresholds can differ from guideline-based practice.","A base-model average-cost objective is given by (Eq. 1) $E(C)=\frac{c_s+p_3c_f+p_4c_r}{h}+p_2c_o+p_4c_o B$, where $p_i$ are stationary probabilities of Markov states (in-control/out-of-control/false-alarm/true-alarm), $h$ is the sampling interval, and $B$ is the expected undetected out-of-control fraction within an interval. In the generalized model, transition probabilities combine the process CDF $\phi(\cdot)$, discretized shift-size pmf $q_h(\cdot)$, repair distribution $R(\cdot,\cdot)$ and sampling-success probability $T_h(\cdot)$ to form a $2V_d\times 2V_d$ transition matrix; the stationary distribution is obtained as the normalized left eigenvector solving $\Pi_0^T f_0=f_0$. The optimized criterion becomes $G=pE(C)+(1-p)\sigma(C)$, and out-of-control cost uses a Taguchi-type squared-deviation loss via expected squared distance between samplings (e.g., Eq. 5 for $E_h(H_j^2)$ under Poisson event counts and exponential shift sizes).","In the LDL monitoring application (Hungary-informed parameters), optimizing expected cost only yields an optimal time between samplings of 56.57 days and a critical LDL increase threshold of 0.143 mmol/l, with average daily cost €0.469 and cost SD €0.562. When also penalizing cost variability (weight p = 0.9 on expected cost), the optimum shifts to 64.76 days and 0.129 mmol/l, with average daily cost €0.477 and cost SD €0.418 (lower variability with only a small mean-cost increase). In a sensitizing-rules simulation (50,000 sampling intervals; burn-in 100), baseline empirical mean cost was 36.51 vs theoretical 37.75, and adding 2/3k warning-limit rules had minimal impact on mean cost, variability, or alarm proportion (alarm proportion roughly ~0.19–0.20). Sensitivity plots indicate out-of-control cost increases generally decrease the sampling interval and increase both expected cost and cost SD, while including cost SD in the objective tends to lower the control limit and increase the sampling interval.","The authors note that estimating a realistic non-compliance distribution is difficult, so results should be regarded as close approximations; patients can differ widely, motivating scenario analyses. They also state that the simulation-based assessment of sensitizing rules can yield somewhat inaccurate empirical SD estimates due to large variance and slow convergence. In the conclusions, they highlight that modeling repair as instantaneous may be inappropriate in many situations.","The framework relies on a discretization of the state space (choice of $\Delta$ and $V_d$), which can materially affect accuracy and computational burden; guidance for selecting these discretization parameters is limited. The process model assumes normal measurement distribution with known $\mu_0$ and $\sigma$ and focuses on one-sided positive shifts, which may not fit many healthcare biomarkers or measurement error structures. Although non-compliance is treated as missed sampling, more general missingness mechanisms (informative visit timing, irregular sampling not aligned to fixed opportunities) and serial correlation in measurements are not explicitly modeled. The method is presented primarily as Phase II design (parameters treated as known/estimated externally), leaving uncertainty propagation from parameter estimation to optimal policies only partially addressed via sensitivity studies.","The authors suggest improving the modeling of the repair procedure, since instantaneous repair may be unrealistic. They also propose developing a continuous model (e.g., incorporating time-series dynamics) to better describe process evolution between samplings.","Develop robust/nonparametric or Bayesian versions that account for uncertainty in $\sigma$, shift-rate $s$, and compliance parameters and propagate this uncertainty into optimal chart settings. Extend the approach to multivariate patient monitoring (e.g., multiple lipids/vitals) and to autocorrelated or longitudinal mixed-effects measurement models. Provide principled guidance (or adaptive schemes) for selecting discretization granularity and for computational scalability as $V_d$ grows. Release an R package or reproducible code and include more external healthcare case studies to validate practicality and generalizability across different biomarkers and care pathways.",1903.06675v1,local_papers/arxiv/1903.06675v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:05:39Z
TRUE,Multivariate,EWMA|Other,Phase II,Manufacturing (general)|Healthcare/medical|Theoretical/simulation only,TRUE,TRUE,FALSE,Simulation study|Case study (real dataset),ATS (Average Time to Signal)|Steady-state ARL|ARL (Average Run Length),"Subgroup sizes studied for grouped monitoring: for p=2, n = 3, 5, 10; for p=10, n = 11, 15, 20. In the real-data illustration they use subgroup size n = 5. Phase I sample size guidance is not provided (parameters are assumed known in the main study).",TRUE,R,Not provided,https://cran.r-project.org/web/packages/MSQC/MSQC.pdf,"The paper studies whether multivariate observations should be monitored individually or temporally aggregated into subgroups when monitoring changes in the process covariance matrix (multivariate dispersion). It compares an individual-observation multivariate EWMA dispersion chart (Huwang et al.’s MEWMS, based on the trace of an EWMA of outer products) to subgroup-based dispersion charts using both non-overlapping and overlapping (moving window) subgroups. For subgroup monitoring, it includes Alt’s generalized variance chart (determinant of sample covariance) and a trace-of-sample-covariance chart for non-overlapping subgroups (NTCC), and develops/uses overlapping counterparts based on the trace of the sample covariance (OTCC) and the trace of a mean squared successive difference covariance estimator (OTMC). Performance is evaluated primarily via steady-state Average Time to Signal (ATS) under sustained shifts in variances and/or correlations using extensive Monte Carlo simulation, supplemented by an industrial-data illustration. Main conclusions are that individual-observation monitoring (MEWMS) is fastest for detecting increases in dispersion, while overlapping-subgroup trace-based charts (OTCC/OTMC) are best for detecting decreases, and overlapping subgroups generally outperform non-overlapping ones.","Data are standardized as $\mathbf{Y}_t=\Sigma_0^{-1/2}(\mathbf{X}_t-\mu_0)$ with in-control $\mathbf{Y}_t\sim N(\mathbf{0},I_p)$. MEWMS uses $E_t=\omega \mathbf{Y}_t\mathbf{Y}_t^\top+(1-\omega)E_{t-1}$ and charts $\operatorname{tr}(E_t)$ against time-varying limits $p\pm L\sqrt{2p\,C_t}$. For subgroup charts, $S_T=(n-1)^{-1}\,\mathbf{Y}_T^{[n]}(\mathbf{Y}_T^{[n]})^\top$ (or overlapping analogue) and either $\det(S_T)$ (GVC) or $\operatorname{tr}(S_T)$ (NTCC/OTCC) is charted; OTMC charts $\operatorname{tr}(\mathrm{MSSD}_{T'})$ where $\mathrm{MSSD}_{T'}=\{2(n-1)\}^{-1}(\mathbf{Y}_{T'}^{[n]}-\mathbf{Y}_{T'-1}^{[n]})(\mathbf{Y}_{T'}^{[n]}-\mathbf{Y}_{T'-1}^{[n]})^\top$.","Across simulated sustained shifts with ATS0 tuned to 370, MEWMS (especially with $\omega=0.2$) is typically the quickest detector for increases in dispersion (e.g., for p=10 and an overall variance inflation $\delta=1.4$, ATS values around 10–25 for MEWMS vs larger for subgroup charts in the reported tables). For decreases in dispersion, MEWMS performs poorly due to symmetric limits, while overlapping subgroup charts OTCC/OTMC produce the smallest ATS values among those compared. Overlapping subgroup charts (OTCC/OTMC) outperform the corresponding non-overlapping trace chart (NTCC) for moderate/large shifts, and the generalized variance chart (GVC) can perform poorly when correlation increases (ρ>0). The paper also reports subgroup-size effects: for larger shifts, smaller subgroup sizes generally reduce ATS, while some small/moderate-shift cases with p=2 and ρ=0 favored larger n for certain subgroup charts.","The authors restrict numerical investigations to two dimensions settings (p=2 and p=10) and only a small set of subgroup sizes (three n values per p) when studying subgroup effects. They also assume the in-control parameters $(\mu_0,\Sigma_0)$ are known “for simplicity,” noting that in practice they are estimated in Phase I but arguing results are not expected to change materially. They do not attempt to find the truly optimal subgroup size, suggesting this as future work.","The main comparison is largely simulation-based with Gaussian, equidistant observations; robustness to non-normality and model misspecification for the newly tuned overlapping charts (OTCC/OTMC) is not fully explored here. Overlapping subgroup methods induce serial dependence; although they calibrate limits by numerical search to achieve ATS0, practical guidance for maintaining calibration under parameter estimation error (Phase I) and real-world autocorrelation beyond overlap-induced dependence is limited. Comparisons are restricted to a small set of dispersion charts (trace/determinant-based), omitting other modern covariance-monitoring approaches (e.g., eigenvalue-based, likelihood/GLR-based, high-dimensional regularized methods) that may be competitive especially when p is large.","They suggest studying the “actual sample size to yield the optimal performance” for subgroup-based multivariate dispersion monitoring, beyond the three subgroup sizes examined. They also indicate that redesigning the MEWMS lower control limit (LCL) could improve its performance for decreases in dispersion.","Extend the study to broader p (including high-dimensional p≫n) and to non-normal/heavy-tailed distributions with robust or rank-based dispersion charts, especially for OTCC/OTMC. Incorporate explicit Phase I estimation effects (including contaminated Phase I data) and provide practical guidance for choosing n and recalibrating limits when parameters are estimated. Evaluate performance under additional dependence structures (true process autocorrelation, not only overlap-induced), and provide software implementations for OTCC/OTMC to support adoption and reproducibility.",1906.08038v1,local_papers/arxiv/1906.08038v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:06:21Z
TRUE,Multivariate|Image-based monitoring|Nonparametric|Other,EWMA|Other,Both,Manufacturing (general)|Theoretical/simulation only,FALSE,NA,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Detection probability|Other,"For Phase II DFEWMA the authors set/advise a Phase I reference set size of about m0=100 (and note Chen et al. recommend at least 50). For Phase I startup evaluation, they simulate datasets of 25 in-control + 25 out-of-control parts and use a nominal false alarm probability α=0.05.",TRUE,MATLAB|R,Supplementary material (Journal/Publisher),NA,"The paper proposes an intrinsic, registration-free SPC framework for 3D part geometry represented as meshes (extendable to point clouds/voxels) by monitoring the lower spectrum of an estimated Laplace–Beltrami (LB) operator. Each scanned part is converted to a discrete LB operator (using a localized mesh Laplacian) and its first p eigenvalues form a multivariate monitoring vector; this avoids requiring point-to-point correspondence and equal mesh sizes. For Phase II online monitoring, the authors adapt Chen et al. (2016) distribution-free multivariate EWMA (DFEWMA) rank-permutation chart to the eigenvalue vectors, targeting small changes in shape and/or size while controlling false alarms under non-normality. For Phase I startup without prior in-control data, they use Capizzi & Masarotto (2017) a distribution-free multivariate Phase I procedure (dfphase1) applied to LB spectra to control false alarm probability and detect initial outliers/shifts. Extensive simulations (ARL/SDRL, alarm probabilities) compare LB-spectrum monitoring against an ICP-registration objective and a GP-based surface monitoring method, and a post-alarm diagnostic uses ICP registration to localize defect regions on the surface.","The geometry descriptor is the spectrum (eigenvalues) of a discrete Laplace–Beltrami approximation computed from a mesh, e.g., a localized mesh Laplacian that yields a Laplacian matrix $L^t_K=D-W$ with weights $W_{ij}\propto A(\cdot)\exp(-d(p_i,p_j)^2/(4t))$ (Eq. 9). Phase II monitoring uses the Chen et al. (2016) DFEWMA rank statistic per component $T_{jn}(w,\lambda)$ based on exponentially weighted ranks of the last window of observations (Eq. 13), and combines them via $T_n=\sum_{j=1}^p T_{jn}^2$. Control limits are obtained by permutation to achieve a target false alarm probability $\alpha$ (geometric in-control run-length).","In-control performance of the Phase II DFEWMA chart is close to geometric: with $\alpha=0.05$ (nominal ARL=20), simulated in-control ARLs are ≈20.46 (cylinder) and ≈20.17 (prototype part) using the LB-spectrum method (Table 1). For barrel-shaped cylinder deformations, LB-spectrum detection is much faster than an ICP-objective chart for small shifts: at $\delta=0.0005$, ARL ≈10.79 (LB) vs ≈83.21 (ICP); at $\delta\ge 0.005$, LB signals in ~2 observations while ICP remains much slower (Table 2). For very localized defects on small meshes, ICP can outperform LB-spectrum (Table 3), but LB improves substantially with mesh densification (Loop subdivision). Phase I results using Capizzi–Masarotto control the nominal false alarm probability near 0.05 (Table 6), with detection power concentrated in lower eigenvalues (best around the first ~15; including up to 100 eigenvalues can reduce power due to noise, Tables 7–8).",The authors note their study focuses on surface/mesh (2-manifold) data; voxel (3-manifold) extensions are left for future work. They indicate more work is needed to develop charts that detect changes in variance/noise level (not only mean geometry). They also assume no systematic local bias from scanner optical aberrations and mention that extensions to handle such bias are of interest if calibration is not sufficient.,"The approach depends on choosing tuning parameters for the discrete Laplacian (e.g., heat-kernel bandwidth t, localization radius r, number of eigenvalues p) and these choices may strongly affect sensitivity/robustness but are not fully optimized or given general selection rules. While the method is registration-free for detection, the post-alarm localization still relies on ICP registration, which may be unstable for subtle global deformations or high/noisy variability and can reintroduce computational complexity. Comparisons are largely simulation-based with specific defect types; broader real-data validation across diverse part families, sensor modalities, and process fault mechanisms is limited. The multivariate rank-permutation chart assumes exchangeability under the in-control condition; practical effects of temporal dependence (e.g., autocorrelated production streams) are not addressed.","They propose extending the framework to voxel (3D volumetric) data using tetrahedralization and FEM-based LB approximations. They also call for developing monitoring methods to detect changes in variance/noise (not just mean geometry). Finally, they mention extensions to address systematic local bias due to scanner optical aberrations when calibration is not effective.","Develop data-driven or adaptive selection of (t,r,p) (e.g., minimizing in-control estimation error while maximizing OC sensitivity) and study robustness to mesh quality and preprocessing choices. Extend the Phase II procedure to explicitly handle autocorrelation (e.g., batch means, prewhitening on spectral features, or rank-based charts for dependent data). Provide scalable, open-source implementations (e.g., an R/Python package) and benchmarking datasets to facilitate adoption. Investigate defect localization methods that remain intrinsic/registration-free (e.g., local spectral signatures or heat-kernel-based anomaly maps) to reduce reliance on ICP after alarms.",1907.00111v3,local_papers/arxiv/1907.00111v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:07:10Z
TRUE,Univariate|Other,Change-point|Other,Phase II,Theoretical/simulation only,TRUE,TRUE,NA,Simulation study|Other,ARL (Average Run Length)|Detection probability|Other,"Not discussed (model parameters assumed known; simulations repeated 20,000 times for evaluation).",TRUE,R,Not provided,https://CRAN.R-project.org/package=spc,"The paper proposes an online (Phase II) level-shift/mean-shift detection procedure for autocorrelated data streams by adapting Tsay’s (1988) offline time-series outlier/level-shift test to a moving-window framework. The method monitors a vector of correlated pointwise Wald statistics, computed from one-step prediction errors of an assumed-known ARMA model, and signals using the within-window maximum (infinity norm). To control the in-control false alarm rate despite dependence across the window, the authors provide simulation-based algorithms to choose a critical value for a desired in-control ARL (target ARL0=370.4) as window size increases. Performance is evaluated on simulated stationary AR(1) processes with injected level shifts and compared against a two-sided CUSUM applied to one-step prediction errors, showing comparable detection speed when tuned well but improved probability of pinpointing the correct change point and easier tuning for more complex ARMA behavior. The study emphasizes that conventional SPC assumptions (independence/standard ARL interpretation) break down under autocorrelation and motivates model-based monitoring tailored to serial dependence.","The ARMA process is written in AR form to compute one-step prediction errors $e_t=x_t-\hat x_t$, with $\hat x_t=\sum_{i=1}^{p^*}\pi_i x_{t-i}$. Under a mean level shift of size $\tau$ at time $t^*$, the error has structure $e_t\approx \tau H(B)I^{t^*}_t+a_t$, implying $\mathbb{E}[e_{t^*+i}]=\tau\eta_i$. For each candidate change time $d$ in the window, an unbiased estimator $\hat\tau_{d,T}=\rho_{d,T}^2\left(e_d+\sum_{i=1}^{T-d}\eta_i e_{d+i}\right)$ yields the Wald statistic $\lambda_{d,T}=\hat\tau_{d,T}/(\rho_{d,T}\sigma_a)$; the chart statistic is $\Lambda=\|\Lambda_T\|_\infty=\max_{d\in\text{window}}|\lambda_{d,T}|$ and signals if $\Lambda\ge h$.","Critical values $h$ depend on both window size $K$ (up to 100) and AR(1) parameter $\phi_1\in[-0.95,0.95]$; estimated values range roughly from about 3.0 up to about 3.4 for target ARL0=370.4. In simulations (20,000 repetitions), both the proposed moving-window method and CUSUM on one-step prediction errors can achieve similar out-of-control ARL (ARL1) when tuned appropriately, but CUSUM is sensitive to the slack parameter while the proposed method is mainly sensitive to too-small window size. The proposed method shows higher probability of identifying the correct change-point location (reported as fraction of signals within $t^*\pm 10$) and is described as easier to tune for more complicated ARMA processes because the relevant window-size search range is process-independent. Reported relative-performance summaries (Table 1) show median ARL1 ratio near 0.96 (CUSUM/proposed) under optimal tuning, with extremes ranging from about 0.57 to as high as ~29–34 depending on settings and conditions.","The authors restrict the empirical evaluation primarily to stationary AR(1) processes, noting they have not “fully exploited” advantages for more general ARMA(p,q) models. They also assume the ARMA model parameters are known (stationary and invertible), which simplifies deployment and critical-value selection compared with typical practice where parameters are estimated.","The method’s performance and false-alarm properties under parameter estimation uncertainty (Phase I estimation error), model misspecification, or nonstationarity are not quantified, yet these are common in practice with autocorrelated streams. Critical values are obtained via simulation for each (model, K) combination, which may be computationally heavy for practitioners and could require careful reproducibility controls; no shared implementation is provided. The approach is developed for Gaussian innovations and may be less robust under heavy tails/outliers beyond the modeled level shift, potentially inflating false alarms or degrading change-point localization.","The authors recommend that future work should investigate extension of the approach to multivariate time series. They also suggest that applying the method to more general ARMA processes is expected to reveal further advantages compared to alternatives (e.g., easier tuning under more complex one-step prediction error mean patterns).","Develop a full Phase I–Phase II workflow that estimates ARMA parameters online or robustly from in-control data, and quantify how estimation error impacts ARL0/ARL1 and critical values. Extend the approach to handle non-Gaussian innovations (e.g., robust or nonparametric versions) and missing/irregular sampling common in sensor streams. Provide open-source software (e.g., an R/Python package) with efficient critical-value calibration, default window-selection guidance, and diagnostics to help practitioners detect model misspecification and interpret signals.",1907.05453v2,local_papers/arxiv/1907.05453v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:07:44Z
TRUE,Univariate|Other,GLR (Generalized Likelihood Ratio)|Change-point,Phase II,Energy/utilities,TRUE,TRUE,FALSE,Simulation study|Other,ARL (Average Run Length)|False alarm rate|Expected detection delay|Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a real-time transmission line outage detection and localization scheme using streaming PMU voltage phase angles and a dynamic, time-varying linearization derived from the AC power-flow Jacobian. It formulates outage detection as quickest distribution-change detection, where the pre- and post-outage distributions of angle increments are multivariate Gaussian with covariance driven by the (time-updated) Jacobian under each outage scenario. A generalized likelihood ratio (GLR) procedure is used to sequentially accumulate evidence over unknown change time and unknown outage scenario, with a detection threshold selected to meet a desired in-control false alarm rate (specified via ARL0/mean time to false alarm). Extensive simulation on IEEE 39-bus and 2383-bus systems (with full and limited PMU deployments) demonstrates fast detection (often sub-second in the 39-bus case) and reasonable identification accuracy, while highlighting sensitivity to PMU placement and topology. The method is positioned as more suitable than quasi-steady-state/DC approaches for capturing transient post-outage dynamics and as computationally efficient for online use.","The monitoring uses the GLR statistic with per-scenario log-likelihood ratio $Z_k(\ell)=\log\{f_\ell(\Delta\theta_k\mid\theta_{k-1})/f_0(\Delta\theta_k\mid\theta_{k-1})\}$. With Gaussian models, $Z_k(\ell)=\ln|J_\ell|-\ln|J_0|+\frac{1}{2\sigma^2}\,\Delta\theta_k^T\,(J_0^T J_0 - J_\ell^T J_\ell)\,\Delta\theta_k$. The recursive per-scenario statistic is $W_{\ell,k}=\max\{0,\,W_{\ell,k-1}+Z_k(\ell)\}$ and the overall statistic is $G_k=\max_{\ell\in\mathcal{L}} W_{\ell,k}$ with stopping time $D=\inf\{k\ge 1: G_k\ge c\}$; the threshold is approximated by $c=\ln(\mathrm{ARL}_0\times p)$ where $p$ is the number of PMUs.","Thresholds for multiple mean-times-to-false-alarm (e.g., 1 to 30 days) and PMU counts are tabulated using $c=\ln(\mathrm{ARL}_0\times p)$ (e.g., for 39 PMUs and ARL0=1 day, $c\approx 18.43$). In a 39-bus limited-PMU example, an outage at 3.0 s is detected at about 3.5 s (≈0.5 s delay), and per-sample processing time on a laptop is reported as ~1 ms for Jacobian evaluation plus ~0.227 ms for updating all scenario statistics. Compared against a DC-model quickest detection method and an Ohm’s-law CUSUM-type method on selected lines, the proposed “AC-limited” method achieves markedly smaller delays (e.g., for line 27: ~0.001–0.004 s vs ~3.3–3.9 s for Ohm’s-law limited, while DC-full can be several seconds). On the 2383-bus system with 1000 PMUs, reported delays for some lines are ~1.37–4.9 s, with several missed detections under the tested setup.","The authors note that with limited PMU deployment, the computed Jacobian diagonal terms can be inaccurate due to unobservable neighbor buses; they treat missing neighbor terms as 0 and suggest careful PMU placement to mitigate this. They also state that optimal PMU placement (to reduce detection delay and improve identification with limited sensors) is beyond the scope of the paper. For large systems (2383-bus), they report that single-line outage detection is more difficult and that several outages are undetected in their experiments.","The statistical model assumes independent Brownian-motion increments for active-power mismatches and leads to conditionally Gaussian angle-increment models; real PMU data may exhibit heavier tails, colored noise, and model mismatch during transients, which could affect false-alarm control. The threshold selection $c=\ln(\mathrm{ARL}_0 p)$ is an approximation and is not validated via in-control run-length simulations under the full dynamic model, so achieved ARL0 may deviate from nominal. The approach requires enumerating outage scenarios and repeatedly evaluating likelihoods for all $\ell\in\mathcal{L}$, which can become computationally and memory intensive for multi-line contingencies or very large networks unless additional pruning/approximation is used.","They propose investigating the optimal number and placement of a limited number of PMUs to reduce detection delays and improve identification accuracy. They also propose incorporating generator dynamics into the system model, aiming for a more detailed physical model to further improve outage detection and identification.","A robust/nonparametric or heavy-tail-aware version of the GLR (or robust covariance modeling) could improve reliability under realistic PMU noise and model mismatch. Extending the method to explicitly handle missing/irregular PMU data streams and communication dropouts would improve deployability. More principled calibration of thresholds via Monte Carlo/Markov-chain/integral-equation ARL estimation under the proposed conditional model, and benchmarking on real outage PMU datasets, would strengthen practical false-alarm guarantees.",1911.01733v2,local_papers/arxiv/1911.01733v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:08:19Z
TRUE,Multivariate|Other,Hotelling T-squared|Machine learning-based|Other,Both,Energy/utilities,NA,FALSE,TRUE,Case study (real dataset)|Other,False alarm rate|Other,Not discussed,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a condition-monitoring/early-diagnostics methodology for hydropower plants based on a novel Key Performance Indicator (KPI) derived from a trained Self-Organizing Map (SOM) modeling nominal multivariate behavior. For each new observation vector, an SOM distortion measure is computed and normalized by the average nominal distortion to yield a bounded KPI; alarms are triggered using a 3-sigma lower control limit computed from the training KPI distribution and the KPI is additionally smoothed with an EWMA-type filter over the last 12 hours. The method also provides variable-level fault localization by comparing normalized per-variable contributions to the distortion against their nominal averages and flagging unusually large ratios. Performance is validated in an online field deployment on two Italian hydropower plants over more than one year (Apr 2018–Jul 2019), during which the system detected 20+ anomalous situations; it is compared against a benchmark multivariate Hotelling $t^2$ control chart. The authors report that the SOM-based KPI yields earlier and cleaner warnings (fewer false positives and earlier detection in the showcased cases) than the Hotelling $t^2$ chart for the same monitoring problem.","The SOM best-matching unit is $c=\arg\min_i\|r-m_i\|$. The SOM distortion for a sample is $DM(r)=\sum_{i=1}^D w_{ci}\|r-m_i\|$ with neighborhood weights $w_{ci}=\exp\{-d(c,i)^2/(2\sigma^2)\}$, and the nominal average is $DM_\Delta=\frac{1}{N}\sum_{r\in\Delta}DM(r)$. The proposed KPI is $KPI(r)=\frac{1}{1+\left|1-\frac{DM(r)}{DM_\Delta}\right|}$ and the alarm threshold is a 3-sigma lower control limit $LCL_{kpi}=\mu_{kpi}-3\sigma_{kpi}$ (computed on filtered training KPI values). For comparison, the Hotelling statistic is $t^2(r)=(r-\mu)C^{-1}(r-\mu)^T$ with 3-sigma limits on the historical $t^2$ sequence.","Across two operating hydropower plants monitored online from Apr 2018 to Jul 2019, the system detected more than 20 anomalous situations that were reportedly not flagged by existing plant monitoring systems. In a generator temperature sensor case (Plant B, Oct 2018), the SOM-KPI produced a sharp warning at the anomaly onset, while the Hotelling $t^2$ chart would have generated several false positives earlier in the timeline. In an HV transformer gas-related case (Plant A, Apr–Jun 2018), the SOM-KPI detected the anomaly about 20 days earlier than the Hotelling chart; after operator feedback and removing previously untagged anomalous periods from training, the method retrospectively indicated the fault started about one month earlier. The prevented unit stop in the showcased sensor case was estimated to save roughly 25k€–100k€.","The authors state that the procedure cannot yet be implemented in a fully unsupervised fashion because iterations with plant operators are still needed when alarms are triggered to confirm anomalies and refine the nominal dataset. They also present the method as a first step toward predictive maintenance, noting that faults are currently detected/diagnosed rather than predicted at an incipient stage.","The alarm limits are set via a heuristic 3-sigma rule on filtered KPI values and an empirical per-variable contribution threshold (1.3), without calibrating to a target in-control false-alarm rate or reporting run-length properties, making performance hard to generalize. The approach relies on the availability of a reasonably clean nominal training set and periodic retraining; concept drift, seasonality, and operating-mode changes may degrade performance without a formal adaptation scheme. Autocorrelation is likely present in 1-minute sensor streams, yet the control-limit logic appears to assume (approximately) independent training residual behavior; smoothing further complicates statistical interpretation. Comparisons are mainly against Hotelling $t^2$; stronger SPC baselines for multivariate monitoring (e.g., MEWMA/MCUSUM/GLR/PCA-based monitoring) are not evaluated.","The authors suggest moving toward fully automatic predictive maintenance schemes where faults are not only observed but predicted ahead of time, ideally at an incipient stage. They also imply future work on reducing the need for operator-in-the-loop iterations so the approach can be more fully unsupervised in practice.","Calibrate thresholds using run-length/false-alarm guarantees (e.g., via bootstrap, block bootstrap for dependence, or setting limits to achieve a desired in-control ARL) and report ARL/ATS benchmarks. Extend the method to explicitly handle autocorrelation and multiple operating modes (e.g., regime-dependent SOMs, adaptive/control-limit scheduling, or state-space residual monitoring). Provide broader comparative studies versus established multivariate SPC methods (MEWMA/MCUSUM/GLR, PCA/PLS-based charts) under realistic shift scenarios and sensor faults. Release an implementation (or at least pseudocode and parameter defaults) and add guidance on selecting SOM size, retraining frequency, smoothing parameters, and handling missing/frozen data.",1911.06242v1,local_papers/arxiv/1911.06242v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:08:58Z
TRUE,Univariate|Nonparametric|Other,Shewhart|Change-point|Other,Phase I,Energy/utilities|Environmental monitoring|Other,FALSE,TRUE,NA,Simulation study|Case study (real dataset)|Other,False alarm rate|Detection probability|Other,"Subgrouped hourly with subgroup size n = 6 (six successive 10-min observations). In RS/P, the minimum segment length between step changes is recommended as l_min = 5; maximum number of change points set to k = 50 in the case study.",TRUE,R,Not provided,https://opendata-renewables.engie.com/pages/home/,"The paper proposes a Phase I SPC procedure for wind-turbine SCADA data that first models the turbine power curve and then applies a distribution-free Phase I control chart to identify out-of-control (OC) historical segments. Power output is adjusted via a nonparametric Multivariate Adaptive Regression Splines (MARS) model that incorporates multiple weather covariates and seasonal effects, and autocorrelation in residuals is reduced using iterative feasible GLS (Cochrane–Orcutt) with an AR(p) error structure until residuals pass Box–Ljung tests. On the (approximately) independent residuals, the authors apply the Recursive Segmentation/Permutation (RS/P) distribution-free Phase I chart to detect mean level changes across a broad class of OC patterns (isolated shifts, single/multiple steps, and trends), with significance assessed via permutations and an aggregated statistic over multiple k-change models. A real-data case study on the La Haute Borne wind farm SCADA dataset shows the method can reveal informative OC segments that are not obvious from the raw power curve after rough filtering. The work advances SPC for SCADA monitoring by combining flexible power-curve adjustment, explicit serial-correlation removal, and a nonparametric Phase I change-segmentation chart with controlled false-alarm behavior without assuming a parametric in-control distribution.","The SCADA power curve is modeled as $Y_t=f(X_t)+u_t$ with autocorrelated errors $u_t=\sum_{\theta=1}^p a_{\theta}u_{t-\theta}+\varepsilon_t$, where $\varepsilon_t$ are i.i.d. of unknown distribution. MARS estimates $\hat f(X_t)=\sum_{s=1}^S \alpha_s B_s(X_t)$ with spline basis functions and selects complexity via GCV; then IFGLS iterates between estimating AR parameters on residuals $\hat u_t=Y_t-\hat f(X_t)$ and refitting coefficients by least squares to obtain independent residuals $r_t=Y_t-\mathbb E[Y_t\mid X_t]$. RS/P groups residuals into $m$ subgroups of size $n$ and uses an isolated-shift statistic $T_0=\max_i|\bar r_i-\bar r|$ plus recursive-segmentation step-shift statistics $T_k$; an aggregated statistic $W=\max_{k=0,\ldots,K}(T_k-u_k)/v_k$ is calibrated by permutation to yield a p-value.","In the La Haute Borne case study, adding IFGLS to MARS reduced power-curve RMSE from 39.18 to 30.08 (dataset 1) and from 42.86 to 35.84 (dataset 2), indicating improved fit after accounting for serial correlation. For Phase I charting, residuals were subgrouped hourly (n=6) and RS/P was run with maximum change points k=50 and significance threshold p=0.05. On the larger dataset, removing the first 4 detected OC segments increased the aggregated-statistic p-value (reported as 0.002 after excluding 4 segments), and excluding 4 more segments raised the p-value above 0.05, suggesting iterative removal can yield an in-control reference set. Detected OC segments exhibited diverse patterns (short-term deviations, down-rating-like low-mean periods, increased variance, pre-shutdown behavior), supporting the chart’s ability to flag patterned anomalies beyond simple outliers.","The authors note they only focus on level (mean) changes in the residual process and do not address scale/variance shifts, which are complicated by stochastic wind patterns and require further development. They also state they did not monitor the coefficients of the power-curve model itself, and that both variance-shift detection and coefficient monitoring are non-trivial extensions. Finally, they acknowledge that precise turbine status cannot be verified from available records in the case study, so explanations for detected segments are only possible interpretations requiring more information and expert diagnosis.","The Phase I procedure depends on the quality of the initial “rough filtering”; if filtering misses large anomalies or removes too much, both the fitted power curve and RS/P segmentation may be biased. The autocorrelation-removal step assumes a stationary AR(p) structure and uses Box–Ljung tests to declare independence; residual dependence, nonstationarity, or conditional heteroskedasticity could inflate false alarms or reduce detection power. RS/P is applied to subgroup means (hourly), so performance and interpretability may be sensitive to the chosen subgroup size n, minimum segment length $l_{min}$, and maximum K; the paper provides limited robustness analysis over these tuning parameters. Computational cost of permutation-based calibration for large T (especially with repeated re-fitting/iteration) could be substantial in practice, but runtime/scalability is not reported.","They propose extending the approach to detect scale (variance) shifts in the residual process, which may contain valuable information but is non-trivial under stochastic wind conditions. They also suggest monitoring the coefficients in the power-curve model (i.e., stability of the power-curve relationship itself), noting this is also non-trivial and requires further development.","A natural extension is a unified framework that simultaneously models autocorrelation and heteroskedasticity (e.g., ARMA/GARCH or state-space errors) before Phase I charting, with theoretical control of false-alarm under remaining dependence. Extending RS/P-style distribution-free Phase I methods to profile monitoring of the power curve (monitoring function/parameter drift directly) could improve diagnostic value compared with residual-only monitoring. More extensive benchmarking against alternative Phase I segmentation and nonparametric charts under serial correlation (with matched false-alarm rates) and reporting of computational complexity would strengthen guidance for practitioners. Packaging the full workflow (filtering, MARS+IFGLS, RS/P iteration) into reproducible software with default tuning guidance for SCADA sampling regimes would aid adoption.",1912.04045v2,local_papers/arxiv/1912.04045v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:09:38Z
TRUE,Multivariate|Nonparametric|High-dimensional,Shewhart|CUSUM|EWMA|MEWMA|MCUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Other,Phase II,Theoretical/simulation only|Food/agriculture,NA,TRUE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|Steady-state ARL|Other,"Focus is on individual observations (often n=1). Phase I size is denoted m; many reviewed charts assume parameters known, while nonparametric charts (MNSE/MSRE) are noted as requiring a large amount of Phase I data to estimate the in-control distribution and set limits.",NA,None / Not applicable,Not applicable (No code used),NA,"This paper is a literature review of multivariate dispersion (covariance matrix) control charts for processes observed as individual vectors (multivariate individual observations), covering 30 articles from 1987–2019. It classifies Phase II monitoring methods into five groups—CUSUM-type, MEWMA-type, Shewhart-type, nonparametric, and high-dimensional—and summarizes the monitoring statistics used (e.g., trace, likelihood-ratio/Alt statistic, norms, Wilks’ statistic, dissimilarity indices, spatial sign/rank). The review highlights that most proposed dispersion charts assume multivariate normality and independent observations, with relatively fewer contributions for CUSUM, nonparametric, and high-dimensional settings. It reports how prior work evaluates performance (primarily via run-length metrics such as ARL/SDRL/MRL, including steady-state ARL for some nonparametric work) and notes practical issues such as parameter estimation effects and difficulty detecting variance decreases. The paper concludes with concrete research gaps (e.g., better handling of skewed statistic distributions/control limits, decreased-variance detection, transient shifts, robustness to non-normality/outliers, and serial dependence).","The paper is a review but presents representative defining equations from reviewed chart families. Examples include the standardization $\mathbf{Y}_i=\Sigma_0^{-1/2}(\mathbf{X}_i-\mu_0)$, a likelihood-ratio CUSUM recursion $T_i=\max\{T_{i-1}+\log(f_B(\mathbf{X}_i)/f_G(\mathbf{X}_i)),0\}$ (simplifying under $\Sigma\to C\Sigma_0$ to a quadratic form minus $K$), and MEWMA covariance recursions such as $\mathbf{S}^{EWMA}_i=\lambda\mathbf{Y}_i\mathbf{Y}_i' + (1-\lambda)\mathbf{S}^{EWMA}_{i-1}$. Trace- or LR-type dispersion statistics discussed include $\mathrm{tr}(\mathbf{S}^{EWMA}_i)$ and Alt-type $c_i=\mathrm{tr}(\mathbf{S}^{EWMA}_i)-\log|\mathbf{S}^{EWMA}_i|-p$, with signals when statistics exceed control limits (often chosen to attain a target in-control ARL).","The review reports qualitative comparative findings rather than a single new method with headline numbers. It notes that Cheng & Thaga’s Max-MCUSUM outperformed competitors for simultaneous small mean-and-covariance shifts in their study, and that Bodnar & Schmid recommend modified MCUSUM-type schemes for covariance monitoring in multivariate time series (including residual-based variants). For MEWMA dispersion monitoring, Gunaratne et al. propose a parallelized Monte Carlo approach to calibrate control limits up to about $p=15$, and Li et al. (2013) show the nonparametric MNSE chart is robust under non-normality (steady-state ARL studied) but can be less efficient than MEWMC for large shifts. Huwang, Lin & Yu (2019) find their MSRE chart often improves detection of increasing variance and simultaneous variance/correlation shifts versus MNSE, at the cost of needing resampling for limits. The paper also highlights known issues in the literature such as symmetric limits being inappropriate for skewed statistics and generally weaker detection for decreases in variance.","As a review focused on dispersion charts for multivariate individual observations, the paper excludes charts primarily designed for mean monitoring (e.g., Hotelling $T^2$ and standard MEWMA for mean), even if they react to variability shifts. It also emphasizes that most reviewed methods assume independent observations and (for many charts) multivariate normality, and notes that many methods assume known in-control parameters, limiting direct practical applicability. For nonparametric methods, it explicitly notes the drawback that a large amount of Phase I data is required to reflect the in-control distribution and set control limits.","Because this is a narrative review rather than a systematic review/meta-analysis, inclusion/exclusion decisions (Google Scholar search, >200 screened, 30 selected) may be difficult to reproduce exactly, and no PRISMA-style flow or full search strings are provided. The paper aggregates comparative statements from different studies that may use different shift models, calibration targets (e.g., ARL0), and initialization (zero-state vs steady-state), which can make cross-paper performance conclusions non-uniform. It does not provide a unified benchmark simulation comparing all 30 methods under the same design assumptions, which would strengthen practitioner guidance. Software/implementation guidance is limited, so adoption barriers (tuning, computational cost, numerical stability for high-dimensional precision estimation) are not deeply addressed.","The paper calls for more research on (i) CUSUM/MCUSUM dispersion charts, (ii) nonparametric dispersion charts, and (iii) high-dimensional covariance monitoring. It highlights needs for correct/asymmetric control limits for skewed monitoring statistics, improved ability to detect decreases in variance, and development of charts for transient shifts. It also recommends evaluating steady-state performance (not just zero-state), investigating robustness to outliers/non-normality, and studying the effects of estimating in-control parameters rather than assuming them known. It further notes the need for methods that address serial dependence when independence is violated.","A valuable extension would be a standardized open benchmark (common data-generating processes, calibration targets, and reporting of ARL/SDRL/MRL/EDD under both zero-state and steady-state) to enable fair, reproducible comparisons across dispersion charts for individual observations. Developing unified diagnostic tools to localize which variances/covariances changed (post-signal interpretation) would increase practical usefulness, especially for sparse/high-dimensional changes. More work is also needed on robust and autocorrelation-aware dispersion monitoring (e.g., model-based residual charts, bootstrap/resampling limits under dependence) with explicit guidance on when each approach is preferable. Finally, providing reference implementations (R/Python packages) for major dispersion-chart families would materially improve real-world uptake.",1912.09755v1,local_papers/arxiv/1912.09755v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:10:18Z
TRUE,Multivariate|Other,Shewhart|Other,Both,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,NA,Markov chain|Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Other,"Design and tables consider subgroup sample sizes n ∈ {5, 10, 15} for monitoring; illustrative Phase II example uses n = 5. No general minimum Phase I sample-size guidance is provided beyond using Phase I to estimate γ0.",TRUE,None / Not applicable,Not provided,NA,"The paper develops one-sided multivariate control charts to monitor the multivariate coefficient of variation (MCV) for p-variate normal data, using a Shewhart-type MCV statistic augmented with supplementary run rules (2-of-3, 3-of-4, and 4-of-5). The MCV is defined via the Mahalanobis distance, and the sample MCV’s distribution is expressed through a noncentral F distribution, enabling probability calculations for in-control/out-of-control regions. Statistical performance (ARL and SDRL) for the proposed run-rules charts is evaluated via an embedded Markov chain approach, and control limits are selected to achieve a common in-control ARL0 = 370.4. Numerical studies show substantial ARL improvements over the baseline Shewhart-MCV chart in most scenarios, especially for detecting decreases in MCV (process improvement), while Shewhart may be better for very large shifts. A spring manufacturing Phase II dataset illustrates that the run-rules charts can signal out-of-control conditions when the Shewhart-MCV chart does not, and the authors argue the approach is simpler to implement than a Run Sum MCV chart while often providing competitive or better overall performance (via EARL over a range of shift sizes).","The population MCV is defined for X ~ N_p(μ,Σ) as γ = (μ^T Σ^{-1} μ)^{-1/2}. With sample mean \bar{X} and sample covariance S, the sample MCV is \hat{γ} = (\bar{X}^T S^{-1} \bar{X})^{-1/2}; its CDF is given using a noncentral F distribution: F_{\hat{γ}}(x|n,p,δ) = 1 - F_F( n(n-p)/((n-1)p x^2) | p, n-p, δ ), with δ = n μ^T Σ^{-1} μ = n/γ^2. Run-rules signaling is r-out-of-s consecutive points beyond a single control limit (lower-sided: \hat{γ}<LCL^-; upper-sided: \hat{γ}>UCL^+), and ARL/SDRL are computed from the Markov chain transient matrix Q via ν1 = q^T(I-Q)^{-1}1 and SDRL from ν2, μ2.","Control limits are tabulated for n ∈ {5,10,15}, p ∈ {2,3,4}, γ0 ∈ {0.1,…,0.5} to achieve ARL0 = 370.4. Across extensive ARL1/SDRL1 tables, the run-rules MCV charts generally reduce ARL relative to the Shewhart-MCV chart for many small-to-moderate shifts; performance gains are especially strong for downward (τ<1) shifts, where the 4-of-5 scheme often performs best. For unknown shift sizes modeled as uniform over [0.5,1) (down) or (1,2] (up), expected ARL (EARL) comparisons still favor run-rules charts in most scenarios, and the paper recommends RR^-_{4,5} for decreases and RR^+_{2,3} for increases. In the spring manufacturing example (Phase II, n=5, estimated γ0≈0.0891), the Shewhart UCL=0.1691 yields no signal, while run-rules UCLs (0.1296, 0.1106, 0.0986) produce out-of-control signals around samples 4–6 depending on the rule.",None stated.,"The approach assumes multivariate normality and (implicitly) independent subgroups; robustness to non-normality, heavy tails, or autocorrelation is not examined. The paper provides extensive numerical tables but does not clearly document software/implementation details or provide reproducible code. The method focuses on monitoring the scalar MCV and does not address diagnosing which variable(s) or covariance/mean components drive an MCV shift, which may limit practical root-cause analysis.",None stated.,"Extend the run-rules MCV charts to settings with autocorrelated multivariate processes (e.g., via residual charts or state-space models) and assess robustness under departures from multivariate normality (e.g., elliptically contoured or skewed distributions). Develop diagnostic add-ons to localize whether shifts are primarily due to changes in mean vector, covariance structure, or specific variables. Provide open-source software (e.g., R/Python) to compute control limits and ARL/EARL via the Markov chain construction for general (r,s), n, and p.",2001.00996v2,local_papers/arxiv/2001.00996v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:10:52Z
TRUE,Univariate|Other,Shewhart|Other,Phase II|Both,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,NA,Simulation study|Markov chain|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Other,Not discussed (examples consider subgroup/sample sizes n = 5 and n = 15; illustrative example references a Phase I dataset to estimate γ0).,TRUE,None / Not applicable,Not provided,NA,"The paper studies Run Rules control charts for monitoring the squared coefficient of variation (CV^2) when measurements are contaminated by measurement error. It proposes replacing the earlier two-sided Run Rules chart on CV with two one-sided Run Rules charts on CV^2 (RR− and RR+), aiming to avoid ARL bias caused by the asymmetry of the CV^2 distribution. Chart performance (ARL and SDRL) is computed via embedded Markov chains for 2-out-of-3, 3-out-of-4, and 4-out-of-5 rules, with control limits set to achieve a fixed in-control ARL0 = 370.4. A linear covariate measurement error model (with precision and accuracy error components and repeated measurements per item) is incorporated by modifying the in-control and out-of-control CV and the noncentral F distribution used for CV^2. Simulations and an industrial sintering-process example show the proposed charts improve detection versus prior Run Rules CV charts and Shewhart CV charts, while measurement errors degrade performance and taking multiple measurements per item yields little benefit.","The chart monitors sample CV squared \(\hat\gamma_i^2\) with one-sided run rules using limits \(\mathrm{LCL}^- = \mu_0(\hat\gamma^2) - k_d\,\sigma_0(\hat\gamma^2)\) and \(\mathrm{UCL}^+ = \mu_0(\hat\gamma^2) + k_u\,\sigma_0(\hat\gamma^2)\), where \(\mu_0\) and \(\sigma_0\) are approximated (Breunig) as in Eqs. (4)–(5). The CDF of \(\hat\gamma^2\) is based on a noncentral F distribution: \(F_{\hat\gamma^2}(x\mid n,\gamma)=1-F_F\big(n/x\mid 1,n-1,n\gamma^2\big)\) (Eq. 2), and with measurement error replaces \(\gamma\) by \(\gamma^*\) (Eqs. 15–18). Run-length moments are obtained from a Markov chain with transient matrix \(Q\): \(\mathrm{ARL}=q^T(I-Q)^{-1}\mathbf{1}\) and \(\mathrm{SDRL}=\sqrt{2q^T(I-Q)^{-2}Q\mathbf{1}-\mathrm{ARL}^2+\mathrm{ARL}}\) (Eqs. 11–12).","With ARL0 fixed at 370.4, the two one-sided RR\(_{r,s}\)–\(\gamma^2\) charts yield out-of-control ARLs that are consistently below ARL0, addressing the ARL-biased behavior noted for two-sided CV charts. Example comparison: for \(\gamma_0=0.05\), \(n=5\), \(\tau=1.10\) using RR\(_{2,3}\)–\(\gamma^2\), the paper reports \(\mathrm{ARL}_1=95.9\) versus 101.6 for the earlier two-sided RR–\(\gamma\) chart in Castagliola et al. (2013). Measurement error increases detection delays: increasing precision error ratio \(\eta\) and accuracy error \(\theta\) increases ARL/EARL (e.g., cited EARL increases from 82.27 at \(\eta=\theta=0\) to 84.49 at \(\eta=0.5,\theta=0.05\) for a specific setting). Repeated measurements per item (larger \(m\)) provide negligible ARL improvement in the examined scenarios, while larger subgroup size (e.g., \(n=15\) vs \(n=5\)) improves performance. In the sintering-process example, the RR\(^+\) run-rules charts signal an out-of-control condition around sample #12 whereas the comparable Shewhart CV chart does not.",The paper notes that the noncentral-t approximation for the sample CV CDF is sufficiently precise only when \(\gamma<0.5\) and proceeds under the rationale that small CV values are common in stable processes. It also remarks that many control limits for other parameter settings are not shown in the paper but are available upon request from the authors.,"The method assumes normality and i.i.d. sampling within subgroups; robustness to non-normality and autocorrelation (common in process data) is not assessed. The measurement-error model parameters (A, B, \(\sigma_M\), hence \(\theta,\eta\)) are treated as known, but in practice they often require estimation and uncertainty in these inputs could materially affect ARL calibration. The work focuses on specific run rules (2/3, 3/4, 4/5) and fixed ARL0=370.4; optimal rule selection and sensitivity to ARL0 choices are not explored. Software/implementation details are not provided, which may hinder reproducibility despite the heavy numerical evaluation.",None stated.,"Extend the charts to handle autocorrelated observations (e.g., via residual-based monitoring or time-series modeling) and assess robustness under non-normal distributions. Develop procedures for estimating measurement-error parameters (\(\theta,\eta,B\)) and propagating that uncertainty into control-limit design (Phase I/II with estimated ME). Explore adaptive or economically designed choices of (r,s) and control-limit parameters to optimize EARL over a practitioner-specified shift distribution. Provide open-source software (e.g., R/Python) to compute limits and ARL/EARL for broader adoption and reproducibility.",2001.01821v1,local_papers/arxiv/2001.01821v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:11:35Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Healthcare/medical|Other,NA,NA,NA,Other,Other,Not discussed,TRUE,R|Other,Not provided,https://eralpdogu.shinyapps.io/msstatsqc/|https://drive.google.com/drive/folders/1jbR5DOv4uOae99K4-oU10Q4T3rLcXXDb?usp=sharing,"This paper reports a usability and plot-interpretability evaluation of the MSstatsQC open-source software, which provides longitudinal system suitability monitoring for targeted mass-spectrometry proteomics using control charts and additional summary visualizations. The study uses 4 domain users to perform structured tasks (data import, metric/peptide selection, setting decision rules, and interpreting control charts and other plots) and collects timing, completion, and subjective ratings. The SPC component centers on XmR-style control charts for monitoring metric mean and variability, along with decision maps and change-point-related outputs within the software, but the paper’s novelty is the human–computer interaction evaluation rather than new chart theory. Results identify major usability problems (especially data import and workflow navigation) and interpretability issues (insufficient titles/labels/explanations), leading to concrete interface redesign recommendations (prominent format guidance, pop-up error messages, clearer plot annotations, and tab numbering). The work contributes to SPC practice by highlighting barriers that can prevent correct use of control charts in proteomics QC software and proposing actionable UI changes to reduce user error.",Not applicable,"With 4 test users, none were able to successfully complete the data-import/format-detection portion of Task 1, indicating the software did not adequately communicate required input format and lacked helpful error feedback. All test users completed the control-chart interpretability task successfully, though at least one user expressed confusion and multiple users requested clearer explanations (e.g., meaning of colored points and threshold lines) and better axis labeling/titles. User ratings (1–5 scale) showed the lowest mean attribute was convenience (reported as 2.7/5), attributed to unclear workflow/tab sequencing and insufficient guidance. Qualitative findings also noted difficulty reading peptide names in radar plots and confusion about which plots are affected by decision rules.","The authors state that a key limitation is the lack of qualified researchers available to participate as test users, noting the proteomics community is small and researchers are typically not interested in a 40-minute unpaid study. They also state it was difficult to design tasks that adequately cover usability and plot interpretability while remaining feasible within 30–40 minutes, and they relied on pilot users to refine tasks.","The study uses a very small sample (n=4) with limited diversity of user backgrounds and no comparison across experience levels, which constrains generalizability of the usability findings. The evaluation focuses on subjective usability/interpretability outcomes and task success, without assessing whether different UI issues lead to statistically meaningful differences in monitoring decisions (e.g., false alarms/missed detections) or downstream QC actions. The paper does not clarify which specific control-limit computations or assumptions (distribution/independence) underpin the charts users interpret, which limits SPC-methodological conclusions.","The authors propose redesigning the MSstatsQC interface based on the study findings and then re-evaluating the revised software with another usability test to demonstrate improvements in convenience, satisfaction, comprehension, and aesthetics. They also suggest creating a set of general rules and techniques that proteomics software designers can follow to design their own usability studies.","Future work could quantify how UI/interpretability improvements affect the correctness and consistency of SPC decisions (e.g., agreement rates on out-of-control points, time-to-intervention, and rates of incorrect actions). Additional studies with larger and more heterogeneous user groups (novice vs expert, different labs) and realistic longitudinal datasets could validate robustness of conclusions. Providing and evaluating standardized training materials and embedded explanations (tooltips, guided workflows) could also be tested experimentally to reduce user error in Phase II monitoring setups.",2002.00511v1,local_papers/arxiv/2002.00511v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:12:00Z
TRUE,Multivariate|Nonparametric|High-dimensional|Other,Shewhart|CUSUM|EWMA|Hotelling T-squared|MEWMA|MCUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Machine learning-based|Other,Both,Manufacturing (general)|Semiconductor/electronics|Healthcare/medical|Service industry|Theoretical/simulation only|Other,NA,TRUE,NA,Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|MRL (Median Run Length)|ATS (Average Time to Signal)|ANOS (Average Number of Observations to Signal)|Detection probability|False alarm rate|Expected detection delay|Steady-state ARL|Conditional expected delay|Other,"Not discussed (the review notes Phase I estimation is challenging for covariance monitoring due to many parameters; it poses the question of what Phase I sample size is needed but does not give specific recommended N, m, or n).",NA,None / Not applicable,Not applicable (No code used),NA,"This paper is a literature review of multivariate statistical process monitoring (MSPM) methods for monitoring the process covariance/dispersion matrix, a topic less developed than mean-vector monitoring. It classifies the literature into four groups: (i) covariance monitoring when subgroup size n≥p (full-rank sample covariance), (ii) covariance monitoring when n<p (including individual observations), (iii) joint monitoring of mean vector and covariance matrix, and (iv) related topics such as diagnostics, robustness/nonparametrics, and Phase I analysis. The review surveys major charting approaches including generalized-variance (|S|) and trace-based Shewhart statistics, likelihood-ratio-test charts (two-sided and one-sided for dispersion increase/decrease), memory-type schemes (CUSUM/MCUSUM, EWMA/MEWMA/MEWMC), penalized-likelihood (LASSO/ridge) and sparsity-aware methods for high-dimensional settings, and change-point/GLR and self-starting schemes. It highlights practical issues such as masking/""misleading"" signals in joint schemes, sensitivity to tuning parameters in penalized methods, difficulties in diagnosing which covariance elements changed, and the impact of Phase I estimation on Phase II performance. The paper concludes with research directions including broader method comparisons, improved diagnostics, adaptive schemes, robust/nonparametric covariance monitoring beyond bivariate cases, and methods for autocorrelated multivariate data.","The review formalizes covariance monitoring as hypothesis tests (e.g., $\Sigma=\Sigma_0$ vs $\Sigma\neq\Sigma_0$ or one-sided PSD alternatives) and summarizes representative chart statistics from the literature. Examples include Alt’s (1985) LRT-based Shewhart statistic $R_i=-(n-1)\left[p+\ln\{|S_i|/|\Sigma_0|\}-\operatorname{tr}(\Sigma_0^{-1}S_i)\right]$ for subgroup covariance $S_i$, the VMAX statistic $\mathrm{VMAX}_i=\max_k \sum_{j=1}^n (X_{ijk}-\mu_{0k})^2/(n\sigma_{0k}^2)$, and EWMA covariance updates such as $Z_i=\omega U_iU_i' + (1-\omega)Z_{i-1}$ (MEWMC). It also describes sparsity-regularized precision-matrix estimation via penalized likelihood (graphical LASSO), e.g. $\hat\Omega_i(\lambda)=\arg\min_{\Omega\succ0}\{\operatorname{tr}(\Omega S_i)-\log|\Omega|+\lambda\|\Omega\|_1\}$, with PLR-type charting statistics compared to a simulated UCL.",Not applicable (review paper; it synthesizes findings across many cited studies rather than reporting a single new set of quantitative results).,"The review notes that most existing covariance-matrix charts assume multivariate normality and independent subgroups/observations, with relatively limited work for non-normal data and for autocorrelated multivariate processes. It also emphasizes that Phase I estimation effects on Phase II performance are under-studied for covariance monitoring and that choosing tuning parameters in penalized/sparse methods lacks clear guidance. The authors further highlight that misleading/masking signals can occur in joint mean–covariance monitoring schemes and that diagnosis (identifying responsible variables/covariance elements) is difficult in higher dimensions.","Because it is a broad literature review, the paper does not provide a unified benchmarking protocol (common shift scenarios, identical ARL0 targets, consistent parameter settings) that would allow practitioners to compare methods on equal footing; conclusions about “best” charts remain context-dependent. It does not provide implementation guidance such as decision trees, default parameter choices (e.g., smoothing constants, window lengths), or computational complexity comparisons that matter in high-dimensional online monitoring. Application coverage is mostly illustrative (examples cited) rather than a systematic set of real-data case studies validating multiple chart classes in practice.","The authors call for broader and more systematic comparisons of existing charts (in both Phase I and Phase II) to guide practitioners, including comparisons across individual observations vs (overlapping/non-overlapping) subgroups. They identify open problems on misleading signals in simultaneous mean–covariance schemes, development of adaptive versions of dispersion charts, principled selection of tuning parameters for penalized/sparse methods, and better diagnostic tools to distinguish mean vs covariance changes and isolate responsible variables. They also point to needs for robust/nonparametric covariance charts beyond the bivariate case, clearer understanding of Phase I estimation impacts (including Phase I sample size needs), and methods for autocorrelated multivariate data.","Develop open-source software implementations (e.g., an R/Python package) that standardize computation of key covariance-monitoring charts (|S|, LRT, VMAX/VMIX, MEWMC/MEWMS/MEWMV, PLR/LASSO/ridge, MaxNorm, eigenvalue charts) with calibrated control limits and diagnostics. Establish public benchmark datasets and a simulation suite (including heavy tails, skewness, autocorrelation, missingness/irregular sampling, and high-dimensional sparse/dense shift regimes) to enable reproducible, fair method comparisons. Develop practical guidance for Phase I sizing/cleaning in high dimensions (e.g., robust covariance estimation with limited N, shrinkage priors) and extend covariance-monitoring methods to handle missing data explicitly (e.g., pairwise likelihood, Kalman/EM updates) since many modern sensor streams are incomplete.",2002.06159v2,local_papers/arxiv/2002.06159v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:12:43Z
FALSE,Other,Shewhart|Machine learning-based|Other,Phase II,Network/cybersecurity|Other,TRUE,FALSE,FALSE,Simulation study,Other,Not discussed,TRUE,None / Not applicable,Not provided,https://italiangrid.github.io/storm/,"The paper proposes a real-time log-based anomaly detection framework for data-center predictive maintenance using an evolving Gaussian Fuzzy Classifier (eGFC) trained online on features extracted from sliding time windows of log-entry rates. A Shewhart-style control chart is used to automatically tag each window’s mean log rate into four severity classes (normal, low, medium, high) based on k-sigma bands around the overall mean, creating labels for supervised online learning. The core methodological contribution is the evolving fuzzy-rule-based classifier that incrementally creates, updates, merges, and deletes Gaussian-rule granules to track non-stationary patterns without storing the full stream. Experiments on StoRM backend logs from the INFN Tier-1 Bologna computing center evaluate classification accuracy, model compactness (number of rules), and runtime across different window sizes (60/30/15/5 minutes). The approach achieves its best reported performance with 60-minute windows, indicating that longer windows smooth noise and improve separability of anomaly patterns for this application.","A control-chart labeling scheme computes window means $\mu_j=\frac{1}{n}\sum_{i=1}^n u_i$ and assigns class by whether $\mu_j$ lies within successive bands $[\bar\mu\pm \sigma_k(\mu)]$ for $k=1,2,3,4$ (normal through higher-severity anomalies). eGFC uses fuzzy rules with Gaussian memberships $A_j^i=G(\mu_j^i,\sigma_j^i)$; new rules are created when the T-norm activation of all rules is $\le \rho^{[h]}$, and rule parameters are updated recursively via streaming mean/variance updates for $\mu_j^i$ and $\sigma_j^i$. The activation threshold is adapted over time as $\rho(\text{new})=\frac{\sigma^{[h]}_{\text{avg}}}{\sigma^{[h-1]}_{\text{avg}}}\rho(\text{old})$, with granule merging based on an inter-granular distance threshold $\Delta$.","Across 5 shuffled runs (1,436 samples; 5 features; 4 classes), the best average accuracy is reported for 60-minute windows: $92.48\%\pm 1.21$ with an average of $13.42\pm 4.32$ rules and runtime $0.36\pm 0.10$ seconds. For shorter windows, accuracy drops: 30-min $88.01\%\pm 4.96$ (17.22 rules), 15-min $82.57\%\pm 5.64$ (18.13 rules), and 5-min $81.97\%\pm 5.02$ (16.09 rules). The paper also shows an example confusion matrix with about 94% accuracy where most confusion occurs between adjacent severity classes (especially Class 1 vs Class 2), consistent with overlapping regions in the learned Gaussian granules.","The authors state that analysis of the log message type/content is out of scope; the approach relies only on timestamp-derived log-rate features. They also note the online classification problem is unbalanced due to the control-chart probability mass across sigma bands, which affects class frequencies. They further frame anomaly detection as context-sensitive, implying that labeling and performance depend on the chosen operational context and windowing.","The use of a control chart for labeling effectively makes the “ground truth” anomalies defined by k-sigma excursions of the mean log rate, which may not correspond to actual incidents/failures and can bias evaluation toward detecting rate shifts rather than true maintenance-relevant anomalies. The control-chart rationale invokes CLT/normality and independence, but log-rate time series in data centers are often autocorrelated and nonstationary; labeling and thresholds may therefore be unstable without explicit time-series modeling. Evaluation is limited to accuracy/compactness/runtime on shuffled datasets; streaming order effects, delay to detection, false-alarm behavior, and robustness to parameter choices (e.g., $\rho^{[0]}$, $\Delta$, $h_r$) are not benchmarked against standard SPC metrics like ARL/ATS or against established anomaly detectors on the same stream order.","The authors propose to identify the types of log messages associated with anomalous time windows and to investigate autonomous feature-extraction procedures from log content. They also position the window-tagging strategy as a way to reduce hand-labeling, suggesting further work to leverage it to improve accuracy in evolving classification frameworks.","A natural extension is to evaluate the approach under true online conditions (original temporal order) using detection-oriented metrics (e.g., time-to-detect, false-alarm rate) and to compare against SPC charts designed for autocorrelated/nonstationary data. Another direction is to replace or augment the k-sigma labeling with incident-confirmed labels or hybrid labels that incorporate multiple signals (rates, message templates, system KPIs) to reduce label bias. Providing an open implementation and reproducible pipelines (windowing, labeling, eGFC training) would improve practical adoption and allow sensitivity analysis for key hyperparameters.",2004.13527v1,local_papers/arxiv/2004.13527v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:13:15Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Phase II,Manufacturing (general)|Theoretical/simulation only|Other,TRUE,FALSE,FALSE,Simulation study,False alarm rate|Detection probability|Expected detection delay|Other,Uses Phase I training data size $N$ to estimate $\bar{x}$ and $S$; examples use $N=5000$ training samples (and 500 test samples in the numerical example; 400 test samples in the CSTR case). No general minimum Phase I sample size guidance is provided beyond requiring $N\ge p$ for the $F$-based limit.,TRUE,None / Not applicable,Not provided,NA,"The paper develops moving-average Hotelling’s $T^2$ control charts for detecting intermittent faults (IFs) with small magnitudes and short durations, proposing a bank of MA-$T^2$ charts with multiple window lengths to trade off sensitivity and alarm delay. It introduces a new notion of “guaranteed detectability” for intermittent faults (extending classical detectability notions largely focused on permanent faults) and derives necessary and sufficient detectability conditions linking fault magnitude/direction, active/inactive durations, and the window length. The authors show that unlike permanent faults, overly large windows can reduce IF detectability when the window exceeds the fault active duration, and they derive two ‘important’/optimal window choices ($W^*$ and $W^\#$) under partial fault knowledge. They also propose rules to reduce false alarms and compensate missing alarms by cross-validating alarm intervals across multiple window lengths and by using derived lower bounds on alarm durations. Effectiveness is demonstrated via simulations on a bivariate Gaussian numerical example and a continuous stirred tank reactor (CSTR) process with intermittent sensor faults.","The MA-$T^2$ statistic at time $k$ with window length $W$ is $T_k^2(W)=(\bar{x}^f_k-\bar{x})^\top S^{-1}(\bar{x}^f_k-\bar{x})$, where $\bar{x}^f_k=\frac{1}{W}\sum_{i=1}^W x^f_{k-W+i}$, $\bar{x}=\frac{1}{N}\sum_{i=1}^N x_i$, and $S=\frac{1}{N-1}\sum_{i=1}^N (x_i-\bar{x})(x_i-\bar{x})^\top$. Under the in-control model with independent multivariate normal data, $T_k^2(W)\sim \frac{p(N+W)(N-1)}{NW(N-p)}F(p,N-p)$, yielding control limit $\delta_W^2=\frac{p(N+W)(N-1)}{NW(N-p)}F_\alpha(p,N-p).$ Detectability results include the window constraint for disappearance $W\le \tau_q^r$ and appearance conditions of the form $\|S^{-1/2}\xi_q f_q\|>2\delta_W$ (or scaled by $\tau_q^o/W$ when $W>\tau_q^o$), plus the derived optimal window choices $W^*=\left\lceil \frac{N}{\frac{N+1}{4\delta^2}\|S^{-1/2}\xi_q f_q\|^2-1}\right\rceil$ and $W^\#=\min\{\tau_{q-1}^r,\tau_q^o,\tau_q^r\}$.","The paper derives necessary and sufficient guaranteed-detectability conditions for intermittent faults, including (i) disappearance detectability iff $W\le \tau_q^r$ and (ii) appearance detectability conditions that depend on whether $W\le \tau_q^o$ or $W>\tau_q^o$ (fault effect diluted by $\tau_q^o/W$). It shows that alarm delays for appearance/disappearance increase with window length and provides formulas/bounds for these delays (e.g., disappearance delay $\nu_q^d(W)=W-1$). In simulations, charts with small windows (e.g., $W=1,2$) exhibit many false/missing alarms, while windows within the theoretically predicted detectable range perform well (numerical example: detectable for $W\in[7,10]$; CSTR example: detectable for $W\in[5,10]$). Using multiple window lengths and intersection-based logic substantially reduces false/missing alarms and yields bounded intervals for inferred fault appearance/disappearance times that contain the true change times in both examples.",None stated.,"The method assumes independent (non-autocorrelated) observations and multivariate normality for the $F$-based $T^2$ limits; many industrial processes exhibit autocorrelation and non-Gaussian behavior, which can inflate false alarm rates or distort detectability conditions. The proposed multi-window fusion and false/missing-alarm logic relies on parameters (or lower bounds) of IF magnitudes/durations to set/limit window ranges, which may be hard to specify reliably in practice. Evaluation is limited to simulations (including a simulated CSTR) rather than real industrial datasets, and the paper does not report standard SPC run-length metrics (e.g., IC/OC ARL), making comparisons to SPC benchmarks harder.","The authors suggest extending MA-TCCs(M) beyond the single steady-state (single-mode) assumption to multi-mode settings by incorporating prior multi-mode knowledge or multi-mode modeling techniques such as Gaussian mixture models (GMM) and hidden Markov models (HMM). They also note applicability to detecting intermittent modes/patterns viewed as intermittent faults, including discriminating them from false alarms and inferring their appearing/disappearing times.","Develop robust/nonparametric or bootstrap-calibrated MA-$T^2$ limits to reduce sensitivity to non-normality and to improve control-limit accuracy with finite $N$. Extend the framework to explicitly handle autocorrelated data (e.g., via residual charts, dynamic models, or block bootstrap) and quantify performance in terms of steady-state and zero-state ARL/ATS. Provide an open-source implementation and evaluate on real industrial intermittent-fault case studies to validate the proposed guaranteed-detectability conditions and multi-window fusion rules under realistic noise, drift, and missing-data scenarios.",2005.06825v2,local_papers/arxiv/2005.06825v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:13:50Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Phase II,Manufacturing (general)|Energy/utilities|Theoretical/simulation only|Other,NA,TRUE,FALSE,Simulation study,False alarm rate|Detection probability|Other,"Training requires collecting N sets of W consecutive in-control observations, with long enough intervals between sets to make different sets approximately independent; examples used include N=5000 sets of W=10 consecutive observations (and 50,000 consecutive observations for training PCA/DPCA/CVA baselines).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes an Optimally Weighted Moving Average Hotelling’s $T^2$ control chart (OWMA-TCC) for detecting intermittent faults (IFs) in multivariate weakly stationary processes, explicitly addressing autocorrelation and cross-correlation within a fixed window of recent observations. Unlike standard MA/EWMA smoothing that uses equal or exponential weights (optimal only under independence), it derives an optimal weight vector by maximizing an IF-detectability objective based on $\|\tilde S_W^{-1/2}\,\xi\|^2$ subject to weights summing to one. The optimal weights are characterized as a solution to a system of nonlinear equations; existence is proven using Brouwer’s fixed-point theorem, and the weights are shown to have a symmetry structure for large training sample sizes. For Gaussian data, the resulting $T^2$ statistic’s control limit is set via an $F$ distribution; for non-Gaussian weakly stationary data, empirical or KDE-based limits are suggested. Simulation studies on a multivariate AR(1) example and a CSTR benchmark show OWMA-TCC achieves clearer signaling of intermittent faults while maintaining low false alarm rates compared to PCA/MA-PCA, DPCA, CVA, and MW-KD baselines.","Within a window of length $W$, the weighted average is $\tilde X_{f,k}=\sum_{j=1}^W a_j X^f_{k-j+1}$ with constraint $\sum_{j=1}^W a_j=1$. The Phase II monitoring statistic is $\tilde T_k^2(W)=(\tilde X_{f,k}-\tilde X)^T \tilde S_W^{-1}(\tilde X_{f,k}-\tilde X)$ and, under stationary Gaussian assumptions with training size $N$, $\tilde T_k^2(W)\sim \frac{p(N^2-1)}{N(N-p)}F(p,N-p)$ giving control limit $\delta^2=\frac{p(N^2-1)}{N(N-p)}F_\alpha(p,N-p)$. The optimal weight vector solves a nonlinear fixed-point system (Eq. 16) derived from KKT conditions to maximize $\beta(a)=\tfrac12\|\tilde S_W^{-1/2}\xi\|^2$.","In the numerical AR(1) simulation with Gaussian noise, OWMA-TCC with $W=10$ achieved FAR 0.25% while providing clearer intermittent-fault signaling than the equally weighted MA-TCC (also FAR 0.25%); MA-PCA with $W=10$ exhibited high FAR (11%) under autocorrelation. In the CSTR simulation (3s sampling), OWMA-TCC($W=10$) had FAR 0.25% versus MA-based Mahalanobis distance with $W=10$ having FAR 10.25%, illustrating reduced false alarms under autocorrelation. Under non-Gaussian settings, control limits were set empirically and OWMA-TCC still outperformed MA-TCC and MW-KD qualitatively in intermittent-fault detection plots. The paper also proves analytically that when observations are independent, the unique optimal weights reduce to equal weights $a_j=1/W$ (recovering MA as optimal in that special case).","The paper notes that without Gaussianity the $T^2$ statistic no longer follows an $F$ distribution, so the control limit in Eq. (7) is not applicable and must be estimated empirically or via KDE. It also notes that overly large window lengths can cause detection delays, and that guaranteed detectability conditions rely on choosing $W$ relative to IF active/inactive durations.","The method’s optimal weighting depends on reliable estimation of lagged covariance blocks from training data (Assumption 1 and large-$N$ approximations), which may be challenging with limited in-control data or time-varying correlation structure. The approach presumes a fixed window length and a stationary/weakly stationary regime; performance under regime shifts, drifting dynamics, or nonstationary autocorrelation is not established. Practical guidance for selecting $W^\#$ and lower bounds on IF parameters may be hard to obtain in many applications, and no publicly available implementation is provided.","The authors suggest combining OWMA with recursive methods, other statistics, kernel methods, dynamic data modeling methods, and alternative selection criteria to handle processes with slightly varying operation points, varying noise levels, nonlinearities, and nonstationarity, and to address detection of faults with unknown characteristics.",Developing a self-starting/online Phase I-to-II procedure that updates the optimal weights and limits under gradual changes in autocorrelation would improve deployability. Robust or nonparametric covariance estimation (and corresponding weight optimization) could strengthen performance with heavy tails or outliers. Providing open-source software and benchmarking on additional real industrial datasets with labeled intermittent faults would help validate generalizability and practical tuning.,2005.06832v2,local_papers/arxiv/2005.06832v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:14:28Z
TRUE,Multivariate|High-dimensional|Profile monitoring|Other,Hotelling T-squared|Other,Both,Manufacturing (general),TRUE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|Detection probability,Simulation uses M = 200 samples with C = 4 channels and K = 128 points per channel; targeted Type I error α = 0.01. Case study training set includes 308 normal samples and 69 samples for each of five fault classes (345 total faults).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a real-time process monitoring and fault diagnosis approach for high-dimensional multi-channel sensor/profile (tensor) data using uncorrelated multilinear discriminant analysis (UMLDA) for feature extraction followed by a multivariate Hotelling $T^2$ control chart for monitoring. Multi-channel profiles are represented as tensors and projected via tensor-to-vector projections to obtain multiple (uncorrelated) discriminant features that incorporate class information (normal vs fault types). The monitoring statistic is $T^2$ computed from extracted features using in-control training mean and covariance, with control limits set from an $F$ distribution at a specified Type I error level. Performance is evaluated via Monte Carlo simulations across several fault scenarios (signal mean shifts, sinusoid superposition, localized weakening, model-parameter mean shifts, and noise variance increases) using ARL as the primary metric. A real forging-process case study with four force sensors demonstrates improved fault detection versus competing tensor/PCA-based feature extraction methods (VPCA, UMPCA, MPCA), while remaining fast enough for real-time deployment.","UMLDA extracts scalar features $y_{ml}=\mathcal{X}_m\times_1\mathbf{v}^{(1)T}_l\times_2\mathbf{v}^{(2)T}_l$ and chooses projection vectors to maximize Fisher’s scatter ratio $F_l^y=S_{B,l}^y/S_{W,l}^y$ subject to unit-norm projections and feature uncorrelatedness ($\mathbf{h}_l^T\mathbf{h}_j/(\|\mathbf{h}_l\|\|\mathbf{h}_j\|)=\delta_{lj}$). Monitoring uses Hotelling’s statistic $T^2=(\mathbf{g}_{new}-\bar{\mathbf{g}})^T\mathbf{S}^{-1}(\mathbf{g}_{new}-\bar{\mathbf{g}})$, where $\bar{\mathbf{g}}$ and $\mathbf{S}$ come from in-control training features. Control limits are set by the $(1-\alpha)$ quantile of an $F$ distribution (with degrees of freedom described as $J$ and $M-J$ in the paper).","In simulation, performance is compared using ARL at targeted $\alpha=0.01$ with 1000 Monte Carlo experiments per projection setting; UMLDA generally yields the lowest (best) out-of-control ARLs in scenarios involving signal-shape/mean changes (scenarios 1–3). For scenarios involving shifts in model-parameter distributions or increased noise (scenarios 4–5), MPCA is a close competitor and is slightly better in 3 of 20 tested cases. In the forging case study, the number of detected out-of-control samples (out of 345 fault samples) is VPCA: 163, UMPCA: 222, MPCA: 224, UMLDA: 311, indicating substantially higher detection for UMLDA. Average online monitoring time per sample is on the order of milliseconds or less (reported for 1000 samples: UMLDA $1.4\times10^{-3}$ s; VPCA $1.3\times10^{-3}$ s; UMPCA $9.5\times10^{-5}$ s; MPCA $5.4\times10^{-4}$ s).",None stated.,"The method relies on labeled fault classes to train UMLDA, which may be impractical when faults are rare/unknown or when only in-control data are available. The $T^2$ chart assumes the extracted feature vectors are adequately modeled with stable mean/covariance (and uses an $F$-based limit), which can be sensitive to non-normality, autocorrelation within profiles, or covariance estimation error in high dimensions. The paper does not describe strategies for concept drift or re-training when the in-control process changes over time. Comparisons focus on a small set of PCA/tensor baselines; other strong monitoring approaches for profiles (e.g., GLR/CUSUM/EWMA on features, functional data methods) are not evaluated.",None stated.,"Develop an unsupervised or one-class/self-starting version that can be trained primarily on in-control data and still detect novel/unknown faults. Extend the monitoring scheme to explicitly address serial dependence within cycles (autocorrelation) and to provide robust control limits under non-normal feature distributions. Provide a principled approach for selecting the number of UMLDA features and for adaptive updating of the in-control covariance matrix under drift. Release an implementation (e.g., an R/Python/MATLAB toolbox) and validate across additional industrial datasets to benchmark against modern profile monitoring and change-detection methods (e.g., EWMA/CUSUM/GLR on extracted features).",2005.12585v1,local_papers/arxiv/2005.12585v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:15:04Z
TRUE,Multivariate|Bayesian|Profile monitoring,EWMA|MEWMA|Other,Phase II,Healthcare/medical,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length),For simulations they generate datasets of size about 500 patients (motivated by ~533 deliveries/month) and state this size fulfills the asymptotic assumption of the score-based MEWMA chart.,TRUE,None / Not applicable,Not provided,NA,"The paper develops a multi-stage monitoring approach for healthcare procedures that tracks intermediate and final outcomes simultaneously rather than only end-stage outcomes (e.g., mortality). It proposes a score-based multivariate EWMA (MEWMA) control chart that monitors the stability of a multistage generalized linear model (logistic regression for binary outcomes) linking outcomes to upstream outcomes, process variables, and patient risk factors. The chart statistic is a Hotelling-type quadratic form of an EWMA of likelihood score vectors, with control limits chosen to achieve a target in-control ARL (set to 200 in their study). Performance is evaluated via Monte Carlo simulation under several shift types (additive coefficient shifts, simultaneous shifts, mean shifts, and odds-ratio shifts), reporting out-of-control ARLs that decrease as shift magnitudes increase. A maternity-unit delivery process is used as the motivating application, with parameter estimates informed by real hospital data to set simulation baselines.","A multistage logistic model is specified as $\text{logit}(\mu_{vt})=\alpha_v+x_{vt}^\top\beta_v+y_{\text{pa}(v)t}^\top\gamma_v+z_{vt}^\top\delta_v$. The score vector is $s(\theta_v)=\sum_{t=1}^n u_{vt}\left[y_{vt}-\frac{\exp(\theta_v^\top u_{vt})}{1+\exp(\theta_v^\top u_{vt})}\right]$, with a block-diagonal Fisher information structure across stages. The MEWMA recursion is $W_t=RS_t+(I-R)W_{t-1}$ and the charting statistic is $T_t^2=W_t^\top\Sigma_{W_t}^{-1}W_t$, signaling when $T_t^2>h$ (with $h$ set by simulation to achieve a desired in-control ARL).","Control limits are calibrated to an in-control ARL of 200 (assuming Phase I parameters known without error) and out-of-control ARLs are estimated from 5,000 simulated runs per shift scenario. For additive shifts in process-effect coefficients, out-of-control ARL declines as the change factor increases; e.g., shifting $\beta_{24}$ by factor $c=4.0$ yields ARL 66.3, while $c=1.0$ yields ARL 137.6 (Table 2). For upstream-outcome effect shifts, larger baseline coefficients tend to be detected faster; e.g., shifting $\gamma_{34}$ can yield much smaller ARLs (down to about 22.0 at $c=2.0$ in Table 3) than comparable shifts in smaller coefficients. Mean/odds-ratio shifts in an upstream outcome (illustrated for $\mu_{3t}$) also reduce ARL with increasing shift size; at $c=4.0$ ARL is about 46.9 (additive) and 61.1 (odds-ratio) (Table 4).",None stated.,"Control limits and reported performance largely rely on simulations with an assumption that Phase I parameters are known without error; the impact of estimation uncertainty on false-alarm rates and ARL is not fully developed. The method is presented for i.i.d. observations and does not address serial dependence common in healthcare processes (e.g., temporal autocorrelation or seasonal effects). Practical implementation guidance is limited (e.g., how to choose the smoothing matrix $R$ in multivariate settings, and how to diagnose which stage/parameter drove a signal). Despite a real-data motivation, there is no full prospective real-world monitoring case study demonstrating deployment and actionability.",None stated.,"Extend the procedure to explicitly incorporate parameter-estimation uncertainty (Phase I/Phase II transition) and provide analytic or bootstrap-based adjustments to maintain nominal in-control ARL. Develop versions robust to autocorrelation, time-varying baseline risk, and nonstationarity (e.g., seasonal risk patterns). Add post-signal diagnostic tools to attribute signals to specific stages/coefficients and to guide root-cause investigation. Provide open-source software (e.g., an R/Python implementation) and evaluate the method on multiple real healthcare datasets to validate operational performance.",2006.14737v1,local_papers/arxiv/2006.14737v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:15:31Z
TRUE,Univariate|Bayesian,CUSUM|EWMA,Phase II,Theoretical/simulation only,NA,FALSE,FALSE,Simulation study,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|ATS (Average Time to Signal)|Other,"Simulation sample sizes considered include n = 5, 10, 20, 30 (with baseline simulations using n = 10), with charts designed to achieve ARL0 ≈ 370.",TRUE,None / Not applicable,Not provided,NA,"The paper develops Bayesian versions of the EWMA and CUSUM control charts where the center line and control limits are based on posterior predictive distributions and Bayes estimators under different loss functions. It studies three loss functions—squared error (SELF), precautionary (PLF), and Linex (LLF)—and examines how the loss choice affects monitoring performance. The framework is worked out for multiple likelihood/prior pairs: Normal likelihood with conjugate Normal prior, and Poisson likelihood with conjugate Gamma prior, plus a Poisson likelihood with an Exponential prior to illustrate generality. Performance is evaluated primarily via Monte Carlo simulation targeting in-control ARL0 ≈ 370 and comparing out-of-control behavior across shift sizes. Sensitivity analyses are reported for prior hyperparameters and sample size, showing that detection improves with increasing shift size and generally with larger n, and that results are relatively stable across hyperparameter choices, with LLF often yielding smaller time-to-signal metrics.","Bayesian EWMA uses posterior-predictive-based limits $\text{UCL/LCL}=\mu_{LF}\pm L\,\sigma_{\bar Y}\sqrt{\tfrac{\tau}{2-\tau}}$ with center line $\text{CL}=\mu_{LF}$ and plotting statistic $z_i=\tau(\bar y\mid x)+(1-\tau)z_{i-1}$. Bayesian CUSUM uses $\text{UCL/LCL}=\pm h\,\sigma_{\bar Y}$ with $\text{CL}=\mu_{LF}$ and recursion $c_i=[(\bar y\mid x)-\mu_{LF}]+c_{i-1}$, where $\mu_{LF}$ is the Bayes estimator under SELF/PLF/LLF and $(\bar y\mid x)$ comes from the posterior predictive. For Normal-conjugate cases, $\mu_{LF}$ is given explicitly for SELF/PLF/LLF; for Poisson-Gamma, posterior is $\lambda\mid X\sim\text{Gamma}(n\bar x+\alpha,n+\beta)$ and posterior predictive is Negative Binomial.","Charts are designed to achieve in-control ARL0 around 370 (e.g., for Normal-conjugate Bayesian CUSUM, using $h=6$ gives ARL0 values near 370–386 across loss functions and hyperparameter settings). In the Normal-conjugate Bayesian CUSUM hyperparameter study (n=10), ARL drops sharply for small mean shifts (e.g., around ARL ≈ 25 at $\delta=0.25$ and ≈ 12 at $\delta=0.5$) and continues decreasing as shift size increases (≈ 2 near $\delta=2.5$). Sample-size sensitivity for the Normal-conjugate Bayesian CUSUM shows that increasing n (with adjusted h to maintain ARL0≈370) improves detection (e.g., for SELF at $\delta=1$, ARL ≈ 8.82 for n=5 vs ≈ 3.01 for n=30). ATS/SDTS are reported (SDTS explicitly) and are generally smallest under the Linex loss function relative to SELF and PLF in the presented normal-conjugate results.",None stated.,"The study appears largely simulation-based with no real-case study, so practical performance on industrial data and implementation details are not validated. The monitoring setup assumes independent observations and does not address common SPC complications such as autocorrelation, parameter estimation uncertainty in Phase I, or robustness to model misspecification (e.g., non-normality beyond the Poisson cases considered). Software/code used to generate the Monte Carlo results is not provided, limiting reproducibility. The CUSUM recursion presented is simplified and it is unclear whether standard one-sided/two-sided reference-value formulations (k, separate $C^+$/$C^-$) are fully incorporated, which may affect comparability with classical CUSUM designs.",None stated.,"Extend the Bayesian EWMA/CUSUM constructions to explicitly handle autocorrelated data (e.g., via Bayesian state-space models or residual charts) and evaluate robustness under model misspecification. Develop Phase I/Phase II procedures that account for estimating prior hyperparameters and in-control parameters from historical data (including self-starting variants). Provide open-source software (e.g., an R/Python package) to compute limits and run simulations, enabling reproducibility and practitioner adoption. Add real-data case studies (healthcare counts, network events, manufacturing defect counts) to demonstrate interpretability and operational impact of different loss-function choices.",2007.09844v1,local_papers/arxiv/2007.09844v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:16:05Z
TRUE,Univariate|Other,CUSUM|Change-point,Phase II,Network/cybersecurity|Other,NA,FALSE,NA,Simulation study|Case study (real dataset),Expected detection delay|False alarm rate|Detection probability|Other,Not discussed (uses 30-day trial windows; experiments use 18 consumers and anomaly thresholds defined as 10%–100% of consumers per trial period).,TRUE,None / Not applicable,Not provided,https://aws.amazon.com/ec2/instance-types/|https://www.cs.ucsb.edu/~rich/workload/,"The paper proposes an Event-Condition-Action (ECA) framework to detect and manage long-term changes in Infrastructure-as-a-Service (IaaS) performance signatures built from aggregated free-trial user experiences. Events are triggered when performance anomalies occur frequently within a fixed trial window, where anomalies are detected by time-series similarity between a user’s normalized trial trace and the current signature (e.g., Pearson correlation, cosine similarity, Euclidean distance). When an event occurs, the condition/action step recomputes a new signature from users in that window and applies a univariate CUSUM control chart to determine whether the signature has changed and should be updated. A feedback-based self-adjustment mechanism updates the anomaly frequency threshold based on true/false positive outcomes to reduce unnecessary re-evaluations while maintaining detection capability. Experiments using a real workload trace (Eucalyptus) combined with published SPEC Cloud IaaS 2016 performance data (and simulated random signature changes) evaluate false positives, detection delay, and detection accuracy under varying thresholds.","Signatures are represented as a QoS time series (matrix form in Eq. (1)); anomaly detection uses similarity between normalized trial series $E'_Q$ and signature segment $S_Q$ via Euclidean distance (Eq. (2)), Pearson correlation (Eq. (3)), or cosine similarity (Eq. (4)). The similarity threshold is initialized as $TS=\min_{i=1}^N S(E_i,S_Q)^M$ over past trial users (Eq. (5)). Signature change is detected by a univariate CUSUM with recursive upper/lower sums $UL_i=\max(0,UL_{i-1}+x_i-m_x-\tfrac{1}{2}ns_x)$ and $LL_i=\min(0,LL_{i-1}+x_i-m_x+\tfrac{1}{2}ns_x)$ (Eqs. (6)–(7)), signaling when $UL_i>cs_x$ or $LL_i<-cs_x$.","Across 100 simulations (360-day horizon; 30-day trial window; 18 consumers; 5 signatures), increasing the similarity threshold increases false positives but reduces average detection delay (reported average delays roughly 30–55 days for the settings shown). Increasing the anomaly-frequency threshold (10%–100% of consumers per month) decreases false positives toward near-zero at 100% but increases average detection delay (reported as ranging roughly 15–180 days in some settings), indicating missed/late detections at strict thresholds. Using a 60-day evaluation window ($T_w=60$), detection accuracy increases from about 40% to about 95% as the similarity threshold increases, while accuracy drops from about 95% to below 10% as the anomaly threshold increases. Minimum detection delays shown are typically about 15–60 days depending on thresholds.","The authors note that, due to difficulty obtaining real long-term workload traces and performance datasets, they “utilize the publicly available workload traces and performance data to mimic the long-term cloud environment.” They also state they focus on only one QoS attribute (throughput) “for simplicity,” making the signature effectively two-dimensional, with multivariate signatures left for future work. They mention page limitations and therefore discuss only the PCC-based similarity results.","The CUSUM procedure is applied after recomputing signatures in fixed 30-day windows, so the monitoring is not fully streaming/self-starting and may delay detection relative to observation-level SPC; also, CUSUM parameter choices (e.g., $c$ and $n$) are described informally and may not correspond to standard design via ARL calibration. The work does not report classical SPC metrics such as in-control/out-of-control ARL/ATS, and false positives are evaluated in an application-specific way, making it hard to compare with SPC baselines. Autocorrelation and nonstationarity in workload/performance time series are not modeled explicitly; similarity thresholds and frequency thresholds appear to require tuning per signature, which may be operationally burdensome without clear guidance.","They plan to extend signatures beyond a single QoS attribute (i.e., support more than two dimensions). They also state an intention to run experiments “on a larger scale to evaluate the impact of the proposed approach in the long-term selection.”","A natural extension is to calibrate the CUSUM (and event-trigger thresholds) to target in-control false alarm rates/ARL, enabling principled parameter selection and comparison with alternative SPC charts (EWMA, GLR, Bayesian change detection). Extending to multivariate/high-dimensional signature monitoring (multiple QoS attributes) with robust methods under autocorrelation would improve realism for cloud telemetry. Providing an implementation and benchmarking on additional real cloud monitoring datasets (with ground-truth change events) would strengthen external validity and reproducibility.",2007.11705v2,local_papers/arxiv/2007.11705v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:16:36Z
TRUE,Univariate|Other,Shewhart,Phase II,Food/agriculture|Manufacturing (general),TRUE,FALSE,FALSE,Approximation methods,ARL (Average Run Length)|Other,"Short production run is modeled with a finite number of inspections I over horizon H; sample size per inspection n is considered (examples/tables use n ∈ {1,5,7,10,15} and I ∈ {10,30,50}; illustrative example uses n=5 and I=15). No minimum Phase I reference sample is required because limits are tabulated from assumed known parameters.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes two one-sided Shewhart-type ratio (RZ) control charts for short production runs to monitor the ratio of two (possibly correlated) normal variables, targeting either decreases (Sh−RZ) or increases (Sh+RZ) in the ratio. Because the production horizon is finite, performance is evaluated using truncated run length (TRL) and truncated average run length (TARL) rather than the traditional infinite-horizon ARL. The design uses an approximation to the CDF/quantiles of the ratio of two normal variables and applies it to the ratio of sample means, with known in-control ratio z0, correlation ρ0, and coefficients of variation (γX, γY) assumed constant. The authors provide ready-to-use tables of probability-based control limits for specified in-control TARL0 (I) and extensive TARL1 tables under ratio shifts τ and correlation changes. A food-industry illustration (muesli ingredient proportions) demonstrates signaling of a 1% upward ratio shift using the Sh+RZ chart.","Monitoring statistic at inspection i is the ratio of sample means: $\hat Z_i=\bar X_i/\bar Y_i=\frac{\sum_{j=1}^n X_{i,j}}{\sum_{j=1}^n Y_{i,j}}$. Control limits use the approximate inverse CDF of the ratio distribution applied to $(\gamma_X/\sqrt n,\gamma_Y/\sqrt n,\omega_0=z_0\gamma_X/\gamma_Y,\rho_0)$: for Sh−RZ, $\mathrm{LCL}^- = F^{-1}_{\hat Z}(\alpha_0|n,\gamma_X,\gamma_Y,z_0,\rho_0)$ and $\mathrm{UCL}^-=\infty$; for Sh+RZ, $\mathrm{LCL}^+=0$ and $\mathrm{UCL}^+ = F^{-1}_{\hat Z}(1-\alpha_0|n,\gamma_X,\gamma_Y,z_0,\rho_0)$. Finite-horizon performance is summarized by $\mathrm{TARL}_0=\{1-(1-\alpha)^{I+1}\}/\alpha$ and $\mathrm{TARL}_1=\{1-\beta^{I+1}\}/(1-\beta)$.","The paper tabulates probability control limits (LCL− and UCL+) designed so that TARL0 equals the finite number of inspections I (examples shown for I=10,30,50) over multiple $(\gamma_X,\gamma_Y)$, correlations $\rho_0\in\{0,\pm0.4,\pm0.8\}$, and sample sizes $n\in\{1,5,7,10,15\}$. Control limits narrow as n increases and widen as I increases; e.g., for $\gamma_X=\gamma_Y=0.01$, $\rho_0=-0.8$, I=10: (LCL−,UCL+)=(0.9615,1.0401) at n=1 and (0.9899,1.0102) at n=15; for I=50 at n=1 they widen to (0.9418,1.0618). TARL1 decreases (faster detection) as the shift magnitude $|\tau-1|$ increases and as n increases, with extensive TARL1 tables reported for both one-sided directions and for cases $\rho_1=\rho_0$ and $\rho_1\ne\rho_0$. In the food example with n=5, $\rho_0=0.8$, H=16 hours and I=15 (hourly inspections), the Sh+RZ chart uses UCL+=1.01421 and signals a 1% ratio increase at sample #11 (\hat Z_{11}=1.017) when the shift occurs between samples #10 and #11.","The authors note the work is developed under a finite-horizon (short-run) framework with specific modeling assumptions (e.g., constant coefficients of variation and known in-control parameters) and focus on one-sided Shewhart-type charts; they indicate other chart families are not covered. They also state that further extensions are needed, such as one-sided EWMA and CUSUM ratio charts for short production runs (implying current scope is limited to Shewhart-type charts).","The design assumes $\gamma_X,\gamma_Y$, the in-control ratio $z_0$, and the in-control correlation $\rho_0$ are known (or treated as fixed), which may be unrealistic in true short-run settings where Phase I data are scarce and estimation error can substantially affect false alarm performance. The ratio distribution is handled via an approximation; accuracy may degrade for larger coefficients of variation, small n, strong correlation, or when $Y$ can be near zero, but robustness of the approximation is not systematically validated across regimes. The paper largely relies on tabulated calculations and does not document reproducible software or simulation code, which limits practical adoption and independent verification. Autocorrelation/serial dependence and non-normality (common in time-ordered production data) are not modeled, so performance may differ in many real processes.","The authors recommend extending the short-run ratio monitoring framework to more sensitive memory-type schemes, specifically proposing one-sided EWMA-type and CUSUM-type ratio charts for short production runs. They also imply further investigation of related designs beyond the presented one-sided Shewhart charts.","Develop self-starting/estimation-aware versions that account for uncertainty in $\gamma_X,\gamma_Y,z_0,\rho_0$ under very limited Phase I data, including adjusted limits to preserve TARL0. Study robustness to non-normality, heavy tails, and autocorrelation (e.g., ARMA errors) and provide guidelines for when the ratio approximation is reliable. Provide open-source implementations (e.g., an R/Python package) that compute limits for arbitrary inputs and generate TARL curves, enabling practitioners to avoid manual table lookup. Extend to multivariate/generalized ratio settings (multiple components, compositional constraints) and to adaptive sampling/variable inspection schedules optimized for finite-horizon objectives.",2010.01297v1,local_papers/arxiv/2010.01297v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:17:22Z
TRUE,Profile monitoring|Functional data analysis|Other,EWMA|Other,Both,Environmental monitoring,FALSE,NA,NA,Simulation study|Case study (real dataset)|Other,False alarm rate|Detection probability|Other,Not discussed,TRUE,None / Not applicable,Not provided,https://ypen.gov.gr/perivallon/poiotita-tis-atmosfairas/dedomena-metriseon-atmosfairikis-rypansis/|https://www.who.int/news-room/fact-sheets/detail/ambient-(outdoor)-air-quality-and-health,"The paper proposes a modelling-and-monitoring framework for functional profiles that targets interpretable “shape shifts” by combining the Fréchet mean (as a generalized notion of mean for metric-space-valued objects) with a deformation model, specifically the Shape Invariant Model (SIM). It estimates a typical in-control (IC) profile as an SIM-compatible approximation to the Fréchet mean via a constrained, regularized Fréchet-variance minimization problem, and registers each profile to the mean to obtain amplitude/phase deformation parameters. For monitoring, it develops an EWMA-type scheme on (i) an underlying-shape deviance statistic (after removing estimated deformations) and (ii) a deformation-deviance process based on an Exponentially Weighted Fréchet Moving Average (EWFMA) variational update restricted to SIM forms. Control limits are set using empirical IC distributions/quantiles rather than parametric ARL calibration. The method is demonstrated on real ambient air-pollution daily profiles (Athens), where hazardous days are identified and shifts can be attributed to amplitude and/or phase components via follow-up EWMA charts on SIM parameters.","Profiles follow the SIM deformation model $f_j(t)=\beta_j+\alpha_j f_0\big((t-\zeta_j)/\kappa_j\big)+\varepsilon_j(t)$. The SIM-compatible Fréchet-mean approximation is $\hat f_0(t;\theta)=\frac1n\sum_{j=1}^n \alpha_j^{-1}\{f_j(\kappa_j t+\zeta_j)-\beta_j\}$ with $\theta$ chosen by minimizing an (optionally regularized) Fréchet-variance objective $V(\theta)=\frac1n\sum_{j=1}^n d_M^2(\hat f_0(\theta),f_j)$ subject to centrality constraints (products/sums) on deformation parameters. Monitoring uses EWMA recursion on shape deviance $D_j^s=d_M^2(\hat f_{0,j},f_0)$: $DE_j^s=\lambda D_j^s+(1-\lambda)DE_{j-1}^s$, and an EWFMA/SIM variational update $f_{e,j}=\arg\min_{g\in M}\{\lambda d_M^2(g,\hat f_j)+(1-\lambda)d_M^2(g,f_{e,j-1})\}$ (implemented by minimizing over SIM parameters), with deformation deviance tracked by $DE_j^\theta=\lambda D_j^\theta+(1-\lambda)DE_{j-1}^\theta$ where $D_j^\theta=\|\hat f(\theta_j)-f_0\|_2^2$.","In the Athens air-pollution case study (train: 2001–2004; test: 2005–2007; focusing on Oct–Dec), overall classification accuracy (“both” stages combined) is reported as 87.22% for CO, 82.58% for NO$_2$, 88.35% for O$_3$, and 95.86% for SO$_2$. Within-class (OOC) detection is reported as 100% for CO and SO$_2$, 78.26% for NO$_2$, and 100% for O$_3$ (noting O$_3$ has very few OOC cases). The authors report near-zero type-I error behavior in several cases (i.e., very few IC profiles flagged as OOC), with more conservatism manifesting as higher type-II errors for some pollutants (especially CO/NO$_2$). They also note that the SIM appears adequate for these data since few profiles are deemed “not explainable” by the shape process stage, and that amplitude-related parameters ($\alpha$, sometimes $\beta$) tend to drive detected shifts while phase shift ($\zeta$) rarely does.","The authors note the scheme can be conservative in some cases, yielding higher Type II errors (misclassifying OOC as IC), and suggest this could be improved by incorporating additional covariates (e.g., temperature, humidity), by better modelling interdependencies within/across pollutants, or by using a different deformation model when IC profiles have more diverse patterns. They also state that choosing the EWMA weight $\lambda$ was done by testing a grid and selecting an ‘optimal’ value per pollutant, while more elaborate re-tuning (e.g., cross-validation) is beyond the paper’s scope.","Control limits are based on empirical IC quantiles rather than calibrated in-control run-length (ARL/ATS), so false-alarm behavior under process drift/finite Phase I uncertainty is not theoretically characterized and may depend strongly on Phase I sample size and nonstationarity. The approach relies on successful SIM registration/optimization (nonconvex in phase parameters), which may be sensitive to initialization, local minima, and computational cost for larger datasets or more complex deformations. Autocorrelation (common in daily environmental profiles) is not explicitly modeled in the monitoring statistic design, which could affect signal rates and interpretation. The framework is shown primarily for a simplified SIM variant (e.g., omitting $\kappa$ in the case study), so general performance for full SIM and other deformation families needs broader validation.","The authors suggest reducing Type II errors and improving monitoring by including extra environmental information (e.g., temperature, humidity), modeling interdependencies among deformation features within and across pollutants, and considering alternative deformation models beyond SIM (e.g., landmark deformation models) when IC patterns are more heterogeneous. They frame these as natural extensions of the current Fréchet-mean/deformation-model monitoring framework.","Developing principled design/calibration of control limits to target specified in-control ARL/false-alarm probabilities (including Phase I estimation effects) would make the method easier to deploy in SPC practice. Extending the procedure to explicitly handle serial dependence and seasonality (e.g., through residualization, dynamic SIM parameters, or state-space registration) would better match environmental monitoring realities. Providing open-source software and computational guidance (convergence diagnostics, initialization strategies, runtime scaling) would materially improve reproducibility and adoption. Finally, extending to multivariate functional monitoring (jointly modeling CO/NO$_2$/O$_3$/SO$_2$ curves) could leverage cross-pollutant correlation to improve detection and explanation.",2010.02968v4,local_papers/arxiv/2010.02968v4.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:18:02Z
FALSE,Other,EWMA|Other,NA,Energy/utilities,NA,NA,NA,Simulation study|Case study (real dataset),Other,Not discussed,TRUE,Python|Other,Not provided,NA,"The paper proposes a decentralized ADMM-based framework for mixed-integer generation maintenance and unit commitment in power systems that preserves privacy of inter-region network flow estimates via differential privacy. Privacy is achieved by adding engineered exponential noise to shared phase angles; via the linear DC flow–angle relationship, this induces Laplace noise on line flows and yields formal \(\varepsilon\)-differential privacy guarantees. To improve convergence under iterative noisy communications, the authors use an EWMA-based consensus mechanism (smoothing neighbor-shared noisy phase angles/flows) and introduce a CLT-based control-chart-style stopping/stability criterion that limits instability from noise. Experiments on IEEE 118-bus test cases with 8- and 12-region decompositions show solution quality (optimality gap vs a centralized non-DP benchmark) is generally within about 5% in many scenarios and remains robust over a range of noise scales and convergence limits. Implementation uses MPI (mpi4py) with Gurobi solving regional MIQP subproblems, demonstrating computational scalability and robustness to noise.","Key elements include (i) DC flow relation \(f_{uv,t}=\Gamma(uv)(\theta_{u,t}-\theta_{v,t})\); (ii) privatization of phase angles \(T(\theta_{b,t})=\theta_{b,t}+\alpha_{b,t}\) with \(\alpha\sim\mathrm{Exp}(\omega/(|\Gamma|\varepsilon))\), which yields Laplace noise on flow differences and thus \(\varepsilon\)-DP for flows; (iii) EWMA consensus updates \(\tilde\theta_{b,k}=\eta\hat\theta_{b,k}+(1-\eta)\tilde\theta_{b,k-1}\) and similarly for flows; and (iv) CLT-based chart statistic \(\Theta=\sum_{k=1}^{S_w}(\theta_{u,k,r}+\alpha_{u,k,r}-\theta_{u,k,r'}-\alpha_{u,k,r'})\) with alarm thresholds based on \(\mathcal N(0,2\tilde\omega^2/S_w)\).","On IEEE 118-bus experiments with 8- and 12-region decompositions, the proposed differentially private approach often achieves optimality gaps below about 5% across many combinations of noise scale and convergence-limit settings (surface plots in Figs. 2–3). Flow perturbation magnitude (2-norm difference between real and DP flows at convergence) increases monotonically with the noise scale parameter (Fig. 4), indicating stronger obfuscation at higher scales. A lookback size of 20 for the control-chart criterion reduces variability in optimality gap compared to a lookback of 10 at the tested convergence limit (Fig. 5). Reported mean compute times (including non-converged runs capped by a runtime limit) range roughly from ~1,000s at low noise to several thousand seconds at higher noise, with higher variance under high noise (Table 2).","The authors note sensitivity of ADMM performance to the penalty parameter \(\rho\), stating that it was chosen empirically through repeated trials and that existing adaptive-\(\rho\) methods for convex problems may not apply to their mixed-integer setting. They also indicate that higher noise levels make Lagrangian balancing harder, increasing iterations and making non-convergence within a fixed runtime more likely (affecting mean/variance of computation time).","Although the method uses a control-chart-like criterion, the work is not an SPC/control-chart paper and does not characterize in-control/out-of-control run-length properties (e.g., ARL) or false-alarm behavior of the charting rule. The DP guarantee is provided for flows induced by privatized phase angles, but practical calibration of sensitivity \(\omega\) and the privacy budget \(\varepsilon\) for real utility settings (including composition over many iterations/lines) is not fully quantified in terms of end-to-end privacy loss vs utility. The approach relies on a linearized (DC) flow–angle relationship; performance and privacy/utility tradeoffs may change under AC power flow or model mismatch. Comparisons are primarily against a centralized non-DP benchmark rather than against alternative privacy-preserving distributed optimization baselines across identical conditions.",They explicitly identify automatic/adaptive adjustment of the ADMM penalty parameter \(\rho\) during runtime for improved performance in the mixed-integer formulation as a key component of future work.,"Extending the privacy mechanism and convergence/stability analysis to AC power flow models and to more realistic communication issues (delays, packet loss, asynchronous updates) would strengthen applicability. Providing formal privacy accounting across iterations/regions (composition over time and across multiple shared variables) and guidance for choosing \(\varepsilon\) based on operational risk would improve deployability. Evaluating robustness under autocorrelation/temporal dependence in operational conditions and under adversarial inference models tailored to grid communications would better validate security claims. Releasing reproducible code (e.g., MPI/Gurobi scripts and data preprocessing) or an open-source surrogate solver setup would facilitate broader benchmarking and adoption.",2010.09099v1,local_papers/arxiv/2010.09099v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:18:30Z
TRUE,Multivariate|Other,CUSUM|EWMA|Hotelling T-squared|Other,Both,Transportation/logistics|Network/cybersecurity|Theoretical/simulation only,TRUE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Conditional expected delay|Other,"Phase I/II split is discussed conceptually. In simulations, Phase I uses 2500 in-control temporal graphs (with a burn-in of 1000 time points) and then calibrates charts; for the real-data example, Phase I uses year 2018 with z=7 giving 358 observations (Phase II has 486 observations).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes an online network surveillance framework that combines temporal exponential random graph models (TERGM) with multivariate statistical process control to detect structural anomalies in temporal networks. Network evolution is summarized through either estimated TERGM parameters or averaged network statistics computed via a moving window, reducing dimensionality while modeling temporal dependence. The monitoring step applies multivariate CUSUM (Crosier-type MCUSUM) and multivariate EWMA (MEWMA), using Mahalanobis-distance-based statistics and calibrating upper control limits via Monte Carlo simulation to achieve target in-control ARL under autocorrelation/overlapping windows. Performance is assessed via average run length and conditional expected detection delay across simulated anomaly scenarios (persistent changes and point events). An empirical case study monitors daily U.S. flight networks and detects anomalous periods corresponding to seasonal peaks and the onset of COVID-19-related disruptions.","The TERGM is defined by $P_\theta(Y_t\mid Y_{t-1},\ldots,Y_{t-v})\propto \exp\{\theta^\top s(Y_t,\ldots,Y_{t-v})\}$. Monitoring uses Mahalanobis-type distances, e.g., $D^{(1)}_t=(\hat c_t-c_0)^\top\Sigma^{-1}(\hat c_t-c_0)$. The Crosier MCUSUM computes $C_t=\{(r_{t-1}+\hat c_t-c_0)^\top\Sigma^{-1}(r_{t-1}+\hat c_t-c_0)\}^{1/2}$ with $r_t=0$ if $C_t\le k$ and otherwise $r_t=(r_{t-1}+\hat c_t-c_0)(1-k/C_t)$, charting $D^{(2)}_t=r_t^\top\Sigma^{-1}r_t$. The MEWMA computes $\ell_t=\lambda(\hat c_t-c_0)+(1-\lambda)\ell_{t-1}$ and charts $D^{(3)}_t=\ell_t^\top\Sigma_{\ell_t}^{-1}\ell_t$ with $\Sigma_{\ell_t}=\frac{\lambda}{2-\lambda}(1-(1-\lambda)^{2t})\Sigma$.","Upper control limits (UCLs) are tabulated for MEWMA and MCUSUM for target $\mathrm{ARL}_0\in\{50,75,100\}$ and window sizes $z\in\{7,14\}$, separately for monitoring TERGM parameter estimates $\hat\theta_t$ versus averaged network statistics $\hat s_t$. In simulation (250 replications, change time $\tau=101$) the charts show decreasing conditional expected delay (CED) as anomaly intensity increases; MEWMA often has smaller CED than MCUSUM for moderate-to-large persistent changes, while MCUSUM can be competitive for weaker changes with small reference value $k$. For point-event anomalies (Type C), monitoring $\hat\theta_t$ can substantially outperform monitoring $\hat s_t$ for moderate event sizes (e.g., around $\zeta=0.01$–0.02 in their tested settings). In the U.S. flight network case study (Phase I=2018, Phase II=2019–2020, $z=7$), both MCUSUM ($k=1.5$) and MEWMA ($\lambda=0.9$) signal anomalous periods, including a sustained signal sequence beginning in late March 2020 consistent with COVID-19 disruptions.","The authors note that TERGM term selection is difficult and that TERGM is not suitable for very large networks. They also state that temporal dependency terms depend on the chosen lag $v$ and window size $z$, so accurate modeling relies heavily on analyst knowledge of the network. They further mention that multivariate control charts are typically effective only when the number of monitored variables is not too large (often up to about 10).","The method calibrates UCLs via Monte Carlo under specific TERGM specifications (terms, lag, windowing), so transfer to new networks likely requires substantial re-simulation and may be sensitive to model mis-specification. Phase I is assumed clean (in-control) and uses simple sample mean/covariance for $c_0,\Sigma$; robustness to Phase I contamination/outliers and uncertainty in $\Sigma$ is not developed. The approach signals changes but provides limited built-in diagnostics for attributing which network terms/edges/nodes drive the signal beyond post-hoc investigation. No implementation details (software/code) are provided, which may hinder reproducibility given the computational complexity of TERGM fitting and repeated simulations.","They suggest extending the approach to STERGM to separate formation and dissolution processes and improve interpretability of detected changes. They propose further investigation of when averaged network statistics $\hat s_t$ can reliably replace parameter estimates $\hat\theta_t$ for monitoring, and exploring alternative estimators for $\hat s_t$. They also mention extending to settings where $\Sigma$ may differ between in-control and out-of-control states and developing adaptive control charts to improve anomaly detection performance.","Develop robust Phase I procedures for estimating $c_0$ and $\Sigma$ under possible contamination and to quantify estimation uncertainty in control limits (e.g., bootstrap/adjusted limits). Extend the framework to high-dimensional settings (many network terms/covariates) via regularization or sparse monitoring statistics. Provide automated diagnostic/attribution tools (e.g., contribution analysis, term-wise decomposition, or edge/node-level localization) to explain signals in operational monitoring. Publish open-source software (e.g., an R/Python package) with workflows for TERGM fitting, simulation-based calibration, and monitoring to support reproducible deployment.",2010.09398v2,local_papers/arxiv/2010.09398v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:24:10Z
TRUE,Multivariate|Nonparametric|Other,Shewhart|CUSUM|EWMA|GLR (Generalized Likelihood Ratio)|Machine learning-based|Other,Both,Environmental monitoring|Other,FALSE,TRUE,TRUE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate|Other,"Not discussed (no general Phase I minimum sample size guidance; in the application they use a panel of N=21 series, select P1=12 and P2=6 IC series, and choose K=200 nearest neighbours for estimating IC patterns).",TRUE,None / Not applicable,Not provided,http://www.sidc.be/silso/,"The paper proposes a nonparametric, robust SPC framework to monitor a panel of serially correlated time series with time-varying mean/variance, strong noise, missing values, and no clearly identified in-control (IC) period. Phase I selects an IC pool from the panel using robust stability criteria and clustering, then estimates IC longitudinal mean and variance patterns via smoothing and K-NN methods. Phase II standardizes each series by these IC patterns and monitors it primarily with a two-sided CUSUM chart whose control limits are calibrated using a moving block bootstrap to preserve autocorrelation and avoid parametric distributional assumptions. The approach is augmented with support vector regression/classification (SVR/SVC) to estimate the magnitude and form of detected deviations (e.g., jumps, trends, oscillatory shifts) after a signal. The methodology is demonstrated on a panel of sunspot-number observing stations, where it detects known prominent deviations and additional smaller persistent shifts; the chart is calibrated to achieve a target in-control ARL (e.g., ARL0=200).","The panel is standardized by removing a common signal via a robust median ratio $\hat\eta_e(i,t)=X(i,t)/\mathrm{med}_{1\le i\le N}X(i,t)$ and then de-meaning by a long-window moving average to remove slow station level effects. IC longitudinal patterns are estimated as smoothed mean/variance $\hat\mu_0(t),\hat\sigma_0^2(t)$ over a selected stable subset of series, and residuals are formed as $\hat\epsilon(i,t)=(\hat\eta(i,t)-\hat\mu_0(t))/\hat\sigma_0(t)$. Monitoring uses a two-sided CUSUM: $C_j^+=\max(0,C_{j-1}^+ + \hat\epsilon(t)-k)$ and $C_j^-=\min(0,C_{j-1}^- + \hat\epsilon(t)+k)$, signaling when $C_j^+>h^+$ or $C_j^-<h^-$. Control limits are selected to match a desired $\mathrm{ARL}_0$ using (moving) block-bootstrap resampling of IC residuals.","In the application to sunspot-number stations (N=21), the CUSUM chart is calibrated via block bootstrap with block length 27 (one solar rotation) and reset at missing values; for $\mathrm{ARL}_0=200$ and $k=\delta_{\min}/2$ the two-sided control limits are reported as approximately $h^+=8.5$ and $h^-=-8.5$. The SVR/SVC layer is trained on 63,000 simulated instances (80% train/20% validation) created by block-bootstrapping IC residuals and injecting three deviation types; the SVR achieves MAPE ≈ 26 and NRMSE ≈ 0.26 on validation, while the SVC achieves ≈86% classification accuracy (confusion matrix provided). Case-study figures show the procedure detects previously documented station deviations (e.g., identified shifts in FU around 2007 and strong variations in SM after ~1977) and many additional smaller/persistent shifts. A simulation comparison in an appendix indicates the block-bootstrap calibration yields closer-to-target $\mathrm{ARL}_0$ than a parametric ARMA-based calibration under complex autocorrelation.",None stated.,"The method relies on the panel’s cross-sectional median and on clustering-based selection of stable series; if a large fraction of series are simultaneously degraded or share common biases, the robust reference/IC pool selection may be compromised. Control-limit calibration and SVM training depend on bootstrap design choices (block length, smoothing windows, IC pool definition) that may be data-specific and require careful tuning to avoid mis-calibration. The shift-form classifier is restricted to a finite, user-specified set of deviation types, so real deviations outside these templates may be mischaracterized even if they are detected. No implementation details (software/code) are provided, which may hinder reproducibility for practitioners.","The authors plan to extend the approach from sunspot counts $N_s$ to other components used to build the international sunspot number (e.g., $N_g$ and composites such as $N_g+10N_s$) toward fully automated online monitoring with alerts to observing stations. They also state an extension to monitoring process variances (in addition to mean/level deviations). They suggest further analysis of predicted shift forms (e.g., linking trends to instrument ageing or oscillations to alternating observers) as a research perspective.","Developing principled, data-driven block-length and smoothing-window selection (with sensitivity analyses) would improve robustness and portability across domains. Extending the scheme to high-dimensional or streaming panel settings with changing membership (stations entering/leaving) and to joint multivariate monitoring of multiple related indicators (e.g., counts and groups) would broaden applicability. Providing diagnostic tools for root-cause analysis and automatic re-estimation/re-initialization rules (adaptive/self-starting re-calibration) would help long-term deployments. Releasing open-source software and benchmark datasets would enable reproducibility and more comprehensive comparisons against modern robust/change-point methods.",2010.11826v1,local_papers/arxiv/2010.11826v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:24:44Z
TRUE,Univariate|Multivariate|Image-based monitoring|High-dimensional|Other,Hotelling T-squared|Machine learning-based|Other,Both,Healthcare/medical|Transportation/logistics|Theoretical/simulation only|Other,TRUE,FALSE,FALSE,Simulation study|Other,ARL (Average Run Length)|False alarm rate|Other,"Phase I calibration uses 2500 in-control samples to estimate $Q_0$ and $\Sigma_0$ (via sample mean and covariance). For daily quantile estimation, each day simulates between 10 and 100 accidents (response times) to compute the 0.8 and 0.95 sample quantiles.",TRUE,Python,Not provided,NA,"The paper discusses statistical learning approaches for change point and anomaly detection in dynamic graphs, emphasizing a hybrid workflow that combines statistical process control (SPC) with deep learning on graphs. For monitoring, it proposes using the control chart for quantile function values to detect deviations in ambulance-service response times, then applying a graph convolutional network (GCN) to classify the network state into causal categories (stable, manpower shortage, construction works, traffic jams). The SPC component uses a multivariate statistic built from the daily 0.8 and 0.95 response-time quantiles, calibrated in Phase I with 2500 in-control samples and monitored in Phase II over a simulated 100-day period. The GCN uses message passing with a Gaussian mixture model convolution operator (via PyTorch Geometric) and is trained on simulated labeled graphs to support post-signal diagnosis. Results from the simulation indicate the chart can flag out-of-control periods (with one reported false signal late in the series), while the GCN achieves an overall weighted F-score around 83% on Phase II data, performing well on classes 0/2/3 but struggling with the manpower-shortage class.","The monitoring statistic is the quadratic form $a_t=(\hat Q_t-Q_0)^\top\Sigma_0^{-1}(\hat Q_t-Q_0)$, where $\hat Q_t=(\hat Q_{0.8,t},\hat Q_{0.95,t})^\top$ are daily sample quantiles of response times. In Phase I, $Q_0$ is estimated by the sample mean $\bar Q$ and $\Sigma_0$ by the sample covariance matrix $S$ from 2500 in-control samples. For large samples, $a_t\sim\chi^2_c$ with $c=2$, and the upper control limit is set to $\chi^2_{\alpha}(2)$ with $\alpha=1/\text{ARL}$ (here ARL=1000 so UCL $=\chi^2_{0.001}(2)=13.82$).","The chart is calibrated with in-control ARL set to 1000 (equivalently $\alpha=0.001$), yielding a control limit of 13.82 for $c=2$ quantile components. In the 100-day Phase II simulation (30 IC days, then 10 manpower-shortage, 30 construction, 20 traffic-jam days, then 10 IC days), the chart shows one false signal in the final in-control segment (slightly above the limit). The GCN is trained with 2500 labeled graphs (balanced across 4 classes) and validated on 800 graphs, with best model at epoch 102 achieving weighted F-scores of about 93% (train) and 87% (validation). On Phase II test data, the GCN achieves an overall weighted F-score of about 83%, classifying labels 0/2/3 nearly perfectly but misclassifying many label-1 (manpower shortage) cases.","The authors note that graph-structured data analysis is still novel and that key open challenges remain, including how to represent graph data, how to convolve information, which approach to use in which case, and how to measure performance. They also argue that using GCNs alone for the full monitoring task may be impractical for real-world settings due to required model complexity and training time, which would limit applicability. In the simulation results discussion, they observe that the manpower-shortage class (label 1) is not well learned, possibly because its representation is not sufficiently clear in the simulated data.","The monitoring method relies on asymptotic $\chi^2$ behavior of the quantile-based quadratic form and uses Phase I estimates as known in Phase II; finite-sample effects for sample quantiles and covariance estimation (and their impact on in-control ARL) are not fully assessed. The study is simulation-based with a simplified road network and stylized data-generating mechanisms, so external validity to real ambulance operations (heterogeneous demand, nonstationarity, reporting delays, dependence) is uncertain. The hybrid approach is only loosely integrated: the GCN is used for post-signal classification rather than jointly optimizing detection/diagnosis, and no decision-theoretic or cost-sensitive evaluation is provided. Comparisons against alternative SPC charts (e.g., nonparametric quantile charts, EWMA/CUSUM on quantiles, Bayesian predictive limits) and alternative graph anomaly models are not systematically reported.","The paper concludes that many open questions remain in unifying SPC with graph deep learning, specifically around graph representation, convolution/aggregation choices, method selection by context, and performance measurement. It also discusses the possibility of extending graph deep learning methods (e.g., GCNs) to cover the entire monitoring procedure without control charts, but suggests practical constraints make a hybrid approach more feasible at present. The authors state they present several directions for future research aimed at expanding joint applications of machine learning and classical statistical tools for graph monitoring.","A valuable next step would be to develop a principled joint detection-and-diagnosis framework where the GCN output informs adaptive charting (e.g., class-conditional limits, multi-stream charts, or GLR/CUSUM driven by learned embeddings) rather than only post-signal classification. Robustness studies should examine non-normality, serial dependence in daily statistics, irregular sampling/missing data, and the impact of Phase I estimation error on achieved ARL, including bootstrap or Markov-chain/integral-equation ARL estimation. Extending to real datasets from emergency medical services (with ground-truth incidents and interventions) would validate practicality and interpretability, and enable economic design (cost of false alarms vs delayed detection). Finally, releasing reproducible software (e.g., a PyTorch Geometric implementation plus chart calibration scripts) would accelerate adoption and benchmarking against competing network monitoring methods.",2011.06080v1,local_papers/arxiv/2011.06080v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:25:16Z
TRUE,Profile monitoring|Multivariate,Hotelling T-squared|Other,Both,Semiconductor/electronics|Pharmaceutical|Manufacturing (general),TRUE,NA,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|Detection probability|False alarm rate|Other,"Pharmaceutical example notes dissolution testing guidance that sample sizes “have to be 12” (per Ma et al., 2000). Simulation uses Phase I with k=100 samples of size n=5; semiconductor example uses Phase I: 18 samples of size n=11 and Phase II: 14 samples of size n=11.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes profile monitoring control charts for simple linear profiles by monitoring the intercept and slope simultaneously rather than a process mean. It compares two coefficient-estimation approaches—classical linear regression (least squares) versus a maximum-entropy-based estimation that first fits a (bivariate) maximum entropy joint density under moment constraints—then converts the 2D coefficient vector to a scalar using the Hotelling $T^2$ statistic. Control limits are set either via an $F$-distribution-based UCL for $T^2$ or via an empirical $(1-\alpha)$ quantile of the Phase I $T^2$ values (separately for ME and LR). Performance is evaluated via Monte Carlo simulations under small shifts in intercept, slope, or both, reporting Type II error ($\beta$) and ARL (ARL0/ARL1). Two case studies (semiconductor DRIE profiles and pharmaceutical dissolution profiles) illustrate that the maximum-entropy approach can detect changes that the regression/mean-based limits may miss, particularly in the pharmaceutical example.","Simple linear profile: $\tilde y_j = a + b\tilde x + \tilde\varepsilon_j$ with $\varepsilon\sim N(0,\sigma^2)$. Maximum entropy joint density under constraints: $f_{X,Y}(x,y)=\exp\{-1-\lambda_0-\sum_{i=1}^r \lambda_i h_i(x,y)\}$. ME coefficient estimates: $\hat b_{j,ME}=\frac{E_j[(X-EX)(Y-E_jY)]}{E[(X-EX)^2]}$ and $\hat a_{j,ME}=E_j(Y)-\hat b_{j,ME}E(X)$; LR uses standard least-squares formulas. Monitoring statistic: $T^2_{j}= (\mathbf{c}_j-\bar{\mathbf{c}})'S^{-1}(\mathbf{c}_j-\bar{\mathbf{c}})$ for coefficient vector $\mathbf{c}_j=(\hat a_j,\hat b_j)$, with UCL either $UCL_F$ (via $F_{\alpha,p,k-p}$) or empirical quantile $q_{ME}$/$q_{LR}$ of Phase I $T^2$ values.","In simulation with 100 Phase I samples (n=5) from $Y=2+3X+\varepsilon$, reported control limits were $UCL_F=6.303865$, $UCL_{ME}=5.591411$, and $UCL_{LR}=5.80042$; estimated in-control ARLs were about 17 (with $UCL_F$) and 13 (with $UCL_{ME}$) for ME, and about 17 (with $UCL_F$) and 15 (with $UCL_{LR}$) for LR (based on estimated false-alarm probabilities). Across intercept/slope/mixed shift models, plots/tables show ME (especially using quantile-based UCL) generally yields lower $\beta$ and smaller ARL1 for small shifts than LR, indicating faster detection. In the semiconductor case (Phase I: 18 samples; Phase II: 14), $UCL_F=8.150644$ while $UCL_{ME}=UCL_{LR}=4.857291$; the last Phase II sample had $T^2\approx 5.773$ and was signaled by ME/LR quantile limits but not by $UCL_F$. In the pharmaceutical dissolution example, limits at confidence 0.9973 were $UCL_F=26.97728$, $UCL_{ME}=6.812622$, $UCL_{LR}=5.883782$, and ME more clearly separated post-change batches from the reference than LR per the reported $T^2$ tables.",None stated.,"The maximum-entropy coefficient estimation relies on choosing moment/constraint sets (e.g., six constraints in Eq. (5), reduced constraints in the toy example), but sensitivity to constraint choice and numerical stability of solving the Lagrange-multiplier systems is not systematically analyzed. Although the profile model assumes i.i.d. normal errors, robustness to non-normality and serial correlation (common in profile data collected over time) is not evaluated. Quantile-based UCLs are empirical and may depend strongly on Phase I sample size/cleanliness, but guidance on Phase I contamination and parameter uncertainty is limited. No software or implementation details are provided, which may hinder reproducibility of the ME estimation steps.",None stated.,"Assess robustness of the maximum-entropy profile chart to non-normal errors, autocorrelation, and outliers, and develop robust/regularized constraint choices for stable ME estimation. Provide principled guidance (or adaptive selection) for the constraint set and numerical solvers for the Lagrange-multiplier system, including convergence diagnostics. Extend the approach to multiple/polynomial/generalized linear profiles and to multivariate profiles with more coefficients (including high-dimensional coefficient vectors). Release reproducible code and benchmarking studies against established profile-monitoring methods (e.g., MEWMA/MCUSUM-based profile charts) under standardized scenarios.",2012.14289v4,local_papers/arxiv/2012.14289v4.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:25:51Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Both,Manufacturing (general)|Environmental monitoring|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate|Other,"Phase I is illustrated using the first 20 samples (out of 30) in a manufacturing example, and the first 70 samples in a flood example; iterative removal of out-of-control Phase I points is used to refine the UCL until remaining Phase I points are in-control.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a multivariate dependence-aware control-charting approach that estimates an unknown joint distribution by combining maximum entropy with maximum copula entropy, thereby preserving the observed dependence structure in the data. Using the resulting copula-based joint density (via Sklar’s theorem) and maximum-entropy marginals, it computes an elliptical Hotelling’s $T^2$ upper control limit (UCL) by numerically solving $P(T^2\le\mathrm{UCL})\ge 1-\alpha$ without relying on the classical normal/F-approximation. Dependence constraints are imposed through rank-based measures (Spearman’s rho and Blest measures), yielding an exponential-form copula density with Lagrange multipliers. Performance is evaluated through Monte Carlo estimation of ARL0/ARL1 across several dependence scenarios and mean-shift magnitudes, and via two real-data case studies (a manufacturing quality dataset and flood duration–volume data). Results show that incorporating dependence via maximum copula entropy yields different (typically more sensitive) control limits than a dependence-ignoring maximum-entropy joint model and differs from traditional Fisher-distribution-based limits under non-normal dependence.","The maximum-copula-entropy copula density is obtained by maximizing Shannon entropy subject to copula constraints and dependence-moment constraints, giving an exponential family form $c(u,v)=\exp\{-1-\lambda_0-\sum_{i=1}^r \lambda_i(u^i+v^i)-\lambda_{r+1}uv-\lambda_{r+2}(u^2v+uv^2)-\lambda_{r+4}u^2v^2\}$. The joint density is built as $f_{X,Y}(x,y)=c(F_X(x),F_Y(y))f_X(x)f_Y(y)$, with marginals $f_X(x)=\exp(-\lambda_0-\sum_i\lambda_i g_i(x))$. Monitoring uses Hotelling’s statistic $T^2=(\mathbf{X}-\boldsymbol\mu)^\top\Sigma^{-1}(\mathbf{X}-\boldsymbol\mu)$ and defines UCL by solving $P(T^2\le\mathrm{UCL})\ge 1-\alpha$ under the estimated joint density.","In simulation with $\alpha\approx0.05$, UCL values varied strongly with dependence: for $(\mu_X,\mu_Y)=(2,1)$, UCL ranged from 2.571 (Group 1) to 8.622 (Group 4), demonstrating dependence-driven control-region changes. ARL0 (estimated with 1000 replications) generally exceeded the nominal 20; e.g., for $(2,1)$ ARL0 means ranged from 22.404 to 31.355 across dependence groups, and for $(7,6)$ reached 41.045 in Group 1. ARL1 decreased sharply with larger mean shifts; for example at $(2,1)$ and Group 1, ARL1 for $(\delta_X,\delta_Y)=(1,1)$ was about 4.171 (implying $\beta\approx0.24$ per the authors’ discussion). In the first real manufacturing example (Phase I = first 20 samples), the copula-based UCL at 95% was 3.03649 versus 7.716048 for a dependence-ignoring maximum-entropy joint density, indicating the latter is less sensitive. In the flood example, the final copula-based UCL after removing four Phase I outliers was 6.89478 (first-stage UCL 6.85875).",None stated.,"The proposed UCL computation requires solving a 2D integral over an elliptical region under an estimated joint density; the paper does not provide numerical integration details or stability/complexity analysis, which may affect reproducibility and practical deployment. The approach is developed and demonstrated primarily for the bivariate case, so extension to higher-dimensional monitoring (p>2) may face rapidly increasing constraint/optimization complexity. Dependence is summarized through a small set of rank-based moments (Spearman/Blest), which may not fully capture tail dependence or complex dependence structures relevant to false-alarm behavior. No explicit treatment is provided for autocorrelation/streaming dependence over time, which is common in process data and can distort ARL properties.",None stated.,"Develop and validate a scalable extension to higher-dimensional (p>2) monitoring, potentially via vine copulas or dimension-reduction plus copula-entropy constraints. Provide a fully specified numerical recipe (and open-source implementation) for estimating Lagrange multipliers and solving for UCL, including convergence diagnostics and sensitivity to constraint choice (r and dependence measures). Study robustness under autocorrelation and nonstationary dependence, and incorporate time-series modeling or residual charts to maintain nominal ARL0. Add broader empirical validation across industrial datasets and compare against modern nonparametric/multivariate charts (e.g., depth-based, kernel, or robust $T^2$ variants) under non-normality and dependence.",2012.14759v5,local_papers/arxiv/2012.14759v5.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:26:27Z
TRUE,Univariate,EWMA,Both,Semiconductor/electronics|Theoretical/simulation only,TRUE,FALSE,FALSE,Integral equation|Approximation methods|Simulation study|Markov chain,ARL (Average Run Length)|MRL (Median Run Length)|False alarm rate|Detection probability|Other,Phase I uses m subgroups each of size n; the paper recommends m ≥ 50 (and suggests m ≥ 100 based on CDF/ARL behavior) to retain reasonable detection performance when limits are adjusted for estimation uncertainty.,TRUE,R,Package registry (CRAN/PyPI),https://cran.r-project.org/|https://kassandra.hsu-hh.de/apps/knoth/s2ewmaP/,"The paper studies EWMA control charts for monitoring a normal process variance using the sample variance $S^2$ when the in-control variance $\sigma_0^2$ is unknown and must be estimated from Phase I data. It shows that simply plugging in an estimate and/or calibrating to an unconditional in-control ARL can yield misleading false-alarm behavior due to heavy-tailed unconditional run-length (RL) distributions, including counterintuitive “improved” out-of-control ARLs for smaller Phase I sample sizes. As an alternative, it proposes designing chart limits by controlling the unconditional probability of a false alarm within a planned monitoring horizon, i.e., fixing an unconditional in-control RL quantile via $P(L\le \bar \ell)=\alpha$. Numerical algorithms are developed to compute unconditional RL distributions/quantiles and to adjust one-sided and two-sided EWMA $S^2$ limits, using a collocation approach for the underlying integral recursions and comparing favorably against Markov chain approximations (more accurate and faster). The paper studies how the EWMA smoothing parameter $\lambda$ and Phase I size $m$ affect required limit widening and detection performance, recommending $\lambda\in\{0.1,0.2\}$ and Phase I sizes at least $m\ge 50$ for detecting small variance changes under parameter uncertainty.","Subgroup variances: $S_i^2=\frac{1}{n-1}\sum_{j=1}^n (X_{ij}-\bar X_i)^2$. EWMA update: $Z_i=(1-\lambda)Z_{i-1}+\lambda S_i^2$ with $Z_0=\sigma_0^2$ (estimated in practice). Stopping times: one-sided $L=\min\{i\ge1: Z_i>c_u\}$ and two-sided $L=\min\{i\ge1: Z_i>c_u \text{ or } Z_i<c_l\}$, with limits chosen to satisfy the unconditional design rule $P(L\le \bar\ell)=\alpha$. Phase I estimator: pooled variance $\hat\sigma_0^2=\frac{1}{m}\sum_{i=1}^m s_i^2$ (chi-square with $m(n-1)$ df), and unconditional survival is computed by mixing conditional RL survival over the distribution of $\hat\sigma_0^2$: $p_{\ell,\text{unc}}(z;\sigma^2,c_u)=\int_0^{\infty} f_{\hat\sigma_0^2}(s^2)\,p_\ell(z;\sigma^2/s^2,c_u)\,ds^2$.","Using a design targeting $P(L\le 1000)=0.25$ (often with $n=5$, $\lambda=0.1$), the required control limits are widened relative to the known-parameter case; for realistic Phase I sizes (about $m\ge 50$) the widening becomes modest, and by $m\gtrsim 100$ the limits are close to the known-$\sigma_0^2$ benchmark. The paper documents that small Phase I sizes produce heavy-tailed unconditional IC RL distributions and high probabilities of early false alarms; calibrating to unconditional IC ARL can even tighten limits and worsen early false alarms (“unconditional ARL puzzle”). In accuracy comparisons for computing unconditional RL properties, collocation with $N=50$ achieves much higher accuracy than a Markov chain with $N=500$ and is substantially faster (about 1s vs tens of seconds in the reported setup). For performance guidance, it recommends $m\ge 50$ (and often $m\ge 100$) to maintain reasonable detection of small shifts and suggests $\lambda=0.1$ or $0.2$ as good practical choices.",None stated.,"The methodology is developed for normally distributed, independent subgroup data; robustness to non-normality, autocorrelation, or model misspecification is not established. The approach requires nontrivial numerical machinery (integral recursions, quadrature, collocation) and may be less straightforward to deploy/validate in settings with varying subgroup sizes, irregular sampling, or additional nuisance parameters beyond $\sigma_0^2$. Real-data validation is limited (an illustrative SEM context is described, but most results are algorithmic/simulation-based), so practical impact across diverse industries is not fully demonstrated.",None stated.,"Extend the false-alarm-probability (RL-quantile) calibration framework to non-normal dispersion models (e.g., gamma/lognormal) and to autocorrelated processes (e.g., EWMA on residuals from time-series models). Develop analogous procedures for other memory charts (CUSUM/MEWMA) for variance and for joint mean–variance monitoring under estimated parameters, and provide systematic guidance on choosing $(\bar\ell,\alpha)$ based on economic or risk criteria. Provide broader empirical case studies and user-friendly software defaults/diagnostics (e.g., sensitivity to Phase I contamination, robust estimators) to improve adoption.",2101.04011v1,local_papers/arxiv/2101.04011v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:27:05Z
TRUE,Univariate|Other,Shewhart|Other,Both,Healthcare/medical|Theoretical/simulation only,FALSE,FALSE,NA,Exact distribution theory|Simulation study,False alarm rate|Other,Not discussed.,TRUE,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=rQCC,"This note revisits geometric-distribution-based g and h control charts for monitoring the number of conforming cases between successive nonconformities. It shows that a commonly cited “MVU” estimator used in the quality engineering literature is incorrect, derives the correct minimum-variance unbiased (MVU) estimator for the geometric parameter p via Rao–Blackwellization, and establishes ordering relationships among the ML, incorrect-bias-corrected, and MVU estimators. The paper derives exact first and second moments (and hence biases and variances/MSEs) for the estimators using hypergeometric-function expressions and validates the bias behavior with Monte Carlo simulations. It further addresses a practical limitation of conventional g/h charts (assumption of balanced subgroup sizes) by providing a method to construct g and h charts when subgroup/sample sizes are unequal, giving ML-based and MVU-based control limits using appropriate estimators for the mean and variance. The work clarifies correct estimation for geometric SPC charts and provides implementable formulas for unbalanced-sample applications, especially relevant to healthcare adverse-event monitoring.","The shifted geometric model is $P(Y=a+k)=p(1-p)^k$ with mean $\mu=(1-p)/p+a$ and variance $\sigma^2=(1-p)/p^2$. The ML estimator is $\hat p_{ml}=1/(\bar Y-a+1)$, while the correct MVU estimator is $\hat p_{mvu}=(n-1)/(\sum_i Y_i-na+n-1)=((n-1)/n)/(\bar Y-a+1-1/n)$. For unbalanced subgroup sizes, with total $N=\sum_i n_i$ and grand mean $\bar{\bar X}$, the paper constructs h- and g-chart limits using $\mu$ estimated by $\hat\mu=\bar{\bar X}$ and $\sigma^2$ estimated by $\hat\sigma^2_{ml}=(\bar{\bar X}-a)(\bar{\bar X}-a+1)$ or $\hat\sigma^2_{mvu}=\frac{N}{N+1}(\bar{\bar X}-a)(\bar{\bar X}-a+1)$, plugged into Shewhart-type limits (e.g., $\mathrm{UCL}=\mu+g\sqrt{\sigma^2/n_k}$ for the h chart and $\mathrm{UCL}=n_k\mu+g\sqrt{n_k\sigma^2}$ for the g chart).","The paper proves strict inequalities (for $0<p<1$) among estimators used in practice: $\hat p_b<\hat p_{mvu}<\hat p_{ml}$, implying the commonly used “bias-corrected” estimator $\hat p_b$ systematically underestimates p while $\hat p_{ml}$ overestimates p. It derives exact expectations and second moments, e.g., $E(\hat p_{ml})=p^N\,{}_2F_1(N,N;N+1;1-p)$ and analogous expressions for $E(\hat p_b)$ and second moments (including a ${}_3F_2$ expression), while $\hat p_{mvu}$ has zero theoretical bias. Monte Carlo results with 10,000 replications show severe bias for $\hat p_b$ when N is small and p is large (e.g., for $(n_1,n_2)=(1,1)$ and $p=0.9$, empirical bias about -0.434), and positive bias for $\hat p_{ml}$ (e.g., about 0.118 when $p=0.5$ and $(1,1)$). The proposed $\hat p_{mvu}$ exhibits empirical biases very close to zero across all simulated scenarios and supports improved estimation for constructing g/h chart limits.","The note points out that conventional g and h charts rely on an underlying assumption of balanced samples (equal subgroup sizes), which is often not satisfied in practice; this motivates their unbalanced-sample construction method. It also cautions that although $\hat p_{mvu}$ is unbiased for p, transformations such as $1/\hat p_{mvu}$ are not unbiased, so care is needed when estimating $\mu$ and $\sigma^2$ that depend on $1/p$.","The proposed unbalanced-sample limits rely on asymptotic normal approximations for subgroup means/totals, which may be inaccurate for very small subgroup sizes or very rare/very frequent events (extreme p), precisely where geometric charts are often used. The work focuses on iid Bernoulli/geometric assumptions and does not address serial dependence, overdispersion/heterogeneity (e.g., varying risk in healthcare), or other departures (e.g., clustering) that can materially affect false-alarm behavior. Performance is evaluated via estimator bias/MSE rather than control-chart run-length metrics (e.g., in-control ARL/ATS under parameter estimation), so practical signaling performance under estimation error is not fully quantified. No implementation code for the specific unbalanced g/h chart construction is provided beyond mentioning an R package to be updated.",The authors state that they developed the rQCC R package and plan (in ongoing work) to add these g and h control charts in the next update to facilitate practitioner use.,"A natural extension is to study in-control and out-of-control run-length (ARL/ATS) properties of the ML-based vs MVU-based limits under parameter estimation, especially with unbalanced subgroup sizes. Robust or hierarchical extensions could address heterogeneity and risk adjustment common in healthcare (e.g., varying baseline p across units/time), and methods that account for autocorrelation or seasonality could broaden applicability. Exact or simulation-calibrated control limits (rather than asymptotic normal limits) for small $n_k$ would likely improve false-alarm control in sparse-event settings. Publishing reproducible code and a worked real-data case study demonstrating unbalanced-sample charts would strengthen practical adoption.",2101.07575v2,local_papers/arxiv/2101.07575v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:27:43Z
TRUE,Multivariate|High-dimensional|Nonparametric,Change-point|Other,Phase II,Semiconductor/electronics|Theoretical/simulation only,NA,FALSE,TRUE,Simulation study|Case study (real dataset),Detection probability|Expected detection delay|False alarm rate|Other,"Monitoring can start with at least 6 observations (each of the two partitions must have at least 3 observations); in the moving-window version each window requires W>=6 (they study W=20,30,40).",TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/wyfwzz/Supplementary-code-for-A-change-point-based-control-chart...-,"The paper proposes a change-point based control chart (NSW) to detect sparse shifts in the mean vector of high-dimensional processes under heteroscedasticity and possibly non-normal distributions, targeting HDLSS/short-run settings where Phase I reference samples are limited. The charting statistic is a supremum-based two-sample split test (adapted from Chang et al., 2017) maximized over variables and over candidate split points, and implemented with a fixed moving window (size W) and step size s to reduce computation and stabilize control limits. Control limits are set via a data-driven bootstrap to control a prespecified false alarm probability (FAP), rather than ARL. Extensive simulations evaluate detection rate (power), conditional expected detection delay, and post-signal diagnosis (change-point estimation and identifying affected variables) under baseline mean shift, heteroscedastic variance changes, correlation, and heavy-tailed data. A semiconductor (SECOM) case study illustrates practical performance, and the authors provide code and supplementary material online.","For split point k, the non-studentized statistic is $T^{NS}_{n,k}=\max_{1\le r\le p}\sqrt{\frac{k(n-k)}{n}}\,|\bar X_{k,r}-\bar X_{n-k,r}|$. The monitoring statistic is $U^{NS}_n=\max_{3\le k\le n-3}T^{NS}_{n,k}$, with estimated change-point $\hat\tau^{NS}_n=\arg\max_k T^{NS}_{n,k}$. With a moving window of size $W$, $T^{NS}_{n,W,k^*}=\max_r \sqrt{\frac{k^*(W-k^*)}{W}}\,|\bar X_{k^*,r}-\bar X_{W-k^*,r}|$ and $U^{NS}_{n,W}=\max_{3\le k^*\le W-3}T^{NS}_{n,W,k^*}$; signal when $U^{NS}_{n,W}>h_{p,W}$ where $h_{p,W}$ is set by bootstrap for target FAP.","In simulations (typically R=1000 runs) the NSW chart shows high detection rates for large sparse mean shifts (e.g., with p=100, W=40, \tau=25, DR≈1 for \delta≥1.5 under multiple models), while DR is low for small shifts (e.g., \delta=1). Robustness studies show only modest degradation under heteroscedasticity, dependence, and heavy-tailed (t) distributions relative to the baseline normal model, with detection delay largely unchanged. Post-signal diagnosis accurately estimates the change-point (CPE near the true \tau) and can recover a large fraction of changed variables for large shifts (DRV around ~0.8 for \delta=2 in reported settings). In a SECOM semiconductor case study (after preprocessing and p reduced to 416), DR is reported as 1 across tested \tau and W settings, with acceptable detection delays depending on W relative to \tau.","The authors note a limitation of the supremum-based method: the chart’s signal is driven by (essentially) one variable attaining the maximum, so abnormal behavior in other variables may be ignored, motivating a separate diagnostic procedure to identify additional suspicious variables.","Control limits are obtained by bootstrap under an approximate-independence assumption induced by the step size; if the process has genuine serial dependence, the achieved false-alarm probability may deviate from nominal. The method focuses on sparse mean shifts; performance for variance-only changes, dense weak shifts, or more complex distributional changes is not the main target and may be inferior to alternatives (as suggested by their comparison to DFEWMA for smaller/moderate shifts). The paper reports code availability but does not clearly state the software environment (e.g., R/Python/MATLAB) for the proposed method, which may hinder immediate reproducibility.","They suggest improving sensitivity as future work, for example by adapting/adding a variable selection algorithm before starting monitoring.","Extending the approach to explicitly handle autocorrelated/high-frequency streams (e.g., via block bootstrap or modeling dependence) would strengthen practical applicability. Developing analytic or fast-approximate control limits (reducing reliance on simulation/bootstrapping across p and W) and providing a packaged software implementation would aid adoption. Broadening the method to jointly detect mean and covariance/variance changes (or other distributional shifts) and to high-dimensional diagnosis with controlled false discovery rates would further advance the framework.",2101.09424v1,local_papers/arxiv/2101.09424v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:28:16Z
FALSE,Univariate|Other,Shewhart|Machine learning-based|Other,Phase II,Energy/utilities|Other,NA,FALSE,TRUE,Case study (real dataset)|Simulation study|Other,False alarm rate|Other,"Training uses the first 2 years of operation assumed healthy, with 2500 randomly selected (uniform by index) training points. X-bar 3σ thresholds are computed from a 6-month period immediately after training, using repeated random sub-sampling (20 subsets) to estimate mean and SD, then setting thresholds as \(\bar\mu \pm 3\bar\sigma\).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a structural health monitoring approach for operational wind-turbine blades that learns healthy relationships between nominally identical blades using Gaussian Process (GP) regression. Using SCADA-derived first edgewise natural frequencies (plus ambient temperature), the method predicts one blade’s frequency from another’s and uses the resulting GP prediction residuals as a damage indicator. For diagnostics, the residuals are aggregated into 28-day means and monitored with an X-bar control chart using ±3σ thresholds estimated from a post-training baseline period. The approach is validated on one synthetic example and two real onshore turbines with known damage events, where threshold exceedances occur months before maintenance actions. The work integrates machine-learning prediction with simple Shewhart-style monitoring to create an early-warning scheme under environmental/nonstationary operating conditions.","Inputs are standardized as \(x_{\text{norm}}=(x-\mu)/\sigma\). Three GPs are trained with kernels \(k(x_p,x_q)=k_{SE}+k_{BL}+k_N\), where \(k_{SE}=\sigma_f^2\exp(-\|x_p-x_q\|^2/(2\lambda^2))\), \(k_{BL}=\sigma_0^2 x_p\cdot x_q\), and \(k_N=\sigma_N^2\delta(x_p,x_q)\). GP predictions use \(\bar f^*=K_*^T K_\theta^{-1}y\) and residuals \(r_e=\bar f - f^*\); X-bar thresholds are set as \(\text{thr}=\bar\mu\pm 3\bar\sigma\) on 28-day averaged residuals.","In Site A, X-bar charts of GP residuals indicate abnormal behavior roughly 6 months before the blade was identified/remedied. In Site B, thresholds are exceeded about 3 months before remedial action. The paper also demonstrates on synthesized data that residuals diverge when one blade’s frequency is forced to degrade, while residuals for a blade not linked to the degraded blade do not diverge. No numerical ARL/ATS values are reported; performance is presented via lead-time to known maintenance events and qualitative discussion of false-alarm considerations (3σ limits).","Damage type and location are not disclosed due to confidentiality, limiting interpretability and reproducibility of the real-case validation. The authors note that after repair/replacement, blade properties differ and correlations break down, implying the model must be retrained after remedial actions. They also note that GP methods should not be used for extrapolation; as inputs move away from the training region when damage develops, prediction quality degrades (which is leveraged for detection).","The X-bar charting design is not calibrated to a target in-control ARL/false-alarm rate, and the dependence induced by 28-day averaging (and SCADA filtering) is not analyzed, so the effective false-alarm behavior is unclear. The method assumes the first two years are healthy and that blades are nominally identical; violations (early defects, manufacturing differences, asymmetric loading) could bias training and thresholds. Autocorrelation and seasonality are handled indirectly via temperature as a covariate, but remaining serial dependence in residuals is not explicitly modeled, which can affect control-limit validity. The approach provides limited diagnostic localization (which blade/where) and relies on frequency sensitivity to damage type/location, which may not hold for all defect mechanisms.","The authors suggest the correlation-based methodology could be applied to other measured signals (e.g., strains and displacements), though these may require additional preprocessing and complementary features.","Develop a principled SPC design that targets a specified in-control ARL (or false-alarm probability) for the residual X-bar chart under serial dependence, possibly via bootstrap/Block bootstrap or time-series control limits. Extend the approach to explicitly handle autocorrelation/nonstationarity (e.g., GP state-space models or dynamic regression) and to multivariate monitoring (joint residual vector across blades) using Hotelling/MEWMA-style charts. Add diagnostic capability to attribute the alarm to a specific blade and to provide root-cause indicators (e.g., change-point estimation on residual mean/variance). Provide open-source implementation and benchmarking across multiple turbines/sites to assess robustness and generalizability.",2101.11711v1,local_papers/arxiv/2101.11711v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:28:49Z
TRUE,Univariate|Bayesian|Other,Shewhart,Phase II,Transportation/logistics,TRUE,NA,FALSE,Simulation study,False alarm rate|Detection probability|Other,Not discussed,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a real-time monitoring framework for autonomous vehicle car-following control that tracks deviations of the (actual) time gap from a desired setting using spacing and speed measurements from onboard sensors. It models spacing via a random-coefficient linear model where the time-gap parameter is a stochastic coefficient, and derives a closed-form Bayesian updating rule for the posterior mean and covariance of the random coefficients. Monitoring is performed by applying a univariate Shewhart control chart to the updated time-gap estimate, with control limits centered at the desired time gap and set by a chosen multiple of an acceptable standard deviation. The approach is demonstrated in a simulation study using a scenario built from NGSIM trajectory/acceleration data, showing that time-gap variations can exceed the limits for upstream vehicles and that adjusting the desired time gap after repeated limit violations reduces variability. The contribution is the coupling of an analytically tractable Bayesian online estimator with standard SPC control limits to trigger parameter retuning in automated-vehicle control.","Spacing is modeled as a random-coefficient regression $S_i(t)=s_0+\tau_i V_i(t)+\epsilon_i(t)$ with $\epsilon_i(t)\sim N(0,\sigma^2)$ and $\Gamma_i=[s_0,\tau_i]^T\sim N(\mu_b,\Sigma_b)$. Given real-time observations $S_a^*=Z_a^*\Gamma_a+E_a^*$, the posterior is $\Gamma_a\mid S_a^*\sim N(\mu_a^*,\Sigma_a^*)$ with $\Sigma_a^*=(\Sigma_b^{-1}+Z_a^{*T}Z_a^*/\sigma^2)^{-1}$ and $\mu_a^*=\Sigma_a^*(Z_a^{*T}S_a^*/\sigma^2+\Sigma_b^{-1}\mu_b)$. Shewhart limits are set as $\mathrm{LCL}=\mu_{desired}-L\sigma_{desired}$, $\mathrm{CL}=\mu_{desired}$, $\mathrm{UCL}=\mu_{desired}+L\sigma_{desired}$.","In the simulation, the baseline time-gap distribution is set to $N(1.6,0.125)$ and limits are computed at 95% confidence ($L=2$): LCL=1.35, CL=1.6, UCL=1.85. Vehicles 1 and 2 exceed these limits, with reported extremes around max 1.92 s and min 1.28 s, including multiple out-of-bounds events over short windows (e.g., four times within 50 s). After repeated violations for Vehicle 1, the desired time gap is changed from 1.6 s to 1.0 s (new limits: LCL=0.75, CL=1.0, UCL=1.25), and the maximum deviation from desired is reduced from 0.32 to 0.17. Downstream vehicles (3–5) show smaller variations, consistent with disturbance damping along the platoon.","The authors note the need for validation using real experimental (field) data from autonomous vehicles rather than simulations. They also state that a non-linear modeling approach could yield more accurate real-time estimates of time gap. They further acknowledge that time gap depends on controller parameters (e.g., feedback/feedforward gains) that are not considered, and that incorporating additional performance metrics could improve the monitoring methodology.","The Shewhart chart is used in a relatively simplified way (fixed normal baseline and fixed $\sigma_{desired}$), without formal calibration of in-control ARL/false-alarm performance or sensitivity to estimation uncertainty in $\tau$ from the Bayesian step. The approach assumes independent Gaussian measurement noise and (implicitly) ignores serial correlation typical in high-frequency vehicle trajectory data, which can inflate false alarms for Shewhart charts. The method focuses on single-parameter thresholding (time gap) and does not provide guidance on diagnosing root causes (sensor issues vs. controller dynamics vs. leader behavior) once a signal occurs. No implementation details or computational benchmarks are provided to substantiate real-time feasibility beyond the closed-form update claim.","The authors suggest (i) using real experimental data on autonomous vehicles to analyze time-gap uncertainty, (ii) considering non-linear modeling to obtain more accurate real-time estimates, (iii) incorporating dependence of time gap on control parameters such as feedback and feedforward gains, and (iv) including additional performance metrics to yield a better monitoring methodology.","A natural extension is to design and evaluate alternative memory charts (EWMA/CUSUM) on the posterior time-gap stream to improve sensitivity to small, persistent drifts while controlling false alarms. Robust/nonparametric variants could be developed to reduce reliance on Gaussian assumptions and to handle outliers from sensor glitches. Explicitly modeling autocorrelation (e.g., state-space/Bayesian filtering) and propagating posterior uncertainty into control limits (credible-interval-based limits) would make signaling behavior more principled. Providing open-source code and testing on multiple driving scenarios (cut-ins, varying traffic, communication delays) would strengthen reproducibility and generalizability.",2102.00375v1,local_papers/arxiv/2102.00375v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:29:24Z
TRUE,Univariate|Other,EWMA|Other,Phase II,Theoretical/simulation only,TRUE,FALSE,FALSE,Simulation study|Integral equation|Other,ARL (Average Run Length)|Conditional expected delay|Steady-state ARL,Not discussed,TRUE,R,Package registry (CRAN/PyPI),https://cran.r-project.org/web/packages/spc/index.html,"This preprint is a critical evaluation of generally weighted moving average (GWMA) control charts for monitoring shifts in a process mean, arguing they have no practical or statistical advantage over appropriately designed EWMA charts. The authors emphasize computational drawbacks of GWMA: no recursive update for the charting statistic (requiring storage of all past data) and lack of a Markov property, which complicates accurate control-limit and performance calculations and forces heavy simulation. They show that prior performance comparisons favoring GWMA are often unfair because the competing EWMA smoothing parameter was not tuned comparably; matching GWMA and EWMA by equalizing asymptotic variances yields EWMA designs with similar or better zero-state and steady-state performance. Performance is studied using ARL, conditional expected delay (CED), and steady-state ARL, with EWMA quantities computed via integral-equation methods and GWMA assessed via large Monte Carlo simulation. The paper concludes practitioners should prefer EWMA charts and notes that if very small shifts are not of concern, alternative design philosophies may be more appropriate.","The GWMA statistic is defined as $G_t=\sum_{i=1}^{t}(q^{(i-1)\alpha}-q^{i\alpha})X_{t-i+1}+q^{t\alpha}\mu_0$, with in-control variance $\mathrm{Var}(G_t)=Q_t\sigma_0^2$ where $Q_t=\sum_{i=1}^{t}(q^{(i-1)\alpha}-q^{i\alpha})^2$ and asymptotic $Q=\lim_{t\to\infty}Q_t$. Control limits are $\mu_0\pm L_G\sqrt{Q_t}\,\sigma_0$ (or $\mu_0\pm L_G\sqrt{Q}\,\sigma_0$ asymptotically). The EWMA statistic is $Z_t=(1-\lambda)Z_{t-1}+\lambda X_t$ with asymptotic scaled variance $Q_E=\lambda/(2-\lambda)$, which they use to match EWMA to a given GWMA by setting $Q=Q_E$.","Using ARL tables based on $q=0.75$, they show that when EWMA is fairly tuned (e.g., matching asymptotic variance), EWMA typically matches or improves on GWMA zero-state ARLs across a range of mean shifts; for example, for shifts 0.25–3.0$\sigma_0$, EWMA with $\lambda=0.152$ often yields smaller ARLs than GWMA with $\alpha=0.5$ and also dominates the other considered GWMA designs in Table 2. Conditional expected delay profiles $D_\tau$ (approximated up to $\tau=100$) indicate the matched EWMAs ($\lambda=0.206$ for $\alpha=0.8$ and $\lambda=0.152$ for $\alpha=0.5$) have comparable or lower detection delays than their GWMA counterparts for small-to-moderate shifts. For large shifts, GWMA can be slightly better in some cases (notably around a 3$\sigma_0$ shift), attributed to GWMA placing relatively higher weight on the most recent observation in some parameter settings. The paper reports that simulating GWMA steady-state behavior required storing up to 10,000 past observations and using $10^8$ replications per setup to obtain smooth estimates, underscoring the computational burden.",None stated,"As a critique paper, it focuses on mean-shift detection under i.i.d. normal data with known and constant $\sigma_0$, so conclusions may not transfer directly to settings with variance changes, non-normality, parameter estimation (Phase I), or autocorrelation. The recommended EWMA matching principle (equalizing asymptotic variances) is plausible but not proven optimal under alternative criteria (e.g., minimizing worst-case delay, economic design, or robustness constraints). Real-data case studies are not provided, so practical impacts (implementation effort, interpretability, and behavior under process complications) are not empirically demonstrated.",None stated,"Extend the critique to broader data-generating conditions common in SPC—unknown parameters (Phase I/II combined), autocorrelated observations, and non-normal or heavy-tailed distributions—to test whether GWMA ever offers robustness advantages. Provide systematic design guidance beyond asymptotic-variance matching (e.g., optimization over $\lambda$ for specified steady-state criteria, head-start/FIR adjustments, or constrained designs for large-shift sensitivity). Develop or compare diagnostic tools and computational approximations for GWMA (if it is used) to reduce the heavy simulation burden, and validate findings on benchmark industrial datasets.",2107.00224v1,local_papers/arxiv/2107.00224v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:29:55Z
TRUE,Multivariate|High-dimensional|Other,MEWMA|Other,Both,Healthcare/medical,NA,NA,NA,Simulation study|Case study (real dataset),Detection probability|False alarm rate|Expected detection delay|Other,Not discussed,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a dynamic functional continuous-time Bayesian network (D-FCTBN) to model how patients’ modifiable lifestyle behaviors (diet, exercise, smoking, drinking) and non-modifiable factors affect the evolution of multiple chronic conditions over time. It formulates time-varying FCTBN edge/transition-rate parameters as a nonlinear state-space model and estimates them sequentially using an Extended Kalman Filter (EKF). For monitoring, it introduces a tensor-based control-charting framework: predicted regression-coefficient tensors are reduced via multilinear PCA (MPCA), reconstruction residuals are computed, and a MEWMA chart is applied to the residual feature vector to signal meaningful behavioral-driven changes in risk. The method is validated via both simulation experiments (injecting behavioral changes) and a real case study using 385 patients from the Cameron County Hispanic Cohort, showing sensitivity to changes in lifestyle factors with low false alarms in in-control scenarios. Overall, the work extends SPC-style monitoring to dynamic network-parameter streams in a healthcare risk modeling context rather than traditional manufacturing process variables.","FCTBN conditional intensities are modeled via Poisson regression, e.g., \(\log q_{x_i,x_j\mid u}=\mathbf{z}\,\boldsymbol{\beta}_{x_i,x_j\mid u}\) and \(q_{x_i\mid u}=\exp(\mathbf{z}^T\boldsymbol{\beta}_{x_i\mid u})\). The EKF state evolution is \(\boldsymbol{\beta}_t=F\boldsymbol{\beta}_{t-1}+\varepsilon_t\) with prediction/update equations for \(\boldsymbol{\beta}_{t\mid t-1},P_{t\mid t-1}\) and the Kalman gain \(K_t\) using the Jacobian \(G_t\). Monitoring uses MPCA reconstruction residual \(E_{new}=A_{new}-Z_{new}\times_1 U^{(1)T}\times_2 U^{(2)T}\times_3 U^{(3)T}\), then MEWMA \(Z_i=\lambda x_i+(1-\lambda)Z_{i-1}\) and chart statistic \(T_i^2=Z_i^T S_I^{-1} Z_i\) with \(S_I^{-1}=\frac{\lambda}{2-\lambda}[1-(1-\lambda)^{2i}]S\).","In simulated monitoring experiments, the proposed tensor-control-chart scheme produces no out-of-control signals under an in-control scenario across 48 months, indicating low Type I error. When a single behavioral factor is changed at month 17, the chart signals after about 3 observations for diet change (around month 20) and about 4 observations for drinking change (around month 21). When two behavioral factors are changed simultaneously (introduced at month 19 in the described setup), the first out-of-control signal occurs much later (around month 39), attributed to complex/offsetting interactions. In a real-case demonstration with yearly intervals, an out-of-control signal is produced after one observation following a behavioral change reported around year 13 (signal at year 14).",None stated,"The monitoring design is demonstrated mostly through illustrative case studies; standard SPC metrics (e.g., in-control ARL, out-of-control ARL/ATS) are not systematically reported, making it hard to benchmark against established charts. Chart parameter choices (e.g., MEWMA \(\lambda\) and control-limit width \(L\)) are selected “based on simulation analysis,” but the design procedure and robustness across scenarios/patients are not fully detailed. The approach depends on multiple modeling layers (FCTBN structure learning, EKF state-space assumptions, MPCA feature extraction), so performance may be sensitive to misspecification (e.g., Poisson/regression form, noise covariance choices) and to limited/irregular follow-up data typical in healthcare cohorts. No implementation details or shared code are provided, which limits reproducibility for practitioners.",None stated,"Provide a full SPC-style design and evaluation (steady-state and zero-state ARL/ATS, false-alarm control, and comparison to alternative multivariate/tensor monitoring methods) across a wide range of shift types and magnitudes in behaviors/network parameters. Extend the framework to explicitly handle irregular visit times, missingness, and informative dropout common in longitudinal patient data, and assess robustness to autocorrelation and nonstationarity in the estimated parameter streams. Develop diagnostic tools to localize which risk-factor coefficients/edges drive an alarm (root-cause attribution) and to recommend targeted interventions. Release open-source software (e.g., R/Python) implementing D-FCTBN estimation and tensor monitoring to support reproducibility and adoption in healthcare analytics.",2107.13394v1,local_papers/arxiv/2107.13394v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:30:28Z
TRUE,Univariate|Other,Shewhart|Other,Both,Manufacturing (general)|Food/agriculture|Theoretical/simulation only,TRUE,TRUE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Detection probability|False alarm rate|Other,"Not discussed (they use Phase I data to estimate VAR(1) parameters; examples include a bivariate time series of size 186 and subgroup sizes n ∈ {2,5,7,10,15} with an illustrative example using n = 5).",TRUE,R|SAS,Not provided,https://doi.org/10.1002/qre.2260,"The paper studies how within-subgroup autocorrelation affects the performance of the Shewhart-RZ control chart, which monitors the ratio of two (approximately) normal variables using probability (quantile) control limits. Autocorrelation between successive observations of the bivariate measurements is modeled via a VAR(1) process, and the authors derive the variance–covariance structure of subgroup means under VAR(1) to obtain the ratio-distribution parameters needed for control-limit calculation. Control limits are set using the inverse CDF of the normal-ratio distribution (using the Celano–Castagliola approximation), and out-of-control performance is evaluated via the geometric run-length model when subgroup ratio statistics are assumed independent across sampling epochs. Extensive numerical results (tables/figures) show that increasing autocorrelation parameters (Φ11, Φ22) inflates ARL1 and EARL (worse detection), i.e., autocorrelation reduces sensitivity to shifts in the mean ratio and/or correlation. An illustrative application is given for a food-mixture ratio-monitoring problem (simulated autocorrelated data), and a VAR(1) fitting example is shown using a real furnace pressure dataset with VAR estimation performed in R (MTS) and compared to SAS output.","The monitored statistic is the subgroup ratio $\bar Z_i = \bar X_i/\bar Y_i = \frac{\sum_{j=1}^n X_{i,j}}{\sum_{j=1}^n Y_{i,j}}$. Within-subgroup autocorrelation is modeled by VAR(1): $W_{i,j}=\mu_W+\Phi(W_{i,j-1}-\mu_W)+\varepsilon_{i,j}$, which induces a closed-form variance for subgroup means and hence CVs $\gamma_{\bar X},\gamma_{\bar Y}$ and correlation $\bar\rho$. Because the ratio of normals has no moments, probability control limits are used: $\mathrm{LCL}=F_Z^{-1}(\alpha/2\mid \gamma_{\bar X},\gamma_{\bar Y},\bar\omega,\bar\rho)$ and $\mathrm{UCL}=F_Z^{-1}(1-\alpha/2\mid\cdot)$, with $F_Z^{-1}$ approximated using the Celano–Castagliola quadratic form. Assuming independence of successive $\bar Z_i$ across sampling times, the run length is geometric with $\beta=P(\mathrm{LCL}\le \bar Z_i\le \mathrm{UCL})$ and $\mathrm{ARL}_1=1/(1-\beta)$.","Using ARL0 = 200 (α = 0.005) and simulation/numerical evaluation over n ∈ {2,5,7,10,15}, γX,γY ∈ {0.01,0.2}, ρ0 ∈ {−0.8,−0.4,0,0.4,0.8}, and Φ11,Φ22 ∈ {0.1,0.7}, the paper shows ARL1 increases as autocorrelation increases (worse detection). Example reported: with n = 5, (γX,γY)=(0.01,0.01), ρ0=ρ1=−0.8, τ=0.99, ARL1 = 23.1 for (Φ11,Φ22)=(0.1,0.1) versus ARL1 = 59.7 for (0.7,0.7), both exceeding the no-autocorrelation benchmark ARL1 = 19.1 from prior work. EARL surface plots over Φ11,Φ22 similarly increase with stronger autocorrelation for both decreasing (τ ∈ [0.9,1)) and increasing (τ ∈ (1,1.1]) shift ranges. In the food-mixture illustration with Φ11=Φ22=0.5 and n=5, computed limits are LCL = 0.9723582 and UCL = 1.0284276, and a 2% ratio increase is signaled at samples 14–15 under autocorrelation (later than the independence-based chart, which signals at sample 12 per the authors’ comparison).","The authors assume that the sampling interval is large enough that successive subgroup ratio statistics $\bar Z_i$ are independent even though observations within each subgroup are autocorrelated. They also restrict most of the study to the case of no lag cross-dependence between X and Y (Φ12 = Φ21 = 0) to simplify stationarity and modeling, following earlier SPC literature. They note that Shewhart-RZ is not efficient for small shifts when the coefficients of variation are high (large γX, γY).","The work relies on an approximation to the inverse CDF of the ratio of normals; the accuracy of the resulting control limits and ARL calculations under strong autocorrelation and/or larger γ values is not thoroughly validated against exact/numerical inversion. Treating $\bar Z_i$ as independent across time may be unrealistic in many high-frequency settings; if $\bar Z_i$ is autocorrelated, the geometric run-length model and ARL formulas will be biased. Phase I estimation uncertainty for VAR(1) parameters is discussed only briefly, but its impact on achieved in-control performance (ARL0/false-alarm inflation) is not systematically studied. Comparisons are largely against a no-autocorrelation baseline rather than against alternative autocorrelation-robust SPC strategies (e.g., residual charts, skip-sampling, EWMA/CUSUM with time-series adjustment) in the same study.","They suggest designing more advanced control charts to reduce the negative impact of autocorrelation on detection performance, and investigating autocorrelation effects for the EWMA-RZ chart along similar lines. They also point to Phase I implementation issues for RZ-type charts as an interesting future research topic.","Extend the framework to allow lag cross-dependence (non-diagonal Φ) and assess how cross-lag effects change ratio-chart behavior and design. Develop and evaluate a fully time-series-aware monitoring scheme for $\bar Z_i$ when subgroup ratio statistics remain serially correlated, including Markov-chain/integral-equation ARL computation under dependence. Incorporate Phase I estimation error explicitly (e.g., bootstrap/Bayesian predictive limits) to maintain nominal ARL0 under estimated VAR parameters. Provide open-source software (R package) implementing limit computation, VAR fitting guidance, and ARL/EARL evaluation to improve practical adoption and reproducibility.",2108.05239v1,local_papers/arxiv/2108.05239v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:31:08Z
TRUE,Univariate|Multivariate|Other,Hotelling T-squared|Shewhart|EWMA|Other,Both,Manufacturing (general)|Other,TRUE,TRUE,NA,Exact distribution theory|Simulation study|Case study (real dataset),ARL (Average Run Length)|False alarm rate|Detection probability|Other,"Phase I uses I historical in-control batches; simulation varies I = 30, 50, 100 (and in the real-data study uses I = 10, 30, 100, 200, 300, 500). Batch run length varies from T = 100 to 1000 in simulations (real data uses T = 500).",TRUE,R,Package registry (CRAN/PyPI),https://www.R-project.org/|http://www.timeseriesclassification.com/description.php?Dataset=FordA,"The paper proposes a Phase I/Phase II SPC methodology for batch processes that treats each batch as a time series and monitors changes in process dynamics via ARMA model coefficients. For each in-control batch, ARMA(v,w) parameters are estimated by OLS; across batches, the mean vector and covariance of the coefficient estimates are pooled. The authors derive modified t-Student and modified Hotelling-type (T^2) reference distributions that account for having I replicated time series (batches), yielding control limits based on t and F distributions. In Phase II, a multivariate T^2 chart monitors the vector of ARMA coefficients for each new batch; if signaled, a set of univariate coefficient charts (t-based) supports fault diagnosis by identifying which coefficients shifted. Monte Carlo studies (various ARMA orders and disturbance magnitudes) show the coefficient-based T^2 chart generally outperforms residual-mean charts (including an EWMA residual approach) for detecting dynamic changes; a real application to engine-noise batches (FordA) demonstrates good detection power and calibration improving with more Phase I batches.","Fit ARMA(v,w): $x_t=\phi_0+\sum_{j=1}^v \phi_j x_{t-j}+\varepsilon_t+\sum_{k=1}^w \theta_k\varepsilon_{t-k}$. For Phase I, compute per-batch OLS estimates $\hat\beta_i$ and pooled estimates $\bar\beta=\frac1I\sum_{i=1}^I\hat\beta_i$, $S_{\hat\beta}=\frac1{I-1}\sum_{i=1}^I(\hat\beta_i-\bar\beta)(\hat\beta_i-\bar\beta)'$. Phase II monitoring uses $T^2_{\beta}=(\hat\beta_i-\bar\beta)'S_{\hat\beta}^{-1}(\hat\beta_i-\bar\beta)\sim \frac{(I-1)(I+1)p}{I(I-p)p}F_{p,I-p}$ and coefficient diagnostics use $t_\beta=(\hat\beta^*_{i}-\bar\beta^*)/S_{\hat\beta^*}\sim \sqrt{\frac{I+1}{I}}\,t_{I-1}$ (normal-data case yields exact distributions).","In simulations (ARMA models up to ARMA(2,2), with Phase I I typically 30–100 and T from 100–1000, 1000 replications), the proposed $T^2_{\beta}$ chart achieves markedly smaller out-of-control ARLs than a residual-mean chart (with EWMA) for disturbances in AR/MA coefficients (dynamic changes), while maintaining in-control ARL near the nominal value (e.g., ARL0≈100 at α=0.01). For mean/intercept shifts, the residual-based chart performs best as expected, but $T^2_{\beta}$ remains competitive when I and T are large. In the FordA engine-noise application (reference group size 1755, monitoring group 1846, T=500), an AR(12) was fit and the chart built on the first three significant AR coefficients; across 200 replications, Phase II detection rates were high (e.g., r1≈0.76 at α=0.10 with I=10 increasing to ≈0.89 with I≥100; at α=0.01 r1≈0.31 with I=10 increasing to ≈0.70 with I=500). Empirical false-alarm rates approached nominal α as I increased (e.g., around 0.10/0.05/0.01 for larger I).",None stated.,"The approach is presented primarily for one variable at a time; extending to multiple variables requires additional modeling (e.g., VAR) and may not capture cross-variable dynamics. It relies on (approximate or exact-under-normality) normality of coefficient estimators and on successful ARMA model specification/order selection per batch; misspecification or non-Gaussian noise could affect control limit validity. The method assumes aligned batch lengths and regular sampling; missing/unequally spaced time points are not addressed, which is common in industrial batch data. Practical guidance for ARMA order selection and robustness to changing within-batch variance (heteroskedasticity) is limited.",None stated.,"Develop a multivariate extension that jointly monitors several correlated batch variables (e.g., VAR/VARMA coefficient charts) while preserving diagnostic interpretability. Study robustness and/or propose nonparametric or bootstrap-calibrated limits for non-normal, heavy-tailed, or heteroskedastic innovations and for model misspecification. Add methodology for irregularly sampled batches and missing data (state-space/Kalman-filter based estimation with corresponding monitoring limits). Provide software and workflow guidance for automated order selection, Phase I screening of atypical batches, and real-time implementation, ideally validated on additional industrial case studies.",2109.00952v1,local_papers/arxiv/2109.00952v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:31:39Z
TRUE,Univariate|Other,CUSUM|EWMA|Other,Phase II,Theoretical/simulation only,TRUE,FALSE,NA,Simulation study|Markov chain|Integral equation|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Conditional expected delay|Steady-state ARL|Expected detection delay|Other,Not discussed,TRUE,R|Other,Not provided,https://cran.r-project.org/web/packages/spc/index.html,"This paper critiques a recent literature on “compound” or “memory-based” process monitoring schemes that modify standard SPC charts (especially EWMA and CUSUM) via recursive constructions (e.g., DEWMA/TEWMA), moving-average nesting (DMA/TMA/QMA), progressive mean variants (PM/DPM), and mixed/hybrid charts (e.g., EWMA input to CUSUM) and run-rule augmentations. Using a common analytical framework for an i.i.d. normal mean-shift model with known variance, the authors argue many proposed methods are ad hoc and unjustified because they employ unreasonable weighting patterns that can overweight older data. They emphasize that performance claims in the literature are often based on zero-state run length only, whereas steady-state behavior and conditional expected delay (CED) are more realistic; under these criteria, the added complexity generally yields no benefit versus well-designed conventional EWMA/CUSUM competitors. The paper re-evaluates representative methods (mixed EWMA–CUSUM, run-rule EWMA/CUSUM, DMA, DEWMA/TEWMA, and DPM) and shows that fairer competitor tuning (e.g., matching weighting/variance) eliminates the perceived advantages of compound charts. The practical recommendation is to avoid these complex compound designs and instead use simpler, properly tuned conventional charts, focusing on shifts of practical importance rather than statistical detectability of arbitrarily small changes.","The benchmark EWMA uses $Z_0=\mu_0$, $Z_i=(1-\lambda)Z_{i-1}+\lambda X_i$ with time-varying exact limits: signal if $|Z_i-\mu_0|>c_E\sqrt{\frac{\lambda}{2-\lambda}(1-(1-\lambda)^{2i})}$. A representative compound example is the mixed EWMA–CUSUM (MEC): compute an EWMA $Q_i=(1-\lambda_q)Q_{i-1}+\lambda_q X_i$ and feed $Q_i$ into two-sided CUSUM recursions $M_i^+=\max\{0,M_{i-1}^+ + Q_i-a_i\}$ and $M_i^-=\max\{0,M_{i-1}^- - Q_i-a_i\}$, signaling when $\max(M_i^+,M_i^-)>b^*\sigma_{Q,i}$ with $\sigma_{Q,i}=\sqrt{\frac{\lambda_q}{2-\lambda_q}(1-(1-\lambda_q)^{2i})}$. The paper also uses conditional expected delay $D_\tau=\mathbb{E}_\tau[(L-\tau+1)\mid L\ge \tau]$ and its steady-state limit $D=\lim_{\tau\to\infty}D_\tau$ as primary performance measures beyond zero-state ARL.","For mixed EWMA–CUSUM charts (e.g., with $\lambda_q\in\{0.1,0.25\}$ and in-control ARL $A=170$), a fairly tuned standard CUSUM competitor (choosing $k=a^*\sigma_{Q,\infty}$ rather than an arbitrarily large $k$ such as 0.5) matches or improves MEC performance: MEC shows at best slight zero-state gains for very small shifts, while CUSUM is uniformly better for medium/large shifts and for steady-state ARL. For DEWMA vs. EWMA at in-control ARL 200, matching asymptotic variances (e.g., DEWMA $\lambda=0.1$ vs. EWMA $\lambda\approx0.05$) yields similar zero-state ARLs but markedly worse CED/steady-state ARL for DEWMA; the steady-state ARL table shows EWMA dominates DEWMA and TEWMA across shifts. For DPM/PM schemes (in-control ARL 200), the paper shows zero-state comparisons can make DPM look strong, but CED grows very large for later change points (suggesting inability to detect delayed changes), while a suitably tuned EWMA can uniformly outperform DPM even in zero-state ARL. Across run-rule EWMA/CUSUM and DMA analyses, the authors demonstrate that either (i) conventional charts with better-tuned parameters outperform the compound versions, or (ii) simpler MA/EWMA alternatives dominate DMA in steady-state behavior, undermining the rationale for these compound designs.","The authors note that they “obviously cannot study the performance of all of these methods” in the rapidly growing compound-chart literature and therefore analyze in detail only five illustrative examples. They also remark that for compound charts “a Markov chain approach is no longer easily derived,” making some standard analytical evaluations harder and motivating simulation-based comparisons.","The empirical re-evaluations are conducted under a simplified i.i.d. normal mean-shift model with known variance, so conclusions may not fully transfer to common industrial complications (autocorrelation, variance shifts, estimation error in Phase I, nonstationarity). Because the paper is primarily a critique, the fairness of “best tuned” competitors can depend on the chosen matching principle (e.g., asymptotic variance matching), and alternative matching criteria (e.g., minimax delay, headstart constraints, or cost) could change some relative rankings. The paper largely focuses on detection performance metrics (ARL/CED) and does not provide a systematic economic design or practitioner-facing guidance for selecting designs under application-specific loss functions.",None stated.,"Extend the critique framework to Phase I/parameter-estimation settings (unknown $\mu_0,\sigma_0$) to assess whether compound charts behave differently when estimation error dominates. Investigate robustness of the conclusions under autocorrelated and non-normal data models (e.g., ARMA errors, heavy tails) using steady-state and worst-case delay metrics. Provide open-source, standardized benchmarking code and a curated set of fair-comparison protocols (matching rules, constraints, and reporting of zero-/steady-state/worst-case) to reduce recurring flaws in future compound-chart proposals.",2110.10680v1,local_papers/arxiv/2110.10680v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:32:17Z
TRUE,Multivariate|High-dimensional|Nonparametric,Other,Both,Manufacturing (general)|Healthcare/medical|Theoretical/simulation only,TRUE,TRUE,FALSE,Simulation study|Case study (real dataset),Detection probability|False alarm rate|Other,"Targets settings with limited Phase I samples, typically with p>m (high-dimensional). The robust subset size is set as h=\lfloor m(1-\gamma)\rfloor to achieve a 100\gamma% breakdown value (0\le\gamma\le0.5); examples include m=100 for p up to 200 in simulations and m=24, p=314 in the VDP case study.",TRUE,R,Not provided,http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/,"The paper develops a robust Phase I methodology for monitoring high-dimensional multivariate processes with individual observations when the number of variables exceeds the Phase I sample size (p>m) and the reference data may be contaminated by outliers. It replaces Hotelling’s T^2 (infeasible when the sample covariance is singular) with a modified Mahalanobis distance that uses only the diagonal covariance (variances), and standardizes it using trace functionals of the correlation matrix to obtain an asymptotically N(0,1) statistic. To improve tail accuracy for small false-alarm levels, it applies a Cornish–Fisher correction requiring consistent high-dimensional estimators of tr(\rho^2) and tr(\rho^3), and proposes a finite-sample correction factor for better Phase I calibration. Robust parameter estimation is achieved via a reweighted minimum diagonal product (RMDP) subset-and-reweighting algorithm designed to resist outliers while estimating the mean, variances, and trace terms. Simulation studies evaluate Phase I swamping/masking behavior and show improved Type I control (especially with Cornish–Fisher) and good detection of mean shifts; two real-data examples (wood board density profiles and breast cancer data) illustrate applicability.","The chart uses a diagonal-covariance modified Mahalanobis distance: $M_i^2=(X_i-\mu)^\top D^{-1}(X_i-\mu)=\sum_{j=1}^p (X_{ij}-\mu_j)^2/\sigma_{jj}$. It is standardized as $U_i=(M_i^2-p)/\sqrt{2\,\mathrm{tr}(\rho^2)}$ (asymptotically $N(0,1)$ for large p under stated eigenvalue/trace conditions) and then Cornish–Fisher adjusted using $\omega_{\alpha,p}\approx z_\alpha+\frac{4\,\mathrm{tr}(\rho^3)(z_\alpha^2-1)}{3[2\,\mathrm{tr}(\rho^2)]^{3/2}}$, yielding a signal rule based on $Z_i$ (Eq. 5). Robust Phase I estimation uses MDP/RMDP subset selection minimizing the product of diagonal variances and consistent high-dimensional estimators such as $\widehat{\mathrm{tr}(\rho^2)}=\mathrm{tr}(R^2)-p^2/m$ and $\widehat{\mathrm{tr}(\rho^3)}=\mathrm{tr}(R^3)-\frac{3p}{m}\mathrm{tr}(R^2)+\frac{2p^3}{m^2}$ (with reweighted analogues).","Monte Carlo experiments (typically 10,000 replications) show the Cornish–Fisher-adjusted statistic $Z_i$ matches the standard normal CDF far better than $U_i$, especially in tails for moderate p. In a calibration comparison (m=200), using Cornish–Fisher within the algorithm yields simulated Type I error close to nominal: e.g., for Scenario 1 with $\alpha=0.01$, p=30 gives 0.010 (with CF) vs 0.022 (without), and for $\alpha=0.05$, p=30 gives 0.052 (with CF) vs 0.071 (without); similar improvements occur under correlated Scenario 2. Power studies under contamination (e.g., Scenario 2 with p=30 or 100 and r=0.1–0.3) show detection probability increases with mean shift size \delta and decreases as contamination rate r increases. In a Phase I comparison to an MCD-based T^2 approach, the proposed RMDP procedure achieves much better Type I control (reported example: near 0.05 vs about 0.25 for MCD at nominal 0.05) and generally higher detection power, particularly for larger p.","The authors note that Assumption 1 (bounded trace growth / limited cumulative correlation) may appear to restrict applications, and that convergence to nominal false-alarm rates can be slower when variables are highly correlated (e.g., strong AR-type correlations). They also acknowledge the Cornish–Fisher adjustment is derived under normality, and suggest additional work is needed for non-normal processes and highly correlated settings.","The proposed chart discards off-diagonal covariance information in the main distance (uses only D), so detection and diagnostic performance may degrade when correlation structure is informative or when shifts occur primarily through changes in dependence rather than mean. The method relies on estimating trace functionals of the correlation matrix; in very small m, these estimators and the finite-sample correction may be unstable and sensitive to pre-processing choices. The robust subset/reweighting algorithm introduces tuning choices (e.g., \gamma/h, \alpha splits) and computational complexity that may affect reproducibility and practicality without published code.","They propose extending the approach to non-normal high-dimensional processes, to highly correlated multivariate processes, and to settings with batch size greater than one. They also point to exploring alternative distributional approximations (e.g., Welch–Satterthwaite type approximations for weighted chi-square sums) to improve behavior under strong correlation.","Useful extensions include developing a self-starting/online version for Phase II deployment that updates robust estimates sequentially, and providing diagnostic tools to identify which variables drive a signal in high dimensions. Robustness studies beyond mean-shift contamination (e.g., variance shifts, heavy tails, dependence across time, missingness/irregular sampling) and broader real industrial benchmarks would strengthen practical guidance. Publishing an R package implementing RMDP with recommended defaults and computational optimizations would improve adoption and facilitate fair comparisons.",2110.13689v2,local_papers/arxiv/2110.13689v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:33:01Z
TRUE,Multivariate|Self-starting|High-dimensional,Shewhart|Hotelling T-squared|Other,Both,Semiconductor/electronics|Theoretical/simulation only,TRUE,TRUE,FALSE,Approximation methods|Simulation study|Case study (real dataset),ARL (Average Run Length)|False alarm rate|Detection probability|Other,"Recommends Phase I sample size about 200–300 for stable in-control performance (ARL0), noting performance improves as more in-control Phase II observations are incorporated in the self-starting updates; diagonal variances can be estimated with as few as 2 observations.",TRUE,R,Not provided,http://archive.ics.uci.edu/ml/datasets/SECOM,"The paper proposes a new Shewhart-type control chart for Phase II monitoring of high-dimensional multivariate processes with individual observations when Phase I sample size is small relative to dimension p, making the sample covariance singular for Hotelling’s T^2. The chart replaces Σ^{-1} with the inverse of the diagonal variance matrix D^{-1}, yielding a modified Mahalanobis distance and a standardized statistic whose null distribution is approximated using a Cornish–Fisher expansion to improve tail accuracy for small α used in SPC. A unified Phase I/Phase II self-starting procedure is developed that uses robust Phase I estimation (via a modified re-weighted minimum diagonal product, RMDP, approach) and then updates parameter estimates sequentially with incoming in-control observations. Performance is evaluated primarily via ARL (in- and out-of-control) under independent and AR(1)-correlated covariance scenarios, with and without Phase I outliers, showing improved adherence to nominal ARL0 and effective detection of mean shifts. A semiconductor manufacturing case study (SECOM) illustrates applicability on real high-dimensional data after preprocessing and marginal normal-score transformation.","The proposed monitoring distance uses only diagonal variances: $M_i^2=(X_i-\mu)^\top D^{-1}(X_i-\mu)=\sum_{j=1}^p (X_{ij}-\mu_j)^2/\sigma_{jj}$. It is standardized as $U_i=(M_i^2-p)/\sqrt{2\,\mathrm{tr}(\rho^2)}$, with a Cornish–Fisher corrected threshold; the signaling rule is $Z_i=U_i-\frac{4\,\mathrm{tr}(\rho^3)(z_\alpha^2-1)}{3[2\,\mathrm{tr}(\rho^2)]^{3/2}}>z_\alpha$. In-control and out-of-control ARLs for this Shewhart-type rule are given as $\mathrm{ARL}_0=1/(1-\alpha)$ and $\mathrm{ARL}_1=1/(1-\beta)$.","Using Monte Carlo (10,000 replications), the Cornish–Fisher (CF) corrected chart produces simulated ARL0 values close to nominal across dimensions (e.g., Scenario 1, $p=100$, $\alpha=0.005$: ARL0≈199.8 with CF vs 109.1 without CF; $\alpha=0.0027$: ARL0≈373.2 with CF vs 173.4 without CF). For mean shifts affecting 20% of variables, CF similarly yields ARL1 closer to nominal than the uncorrected statistic (e.g., Scenario 1, $p=50$, $\alpha=0.005$: simulated ARL1≈16.8 with CF vs 10.1 without CF; nominal 17.4). Correlation sensitivity analysis with AR(1)-type covariance ($\sigma_{ij}=a^{|i-j|}$) shows ARL0 generally remains near nominal even as $a$ increases, with some inflation when $a=0.9$ and small $p$ (e.g., $p=30$, $\alpha=0.01$: ARL0≈124). In comparisons against an RMCD-based robust Hotelling chart (with very large Phase I m for fairness), the proposed RMDP-based diagonal chart yields smaller ARL1, particularly under Phase I contamination rates $r=0.1$ and $0.2$, where RMCD detection degrades as $p$ increases.","The authors note that the asymptotic normal approximation for $U_i$ can fail in the tails for moderate p and small nominal Type I errors, motivating the Cornish–Fisher correction. They also acknowledge that when variables are highly correlated (e.g., large AR correlation parameter a) the in-control ARL0 can deviate upward from nominal in some small-p cases, and suggest alternative approximations/corrections to address this. In the real data example, they note nonconforming items may not be explained by mean shifts alone, which can limit detection by a mean-focused chart.","Using only diagonal variances discards cross-covariance information, so power can suffer for shifts that primarily manifest through correlated directions rather than marginal mean changes. The method relies on multivariate normality (with additional marginal normal-score transformation in the case study), which may not preserve dependence structure or guarantee the assumed asymptotic behavior under complex non-Gaussian dependence. The self-starting update rule is described procedurally, but practical guidance on how to robustly update $\mathrm{tr}(\rho^2)$ and $\mathrm{tr}(\rho^3)$ online in the presence of ongoing contamination is limited. Handling of missing data is not built into the method despite the motivating SECOM dataset having substantial missingness (addressed via preprocessing/cleaning rather than a modeled missing-data approach).","They suggest addressing highly correlated processes by adopting a Welch–Satterthwaite $\chi^2$ approximation for the weighted sum representation of $M_i^2$, and/or developing a finite-sample correction coefficient specifically for highly correlated settings via extensive simulation to better attain nominal ARL0. They also emphasize that high-dimensional process monitoring remains a promising area where substantially more work is needed.","Extend the approach beyond mean shifts to simultaneously monitor variance/covariance changes or distributional shape changes common in high-dimensional sensors. Develop a principled missing-data/irregular-sampling extension (e.g., model-based imputation or likelihood under MAR) so the chart can be applied without ad hoc data cleaning. Provide a fully specified online robust updating scheme (with safeguards against persistent outliers) and study its steady-state ARL properties under parameter estimation. Implement and release reproducible software (e.g., an R package) with recommended defaults for $\alpha$, warm-up/learning time, and robust tuning parameters, and benchmark against additional high-dimensional SPC competitors.",2110.13696v2,local_papers/arxiv/2110.13696v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:33:39Z
TRUE,Multivariate|Other,CUSUM|Other,Both,Transportation/logistics|Other,NA,TRUE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),"Uses Phase I in-control data of size m = 1000 in simulations to estimate the in-control mean vector, covariance matrix, and correlation coefficients; requires m ≫ B (with B ≤ Bmax, and Bmax set to 4 in examples). For the metro case study, Phase I uses the first 11 months of 2017 to estimate parameters and control limits (Bmax = 4).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a robust online SPC method for monitoring serially correlated directed networks represented as time-indexed transition matrices. It transforms the raw transition-count matrix N(t) into a transition probability matrix P(t) to reduce “double” serial correlation and standardize each row (row sums equal 1), then monitors mean shifts in the rows of P(t). A spring-length-based approach is extended to multivariate row vectors to adaptively estimate the effective correlation order (up to a user-specified Bmax) and to decorrelate the row statistics without fitting a parametric time-series model. Using these decorrelated statistics, the authors construct an adaptive two-sided multivariate CUSUM per row and aggregate them with data-driven weights proportional to each node’s total outgoing transitions to form a whole-network weighted CUSUM chart (WSCUSUM). Extensive Monte Carlo simulations (10,000 replications) compare ARL/SDRL against alternatives (top-1 CUSUM variants and a network EWMA), and a metro transportation case study demonstrates fewer false alarms in Phase I back-testing and faster detection in Phase II monitoring.","The directed network is observed as a transition count matrix $N(t)=(n_{ij}^{(t)})$, converted to a transition probability matrix $P(t)=(p_{ij}^{(t)})$ with $p_{ij}^{(t)}=n_{ij}^{(t)}/n_i^{(t)}$ and $\sum_j p_{ij}^{(t)}=1$. For row $i$, a two-sided CUSUM uses a (decorrelated) score $\tilde e_{i,t}$ and updates $\tilde C^+_{i,t}=\max\{0,\tilde C^+_{i,t-1}+\tilde e_{i,t}-k_i\}$ and $\tilde C^-_{i,t}=\min\{0,\tilde C^-_{i,t-1}+\tilde e_{i,t}+k_i\}$ with $k_i=\tfrac12(\mu_{1,i}-\mu_{0,i})^T\Sigma_i^{-1}(\mu_{1,i}-\mu_{0,i})$. The whole-network statistic is a weighted sum $\tilde C_t=\sum_{i=1}^K w_i^{(t)}\tilde C_{i,t}$ where $w_i^{(t)}=n_i^{(t)}/\sum_i n_i^{(t)}$, and it signals when $\tilde C_t>h$.","Performance is evaluated primarily by ARL and SDRL, computed from 10,000 Monte Carlo replications, targeting $\text{ARL}_0\approx 200$. Across multiple scenarios (single and double serial correlation and correlated rows), the proposed WSCUSUM generally yields more reliable behavior (SDRLs smaller than corresponding ARLs) and avoids non-monotone ARL behavior seen in some competing methods under serial dependence. In the metro case study, using control limits (Table 5) with WSCUSUM threshold $h=638.5$, Phase I back-testing (Dec 7–13, 2017) shows conventional methods (e.g., TCUSUM and NEWMA) trigger alerts while WSCUSUM does not; in Phase II monitoring (Nov 11–15, 2019) WSCUSUM and TCUSUM signal at the 4th observation whereas the decorrelated CUSUM variant signals at the 19th and some methods fail to accumulate under persistent shifts. The paper also reports that monitoring the transition probability matrix $P(t)$ is preferable to monitoring the raw transition matrix $N(t)$, as the latter can have much larger SDRLs and unstable signaling.","The authors note that estimating the autocorrelation terms $\gamma(B)$ for $B\le B_{\max}$ requires a large in-control sample size, especially when $B_{\max}$ and the number of nodes are large; they indicate a need for decorrelation methods suitable for small Phase I sample sizes. They also acknowledge that while the method is designed for robustness and small-shift detection, it can be less sensitive than conventional control charts for large shifts, motivating improved designs that better balance robustness and sensitivity under complex serial correlation.","The method appears to require specifying an expected out-of-control mean $\mu_{1,i}$ (or direction of shift) and uses covariance matrices $\Sigma_i$, which may be difficult to elicit/estimate reliably for large networks or evolving baselines; robustness to misspecification of $\mu_1$ is not clearly established. Although the approach is described as having weak assumptions, parts of the construction use mean/covariance structure and Cholesky-based decorrelation; behavior under heavy tails, zero inflation, or strong nonstationarity in $P(t)$ may degrade without robust estimators. The weighted aggregation $\sum_i w_i^{(t)}\tilde C_{i,t}$ may dilute localized changes affecting low-traffic nodes and may complicate diagnosis (which nodes/edges changed) unless additional attribution steps are provided. Computational and memory costs for maintaining lagged vectors up to $B_{\max}$ and estimating cross-covariances per row could become substantial for large $K$, but scaling limits and runtime benchmarks are not reported.","They propose developing a decorrelation method that works when the Phase I in-control sample size $m$ is small, since estimating $\gamma(B)$ becomes demanding as $B_{\max}$ and the number of nodes increase. They also suggest improving the method to better balance robustness and sensitivity, acknowledging that their approach may be less sensitive than conventional charts for large shifts under complex serial correlation.","Extending the framework to handle unknown/unspecified shift directions (e.g., GLR/adaptive CUSUM over multiple alternatives) would reduce reliance on $\mu_1$ and improve usability. Robust/regularized covariance estimation for high-dimensional rows (large $K$) and theory for $K$ growing with time would help scalability to large directed networks. Adding structured diagnostics (e.g., edge- or community-level contributions, multiple-testing style follow-up) could improve interpretability after a signal. Incorporating mechanisms for missing edges/irregular sampling and for nonstationary baselines (e.g., seasonal metro patterns) would make the method more deployable in real streaming settings.",2111.02653v2,local_papers/arxiv/2111.02653v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:34:18Z
TRUE,Univariate|Other,EWMA|Shewhart|CUSUM|Other,Phase II,Theoretical/simulation only,TRUE,FALSE,FALSE,Markov chain|Simulation study|Approximation methods,ARL (Average Run Length)|Conditional expected delay|Steady-state ARL|Other,Not discussed,TRUE,R,Not provided,NA,"This preprint provides a critical review and re-analysis of “synthetic-type” control charts (runs/scan-rule designs requiring two signals within H observations), comparing eight variants (with/without head-start) to classical EWMA and, in an appendix, to CUSUM and Shewhart-EWMA combinations. Assuming i.i.d. normal observations with known and constant variance, it evaluates detection performance for mean shifts using zero-state ARL, conditional expected delay (CED) profiles, and especially conditional steady-state ARL, arguing that zero-state ARL can be misleading for head-start designs. Performance is computed exactly for synthetic-type charts via finite-state Markov chain models (with some explicit formulae for the simplest synthetic chart) and via accurate numerical approximations for EWMA/combination charts. Across a wide range of shifts, EWMA (notably with λ≈0.25) is shown to uniformly dominate the pointwise-optimal synthetic-type #4 charts in steady-state ARL, while synthetic-type charts look favorable mainly under zero-state/early-change scenarios created by head-starts. The paper concludes there is little practical reason to prefer synthetic-type charts over established EWMA/CUSUM (or Shewhart-EWMA combos), and stresses that steady-state (or worst-case) measures should be central in chart evaluation.","EWMA statistic: $Z_0=\mu_0$, $Z_i=(1-\lambda)Z_{i-1}+\lambda X_i$, signaling when $|Z_i-\mu_0|>c_E\sqrt{\frac{1-(1-\lambda)^{2i}}{2-\lambda}\lambda}\,\sigma_0$ (time-varying limits). Mean-shift model: $\mu_t=\mu_0$ for $t<\tau$ and $\mu_t=\mu_1=\delta$ for $t\ge\tau$ with $\sigma=\sigma_0=1$. For Markov-chain chart models with transient matrix $Q$, the ARL vector is $\ell=(I-Q)^{-1}\mathbf{1}$ and steady-state ARLs are computed as $D_i=\psi_i^\top\ell$ using appropriate quasi-stationary/steady-state vectors (conditional vs cyclical).","For in-control ARL calibrated to 500, CED and ARL envelope comparisons show EWMA (especially with $\lambda=0.25$) lies below all synthetic-type head-start charts’ CED profiles for small/moderate shifts (e.g., δ=1), and it uniformly dominates the pointwise-best (#4) synthetic-type envelopes in conditional steady-state ARL over δ∈(0,5]. Synthetic-type charts can appear competitive in zero-state ARL for δ>1 due to head-start effects, but this advantage largely disappears for later change points; steady-state ARL provides a more representative comparison. Adding a Shewhart rule improves large-shift (δ≳3) performance for both synthetic-type and EWMA, but Shewhart-EWMA retains better performance for δ≤2 while matching synthetic-type combos for large shifts. In an appendix worst-case comparison, a standard CUSUM (k=1 tuned for mid-size shifts) is reported as uniformly better than an R4 synthetic-type chart, and a Shewhart-CUSUM combo matches or beats Shewhart-synthetic combos.",None stated,"The conclusions are based on an idealized i.i.d. normal mean-shift model with known, constant variance; robustness to parameter estimation (Phase I), variance shifts, heavy tails, and autocorrelation is not investigated. Although the paper includes extensive ARL/CED comparisons, it does not provide real-data case studies demonstrating practical impacts (e.g., interpretability, diagnostics, implementation constraints). Only selected EWMA settings (notably λ=0.25 and 0.1) are emphasized, and broader tuning strategies or alternative optimality criteria could alter some comparative margins, especially for large shifts where Shewhart dominates.",None stated,"Extend the critique to more realistic settings: unknown parameters with Phase I estimation, autocorrelated processes, and non-normal or contaminated distributions to see whether synthetic-type charts ever gain robustness advantages. Provide benchmarked open code and reproducible experiments (including standard testbeds and real industrial/healthcare datasets) to validate the steady-state vs zero-state message in practice. Explore systematic tuning/optimization under alternative loss functions (e.g., weighted expected delay over random change times) and compare against modern adaptive/robust EWMA/CUSUM variants.",2112.02641v1,local_papers/arxiv/2112.02641v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:34:57Z
TRUE,Univariate|Other,Shewhart|Other,Phase II,Healthcare/medical|Theoretical/simulation only,TRUE,FALSE,NA,Markov chain|Economic design|Exact distribution theory,Other,Sample size is fixed at N = 1 (single observation at each sampling). No Phase I reference-sample size guidance is provided; optimization focuses on sampling interval h and one-sided critical value K.,NA,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=Markovchart,"The paper develops closed-form expressions for a key term in Markov chain-based economic (cost-optimal) control-chart design when process mean shifts follow special mixture distributions. The monitoring setup corresponds to a one-sided X chart with sample size N=1, optimized over the sampling interval h and critical value K, motivated by healthcare/patient monitoring applications with random shifts. The main technical contribution is a closed formula for the expected squared deviation from the target accumulated over a sampling interval, i.e., an integral of E(H(t)^2) that appears in Taguchi-type out-of-control cost calculations. The authors derive results for a specific exponential–geometric mixture shift model (with Poisson shift arrivals) and then provide a general two-component mixture formula assuming only finite means/variances. These formulas enable faster computation of the economic objective in Markov-chain implementations (notably the Markovchart R package) and facilitate cost-optimal parameter selection under more realistic random-shift models.","The core quantity is the interval cost term $C^2_{h,j}=\int_{t_0}^{t_0+h} \mathbb{E}\{(H_{t_0,j}(t))^2\}\,dt$, where $H$ is the cumulative mean-shift process and $j$ is the starting deviation at the sampling time. For the exponential–geometric mixture (with Poisson shift counts), they compute $\mathbb{E}(X+JY+j)^2$ in closed form and substitute it into $C^2_{h,j}=\int_0^h\left[e^{-ts}j^2+\sum_{k\ge1}\frac{(ts)^k e^{-ts}}{k!}\,\mathbb{E}(X+JY+j)^2\right]dt$ to obtain an explicit expression (Eq. (6)). In the general 2-component mixture case, they derive $\mathbb{E}(X+Y)^2$ (Eq. (8)) and then $C^2_{h,0}=\tfrac{1}{6}h^2 s\left(3(m_X^2+v_X+\zeta(m_Y^2+v_Y-m_X^2-v_X))+2h s(m_X-\zeta(m_X-m_Y))^2\right)$ (Eq. (9)).","They provide an explicit closed-form for the accumulated expected squared deviation $C^2_{h,j}$ under a Poisson shift-arrival model when shift sizes are a mixture of exponential and geometric components (including scaling of geometric jumps by $J$), yielding the final closed expression in Eq. (6). They also derive a general closed-form $C^2_{h,0}$ for any two-component mixture where the stacked-shift components have finite means/variances (Eq. (9)), and show it reduces to the earlier pure-exponential result when $\zeta=0$ (Eq. (10)). As a consistency check, they show that if the two components have identical mean and variance ($m_X=m_Y, v_X=v_Y$), the mixing weight $\zeta$ cancels from the formula (Eq. (11)). The paper claims these closed forms speed up computation in the Markovchart implementation used for cost-optimal control-chart design.",None stated.,"The results are tied to a specific economic-model structure (Taguchi-type squared loss) and a Poisson shift-arrival assumption in the worked mixture example; performance under non-Poisson shift arrivals is not analyzed. The note focuses on deriving a cost-term formula rather than providing comprehensive empirical comparisons (e.g., ARL/ATS tradeoffs) against alternative charts or robustness studies for misspecified mixture parameters. It also assumes independence and normal measurement error with known $\sigma$, which may be unrealistic in healthcare monitoring where serial dependence and parameter uncertainty are common.","They state plans to return to implementation details later and mention potential applications to other mixture shift models (e.g., normal and Fréchet mixtures mixing light- and heavy-tailed components).","Extending the derivations to autocorrelated observations or irregular/missing sampling common in healthcare would broaden applicability. A systematic evaluation linking the closed-form cost optimization to traditional detection metrics (e.g., ARL/ATS under mixture shifts) and robustness to misspecification of mixture weights and component parameters would strengthen practical guidance. Providing reproducible code/examples in a public supplement or vignettes demonstrating the speed/accuracy gains would improve adoption.",2112.05940v1,local_papers/arxiv/2112.05940v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:35:25Z
TRUE,Multivariate|High-dimensional|Other,EWMA|GLR (Generalized Likelihood Ratio)|Other,Phase II|Both,Semiconductor/electronics|Manufacturing (general),NA,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Other,Simulation uses sample size N = 100 (grouped counts per sample) with λ = 0.1 and ARL0 = 370. Case study Phase II uses subgroup size N = 4 and sets ARL0 = 500 (control limit 3.889 on log scale).,TRUE,None / Not applicable,Not provided,http://archive.ics.uci.edu/ml/datasets/SECOM,"The paper proposes a global monitoring scheme for a large number of heterogeneous categorical data streams, including nominal streams (probability-vector changes) and ordinal streams (latent continuous location shifts). For each stream it builds a local likelihood-ratio-based statistic and applies an EWMA smoothing update to improve sensitivity to small/moderate shifts. Because streams can have different numbers of categories and different in-control probabilities, the method normalizes each stream’s EWMA LRT using a chi-square CDF transform so that all local statistics are approximately i.i.d. Uniform(0,1) in control. A global charting statistic is then constructed by applying Zhang’s (2002) powerful goodness-of-fit likelihood-ratio test to the ordered Uniform-transformed values, triggering a signal when the statistic exceeds a control limit chosen for a target in-control ARL. Monte Carlo studies (e.g., p=1000 streams) show the proposed global statistic generally outperforms max- and sum-based aggregations, and a SECOM semiconductor manufacturing dataset illustrates Phase II monitoring and signals out-of-control behavior.","For nominal stream i with h_i levels, the grouped-count LRT is $R_{ik}=2\sum_{j=1}^{h_i} n_{ijk}\ln\{n_{ijk}/(N\pi^{(0)}_{ij})\}$. For ordinal streams, the LRT reduces to a 1-df form $R_{ik}=(\alpha_i^\top n_{ik})^2/(N\,\alpha_i^\top\Lambda_i\alpha_i)$ with $\alpha_{ij}$ computed from a latent model (taken as standard normal in the paper). EWMA is applied to counts $w_{ik}=(1-\lambda)w_{i,k-1}+\lambda n_{ik}$ to form $A_{ik}$, then normalized via $U_{ik}=F_{\chi^2_{df(i)}}\{((2-\lambda)/\lambda)A_{ik}\}$ to approximate $U(0,1)$ in control. The global statistic is Zhang’s GOF LRT-based form $T_k=\sum_{i=1}^p [\ln((U_{(i)k}^{-1}-1)/((p-1/2)/(i-3/4)-1))]^2\,\mathbf{1}\{U_{(i)k}\ge (i-3/4)/p\}$, signaling if $T_k>L$.","In simulations with 10,000 replications, N=100, λ=0.1 and ARL0=370, the proposed $T_k$ typically yields smaller out-of-control ARLs than max-based $Q_k=\max_i U_{ik}$ and sum-based $S_k=\sum_i U_{ik}$ across a range of numbers of affected streams and shift magnitudes. For example (Table 1, two-level nominal streams with shift $[0.02,-0.02]^\top$), when a=10 shifted streams, OC ARL is 74.4 for $T_k$ versus 102 for $Q_k$ and 173 for $S_k$. For ordinal streams (Table 3, four levels, δ=0.10, d=10 shifted), OC ARL is 11.1 for $T_k$ versus 12.8 ($Q_k$) and 104 ($S_k$). In the SECOM case study (461 dichotomized streams, Phase II subgroup size N=4), with ARL0 set to 500 and control limit 3.889 on log($T_k$), the chart signals at the 22nd sample and remains above the limit thereafter.","The authors note that while the global statistic is sensitive to various changes, diagnosing which specific streams are out-of-control and identifying root causes remains an open problem. They also assume all categorical data streams shift at the same time, which may be unrealistic in practice.","The approach relies on approximate in-control chi-square behavior of the EWMA LRT and the resulting Uniform(0,1) transform; accuracy may degrade with small N, rare categories, or strong heterogeneity in probabilities. The method assumes mutual independence across streams for the GOF aggregation; cross-stream dependence can inflate false alarms unless explicitly modeled/adjusted. For ordinal streams, the latent-distribution choice (normal vs logistic/other) can affect power and calibration, and robustness is only lightly explored. No implementation details/software are provided, which may hinder adoption and reproducibility.",They propose developing methods to diagnose out-of-control streams and perform root-cause identification after a global signal. They also suggest relaxing the assumption of simultaneous shifts and studying efficient monitoring for multiple change-points across many categorical streams.,"Develop dependence-robust versions of the Uniform normalization/GOF step (e.g., using copula modeling, permutation/bootstrapped limits, or false-discovery-rate control across streams). Extend the framework to handle serial dependence within streams explicitly (categorical time series/Markov models) while retaining scalable computation. Provide Phase I estimation guidance (e.g., required historical sample sizes for stable π^(0) and thresholds for ordinal categorization) and study the impact of parameter estimation error on ARL0. Release reference software (R/Python) and add more real industrial case studies with heterogeneous numbers of levels and missingness patterns.",2112.09077v1,local_papers/arxiv/2112.09077v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:36:01Z
TRUE,Univariate|Other,EWMA,Phase II,Manufacturing (general)|Food/agriculture|Theoretical/simulation only,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length),"Designed to handle small subgroup sample sizes; simulations consider n = 5, 10, 15, 20. Real-data example uses subgroups of size n = 50 (30 samples of 50 cans each; authors treat IC monitoring time T = 24 and OC monitoring time as 30).",TRUE,R,Public repository (GitHub/GitLab),https://github.com/lchen723/SPC-ME-R-code.git,"The paper proposes a measurement-error (misclassification) corrected EWMA p-control chart for monitoring the proportion of nonconforming items when binary outcomes are recorded with classification errors. The method introduces an error-eliminated (corrected) proportion and corresponding corrected EWMA statistic, leading to asymmetric control limits (with LCL set to 0 and an adjusted UCL). The approach targets Phase II monitoring where the in-control proportion is assumed known, and it is intended to work well even with small subgroup sizes. Control-limit coefficients are calibrated to achieve a nominal in-control ARL (e.g., ARL0 = 370) via Monte Carlo simulation, and out-of-control ARL (ARL1) is then estimated similarly. Simulations show the naive (uncorrected) EWMA p-chart can have substantially worse detection (larger ARL1), while the corrected chart closely matches performance attainable with the unobserved true binary status; a case study on orange-juice can defect data illustrates the procedure under sensitivity analysis for misclassification rates.","The corrected (error-eliminated) in-control proportion is defined by inverting the misclassification relationship: $p_0^{**}=(p_0^*-\pi_{10})/(1-\pi_{10}-\pi_{01})$, equivalently $X_{it}^{**}=(X_{it}^*-\pi_{10})/(1-\pi_{10}-\pi_{01})$ and $\hat p_{0,t}^{**}=\frac1n\sum_i X_{it}^{**}$. The corrected EWMA statistic is $\mathrm{EWMA}_{0,t}^{**}=\lambda\hat p_{0,t}^{**}+(1-\lambda)\mathrm{EWMA}_{0,t-1}^{**}$. Using $\mathrm{Var}(\mathrm{EWMA}_{0,t}^{**})=\frac{p_0^*(1-p_0^*)\lambda(1-(1-\lambda)^{2t})}{n(1-\pi_{10}-\pi_{01})^2(2-\lambda)}$, the upper limit is $\mathrm{UCL}^{**}=p_0^{**}+L^{**}\sqrt{\mathrm{Var}(\mathrm{EWMA}_{0,t}^{**})}$ with $\mathrm{LCL}^{**}=0$.","Across simulated settings (e.g., $n\in\{5,10,15,20\}$, $p_0\in[0.05,0.5]$, $\lambda\in\{0.05,0.2\}$, misclassification $\pi\in\{0.95,0.99\}$ with $\pi_{00}=\pi_{11}=\pi$), the naive EWMA p-chart produced larger (wider) UCLs and consistently larger ARL1 than both the corrected chart and the ideal chart using the true (unobserved) $X$. The corrected chart’s UCL values match those from the true-$X$ chart in the reported tables, indicating effective recovery of the control limits under misclassification. Example: with $\lambda=0.05$, $\pi=0.95$, $n=10$, $p_0=0.10$, the UCL is 0.135 for the true and corrected charts, while the naive chart’s UCL* is 0.180; corresponding ARL1 values are close for corrected vs true but inflated for naive. In the orange-juice case study (observed $p_0^*=0.111$), sensitivity analysis yields corrected $p_0^{**}=0.068$ (\pi=0.95) and 0.103 (\pi=0.99); the naive chart gives wider limits (e.g., for $\lambda=0.05$, UCL*=0.126 vs UCL**=0.080 at \pi=0.95), and the corrected chart detected an OC point that the naive chart missed under $\lambda=0.05$.","A key limitation noted is that the misclassification matrix $\Pi$ is typically unknown because it involves the unobserved true status $X_{it}$. When auxiliary information is unavailable, the paper relies on sensitivity analysis by specifying plausible misclassification levels (e.g., setting $\pi_{00}=\pi_{11}=\pi$). The authors note that $\Pi$ can be estimated if external validation data are available, via a 2×2 confusion table estimator.","The proposed control-limit calibration relies on Monte Carlo simulation and a user-specified search range for $L^{**}$, which may be computationally heavy for routine use and sensitive to simulation settings (M, T) and stopping rules. The paper focuses on Phase II with known in-control $p_0$; performance when $p_0$ must be estimated (common in practice) and the impact of estimation error on ARL are not developed. The misclassification model used in experiments is symmetric (often $\pi_{00}=\pi_{11}$ and $\pi_{10}=\pi_{01}$), which may not reflect real inspection systems where false positives/negatives differ or vary over time. The chart sets LCL to 0 and emphasizes upward shifts; it is less clear how well the approach generalizes to two-sided monitoring or to time-varying misclassification rates.","The authors suggest exploring alternative measurement-error correction strategies such as SIMEX for SPC settings. They also propose extending the corrected EWMA p-chart idea to distribution-free SPC methods that transform continuous variables into binary indicators, where measurement error would contaminate the translated binaries. Another proposed direction is extending the correction approach to profile monitoring, e.g., logistic regression profiles for conforming/nonconforming outcomes with misclassification.","Develop a Phase I/Phase II integrated procedure that jointly estimates $p_0$ and misclassification parameters (or propagates their uncertainty) and studies resulting ARL properties. Extend the method to asymmetric (non-symmetric) misclassification with separate false-positive/false-negative rates and potentially time-varying $\Pi_t$, including robustness diagnostics for misclassification misspecification. Provide analytic/approximate ARL calculations (e.g., Markov chain or integral-equation methods) to reduce reliance on Monte Carlo calibration and enable faster design. Package the method as an R package with functions for design (choose $\lambda,L$) and for sensitivity-analysis reporting to facilitate practitioner adoption.",2203.03384v1,local_papers/arxiv/2203.03384v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:36:44Z
TRUE,Multivariate|Other,MEWMA|Other,Phase II,Manufacturing (general)|Theoretical/simulation only,TRUE,FALSE,NA,Markov chain|Simulation study|Other,ATS (Average Time to Signal)|ARL (Average Run Length)|Other,Not discussed (examples/design scenarios use n = 1 and p = 3 for performance comparisons; general framework allows sample size n).,TRUE,None / Not applicable,Not provided,NA,"The paper develops a Phase II variable-sampling-interval (VSI) multivariate EWMA (MEWMA) control chart for monitoring compositional data, using an isometric log-ratio (ilr) transformation to map compositions from the simplex to an unconstrained Euclidean space. The proposed VSI-MEWMA-CoDa chart monitors shifts in the compositional center (mean) via a MEWMA statistic computed on the ilr-transformed sample means, with a warning limit that switches between long and short sampling intervals. Chart performance is evaluated primarily using Average Time to Signal (ATS), computed via a Markov-chain approximation (including a modified two-dimensional chain for out-of-control ATS). The authors also propose an optimization/near-optimization procedure to select the smoothing parameter, control limit, and long sampling interval under constraints on in-control ATS and average sampling interval. Numerical comparisons (for p=3, n=1, ATS0=200) show the VSI-MEWMA-CoDa chart consistently achieves smaller out-of-control ATS than the fixed-sampling-interval MEWMA-CoDa chart, with greater gains for small-to-moderate shifts and for smaller short-interval choices (e.g., hS=0.1).","Compositional observations are transformed by $X^* = \mathrm{ilr}(X)=\mathrm{clr}(X)B^\top$ and monitoring is performed on the ilr sample mean $\bar X_i^*$. The MEWMA recursion is $W_i=r(\bar X_i^* - \mu_0^*)+(1-r)W_{i-1}$ and the plotted statistic is $Q_i=W_i^\top \Sigma_{W_i}^{-1} W_i$ with $\Sigma_{W_i}=\tfrac{r}{n(2-r)}\Sigma^*$. A signal occurs when $Q_i>H$; VSI operation uses an additional warning limit to switch between $h_L$ and $h_S$, and ATS is computed via Markov-chain formulas $\mathrm{ATS}=s^\top (I-P)^{-1}h$ (with 1D in-control and a modified 2D out-of-control construction).","In the reported scenario (p=3, n=1, ATS0=200, E0(h)=1), the proposed VSI-MEWMA-CoDa uniformly improves detection speed versus the FSI MEWMA-CoDa in terms of out-of-control ATS. For example, at shift size $\delta=0.25$, FSI ARL1 is 64.6 while VSI ATS1 is 56.8 for $h_S=0.1$ (and 63.5 for $h_S=0.5$). At $\delta=0.50$, VSI ATS1 is 19.9 for $h_S=0.1$ versus 26.4 for FSI (and 23.5 for $h_S=0.5$). Improvements persist for larger shifts (e.g., $\delta=2.0$: FSI 3.5 vs VSI 2.4 for $h_S=0.1$), though gains diminish as shift size increases.",None stated.,"The performance study is largely restricted to a narrow design setting (notably p=3 and n=1 in the main comparison table), so it is unclear how sensitive the gains are to higher dimensions, different covariance structures, or subgroup sizes. The approach assumes multivariate normality after ilr transformation and i.i.d. sampling; robustness to non-normality, autocorrelation, and model misspecification is not assessed. Implementation details (software/code) are not provided, which can hinder reproducibility, especially for the Markov-chain ATS calculations and optimization routine.","The authors suggest extending the approach to a VSI MCUSUM-CoDa chart, studying the effect of measurement error on the proposed (and related) charts, and further investigating alternative transformations that map compositional data to (approximately) normal data before charting. They also note that online monitoring of compositional data in real-life applications is a worthwhile direction given the wide applicability of CoDa.","Validate the VSI-MEWMA-CoDa on real compositional datasets (e.g., chemical, food, or economic compositions) and benchmark against additional modern competitors (robust/nonparametric and Bayesian CoDa monitoring). Extend the design to handle autocorrelated/irregularly sampled compositional streams and to incorporate parameter estimation uncertainty (Phase I-to-Phase II effects). Provide open-source software (e.g., an R package) implementing ilr basis selection, Markov-chain ATS computation, and the constrained optimization for practical deployment.",2203.15438v1,local_papers/arxiv/2203.15438v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:37:03Z
TRUE,Univariate|Other,Other,Phase II,Manufacturing (general)|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study,False alarm rate|Other,Not discussed,TRUE,C/C++|Other,Public repository (GitHub/GitLab)|Personal website,https://github.com/Khaled-Janada/AC_Charts|https://doi.org/10.5281/zenodo.6512066,"The paper proposes the Angular Control Chart (ACC), a probability-limits time-between-events (TBE) control chart for monitoring reliability of multi-state systems (MSSs) where the system can fail into multiple performance-degraded states. Each state transition is plotted on its own “state line” (vertical coordinate is the transition median time-to-failure, horizontal coordinate is the observed time-to-failure), and monitoring is performed using the angle of the point relative to the origin with angular control limits (ACL/ALCL/AUCL). A standard ACC is given for cases where all state transitions share a distribution family differing only by scale (e.g., exponential), yielding common ACL angles across states; a generalized ACC allows state-specific ACL segments when transitions differ by distribution family and/or shape parameters (e.g., Weibull, lognormal, Fréchet, gamma). The method targets detection of changes in failure behavior (TTF increases indicating improvement and decreases indicating degradation) for each transition and for the overall MSS. Performance and behavior are illustrated via simulated examples (including an Erlang/cumulative-TTF variant analogous to a $t_r$-chart), and an open-source C# implementation (“AC Charts Software”) is provided.","For an observation with time-to-failure $t$ on a state with median $T_C=F^{-1}(1/2)$, the plotted monitoring statistic is the angle $\theta=\arctan(T_C/t)$. Probability limits are defined via quantiles: $T_L=F^{-1}(c/2)$ and $T_U=F^{-1}(1-c/2)$, with angular limits $\theta_L=\arctan(T_C/T_L)=\arctan(\rho(1/2,c/2))$ and $\theta_U=\arctan(T_C/T_U)=\arctan(\rho(1/2,1-c/2))$, where $\rho(a,b)=F^{-1}(a)/F^{-1}(b)$; the center line angle is always $\theta_C=45^\circ$. For the exponential case, $F^{-1}(p)=-\alpha\ln(1-p)$ giving scale-free ACL angles (for $c=0.27\%$): $\theta_L\approx 89.89^\circ$ and $\theta_U\approx 5.99^\circ$.","With the standard choice $c=0.27\%$ (3-sigma equivalent), the exponential ACC has fixed angular control limits $\theta_L\approx 89.89^\circ$ and $\theta_U\approx 5.99^\circ$ independent of the exponential scale parameter, enabling simultaneous monitoring of multiple exponential state transitions on one chart. For Weibull, lognormal, Fréchet, and gamma transitions, the ACL angles depend on the distribution shape parameter (and for gamma are obtained numerically due to lack of a closed-form quantile). Simulation examples (50 TTFs) show the ACC can flag state-specific improvements (points below AUCL / larger TTF) and degradations (points above ALCL / smaller TTF), and can also indicate overall system change via imbalance of points above vs. below the 45° center line (e.g., 33/50 above ACL interpreted as overall improvement). The paper notes that using cumulative TTF over $r$ events (analogous to $t_r$) may reduce sensitivity in the presented example (fewer out-of-control points observed).","The authors state that further investigation is needed on how the choice of $r$ (when monitoring cumulative TTF every $r$ occurrences, analogous to a $t_r$-chart) affects ACC sensitivity, and that this use is restricted to state-transition distributions with a limiting sum distribution. They also identify practical obstacles still under study: clarifying the chronological order of failure events across states and handling potential coincidences where two or more observation points overlap.","The paper largely demonstrates ACC behavior via illustrative simulations rather than reporting standard SPC performance metrics (e.g., in-control/out-of-control ARL or detection delay) across a broad range of shift sizes and scenarios, limiting quantitative comparability to established TBE charts (CUSUM/EWMA/t- and $t_r$-charts). Parameter estimation/uncertainty for the assumed transition distributions (especially shape parameters in the generalized ACC) is not treated in an SPC Phase I framework, so practitioners may face additional false-alarm inflation when parameters are estimated from limited historical data. The method assumes independence within state-transition samples and a fully repairable MSS that resets to the perfect state after each failure; many real MSS settings include dependence, imperfect repair, minor-failure transitions among degraded states, or competing risks that could complicate applicability.",The authors propose studying the effect of the parameter $r$ on ACC sensitivity when monitoring cumulative TTF every $r$ occurrences. They also point to ongoing work on practical issues: clarifying the chronological order of failure events and addressing potential coincidences (overlapping points) on the chart.,"A natural extension is to provide formal performance characterization (in-control/out-of-control ARL, detection delay, and false-alarm control) for ACC under common shift models, including comparisons to EWMA/CUSUM TBE charts for small and moderate shifts. Developing Phase I estimation procedures (and self-starting or robust variants) for multi-state transitions—especially for shape parameters in generalized ACC—would improve real-world usability. Extending ACC to settings with autocorrelated failure times, imperfect repair, or allowed transitions among degraded states (minor failures), and providing diagnostic tools to identify which transition changed and by how much, would broaden applicability and interpretability.",2205.02024v1,local_papers/arxiv/2205.02024v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:37:40Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,2205.10447v2,local_papers/arxiv/2205.10447v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:38:23Z
TRUE,Profile monitoring|Nonparametric,Change-point|Other,Both,Manufacturing (general)|Transportation/logistics|Theoretical/simulation only|Other,FALSE,TRUE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate|Expected detection delay|Other,"Uses Phase I historical in-control profiles of size m; window size w with w  m. Examples include m  {20, 40} in simulations and w ranging up to 40; also a robot dataset with only 18 usable in-control profiles (after cleaning) and small (w,m)  {4,5,6}{11,12,13}. The paper notes FAR decreases with larger n, m, and larger m/w, and with larger Var[f]/2.",TRUE,R,Supplementary material (Journal/Publisher),NA,"The paper proposes an Eigenvector Perturbation (EP) Control Chart for fast, nonparametric profile monitoring when each time point yields a noisy functional profile y_t = f_t(x_t) + 5_t with possibly multivariate predictors. The method computes the sample correlation matrix R of a moving window of w recent profiles and monitors the statistic ||v_1(R) - (1/w)||_2, where v_1(R) is the leading eigenvector; under in-control (IC) profiles the population correlation matrix has compound-symmetry structure with leading eigenvector proportional to , while a mix of IC and out-of-control (OOC) profiles induces a block structure that perturbs the leading eigenvector. To avoid loss of sensitivity when the window becomes entirely OOC, the chart replaces k_1 oldest window profiles with sampled historical IC profiles and uses max_{k_1 in K} of the eigenvector-distance statistic. Control limits are set using a bootstrap/""quantile trick"" fitted to IC monitoring statistics rather than Monte Carlo ARL calibration, enabling extremely large in-control ARL (reported > 10^6) while maintaining near-immediate detection (ARL1  1) in simulations. The paper reports extensive simulations comparing against SVR-, SIM-, wavelet-, and PCA-based profile monitoring methods and a real-data application on a robot-arm failure dataset, showing strong FAR control and sensitivity particularly when n and Var[f]/2 are sufficiently large.","Profiles follow y_{t,i} = f_t(x_{t,i}) + 5_{t,i}. For monitoring at time t, form the ww sample correlation matrix R of profiles y_{t-w+1},...,y_t and compute its leading eigenvector v_1; the charting statistic is S(t)=max_{k_1K} ||v_1(R(k_1)) - (1/w)||_2, where R(k_1) is R after replacing k_1 oldest window profiles with sampled historical IC profiles. Signal when S(t) exceeds an upper control limit U, which is obtained by bootstrapping IC profiles to generate many IC S values and taking an aggressive high quantile of a fitted normal approximation (""quantile trick"").","In multi-predictor simulations, the EP chart achieved ARL1 = 1 across all scenarios while maintaining ARL0 > 510^6 with essentially zero FAR (0 to 0.01 even when the change point was delayed to =10^4). By contrast, competing SIM and SVR methods showed noticeable increases in ARL1 when ARL0 was increased from 200 to 370, and were infeasible or degraded when targeting ARL0 on the order of 10^6. In a univariate-predictor comparison, PCA sometimes had low ARL1 but often failed to signal (ARL1 = ), and the wavelet method showed ARL1 inflation with large , whereas EP maintained workable FAR and detection (reported max ARL1 around ~1213 under very large ARL0 settings in that study). On the robot-arm dataset, EP yielded ARL1=1 in all simulated scenarios with FAR < 0.02 in 4 of 27 scenarios; PCA produced ARL1 averages of 1.46, 1.53, and 2.12 for ARL0 targets 200, 370, and 5,010,603, respectively, with no false alarms observed.","The authors note performance degrades in difficult regimes such as small n, small m, small Var[f]/2, and when the correlation structure changes only slightly (e.g., |Corr[f(x_r),f(x_s)] - Corr[f(x_r),h(x_t)]| is small) so that IC and OOC profiles are nearly indistinguishable via correlations. They also state the method performs poorly under very heavy-tailed errors when the error distribution has fewer than about five moments, consistent with eigenvector-perturbation theory assumptions. They highlight that windowing can cause failure when all w profiles are OOC (mitigated by injecting historical IC profiles), and that correlation-based monitoring can fail when h is essentially proportional to f or differs only by an additive constant.","The control-limit calibration relies on a fitted normal approximation to bootstrapped IC statistics (""quantile trick""), which may be sensitive to tail mis-specification when targeting extreme ARL0 levels (e.g., 10^610^7) and could benefit from explicit tail modeling or validation. The method requires access to reliably in-control historical profiles and assumes the historical set is representative; contamination or drift in Phase I data could inflate false alarms or reduce power. Computational details for very high-frequency streaming settings (memory/latency constraints and eigenvector computation stability) are not fully benchmarked beyond limited runtime comparisons. The approach is fundamentally correlation-structure based, so it may miss OOC changes that do not meaningfully alter cross-profile correlations (e.g., pure level shifts removed by centering or changes orthogonal to correlation patterns).","They propose extending the method to monitor not just correlations and to address the case  = 1 (i.e., when the OOC relationship is effectively the same scaling as IC). They state that achieving similar performance for small n (contrasting with methods needing large m but allowing small n) remains open. They also suggest modifying the EP chart to handle more general autocorrelation structures beyond the equicorrelated noise explored, and relaxing the need for dependent predictors across time points.","Developing robust (non-Gaussian) control-limit calibration for extreme quantiles (e.g., EVT-based tail fitting or calibrated concentration bounds) would better justify ARL0 claims at 10^6+ without relying on a normal fit. Extensions to handle missing/irregularly sampled profiles and to provide diagnostic information (which portion of the profile or which predictors drive the signal) would improve practical usability. A self-starting variant that reduces dependence on large, clean Phase I datasets and adapts online to slow drift could broaden adoption. Finally, benchmarking against additional modern profile-monitoring methods (e.g., functional data depth charts, kernel change-point detectors, or high-dimensional streaming detectors) and releasing a reusable software package would strengthen reproducibility and uptake.",2205.15422v2,local_papers/arxiv/2205.15422v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:39:09Z
TRUE,Multivariate|Profile monitoring|Functional data analysis|High-dimensional|Other,Hotelling T-squared|Other,Both,Transportation/logistics|Manufacturing (general)|Other,TRUE,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"Not discussed (they note small Phase I sample sizes relative to dimensionality can cause overfitting and recommend splitting Phase I into training and tuning sets; in experiments they use Phase I n=4000 with 1000 training/3000 tuning, and in the case study 919 Phase I with 460 training/459 tuning).",TRUE,R,Supplementary material (Journal/Publisher),https://doi.org/10.1080/00401706.2024.2327346,"The paper proposes the Robust Multivariate Functional Control Chart (RoMFCC), a Phase I/Phase II profile-monitoring framework for multivariate functional data that is robust to both casewise and cellwise (componentwise) functional outliers. RoMFCC combines (i) a functional univariate filtering step that flags functional cellwise outliers and replaces them with missing values, (ii) a robust multivariate functional imputation method, (iii) robust multivariate functional PCA via ROBPCA for outlier-resistant dimensionality reduction, and (iv) Phase II monitoring using Hotelling’s $T^2$ on retained scores and a squared prediction error (SPE) chart on the residual space. An extensive Monte Carlo study evaluates true detection rate (TDR) and false alarm rate (FAR) for mean-shift scenarios under different outlier-contamination mechanisms and levels, showing RoMFCC’s performance is largely insensitive to Phase I contamination and typically best among compared functional and non-functional competitors. A real case study monitors dynamic resistance curves from a resistance spot welding process (automotive body-in-white) and shows RoMFCC yields the highest estimated detection rate versus competing schemes. The work advances SPC for Industry 4.0 settings by explicitly addressing simultaneous casewise and cellwise outliers in high-dimensional multivariate functional monitoring.","RoMFCC’s monitoring uses Hotelling’s statistic $T^2=\sum_{l=1}^{L_{mon}} \xi_l^2/\lambda_l$ computed on robustly estimated multivariate functional PCA scores, and an SPE statistic $\mathrm{SPE}=\|Z-\hat Z\|_H^2$ with $\hat Z=\sum_{l=1}^{L_{mon}} \xi_l\psi_l$. Cellwise functional outliers are flagged by a functional-distance measure $D^{fil}_i=\sum_{l=1}^{L_{fil}}(\hat\xi^{fil}_{il})^2/\hat\lambda^{fil}_l$ and a tail-comparison rule against a $\chi^2_{L_{fil}}$ reference, replacing flagged components by missing values. Missing components are imputed by minimizing a PCA-based quadratic form (Eq. (4)), yielding a closed-form imputation of basis coefficients $\hat c_i^m=-(C_{m,m})^+C_{m,o}c_i^o$ plus stochastic noise $\epsilon_i$ (Eq. (6)) to reduce deterministic-imputation bias.","In simulation (50 runs per setting), RoMFCC achieves higher mean true detection rate (TDR) at comparable false alarm rate (target overall $\alpha=0.05$) than MCC/iterMCC/RoMCC and MFCC/iterMFCC across scenarios with Phase I contamination by functional cellwise or casewise outliers; its advantage increases as contamination severity increases, and its performance is reported as largely insensitive to contamination model/level. In the real resistance-spot-welding case study, RoMFCC flags 72.3% of Phase II observations as out-of-control (reported as $\widehat{\mathrm{TDR}}=0.723$). Competing methods have lower estimated detection rates: MCC 0.336, iterMCC 0.462, RoMCC 0.513, MFCC 0.541, iterMFCC 0.632, while RoMFCC’s bootstrap 95% CI is $[0.695,0.753]$, strictly above competitors’ intervals.",None stated.,"The monitoring step relies on an approximate normality assumption for PCA scores to set $T^2$ limits (chi-squared), which may be violated in heavily contaminated or non-Gaussian functional processes. The approach is a framework with multiple hyperparameters (e.g., $\delta_{fil},\delta_{imp},\delta_{mon}$, number of imputations, basis size/smoothing) and may require substantial tuning and computation for very large streaming settings. Autocorrelation/within-profile temporal dependence and between-item serial dependence are not explicitly modeled in the control-limit calibration, which can affect false-alarm behavior in many real industrial monitoring applications.","The authors note that RoMFDI can be embedded in a multiple-imputation framework and that multiple estimated RoMFPCA models could be combined (e.g., averaging robustly estimated covariance functions) to account for uncertainty from stochastic imputation.","Developing explicit control-limit calibration methods under dependence (serial correlation across items/batches) and under broader non-Gaussian score distributions would strengthen in-control performance guarantees. Providing open-source software (e.g., an R package) and scalable implementations for high-frequency/online updates would improve practical adoption. Extending the framework to diagnose the type/location of functional changes (root-cause analysis after signal) and to handle high-dimensional settings with $p\gg n$ more explicitly (regularized robust covariance/precision estimation) would be valuable.",2207.07978v3,local_papers/arxiv/2207.07978v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:39:45Z
TRUE,Multivariate|Profile monitoring|Functional data analysis|Nonparametric,Hotelling T-squared|Other,Phase II,Transportation/logistics|Environmental monitoring|Theoretical/simulation only,NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,False alarm rate,Phase II focus with a reference (Phase I) dataset assumed clean. Simulation defaults use nI=1000 reference curves plus ntun=1000 in-control tuning curves; Phase II example uses nII=60 (3 groups of 20). Real case study uses 159 reference voyages (VN0519–VN1248).,TRUE,R,Not provided,https://doi.org/10.1080/00224065.2023.2219012|https://CRAN.R-project.org/package=funcharts|https://CRAN.R-project.org/,"The paper introduces the R package funcharts, an off-the-shelf toolkit for statistical process monitoring (SPM) of multivariate functional data (profiles), optionally adjusted for scalar/functional covariates. For monitoring multivariate functional quality characteristics without covariates, it uses multivariate FPCA for dimension reduction and then applies two Phase II charts based on Hotelling’s $T^2$ (for the retained score subspace) and squared prediction error (SPE, for the residual subspace), with limits obtained via empirical quantiles (nonparametric) and Bonferroni family-wise error control. For covariate-adjusted monitoring, it implements scalar-on-function regression control charts (adding a chart on the scalar prediction error with $t$-based limits) and function-on-function regression control charts (monitoring residual functions via $T^2$ and SPE, including a studentized residual option). The package also provides real-time versions of these procedures for profiles observed only up to an intermediate domain point, recalibrating limits on truncated reference data. Functionality is demonstrated through built-in simulation generators and a real case study monitoring Ro-Pax ship CO2-emissions-per-mile profiles adjusted for speed, wind components, and trim.","Functional observations are represented via MFPCA with scores $\xi_{im}$ and reconstruction $\hat X_{ip}(t)=\sum_{m=1}^M \xi_{im}\,\psi_{mp}(t)$. Monitoring uses $T_i^2=\sum_{m=1}^M \xi_{im}^2/\lambda_m$ and $\mathrm{SPE}_i=\sum_{p=1}^P\int_T (X_{ip}(t)-\hat X_{ip}(t))^2dt$. In scalar-on-function regression, the truncated score model is $y_i=\beta_0+\sum_{m=1}^M \xi_{im}b_m+\varepsilon_i$ with a prediction-error chart having limits $\pm t_{n-M-1,1-\alpha/6}\,\hat\sigma\sqrt{1+T^{2*}/(n-1)}$. In function-on-function regression, residual functions $e_i(t)=Y_i(t)-\hat Y_i(t)$ (or studentized residuals) are monitored via $T^2$ and SPE on residual PC scores.","In the package’s simulation examples, OC mean-shift scenarios produce multiple out-of-control signals in the $T^2$ and SPE charts (with stronger shifts yielding more signals), while the IC segment largely remains within limits; one function-on-function example reports an SPE false alarm in the IC block. The ship navigation case study shows a clear shift after an energy efficiency initiative: both Hotelling’s $T^2$ and SPE charts on studentized residuals flag many post-intervention voyages as out of control. The paper reports computational timing of about 90 seconds to reproduce Section 3 simulation outputs and about 25 minutes for the real case study on the described hardware/software setup.","The package ‘focuses on the prospective monitoring of the process, referred to as Phase II,’ and assumes availability of a ‘clean’ in-control reference dataset. It notes that registration/alignment for real-time monitoring is ‘beyond the scope’ of funcharts and is assumed pre-applied in the ship dataset (via percent_miles_hat).","Although limits for $T^2$ and SPE are set via empirical quantiles, the approach still relies on representative IC reference/tuning data; performance under contaminated Phase I data or small reference samples is not thoroughly addressed. The methods largely assume independent profiles over time; there is no explicit modeling of autocorrelation across successive profiles/voyages, which could inflate false alarms. Practical guidance on selecting the number of PCs (beyond variance-explained/PRESS/GCV options) and sensitivity to smoothing/basis choices could affect robustness in real deployments.",None stated.,"Extending the package to include Phase I tools (robust estimation, outlier detection, and reference-data cleaning) and to handle serial dependence across profiles would broaden applicability. Additional non-Gaussian and heavy-tailed residual modeling for regression-based charts (beyond the normal-error scalar case) and more systematic guidance/automation for PC and smoothing-parameter selection could improve robustness. Publishing reproducible scripts and/or vignettes for the ship case study and adding benchmark comparisons against alternative FDA/SPM methods would strengthen empirical validation.",2207.09321v2,local_papers/arxiv/2207.09321v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:40:21Z
FALSE,Other,Other,Both,Healthcare/medical|Other,FALSE,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,Other,"Not discussed (the paper fixes an example design with N=180 total prompts, v=4 desired triggers, and recommends choosing S* not too small; in experiments they use S*=6 as a compromise for parameter estimation stability vs. early triggering under low adherence).",TRUE,R,Public repository (GitHub/GitLab),https://github.com/AI-for-Better-Living/adc-tinnitus,"The paper proposes adaptive data-collection triggering algorithms for ecological momentary assessment (EMA) studies where a burdensome secondary task should be administered a pre-specified number of times when a self-reported variable is at extreme (high/low) values, while accounting for subject adherence (missingness). The approach is inspired by control-chart ideas: at each time point it fits a Beta model to the subject’s historical self-reports using method-of-moments estimates and flags the current report as extreme if it falls outside symmetric Beta quantile thresholds. It then uses a simple design-optimization objective to choose the significance level so that the expected number of triggers matches a desired value v, and extends this to an adaptive version that updates the expected remaining sample size using an online estimate of adherence. The methods are evaluated via simulations and a real tinnitus EMA dataset (TrackYourTinnitus), comparing against random and static-threshold schedules; the adaptive adherence-aware algorithm shows higher F1 and better attainment of the target trigger count (utility) than baselines. The work advances adaptive EMA scheduling rather than proposing a classical SPC chart for industrial process monitoring, despite using control-chart-inspired thresholding.","The control-chart-inspired extreme-value rule triggers at time t (t≥S*) if $x_t<z_{\alpha/2}(\hat\delta_{t-1},\hat\xi_{t-1})$ or $x_t>z_{1-\alpha/2}(\hat\delta_{t-1},\hat\xi_{t-1})$, where $z_q(\cdot)$ is a Beta quantile and $(\hat\delta,\hat\xi)$ are method-of-moments estimates computed from the history $x_{1:t-1}$. Method-of-moments uses $\hat\mu_s=\frac{1}{s}\sum_{r=1}^s x_r$, $\hat\sigma_s^2=\frac{1}{s-1}\sum_{r=1}^s(x_r-\hat\mu_s)^2$, $\nu=\hat\mu(1-\hat\mu)/\hat\sigma^2-1$, and $\hat\delta=\hat\mu\,\nu$, $\hat\xi=(1-\hat\mu)\,\nu$. The significance level is chosen to target v expected triggers: $\alpha^*=v/(N-S^*+1)$ (clipped to [0,1]); with missingness it becomes adaptive via $\hat\chi(t)=\frac{\sum_{i=1}^t a_i}{t}$, $\hat N_0(t)=\hat\chi(t)N$, and $\alpha^*(t)=v/(\hat N_0(t)-S^*+1)$ (with boundary cases).","In both simulated data and real tinnitus data, the adherence-adaptive algorithm (Algorithm 2) is reported to have stochastically larger F1 scores and higher utility than random scheduling, static thresholds, and the non-adherence-adaptive version (Algorithm 1), based on Wilcoxon–Mann–Whitney tests. For simulations, Algorithm 2 vs Algorithm 1 gives p=1.5×10^-3 for F1 and p=3.2×10^-5 for utility; for real data, p=1.2×10^-9 (F1) and p=1.9×10^-13 (utility). The study uses N=180 prompts, desired triggers v=4, starting point S*=6, and adds a stopping rule to cap triggers (e.g., R=10) to avoid excessive burden. The authors also note the real adherence distribution deviates from the Binomial assumption, yet the adaptive method remains beneficial especially under high variability in adherence.","The authors state the algorithms rely on strong statistical assumptions that may not hold in practice: the self-report variable and adherence are modeled as i.i.d. (time effects ignored), the distribution of the self-report variable is assumed known a priori (e.g., Beta), and the approach requires access to distribution quantiles to deploy the triggering rule. They also note that Algorithm 2’s thresholds depend on adherence, creating inconsistency across participants that must be considered in downstream analyses of intra-individual or group differences. They further note that the Binomial adherence model does not fit the real dataset well (some participants interact more than expected).","Although inspired by “control charts,” the method is not evaluated using standard SPC metrics (e.g., in-control/out-of-control ARL/false-alarm calibration), making it hard to compare with SPC literature on sequential monitoring. The method-of-moments Beta parameter estimates can be unstable early on or under bounded/clustered responses; the proposed “add two dummy observations” fix may bias thresholds and is not rigorously justified or analyzed for sensitivity. The ground truth for F1 is defined using the model-based thresholds themselves, which can make performance appear favorable to the proposed modeling assumptions and complicate interpretation as an external validation of detecting “true” extremes. Autocorrelation and nonstationarity (common in EMA) are not modeled, so thresholds may be miscalibrated when symptoms drift or show diurnal patterns.","The authors propose extending the method to time-dependent models for both the quantity of interest and adherence (e.g., autoregressive models) to improve real-world performance. They suggest developing multivariate versions to handle multiple concurrently collected variables, including objective wearable-sensor measures, and exploring hierarchical approaches inspired by anomaly detection. They also propose future work on making the primary (lighter) sampling schedule adaptive—predicting unlikely states and prompting only when needed—to further reduce burden.","A useful extension would be to provide formal calibration guarantees (e.g., controlling expected burden and false-trigger rates under dependence) and to study robustness when the assumed distribution/quantiles are misspecified. Developing nonparametric or semi-parametric thresholding (e.g., empirical quantiles with shrinkage) could reduce reliance on the Beta assumption and improve generality across EMA scales. Providing an explicit treatment of informative missingness (when adherence depends on symptom level) would likely improve triggering logic and reduce bias. Packaging the method as a documented software library with reproducible pipelines and parameter-setting guidance (S*, v, stopping rule R) would facilitate practitioner adoption.",2207.12331v1,local_papers/arxiv/2207.12331v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:40:59Z
TRUE,Multivariate|Nonparametric|Other,Shewhart|Other,Both,Semiconductor/electronics|Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Detection probability|False alarm rate|Other,"Phase I uses per-class reference samples $R_c$ of size $|R|$ (same for all classes), chosen as a subset of correctly classified training embeddings (can be the full training set or selected by highest softmax scores). Example settings include $|R|=100$ (toy example), and $|R|\in\{2000,3000,4000\}$ for CIFAR-10 embeddings ($m_i\in\mathbb{R}^{16}$), $|R|\in\{400,500,600\}$ for TREC embeddings ($m_i\in\mathbb{R}^8$), and $|R|\in\{50,60,70\}$ for sonar embeddings ($m_i\in\mathbb{R}^3$); authors note larger $|R|$ can reduce in-control signals but may reduce out-of-control detection.",TRUE,R|Other,Not provided,https://www.cs.toronto.edu/~kriz/cifar.html|https://cogcomp.seas.upenn.edu/Data/QA/QC/,"The paper proposes an online statistical process monitoring (SPM) method to detect nonstationarity (concept drift/novelty) during deployment of artificial neural networks by monitoring their latent feature representations (embeddings). Because the embedding distribution is generally unknown, it develops a multivariate nonparametric Shewhart-type monitoring scheme using data depth functions and depth-based normalized ranks, yielding Liu’s (1995) $r$ control chart (and also discusses the batch-wise $Q$ chart). Depths considered include robust Halfspace depth, Mahalanobis depth, Projection depth (with multiple approximation algorithms and asymmetric variants), and Simplicial depth; the chart flags out-of-control when the depth-rank falls below a significance threshold $\alpha$. The method is evaluated on a toy example and on three real-data ANN applications (CIFAR-10 CNN image classification with CIFAR-100 OOD classes; TREC question classification with LSTM; sonar signal classification with FNN), and compared to outlier/anomaly benchmarks (LOF, KDEOS, Isolation Forest, Mahalanobis-distance on softmax, NOF). Results show asymmetric Projection depth (notably a Nelder–Mead approximation) is most reliable across settings, while Simplicial depth and symmetric Projection depth can fail when many points lie outside the convex hull or the embedding distribution is asymmetric; misclassification in Phase II is a major driver of high signal rates.","Embeddings $m_i\in\mathbb{R}^k$ are monitored per predicted/true class using a reference set $R_c$. A data depth $D_c(\cdot, R_c)$ (Halfspace/Mahalanobis/Projection/Simplicial) is computed and converted to a normalized rank statistic (Liu’s $r$ chart): $$r_c(m_i)=\frac{|\{t\in R_c: D_c(m_t)\le D_c(m_i)\}|}{|R_c|}\in[0,1].$$ The Shewhart-type decision rule is one-sided with $\mathrm{LCL}=\alpha$: signal if $r_c(m_i)\le \alpha$ (no UCL needed). The batch-wise $Q$ chart uses $$Q_c(m_i)=\frac{1}{n}\sum_{j=1}^n r_c(m_{i_j}),$$ with LCL computed from $\alpha$ and batch size $n$ (closed form when $\alpha\le 1/n!$).","Across toy and real-data experiments with $\alpha=0.05$, depth-based $r$ charts achieve FAR near the nominal level in Phase I, but Phase II in-control signal rates (SR) can be much larger due to misclassification and reference-sample mismatch. In CIFAR-10 (embeddings $\mathbb{R}^{16}$), SR decreases as $|R|$ increases (e.g., for PD2 SR about 0.49 at $|R|=2000$ vs about 0.31 at $|R|=4000$), while CDR also drops (about 0.92 at $|R|=2000$ vs about 0.79 at $|R|=4000$), motivating a trade-off choice of $|R|$. The authors recommend asymmetric Projection depth (Nelder–Mead) as most reliable overall; symmetric Projection depth can fail under asymmetric embedding distributions and Simplicial depth can fail when many observations lie outside the convex hull (leading to many zero depths). Benchmark comparisons show the proposed depth-based approach can outperform LOF/NOF/iForest/KDEOS in the same $r$-chart framework, while KDEOS in particular can show poor out-of-control detection in some settings.","The authors note that high Phase II signal rates can be driven by misclassification and that reliable construction/updating of the Phase I reference sample remains an open challenge (larger reference samples reduce in-control SR but can reduce out-of-control detection). They also highlight computational cost as a limitation: depth computation (especially in higher dimensions and for some depth notions) can be too slow for real-time ANN deployments. They further acknowledge the monitoring performance would improve if additional misclassification information were available, but labels are typically unavailable in deployment.","The method’s effectiveness depends strongly on the stability and representativeness of the embedding distribution; if embeddings drift due to benign covariate shift that does not harm accuracy, the chart may still signal frequently (potentially inflating false alarms). Comparisons are partly constrained by forcing diverse anomaly detectors into the same Shewhart-style thresholding framework, which may not reflect their best operating regimes (e.g., windowed detectors, calibrated scores, or sequential likelihood methods). The approach focuses on one-sided signaling ($r\le\alpha$) and does not provide a systematic diagnostic to localize which embedding dimensions or network layers drive signals, limiting actionable troubleshooting. Autocorrelation/time-dependence and non-i.i.d. streaming behavior are not modeled; this can distort false-alarm properties in many real monitoring streams.","They propose further research on Phase I support using other SPM tools (e.g., multivariate mean-rank chart) and on when/how to update or augment the reference sample over time while avoiding contamination by out-of-control data. They recommend studying whether better train/test splitting that preserves distributional similarity improves both ANN performance and monitoring reliability, and investigating data splitting/compression trade-offs for faster training and representative training subsets. They also suggest developing an additional method to detect misclassification (to combine with monitoring), exploring moving-window variants, and extending the framework to class-imbalanced settings and to semi-supervised/unsupervised learning models.","Develop self-starting or adaptive control limits that maintain a specified in-control ARL under estimated (and evolving) embedding distributions, potentially via online calibration or conformal prediction. Extend the framework to explicitly handle serial dependence (e.g., MEWMA/MCUSUM on depth-ranks, or state-space modeling of rank processes) to control false alarms in temporally correlated streams. Provide principled layer-selection or multi-layer fusion (e.g., monitoring multiple embeddings with a combined statistic) and add diagnostics for root-cause attribution (which layer/feature directions changed). Release reference implementations (e.g., an R/Python package) and benchmark on standardized OOD/drift suites with consistent protocols to improve reproducibility and fairness of comparisons.",2209.07436v2,local_papers/arxiv/2209.07436v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:41:55Z
TRUE,Multivariate|Other,CUSUM|Other,Both,Transportation/logistics|Energy/utilities|Other,NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,ATS (Average Time to Signal)|False alarm rate|Detection probability|Expected detection delay|Other,Training uses k samples of nominal (anomaly-free) data. In experiments they train on 24 h of data and test on a different 24 h of data (sampled at 1 Hz).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an online, data-driven anomaly detection scheme for large Li-ion battery packs using real-time cell voltage and temperature data. For each cell group, mean-based residuals are computed (cell signal minus group mean), standardized using nominal training data, and then processed with PCA to capture multivariate cell-to-cell deviations; the reconstruction error is scalarized via RMSE. A CUSUM control chart is applied to the filtered RMSE (PCA method) to signal anomalies, and a baseline “direct method” applies CUSUM to filtered residual magnitudes without PCA. The approach is validated on experimental data from a battery-electric locomotive pack and on statistically tested synthetic anomalies injected via hybrid experimental/model-based generation. Results show low false-positive rates (<3%) and improved detection compared with direct thresholding, including faster detection and fewer missed anomalies; temperature residuals are especially important for detecting module balancing events that are not visible in voltage residuals. The authors also show voltage-PCA retraining is needed after balancing events because balancing changes the nominal voltage cell-to-cell relationship, whereas temperature PCA remains stable.","Mean-based residual for cell i: $x_i(t)=X_i(t)-\mu_X(t)$ with $\mu_X(t)=\frac{1}{n}\sum_{i=1}^n X_i(t)$. Standardized residual (Z-score) uses training mean and SD: $z_i(t)=\frac{x_i(t)-\mu_{X_r,i}}{\sigma_{X_r}}$. PCA reconstruction in the principal subspace: $\hat{z}(t)=U_rU_r^T z(t)$; anomaly score is based on reconstruction error (RMSE) and then thresholded using one-sided CUSUM: $C^+[t]=\max\{0, C^+[t-1]+(y[t]-\mu_c)-K\}$ (and for the direct method also $C^-$), with $K=4\sigma_c$ and control limit $5\sigma_c$.","On 24 h nominal data across 25 cell groups, average false-positive rates were 1.9% (direct) and 2.9% (PCA). In statistical testing with five families of injected anomalies, the PCA method improved detection time by 56%, reduced false negatives by 42%, and reduced missed anomalies by 60% relative to direct thresholding. For module balancing (all 11 cells experiencing mild ESC), temperature PCA detected events within 13.5 min on average with FNR 2.3%, while voltage PCA was ineffective (99% FNR). The PCA method detected all injected anomalies with voltage deviations ≥4 mV and temperature deviations ≥0.15 °C (zero missed-anomaly rate), and traced anomalous cells with >95% accuracy for deviations >7 mV (voltage) and >0.3 °C (temperature).","The authors note performance depends strongly on sensor quality: their data use highly sensitive sensors (<0.4 mV voltage noise and <0.03 °C temperature noise), and less sensitive/noisier systems would require higher thresholds to control false positives, reducing sensitivity to small deviations. They also note voltage PCA must be retrained after balancing events because balancing changes the nominal voltage relationships.","CUSUM parameters (filter cutoffs and thresholds such as 4.9 mHz/8.4 mHz and 4$\sigma_c$/5$\sigma_c$) are manually tuned, so performance may be sensitive to tuning choices and operating regimes. The method assumes groups of cells behave similarly; if environmental gradients, load sharing differences, or widespread common-cause shifts affect all cells similarly within a group, detection can degrade (especially when anomalies are identical across all cells). Autocorrelation and nonstationarity beyond mean-based residualization are not modeled explicitly, so in-control false-alarm properties (e.g., ARL) are not characterized in the classic SPC sense. No publicly available code or detailed computational reproducibility artifacts are provided, and data access is restricted, limiting independent validation.",None stated.,"Develop principled design/tuning rules (e.g., target in-control ARL/false-alarm rate under autocorrelation) for the CUSUM and filtering choices, and evaluate robustness across operating profiles. Extend to handle missing/irregular sensor streams and explicitly model serial dependence (e.g., ARMA/residual whitening or state-space SPC). Provide self-starting or adaptive retraining strategies (especially around balancing and aging) with automatic change detection for when retraining is needed, and release implementation code (or a package) to support deployment and benchmarking.",2210.15773v2,local_papers/arxiv/2210.15773v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:42:44Z
FALSE,Univariate|Other,Shewhart|Other,Phase II,Finance/economics,NA,FALSE,FALSE,Case study (real dataset)|Other,Other,Not discussed (uses a rolling window of 3 months ≈ 63 trading days to compute connectedness measures).,TRUE,R,Not provided,NA,"The paper constructs dynamic networks of stock returns for 28 OMX30 companies using two connectedness/dissimilarity measures: Pearson correlation coefficient dissimilarity (PCCD) and generalized variance decomposition dissimilarity (GVDD) from a VAR-based framework. For each rolling 3‑month window, the authors build graphs, identify the network center via eccentricity (minimizing the maximum shortest-path distance to others), and create hierarchical clustering trees (single linkage) to summarize market structure. They then compute a one-dimensional time series of distances between consecutive clustering trees using an information-theoretic generalized Robinson–Foulds/“clustering information distance” (implemented via the R package TreeDist) to quantify structural change over time. A Shewhart control-chart idea is used heuristically by comparing tree-distance series to its mean and a 5×SD threshold to flag abnormal jumps, with many large jumps occurring around early 2020 (COVID-19 period). Overall, Investor is most frequently identified as the network center under both PCCD and GVDD, while GVDD yields larger and more volatile tree-distance changes than PCCD.","Key constructions include: (i) PCCD dissimilarity $h^{t,\mathrm{PCCD}}_{ij}=\sqrt{2(1-\rho^t_{ij})}$ where $\rho^t_{ij}$ is the rolling-window Pearson correlation; (ii) GVDD similarity from VAR forecast-error variance decomposition $\hat h^t_{ij}$ (Diebold–Yılmaz) and its conversion to dissimilarity $h^{t,\mathrm{GVDD}}_{ij}=\sqrt{2(1-\hat h^t_{ij})}$. Hierarchical clustering trees are built via single linkage from (symmetrized) dissimilarities, and successive trees are compared using clustering information distance (a generalized Robinson–Foulds information-theoretic metric). The Shewhart-style monitoring is implemented as exceedances over a fixed threshold defined as mean plus $5\times\mathrm{SD}$ of the tree-distance series.","Empirically (Yahoo Finance data, 2017-03-31 to 2022-03-30; 3‑month rolling windows), Investor is the most frequent center of the Swedish market network under both PCCD and GVDD, with Sandvik AB often second. The largest day-to-day change in hierarchical structure (maximum tree-distance jump) occurs on 10/02/2021 for PCCD and 11/03/2020 for GVDD. GVDD-based tree distances are generally larger on average and more volatile than PCCD-based distances. Using a Shewhart-style mean and 5×SD threshold, many exceedances occur after the start of 2020; the authors note 7 out of 20 GVDD exceedances occur in 2020 during the COVID-19 period.",None stated.,"The SPC component is not the methodological focus: the Shewhart chart is used only as a heuristic thresholding device (mean and 5×SD) without control-limit calibration to a specified in-control false-alarm rate/ARL. Because the tree-distance series is explicitly autocorrelated (they fit a VAR(2)), standard Shewhart assumptions of independence are violated and the threshold exceedance rate is not interpretable as a classic control chart. The approach does not address Phase I estimation/robustness (e.g., how the mean/SD threshold is estimated and updated) or sensitivity to window length/VAR order choices; reproducibility is limited because code is not shared.",None stated.,"A natural extension is to design a formally calibrated monitoring scheme for the tree-distance process (e.g., CUSUM/EWMA on tree distances or a control chart with limits accounting for serial dependence) with ARL/ATS guarantees. Another direction is to study robustness to heavy tails and time-varying volatility common in returns (e.g., using robust correlations or GARCH-filtered residuals) and to evaluate sensitivity to rolling-window size and clustering linkage choice. Providing open-source code and a benchmarking study across markets/crisis periods would improve practical adoption and validation.",2210.16679v1,local_papers/arxiv/2210.16679v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:43:10Z
TRUE,Univariate,Shewhart,Both,Manufacturing (general)|Service industry|Healthcare/medical|Theoretical/simulation only|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Detection probability|False alarm rate|Other,"Phase I examples use m=15 subgroups with unequal subgroup sizes under five plans (e.g., Plan-1: five subgroups each of sizes 3, 10, and 17; Plan-5: all subgroups size 10). Phase II monitoring examples use subgroup size n_k=10 in simulations and n_k=5 in the real-data example.",TRUE,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=rQCC,"The paper develops robust Shewhart-type X̄ control charts that accommodate both Phase I data contamination (outliers) and unequal subgroup sample sizes. It proposes optimal pooling estimators for the unknown process mean and standard deviation under normality by deriving best linear unbiased estimators (BLUEs): an inverse-variance weighted BLUE for location and a generalized weighted BLUE for scale, with weights based on finite-sample expectations/variances under the standard normal. Robust versions are built by plugging in high-breakdown-point estimators (median or Hodges–Lehmann for location; MAD or Shamos for scale) along with finite-sample unbiasing factors, then using the resulting pooled BLUEs in standard X̄ chart limits that depend on the Phase II subgroup size n_k. Extensive Monte Carlo studies compare three pooling schemes (A/B/C) and three estimator pairs (mean–SD, median–MAD, HL–Shamos) using ARL/SDRL/percentiles of run length, showing traditional mean–SD charts can be extremely sensitive to even a single contaminated Phase I value, while the proposed robust pooled charts maintain performance. A real piston-ring diameter dataset illustrates that robust methods produce similar nominal limits in clean data but are far less influenced by injected contamination than conventional constructions.","Location pooling BLUE: \(\hat\mu_C=\frac{\sum_{i=1}^m \hat\mu_i/\nu_i^2}{\sum_{i=1}^m 1/\nu_i^2}\), where \(\nu_i^2\) is the variance of the chosen location estimator under \(N(0,1)\). Scale pooling BLUE: \(\hat\sigma_C=\frac{\sum_{i=1}^m (\gamma_i/\tau_i^2)\hat\sigma_i}{\sum_{i=1}^m \gamma_i^2/\tau_i^2}\), with \(\gamma_i=E(\hat\sigma_i)\) and \(\tau_i^2=Var(\hat\sigma_i)\) under \(N(0,1)\) (simplifies to inverse-variance weighting when \(\hat\sigma_i\) is unbiased). X̄ chart limits for Phase II subgroup size \(n_k\): \(\mathrm{UCL}=\hat\mu+g\,\hat\sigma/\sqrt{n_k}\), \(\mathrm{CL}=\hat\mu\), \(\mathrm{LCL}=\hat\mu-g\,\hat\sigma/\sqrt{n_k}\) (often with \(g=3\)).","In Phase II in-control simulations targeting ARL0≈370 with m=15 unequal-size Phase I subgroups, pooling type C (the proposed BLUE pooling) consistently yields ARLs closest to the nominal level compared with pooling A or B across methods. Under a single contaminated Phase I observation (adding δ=100), Method-I (mean/SD) ARLs inflate dramatically (e.g., Plan-1 ARL increases from ~366.8 with no contamination to ~66,089 with contamination for pooling C), demonstrating severe sensitivity. In contrast, robust methods change much less under contamination; for Plan-1 pooling C, Method-II ARL is ~491.4 (no contamination) vs ~540.2 (contaminated) and Method-III is ~382.3 (no contamination) vs ~466.7 (contaminated). A real piston-ring dataset example (Montgomery Table 6.4) shows similar nominal limits across approaches when data are clean, but an influence-style contamination study shows Montgomery/Method-I limits shift substantially with δ while Method-II/III remain comparatively stable.","The authors note the proposed robust X̄ charts rely on a key assumption that the underlying distribution is normal, which limits practical applicability because the true distribution is typically unknown. They also caution that addressing non-normality via transformations can be problematic, citing recent work on pitfalls of nonlinear transformations.","The methodology focuses on Shewhart X̄ charts and does not extend to memory charts (EWMA/CUSUM) where parameter estimation and robustness can behave differently. Although unequal subgroup sizes are handled, the treatment of missingness is indirect (manifesting as unequal n_i) and does not address mechanisms like informative missingness or time-varying sampling schemes. The robust BLUE weights require finite-sample expectations/variances (or empirical tables) for each estimator and sample size, which may be inconvenient for very large ranges of n_i unless comprehensive precomputed tables/software are available.","The authors state ongoing work to study performance of the proposed robust X̄ charts under departures from normality (e.g., skewed distributions). They discuss the possibility of using transformations to achieve approximate normality, while emphasizing the need to investigate non-normal settings carefully.","Extend the robust unequal-n X̄ framework to autocorrelated processes (e.g., via residual charts or time-series modeling) and to adaptive/variable sampling interval schemes. Develop analogous robust pooling/estimation for dispersion charts (S, R) and for multivariate mean monitoring (e.g., Hotelling T²/MEWMA) under unequal subgroup sizes and contamination. Provide a more systematic design/calibration approach for choosing g to achieve nominal ARL0 under estimation error and contamination, potentially with distribution-free or bootstrap calibration.",2212.10731v1,local_papers/arxiv/2212.10731v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:43:46Z
TRUE,Univariate|Other,CUSUM|GLR (Generalized Likelihood Ratio)|Other,Phase II,Healthcare/medical|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Other,Not discussed.,TRUE,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=success,"This R Journal paper introduces the R package success for constructing risk-adjusted control charts tailored to survival (time-to-event) outcomes, motivated by hospital/center performance monitoring where dichotomizing to binary outcomes can delay detection. The package implements (i) a risk-adjusted funnel plot and risk-adjusted Bernoulli CUSUM using fixed follow-up dichotomization, and (ii) continuous-time survival CUSUM charts based on the Cox model: the BK-CUSUM (Biswas–Kalbfleisch) and the CGR-CUSUM (continuous-time generalized rapid response CUSUM). A key contribution is automated parameter and control-limit selection via a simulation-based procedure that chooses limits to control the type I error over a specified time horizon, plus a helper workflow (parameter_assist) and plotting tools including interactive plotly visualizations. CGR-CUSUM avoids prespecifying the out-of-control hazard ratio by using an MLE for the post-change hazard ratio, which can improve robustness to misspecification relative to BK-CUSUM, at the cost of instability early in monitoring and higher computation. The methods are demonstrated on a simulated multi-hospital surgery dataset and on a breast-cancer clinical-trial-derived dataset with multiple centers, illustrating differences in detection timing between discrete-time and continuous-time monitoring.","Bernoulli CUSUM: $S_n=\max(0,S_{n-1}+W_n)$ with $W_n=X_n\log\{p_1(1-p_0)/[p_0(1-p_1)]\}+\log\{(1-p_1)/(1-p_0)\}$ (or in odds-ratio form with $e^\theta$). BK-CUSUM (continuous time, Cox-model-based): $\mathrm{BK}(t)=\max_{0\le s\le t}\{\theta_1 N(s,t)-(e^{\theta_1}-1)\Lambda(s,t)\}$ where $N$ is the failure counting process and $\Lambda$ the cumulative intensity. CGR-CUSUM: $\mathrm{CGR}(t)=\max_{1\le \nu\le n}\{\hat\theta_{\ge\nu}(t)N_{\ge\nu}(t)-[\exp(\hat\theta_{\ge\nu}(t))-1]\Lambda_{\ge\nu}(t)\}$ with $\hat\theta_{\ge\nu}(t)=\max\{0,\log(N_{\ge\nu}(t)/\Lambda_{\ge\nu}(t))\}$.","Control limits are determined by Monte Carlo simulation to control the probability of a false signal (type I error) within a user-specified monitoring horizon (e.g., $\alpha=0.05$ within 1 year), rather than via analytical ARL results for the continuous-time charts. In an example (Bernoulli CUSUM with 30-day follow-up, $\psi=1$ patient/day, $\alpha=0.05$ over 365 days, $\theta=\log 2$), the estimated control limit is reported as $h\approx 5.56$. In the breast-cancer center example, the paper reports different detection times across Bernoulli, BK-, and CGR-CUSUM and notes that detections do not perfectly coincide across methods due to differing hypotheses and outcome usage (fixed follow-up vs continuous-time survival). The authors also report that computing control limits for the breast-cancer data took about 10 minutes on a consumer-grade laptop when simulating 300 in-control centers.","The authors note that analytical run-length/ARL results are largely lacking for continuous-time survival control charts, so control limits in success are obtained via simulation by controlling type I error over a fixed time frame. They also state that the CGR-CUSUM’s MLE for the hazard ratio can be unstable early in monitoring and may be unreliable for low-volume hospitals due to insufficient failure information, and that CGR-CUSUM is computationally more intensive than the other charts.","The framework relies on correct specification of the underlying risk-adjustment models (logistic model for discrete-time outcomes and Cox PH for continuous-time), so model misspecification or non-proportional hazards could materially affect false-alarm and detection performance. The control-limit calibration depends on assumed/estimated patient arrival as a Poisson process with rate $\psi$ and on how covariates are sampled in simulation; real hospital arrivals and case-mix evolution may deviate from these assumptions. The paper primarily calibrates limits via finite-simulation quantiles (controlling a fixed-horizon false-signal probability), but does not provide extensive sensitivity analyses for calibration error under varying censoring mechanisms, delayed entry patterns, or time-varying hazard ratios.",None stated.,"Useful extensions include robustness studies and alternative calibrations under departures from Cox proportional hazards (time-varying effects) and non-Poisson arrivals, plus methods that explicitly handle autocorrelation/seasonality in arrivals or outcomes. Additional work could develop approximate or numerical ARL/ATS theory for the continuous-time charts to reduce reliance on simulation for control-limit selection. Packaging extensions could include built-in guidance for low-volume units (e.g., stabilized early-phase estimation, priors/Bayesian shrinkage for $\theta$, or adaptive caps on $\hat\theta$) and broader real-world validation across multiple clinical registries.",2302.07658v2,local_papers/arxiv/2302.07658v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:44:22Z
TRUE,Univariate|Other,CUSUM|Other,Phase II,Theoretical/simulation only,NA,FALSE,FALSE,Approximation methods|Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a CUSUM-type change-detection/control-chart procedure with observation-adjusted control limits (CUSUM-OAL), where the control limit at time n is a decreasing function of a running (or sliding) average of the log-likelihood ratio statistics. It also discusses an optimal sequential “sum of log-likelihood ratio” (SLR) test and establishes limiting relationships: as a tuning parameter u→∞, certain CUSUM-OAL constructions converge to the optimal SLR-type test and to a combined CUSUM–SLR stopping rule. The authors develop theoretical approximations/estimation formulas for in-control and out-of-control average run lengths (ARL0/ARL1) of the proposed CUSUM-OAL under different post-change regimes (categorized via Kullback–Leibler distance). Extensive Monte Carlo simulations (e.g., normal mean shifts) illustrate substantially smaller out-of-control ARLs for CUSUM-OAL than conventional CUSUM, especially for small shifts, at matched in-control ARL targets (e.g., ≈500 or ≈1000). The work advances SPC/change-detection literature by introducing adaptive, observation-driven control limits for CUSUM and providing both asymptotic ARL approximations and empirical ARL comparisons.","Conventional CUSUM stopping time: $T_C(c)=\min\{n\ge0: \max_{1\le k\le n}\sum_{i=n-k+1}^n Z_i\ge c\}$ with $Z_i=\log\{p_{v_1}(X_i)/p_{v_0}(X_i)\}$. Proposed CUSUM-OAL replaces constant $c$ by an observation-adjusted limit: $T_C(cg)=\min\{n\ge0: \max_{0\le k\le n}\sum_{i=n-k+1}^n Z_i\ge c\,g(\hat Z_n)\}$ (or a sliding-average version $\hat Z_n(ac)$), where $g(\cdot)$ is decreasing. The paper also uses the SLR stopping time $T_{SLR}=\min\{n\ge1: \sum_{j=1}^n Z_j\ge c\}$ and a modified version $T_{SLR}(r)=\min\{n\ge1: \sum_{j=1}^n Z_j\ge n(\mu_0-r)\}$.","In the normal-mean-shift simulation with target in-control ARL0≈1000, conventional CUSUM had ARL1≈439 for a small shift 0.1, whereas CUSUM-OAL (e.g., with large u such as $u=10^2\text{–}10^4$) achieved ARL1 around 7.5–8.2 for shift 0.1 while maintaining ARL0≈1000. For moderate shifts (e.g., 0.5) CUSUM ARL1≈38.9 versus CUSUM-OAL about 1.89–1.92 in the same table. With ARL0≈500 in another simulation across multiple change-points, a CUSUM-OAL configuration (e.g., $TC(cg e_{100})$) yielded much smaller early-detection delays for small shifts (e.g., about 8.06 at τ=1 for v=0.1) compared to CUSUM (about 247.25 at τ=1), albeit with much larger in-control variability (very large SDRL values shown). Theoretical limiting results show $\lim_{u\to\infty}T_C(cg_{u,r})=T^*(r)$ and $\lim_{u\to\infty}T_C(cg e_u)=T_C(c)\wedge T^*(0)$ under stated conditions.","The authors note that while CUSUM-OAL tests can have significantly smaller out-of-control ARLs than conventional CUSUM, they have larger standard deviations (run-length variability) in the in-control state. They also state that the post-change distribution is usually unknown in practice and motivate assuming only a plausible parameter region and a distribution over it for ARL estimation.","The procedure requires selecting the decreasing function $g(\cdot)$ and tuning parameters (e.g., u, a, ac), but practical guidance for choosing these for real applications appears limited, and performance may be sensitive to these choices. Much of the development assumes i.i.d. observations and likelihood-ratio structure; robustness to autocorrelation, model misspecification, or heavy tails is not established. The empirical evaluation is primarily simulation-based (normal mean shifts) and does not clearly document implementation details (e.g., software, reproducibility), making it harder to assess computational stability or replicate results.",None stated.,"Develop data-driven or optimal design rules for selecting $g(\cdot)$ and tuning parameters to balance ARL1 improvements against increased in-control variability (SDRL). Extend the method to autocorrelated processes, nonparametric settings, and other change types (variance/scale, distribution shape) and provide robust versions under model misspecification. Provide open-source software (e.g., an R/Python implementation) and more real-data case studies to support adoption and reproducibility.",2303.04628v1,local_papers/arxiv/2303.04628v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:45:00Z
TRUE,Univariate|Other,EWMA|Other,Both,Semiconductor/electronics|Theoretical/simulation only,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|Conditional expected delay,"Not discussed (data example uses Phase I with T0=50 observations and Phase II with 150 observations, but no general recommendation is given).",TRUE,R,Not provided,NA,"The paper proposes two new classes of generalized EWMA control charts for monitoring Poisson count data based on the Stein–Chen identity, aiming to improve sensitivity to distributional changes beyond pure mean shifts. The first is an “AB-EWMA” chart that estimates the Poisson mean via a ratio of Stein–Chen moment terms; the second is an “ABC-EWMA” chart that normalizes this ratio by an additional EWMA of the mean, making it particularly sensitive to changes in distribution family even when the mean does not change. Chart design uses symmetric two-sided control limits calibrated to a target in-control zero-state ARL (ARL0≈370), and performance is evaluated via Monte Carlo simulations under Poisson in-control and overdispersed out-of-control scenarios (negative binomial and zero-inflated Poisson), including early and late change behavior via conditional expected delay. Simulations show the ordinary Poisson EWMA is best for sole mean shifts, while ABC-EWMA charts (with suitable weight functions) provide substantially faster detection under distributional changes/overdispersion. A semiconductor manufacturing particle-count dataset illustrates that the Stein–Chen-based charts detect model misspecification/overdispersion markedly earlier than the ordinary EWMA chart.","Ordinary Poisson EWMA: $Z_0=\mu_0$, $Z_t=\lambda X_t+(1-\lambda)Z_{t-1}$. Stein–Chen identity for $X\sim\text{Poi}(\mu)$: $\mathbb{E}[X f(X)]=\mu\,\mathbb{E}[f(X+1)]$. AB-EWMA constructs EWMAs $A_t$ of $X_t f(X_t)$ and $B_t$ of $f(X_t+1)$ and charts $Z_t^{AB}=A_t/B_t$; ABC-EWMA adds an EWMA $C_t$ of $X_t$ and charts $Z_t^{ABC}=A_t/(B_t C_t)$ (with $Z_0^{ABC}=1$).","With symmetric limits tuned to in-control ARL0≈370 (using $\lambda=0.10$), simulations (10^4 replications per scenario) show AB-EWMA yields only limited improvements over the ordinary Poisson EWMA under overdispersion, whereas ABC-EWMA often produces much lower out-of-control ARLs across negative-binomial and zero-inflated alternatives (including pure distribution-family changes). The paper reports that discrepancies between zero-state ARL and late-change performance (e.g., CED(100)) are generally small, indicating robustness to the change-point location. In a semiconductor particle-count case study (Phase I T0=50; Poisson model estimated with mean 1.48), the first alarm occurs at t=31 for the ordinary EWMA, at t=11 for AB-EWMA, and as early as t=7 for ABC-EWMA with $f(x)=|x-1|^{1/4}$ or $f(x)=\ln x$. The application suggests strong overdispersion and substantial zero inflation (reported zero frequency about 48%), consistent with ABC-EWMA’s quicker signaling.","The simulation study is limited to a small set of in-control means (µ0=2,5), a fixed smoothing parameter (λ=0.10), and a few out-of-control scenarios focused on overdispersion (negative binomial and zero-inflated Poisson) with a single dispersion setting; more comprehensive analyses are recommended. The paper assumes serial independence and explicitly notes that autocorrelated counts/time series settings need future investigation. It also notes that the impact of Phase I parameter estimation on performance is not analyzed and should be studied in future work.","Control limits are chosen symmetrically for simplicity; this can yield biased two-sided ARL profiles (as noted in related Poisson EWMA literature) and may not be optimal for specific directional alternatives. The proposed methods depend on selecting a weight function $f$; guidance is based on limited scenarios (NB vs ZIP), and performance could be sensitive to misspecification of the expected alternative and to the truncation used for moment calculations. Computational details for setting limits and implementing the charts are not provided as reusable software/code, which may hinder reproducibility and practitioner uptake.","The author suggests broader performance studies including additional out-of-control scenarios such as underdispersion and non-Poisson equidispersion, and exploring more weight functions. They propose adapting the Stein-type (ABC-)EWMA approach to other count distributions using available Stein identities, and extending the idea to continuously distributed variables using continuous Stein identities. They also propose studying the effect of Phase I parameter estimation on chart performance and extending the framework to autocorrelated/time-series count data.","Develop principled, data-driven or adaptive selection of the weight function $f$ (or ensembles of $f$) to reduce reliance on prior knowledge about the overdispersion pattern. Provide analytical or numerical methods (e.g., Markov chain/integral-equation approximations) to compute ARL/limits more efficiently and accurately than brute-force simulation, especially for practitioners. Extend to self-starting implementations and robust Phase I procedures for estimating in-control parameters and limits under model uncertainty, and release an open-source implementation (e.g., an R package) with documented limit-calibration routines and examples.",2305.19006v1,local_papers/arxiv/2305.19006v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:45:43Z
FALSE,Other,Shewhart,Both,Healthcare/medical|Theoretical/simulation only|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset),Other,Not discussed,TRUE,R,Not provided,NA,"The paper proposes SpcShrink, a wavelet-domain denoising method that selects level-dependent thresholds using an iterative procedure inspired by SPC control charts. For each wavelet scale, coefficients are treated as observations under a normal-noise null hypothesis and control limits (LCL/UCL) of the form ±d_j s_j are iteratively recomputed after removing coefficients that exceed the limits, until all remaining coefficients lie within the limits; the final threshold is λ_j = d_j s_j. The method uses a scale-varying significance level α_j (linearly increasing with j via α_j = j·α_1), translating to control-limit distances d_j = \sqrt{2}\,\mathrm{erfc}^{-1}(\alpha_j). Performance is evaluated via Monte Carlo simulations on standard test signals (Blocks, Bumps, Doppler) under additive white Gaussian noise and via two biomedical case studies (inductance plethysmography and ECG), showing improved SNR/RMSE versus VisuShrink, SureShrink, BayesShrink, and S-median in many settings. Although motivated by SPC, the work is primarily a signal-processing/wavelet-thresholding contribution rather than development/evaluation of control charts for process monitoring.","Observations follow y = x + n with n \sim \mathcal{N}(0,\sigma^2 I), and wavelet coefficients satisfy w = Wy = c + z with z \sim \mathcal{N}(0,\sigma^2). For scale j, control limits are LCL = -d_j s_j, CL = 0, UCL = d_j s_j, where s_j is the sample standard deviation of \{w_{j,k}\} and d_j = \sqrt{2}\,\mathrm{erfc}^{-1}(\alpha_j). The iterative algorithm removes coefficients outside [LCL,UCL] and recomputes s_j and limits until convergence; the threshold is \lambda_j = d_j s_j with \alpha_j = j\alpha_1.","An optimization over \alpha_1 in {0.1%,0.2%,...,5%} using Monte Carlo experiments suggests an average optimal value \alpha_1 \approx 1.5% (with reported optima 1.1%, 1.4%, 1.7%, 1.8% across input SNR levels 5, 15, 25, 35 dB), giving example distances d_1=2.432, d_2=2.170, d_3=2.005, d_4=1.881, d_5=1.780 for J_0=5. Simulation studies (e.g., 93,000 signals from 3 prototypes × 31 noise levels × 1,000 replications) show SpcShrink(1.5%) or SpcShrink(1.0%) often attains higher output SNR and comparable or lower RMSE than VisuShrink, SureShrink, BayesShrink, and S-median, with exceptions for Doppler at very low input SNR (<10 dB). Reported SNR gains can exceed 12 dB when input SNR is below 10 dB. Biomedical examples (IPD and ECG) show good visual preservation with relatively few iterations (roughly 40–56 for IPD; ~51–52 for ECG, depending on \alpha_1).",None stated,"The approach relies on the transformed-noise normality assumption (AWGN and orthonormal DWT); performance/validity under non-Gaussian noise, heavy tails, or correlated noise is not established. The ‘SPC’ connection is largely conceptual (using three-sigma-style limits and iterative removal) rather than a full SPC monitoring framework with run-length/false-alarm guarantees, so classical SPC properties (ARL/ATS, steady-state behavior) are not evaluated. Comparisons focus on a small set of wavelet-threshold baselines and limited real datasets; broader benchmarking (more signal classes, other modern shrinkage/denoisers) and sensitivity to wavelet choice or decomposition settings could affect conclusions.",None stated,"Evaluate robustness and adapt the method for non-Gaussian and/or autocorrelated noise (e.g., colored noise, impulsive artifacts) and for non-orthonormal transforms. Derive or approximate false-detection rates for coefficient selection induced by the iterative removal procedure, potentially linking to formal multiple-testing or FDR control across coefficients/scales. Provide an open-source implementation (e.g., an R package) and broader empirical benchmarks, including images/2D signals and additional biomedical datasets, to assess generalizability and practical parameter defaults.",2307.10509v1,local_papers/arxiv/2307.10509v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:46:15Z
FALSE,Multivariate|Other,Other,NA,Other,NA,NA,NA,Case study (real dataset)|Other,NA,Not discussed,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/rostyhn/MolSieve,"The paper presents MolSieve, a progressive visual analytics system for exploring and comparing very long molecular dynamics (MD) simulation trajectories, especially ParSplice-generated ensembles. MolSieve simplifies trajectories into super-states and transition regions using GPCCA (via pyGPCCA) and then progressively computes analyst-defined properties through Python scripts to guide interactive exploration. For transition regions, MolSieve uses small-multiple “control charts” based on moving averages with mean ±1 standard deviation bands and also supports dynamically constructed multivariate control charts to highlight synchronized changes across multiple properties. The system adds specialized views (timeline, transition-region focus+context, state-space aggregation charts) and comparison widgets to help analysts find, inspect, and compare regions/selections across trajectories. The approach is demonstrated via real case studies on platinum nanoparticle simulations (millions of transitions) and a tungsten defect simulation, with qualitative feedback from domain experts emphasizing large productivity gains and noting needs for further customization.","Key charting computations are described procedurally rather than as formal equations: for each transition region and property, the system plots a moving average over an adjustable window (default window length is one tenth of the transition-region length) and colors segments based on deviation from the region mean using control limits at mean ± 1 standard deviation. The trajectory simplification uses GPCCA cluster-membership probabilities per state; states with maximum membership probability above a user-set threshold (default 0.75) are labeled super-states, otherwise labeled transition-region states.","The evaluation is primarily qualitative via two domain case studies rather than reporting SPC metrics such as ARL/ATS. MolSieve successfully helped an expert identify and verify structural transitions in a 750K platinum nanoparticle trajectory (≈18.5 million transitions, ≈25k unique states) and find a similar transition in an 800K trajectory (≈13.3 million transitions, ≈53k unique states) using region-similarity tooling. In a tungsten defect analysis (≈866 transitions), adjusting the simplification threshold to 1.0 and reducing the moving-average window exposed diffusive transitions amid repetitive “fluttering” behavior. Performance/throughput is summarized in a table (e.g., cached load times on the order of seconds to tens of seconds for the nanoparticle datasets) but no run-length/false-alarm operating characteristics are provided.","The authors note that the GPCCA-based simplification can sometimes yield too many regions, cluttering the screen and requiring zooming; they suggest rendering only larger regions until the zoom level is appropriate. They also report issues with color encodings (state-ID colors and clustering colors can be hard to distinguish/overlap) and the inability to list the most frequent states within a super-state. They further relay domain-expert feedback that the simplification approach may not fit all MD analysis modalities, and that the distance metric for comparing states/selections was not effective for vacancy-focused analyses, motivating more customization.","Because the work is a visual analytics system, the “control charts” are used as exploratory anomaly-highlighting tools without calibrating false-alarm behavior (e.g., in-control ARL) or detection performance under defined shift models, so practitioners cannot interpret the colored excursions as statistically controlled signals in the SPC sense. The control-limit choice of mean ± 1 standard deviation and the moving-average window are heuristic and may be sensitive to nonstationarity and dependence in MD trajectories; no robustness analysis is provided. Comparisons to alternative change-detection/segmentation methods (e.g., CUSUM/EWMA/GLR or Bayesian change-point methods) are not quantitatively benchmarked, limiting claims about detection effectiveness beyond the case studies.","They plan to address limitations including cramped visual encoding space and the need for more customization, including better region rendering to reduce clutter, improved state/color encodings, and adding a widget to show frequently occurring states in super-states. They propose additional support for biological simulations to broaden applicability. They also plan to switch rendering from SVG to WebGL for scalability and to improve the 3D rendering pipeline; additional planned features include recalling expert selections, a direct state comparison view, and better 3D rendering support.","A valuable extension would be to formalize the “control chart” layer as a statistically calibrated detection procedure (e.g., set control limits to meet a target false-alarm rate/ARL under a defined in-control model) and evaluate detection delay under controlled perturbations of MD-derived properties. Incorporating methods that account for serial dependence (e.g., residual charts, state-space/AR models, or sequential change-point detectors) could reduce spurious signals driven by autocorrelation. Providing a library of interchangeable detectors (EWMA/CUSUM/GLR/change-point) and guidance for window/parameter selection would improve interpretability and reproducibility, alongside releasing an evaluation benchmark suite and code for the study pipelines.",2308.11724v2,local_papers/arxiv/2308.11724v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:46:49Z
TRUE,Univariate|Other,Shewhart|Machine learning-based|Other,Both,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,NA,TRUE,FALSE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Conditional expected delay|Other,Not discussed.,TRUE,None / Not applicable,Not provided,https://arxiv.org/abs/2309.01978,"The paper proposes a predictive monitoring control-chart framework for time-series data with time-varying variability, using bootstrapped LSTM models to produce one-step-ahead predictions and quantify model (epistemic) uncertainty, plus a separate ANN to estimate time-varying observation noise (heteroscedasticity). A Shewhart-type chart is then constructed using dynamic prediction intervals whose width combines the estimated model variance and the estimated data-noise variance, enabling monitoring when variability changes over time. The method targets anomaly/mean-shift detection in autocorrelated, heteroscedastic processes and is trained without external labels by using the next time step as the training target. Simulation on AR(1)-GARCH(1,1) data shows the proposed chart generally improves detection performance versus NN-based residual chart benchmarks and ablated variants, particularly under strong autocorrelation. Two real sensor case studies (escalator vibration and escalator energy consumption) demonstrate practical abnormality detection, including behavior changes after maintenance and unexpected shutdowns.","The core forecast is an ensemble (bootstrap) LSTM predictor: $\hat f(\vec x_i)=\frac{1}{b}\sum_{j=1}^b f_j(\vec x_i)$, with model-uncertainty variance estimated by $\hat\sigma^2_{\hat f}=\frac{1}{b-1}\sum_{j=1}^b (f_j(\vec x_i)-\hat f(\vec x_i))^2$. A second ANN estimates time-varying noise variance $\hat\sigma^2_{\epsilon,i}=F(\vec x_i)$ by minimizing $C=\tfrac12\sum_{i=1}^n\left(\frac{r_i^2}{\sigma^2_{\epsilon,i}}+\ln\sigma^2_{\epsilon,i}\right)$. Control limits are dynamic prediction-interval limits: $\mathrm{UCL}_{i+w}=\hat f(\vec x_i)+ z\, s(\vec x_i)$ and $\mathrm{LCL}_{i+w}=\hat f(\vec x_i)- z\, s(\vec x_i)$ with $s(\vec x_i)=\sqrt{\hat\sigma^2_{\epsilon,i}+\hat\sigma^2_{\hat f}}$.","In simulation (AR(1)-GARCH(1,1), $T=500$, change at $\tau=401$; tuned to FAP $\approx 0.02$), the proposed LSTM-PI chart achieved FAPs of 0.0185 (\u03d5=0.1), 0.0186 (\u03d5=0.5), and 0.0174 (\u03d5=0.9). Across mean shifts $\delta\in\{0.25,\dots,2.0\}$, it typically had the highest detection rate (DR) and smaller conditional expected delay (CED) than ablated variants and an RNN-residual-chart benchmark, with the biggest advantages under strong autocorrelation (\u03d5=0.9). For example, at \u03d5=0.9 and $\delta=1.5$, DR was 1.00 (proposed) vs 0.94 (ablated B) and 0.98 (RNN), and CED was 5.08 (proposed) vs 16.67 (ablated B) and 10.58 (RNN). In case studies, the vibration application triggered 29 alarms with the first on Sep 7 (linked to maintenance on Sep 6), and the energy application produced 13 signals including alarms corresponding to two unexpected shutdowns (Nov 15 and Nov 21).",None stated.,"The chart limits use a normal-quantile $z$ even though the underlying residuals/prediction errors from neural nets in heteroscedastic, autocorrelated settings may be non-Gaussian, so the nominal false-alarm level may not transfer across regimes without recalibration. Phase I requires substantial model-training choices (window length, bootstrap size $b$, ANN/LSTM architectures) and the paper reports limited guidance on how to set these robustly across applications. The method is presented for a univariate stream; multivariate extension is nontrivial because joint prediction intervals and cross-sensor dependence would need to be modeled/controlled.",The authors state that the model can be developed/extended to monitor multivariate data in future work.,"Develop distribution-free or calibrated conformal prediction intervals to replace the normal-quantile assumption and provide guaranteed in-control coverage under weaker conditions. Add explicit handling of concept drift and periodic/seasonal regime switching (e.g., adaptive retraining or state-space switching) so Phase II performance remains stable over long deployments. Provide open-source software and benchmarking on standard SPC datasets to improve reproducibility and facilitate practitioner adoption.",2309.01978v1,local_papers/arxiv/2309.01978v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:47:23Z
TRUE,Univariate|Nonparametric|Other,Shewhart|Other,Both,Healthcare/medical|Theoretical/simulation only,FALSE,FALSE,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate,Phase I uses k reference subgroups of size n (or m) each: BHC procedure uses k samples of size n (Step-1); simulations use k = 20 with subgroup sizes m = 25 or 40. SHC description uses k subgroups of size m with total n = m×k.,TRUE,R,Not provided,NA,"The paper proposes two Phase I/Phase II monitoring schemes (control charts) for monitoring a specified quantile (p-th percentile) of the generalized Weibull (GW) lifetime distribution when data are hybrid censored. It develops (i) a bootstrap-based hybrid-censored chart (BHC) whose control limits are empirical quantiles of bootstrapped quantile estimates, and (ii) a Shewhart-type hybrid-censored chart (SHC) that uses asymptotic normality of the MLE-based quantile estimator to form symmetric z-based limits. Parameters of the GW distribution under hybrid censoring are estimated via maximum likelihood using an EM algorithm, and the observed Fisher information is derived via the missing-information principle to obtain standard errors for SHC. Performance is evaluated primarily by in-control and out-of-control Average Run Length (ARL) (and SDRL) across censoring plans, quantiles p, subgroup sizes, and false-alarm rates, showing effective detection for parameter shifts. A healthcare case study (bladder cancer remission times) illustrates the charts and compares hybrid-censoring charts to special-case Type I and Type II censoring charts, with hybrid-censoring charts signaling more frequently and earlier under the illustrated shift.","The GW p-th quantile is $\xi_p = F^{-1}(p;\theta,\alpha)=\left[\ln\left(\frac{1}{1-p^{1/\alpha}}\right)\right]^{1/\theta}$ (with scale set to 1). BHC computes $\hat\xi_{p}^{*}=F^{-1}(p;\hat\theta^{*},\hat\alpha^{*})$ from parametric bootstrap samples generated from $F(x\mid\hat\theta,\hat\alpha)$, and sets LCL/UCL as empirical quantiles at $\nu/2$ and $1-\nu/2$. SHC uses $\mathrm{UCL}_{SH}=\bar\xi_p(\hat\Theta_m)+z_{1-\nu/2}\,SE_{\xi_p,m}$ and $\mathrm{LCL}_{SH}=\bar\xi_p(\hat\Theta_m)-z_{1-\nu/2}\,SE_{\xi_p,m}$, where $SE_{\xi_p,m}=\sqrt{\frac1m\nabla\xi_p^T(\hat\Theta_n)\,I_n^{-1}(\hat\Theta_n)\,\nabla\xi_p(\hat\Theta_n)}$.","Simulation settings include B = 5,000 bootstrap replications and 5,000 Monte Carlo runs to estimate ARL/SDRL for multiple censoring schemes (e.g., m = 25 or 40; hybrid parameters like r = 15/20/30/35 and x0 = 55/70). In-control ARL values are close to the nominal $1/\nu$ (e.g., for $\nu=0.0027$, ARL0 values are typically near 370 across scenarios in Table 1). Out-of-control ARL decreases sharply for small parameter shifts; for example, with $\Delta\alpha=0$, a 4% decrease (increase) in $\theta$ yields about 77.6% (36.3%) ARL reduction for the median (50th) quantile (as reported in the text). In the healthcare illustration monitoring the 90th percentile with $\nu=0.0027$, the BHC limits are UCL = 10.564 and LCL = 3.742 (CL = 6.524), while SHC limits are UCLSH = 11.864 and LCLSH = 8.802 (CLSH = 10.333); under a 15% decrease in $\theta$, BHC signals at test samples 1 and 2 (six total signals) and SHC signals first at test sample 1 (six total). Compared to Type I and Type II bootstrap charts in the example, the hybrid-censoring charts signal earlier and/or more often under the same shift.",None stated.,"The SHC chart relies on large-sample asymptotic normality of the quantile estimator under hybrid censoring; for small subgroup sizes or heavy censoring, the normal approximation may be inaccurate and limits may have poor in-control calibration. The approach assumes i.i.d. lifetimes within and across subgroups and does not address serial dependence common in reliability monitoring (e.g., temporal drift or batch effects). Only parametric (model-based) bootstrap limits are considered; robustness to model misspecification of the GW family (or to outliers) is not established, and software/code to reproduce the EM/MLE and chart design is not provided.",None stated.,"Develop robust or semi-parametric versions of the quantile charts (e.g., sandwich/robust SEs for SHC or model-uncertainty/model-averaged bootstrap limits) to reduce sensitivity to distributional misspecification. Extend the hybrid-censoring quantile charts to autocorrelated/streaming settings and to adaptive designs where r and/or x0 may vary over time. Provide self-starting implementations and publicly available software to facilitate practitioner adoption and to benchmark against competing censored-data EWMA/CUSUM quantile charts.",2309.11776v1,local_papers/arxiv/2309.11776v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:48:02Z
TRUE,Profile monitoring|Bayesian|Other,Other|Change-point,Phase II,Energy/utilities|Other,NA,FALSE,TRUE,Simulation study|Case study (real dataset),ARL (Average Run Length)|False alarm rate|Other,"Profiles are built on rolling data segments with segment length N_w = 500 (short-term) or 1000 (medium-term), updated with N_u = N_w/2 new observations. A “starting posterior” is initialized using the first 1000 samples covering the full wind-speed range before sequential updating.",TRUE,None / Not applicable,Not provided,https://doi.org/10.5281/zenodo.5946808,"The paper proposes a Bayesian profile-monitoring approach for wind turbine power curves when sequential data segments have incomplete wind-speed coverage. It introduces copula-based variational inference (CVI) combined with monotone I-spline regression to sequentially estimate posterior distributions of power-curve coefficients, using the previous segment’s posterior as the next segment’s prior. For monitoring, it develops a directional control chart whose statistic is derived from a “KL-divergence factor” comparing the current posterior distribution of coefficients against an IEC standard in-control distribution and a shifted (degradation) alternative. An alarm is triggered when the resulting detection statistic exceeds a threshold chosen to control a prespecified false alarm rate (equivalently, an in-control ARL). Experiments on SCADA data from the Penmanshiel wind farm show improved power-curve modeling accuracy and better degradation detection (precision/recall/F1) than several benchmark approaches.","Profiles are modeled as $\mathbf{Y}_t=\mathbf{Z}_t\boldsymbol\beta_t+\mathbf{e}_t$ with a monotone I-spline regression $f(x)=\sum_j \beta_j I_j^{(p)}(x)$ and sequential prior–posterior updating across time segments. Monitoring is based on a KL-divergence factor $\mathrm{KLF}\propto \frac{\mathrm{KL}(\mathrm{LN}(\mu_\beta,\Sigma_\beta)\,\|\,\mathrm{LN}(\mu_0,\Sigma_0))}{\mathrm{KL}(\mathrm{LN}(\mu_\beta,\Sigma_\beta)\,\|\,\mathrm{LN}(\mu_0-\mathbf{d},\Sigma_1))}$, simplified to a detection statistic $\Lambda_t$ (Eq. 27) formed by quadratic terms $(\mu_\beta-\mu)^T\Sigma^{-1}(\mu_\beta-\mu)$ plus log-determinant and trace terms. Signal when $\Lambda_t>h$, with $h$ set to meet a target false alarm rate / in-control ARL.","On Penmanshiel 02/08 SCADA data, CVI-based sequential power-curve modeling achieved the lowest RMSE and MAE versus MGR, GPR, and WCDF across $N_w\in\{500,1000\}$ (e.g., Penmanshiel 02 at $N_w=500$: RMSE 0.0246 vs 0.0301/0.0271/0.0255). For degradation detection, CVI achieved higher F1 than LWZ, GPR-based monitoring, and LLR (e.g., Penmanshiel 08: F1 0.917 at $N_w=500$ and 0.962 at $N_w=1000$). The chart is tuned with degradation magnitude $\mathbf{d}=0.1\mu_0$ (10% efficiency decline) and a threshold (example used $h=1$) to control false alarms; the authors also describe choosing $h$ via Monte Carlo to target an in-control ARL such as 200.",The authors note the work focuses on incomplete power-curve monitoring for a single wind turbine and suggest extending to monitoring multiple turbines simultaneously. They also indicate that monitoring profile characteristics of different components within a turbine would be valuable for facilitating fault diagnosis and understanding turbine operation.,"The monitoring threshold selection is described empirically (Monte Carlo) but without a reported in-control ARL calibration study over realistic nonstationary conditions, which may affect field deployment. The method appears to assume independent observations within segments after preprocessing; SCADA data commonly exhibit autocorrelation and regime changes that can inflate false alarms if not modeled. Implementation details (software, runtime, numerical stability of Newton–Raphson updates, sensitivity to knot placement and priors) are not provided, limiting reproducibility and practical adoption.",The authors propose extending the approach from single-turbine monitoring to multiple wind turbines monitored jointly. They also suggest monitoring profiles of different components within a turbine to support fault diagnosis and improve understanding of wind-turbine operation.,"Developing a multivariate/multi-turbine extension that explicitly models spatial dependence across turbines (e.g., via hierarchical copulas or spatial priors) would better exploit wind-farm structure. Robust variants that handle autocorrelation, seasonal/nonstationary baselines, and alternative missingness mechanisms (irregular sampling, sensor dropout) would improve real-world reliability. Providing an open-source implementation and a principled ARL/false-alarm calibration procedure (including steady-state ARL) would strengthen reproducibility and facilitate practitioner uptake.",2311.02411v1,local_papers/arxiv/2311.02411v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:48:36Z
TRUE,Univariate|Profile monitoring|Functional data analysis|Other,Shewhart|Machine learning-based|Other,Phase II,Transportation/logistics|Theoretical/simulation only|Other,NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|Other,"Phase I (reference) data are split into training/validation/tuning sets. In simulation: 4000 (training), 1000 (validation), 10000 (tuning) in-control samples; plus 20000 out-of-control samples for ARL1 evaluation. In case study Phase I: 1853 voyages split into 740 (training), 186 (validation), 927 (tuning).",TRUE,R,Public repository (GitHub/GitLab),https://github.com/unina-sfere/FNNCC,"The paper proposes the Functional Neural Network Control Chart (FNNCC) for monitoring a scalar quality characteristic when one or more covariates are functional (profiles) and the response–covariate relationship may be nonlinear. An FNN is trained in Phase I to predict the scalar response from functional (and possibly scalar) covariates; Phase II monitoring is performed on the resulting residuals using an empirical-quantile (two-sided) chart with limits estimated from a tuning set. The chart is evaluated via extensive Monte Carlo simulations against a functional regression control chart (FRCC; linear scalar-on-function model) and a baseline Shewhart chart on the raw scalar response, focusing on detection of mean shifts in the scalar response (with and without shifts in the covariate mean function). Results show FNNCC matches FRCC when the true relation is linear, and substantially outperforms FRCC and the baseline when the relation is nonlinear. A railway HVAC case study demonstrates Phase II detection of a known compressor-related fault using residual signals, and the authors provide data/analysis code online.","The method fits an FNN model for $y$ given functional covariates $X_p(t)$ via a functional first hidden layer: $h^{(1)}_{ik}=g\left(\sum_{p=1}^P\int_{\mathcal T}\gamma_{kp}(t)X_{ip}(t)dt+\sum_{j=1}^J w^{(1)}_{kj}z_{ij}+b^{(1)}_k\right)$, with functional weights expanded in a basis $\hat\gamma_{kp}(t)=\sum_{m=1}^{M_p} c_{kpm}\zeta_{kpm}(t)$. The monitored statistic is the residual $e_{new}=y_{new}-\hat y_{new}$, signaling when $e_{new}$ falls outside two-sided limits $\mathrm{LCL},\mathrm{UCL}$ computed as empirical $\alpha/2$ and $1-\alpha/2$ quantiles of tuning-set residuals.","With $ARL_0=20$ ($\alpha=0.05$), simulations over five scenarios (A linear; B–E nonlinear) show FNNCC and FRCC perform similarly in the linear scenario Scenario A, while FNNCC yields much smaller $ARL_1$ than FRCC/SCC in nonlinear scenarios. For example in Scenario C at shift size $\Delta\mu_y=0.5\,s_y^*$, reported $ARL_1$ values are 3.46 (FNNCC) vs 11.83 (FRCC) and 11.89 (SCC). In Scenario B at $\Delta\mu_y=0.5\,s_y^*$, $ARL_1$ is 9.01 (FNNCC) vs 18.06 (FRCC), and at $\Delta\mu_y=1.5\,s_y^*$, 1.02 (FNNCC) vs 1.84 (FRCC). When the covariate mean shifts, both FNNCC and FRCC generally show increased (worse) $ARL_1$; SCC is unaffected.",None stated.,"The proposed chart reduces to an individuals/residual Shewhart-style rule; it does not incorporate memory (e.g., EWMA/CUSUM) that can improve sensitivity to small persistent shifts. The procedure relies on an empirical-quantile calibration from a tuning set, so performance can be sensitive to the representativeness and size of Phase I data and to dataset shift between Phase I and Phase II (especially with evolving covariate distributions). Autocorrelation/serial dependence in residuals (common in time-ordered industrial data) is not explicitly modeled, which can distort false-alarm properties. Only limited guidance is provided for practitioners on how to choose split proportions, hyperparameter grids, and robustness to non-Gaussian/heteroskedastic residuals.",The authors suggest extending the FNNCC to different and more sophisticated monitoring statistics.,"Develop FNNCC variants with memory (e.g., residual EWMA/CUSUM or GLR-style monitoring) to improve detection of small shifts and gradual drifts. Study robustness and recalibration strategies under autocorrelated residuals, concept drift, and covariate distribution changes (including adaptive/online retraining with safeguards). Extend the framework to multivariate responses and multivariate/higher-dimensional functional covariates with scalable regularization and uncertainty quantification for control limits. Provide standardized software utilities (e.g., an R package) and benchmarking across additional real industrial datasets to support adoption.",2311.11050v1,local_papers/arxiv/2311.11050v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:49:08Z
TRUE,Univariate|Other,CUSUM|Change-point|Other,Both,Energy/utilities|Transportation/logistics|Network/cybersecurity|Environmental monitoring|Other,TRUE,TRUE,FALSE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Expected detection delay|Other,"Simulation design uses burn-in 300; model fitting 600 observations; Phase I m = 200; Phase II = 100 (50 in-control + 50 out-of-control). For the real data example, Phase I uses years 2018–2019 (weekly aggregation) and Phase II uses 2020–2022.",TRUE,None / Not applicable,Not provided,https://transparency.entsoe.eu|https://enmin.lrv.lt/en/news/no-more-russian-oil-gas-and-electricity-imports-in-lithuania-from-sunday|https://www.entsoe.eu/news/2022/09/04/transmission-system-operators-of-continental-europe-decide-to-furtherincrease-trade-capacity-with-the-ukraine-moldova-power-system/,"The paper proposes a framework for statistical monitoring of fixed-topology networks where stochastic processes evolve on edges (Temporal Edge Network, TEN), rather than monitoring changes in the graph structure. TEN edge time series are converted to a node-based representation (via an edge-to-vertex/line-graph-type construction or via sampled adjacency structures), enabling modelling with GNARX (generalized network autoregressive models with time-dependent exogenous variables). Monitoring is performed on one-step-ahead forecast residuals from the fitted GNARX model using residual-based Page-type CUSUM statistics applied to centered squared residuals to detect changes in mean and/or variance. A simulation study evaluates detection under parameter shifts affecting autoregressive, neighbor, or exogenous effects and compares different constructions of the derived adjacency matrix (Erd1sRenyi vs SBM). The method is illustrated on European cross-border physical electricity flows (ENTSO-E), using 20182019 for Phase I and 20202022 for Phase II, detecting periods aligned with COVID-19 disruptions and the RussiaUkraine war-related energy crisis.","GNARX model: $x_{i,t}=\sum_{l=1}^p\big(\alpha_{i,l}x_{i,t-l}+\sum_{r=1}^{s_l}\beta_{l,r}\sum_{j\in N^{(r)}(i)}\omega_{i,j}x_{j,t-l}\big)+\sum_{h=1}^H\sum_{q=0}^{q_h}\gamma_{h,q}z_{h,i,t-q}+\varepsilon_{i,t}$. One-step-ahead residuals: $u_{i,t+1}=x_{i,t+1}-\hat x_{i,t+1}$. Residual-based CUSUM uses centered squared residuals: $Q_\iota(m,k)=\sum_{t=m+1}^{m+k}(u_{\iota,t}-\hat b)^2-\frac{k}{m}\sum_{t=1}^m(u_{\iota,t}-\hat b)$ and $D_\iota(m,k)=\max_{0\le a\le k}|Q(m,k)-Q(m,a)|$ with $\mathrm{UCL}=\hat\sigma_\iota\,\zeta_\alpha\,g(m,k,\nu)$ (\nu=0).","In simulations (500 iterations per scenario), the approach shows essentially no signals during the in-control portion and increasing detection intensity after the introduced change, with faster detection for larger parameter shifts. Detection is more challenging for changes in the neighborhood effect parameter $\beta$ (wider uncertainty and longer time to signal) than for changes in $\alpha$ or exogenous effects (e.g., $\gamma_1$). The cumulative change intensity behaves differently depending on how the derived adjacency $Y'$ is constructed (Erd1sRenyi vs SBM), reflecting whether changes affect all flows or only a community. In the ENTSO-E case study (weekly aggregated 20182022), the method flags 27/33/39 country-pairs (depending on aggregation M1/M2/M3) with change points and highlights periods associated with COVID-19 and early 2022 energy-market disruptions; a system-level signal threshold example uses $W=0.2$ (at least 16 of 76 exchanges flagged).","The authors note the framework is only developed for TENs observed at discrete times and that extension to continuous-time monitoring is open. They also state that defining when to use time-varying adjacency matrices $Y'_t$ (instead of a fixed $Y'$) and whether that improves monitoring requires further research. They emphasize that monitoring effectiveness depends on GNARX being a suitable model; for data types like counts, alternative/extended models would be needed.","The monitoring is implemented as many parallel univariate charts (one per flow) plus an ad-hoc aggregation (cumulative change intensity) and threshold $W$, but the paper does not fully address multiple-testing/control of overall false alarm rate at the network level or how to choose $W$ systematically. Phase I parameter estimation uncertainty and its effect on Phase II control limits/significance is not explicitly quantified for the proposed residual-based chart in this network setting. Real ENTSO-E data motivated choosing a period with less missingness, but the monitoring method itself does not incorporate missing-data handling or irregular sampling, which is common in operational network flow data.","They propose extending the framework from discrete to continuous time monitoring. They suggest investigating when to introduce different adjacency matrices across time and whether that benefits performance. They also mention developing a suitable multivariate monitoring tool (e.g., multivariate control chart) to monitor the TEN process jointly rather than via many univariate charts, and extending/altering the modelling when GNARX is not appropriate (e.g., for count data).","A valuable extension would be a principled network-level false-alarm control (e.g., FDR/Family-wise error control across flows, or a multistream chart with guaranteed in-control ARL). Robust/resistant versions could relax Gaussian error assumptions and improve performance under heavy tails and outliers typical in energy flow data. Incorporating missingness/irregular reporting directly into the forecasting-and-monitoring pipeline (state-space GNARX or Bayesian filtering) would improve applicability to operational settings. Finally, releasing reference implementations (e.g., R/Python) and providing guidance for selecting $\alpha$, $\nu$, and the threshold $W$ based on desired in-control ARL/ATS would help practitioners reproduce and deploy the method.",2312.16357v1,local_papers/arxiv/2312.16357v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:49:49Z
TRUE,Univariate,Shewhart,Both,Manufacturing (general)|Healthcare/medical|Network/cybersecurity|Theoretical/simulation only,FALSE,FALSE,NA,Simulation study,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Other,"Phase I sample sizes studied: m ∈ {100, 200, 500, 1000, 2000, 5000}. Authors suggest Phase I samples with at least 1000 observations for reliable estimation; in some cases even >5000 may be needed to match Case K performance.",TRUE,R,Upon request,https://www.R-project.org/,"The paper studies Shewhart-type attributes control charts for zero-inflated Poisson (ZIP) and zero-inflated binomial (ZIB) processes when in-control parameters are unknown and must be estimated from a Phase I sample (Case U). It focuses on unconditional (marginal) run length performance by averaging the conditional geometric run length over the distribution of estimated parameters, contrasting with the conditional perspective used in some prior work. Two estimation methods are compared—maximum likelihood estimation (MLE) and method of moments (MoM)—and Monte Carlo simulation is used to assess in-control and out-of-control ARL and SDRL over a wide range of Phase I sample sizes and parameter settings. Results show large discrepancies between Case K (known parameters) and Case U performance, especially for small Phase I samples and high zero-inflation, with very large SDRLs indicating substantial uncertainty. To improve practical design, the authors propose an ‘adjusted’ chart constant L* (found via simulation) that targets a desired in-control ARL under Case U, yielding ARLs closer to nominal targets and reduced SDRLs relative to using Case K-tuned limits.","For ZIP(ϕ,λ), Shewhart-type integer limits use mean/variance: UCL_ZIP = ⌈λ0(1−ϕ0)+L\sqrt{λ0(1+λ0ϕ0)(1−ϕ0)}⌉ and LCL_ZIP = max(0, ⌊λ0(1−ϕ0)−L\sqrt{λ0(1+λ0ϕ0)(1−ϕ0)}⌋). For ZIB(ϕ,n,p), UCL_ZIB = ⌈np0(1−ϕ0)+L\sqrt{np0(1−p0+np0ϕ0)(1−ϕ0)}⌉ and LCL_ZIB = max(0, ⌊np0(1−ϕ0)−L\sqrt{np0(1−p0+np0ϕ0)(1−ϕ0)}⌋). In Case K the run length N is geometric with parameter 1−β, so ARL = 1/(1−β) and SDRL = \sqrt{β}/(1−β), where β = P(LCL ≤ Y ≤ UCL); in Case U the paper evaluates unconditional ARL/SDRL by Monte Carlo averaging over parameter estimates.","Using 50,000 Monte Carlo runs, the paper finds that IC ARL/SDRL in Case U can differ drastically from Case K when L is chosen to target ARL0≈370.4 under Case K; e.g., for ZIP with (ϕ0,λ0)=(0.8,4), Case K ARL≈234.04 but with m=200 and MLE-based limits ARL≈566.39 and SDRL≈1116.81. Small Phase I samples (m=100–200) can produce extremely large SDRLs (often in the thousands or more), indicating high uncertainty in achieved false-alarm performance. Even with large m (2000–5000), Case U ARL/SDRL may not converge to Case K values due to discreteness and high zero-inflation. The proposed adjusted constant L* (computed per m and parameter setting) brings IC ARL in Case U close to the target (e.g., same ZIP example uses L*=4.02 at m=200 giving ARL≈234.34 and SDRL≈390.11) and tends to yield OOC ARLs closer to Case K than using unadjusted L.","Due to space economy, only selected out-of-control (OOC) results are presented (primarily for the MLE method). The joint distribution of the estimators (e.g., (\hat{ϕ},\hat{λ}) or (\tilde{ϕ},\tilde{λ})) is not known, so unconditional performance measures rely on Monte Carlo simulation rather than analytic results.","The adjusted L* values are obtained via simulation for specific parameter grids, so portability to other parameter settings (or to practitioner data with model mismatch) is unclear and could require re-simulation. The study assumes independent observations (no serial correlation) and does not examine robustness to autocorrelation, which is common in count processes. Real-data case studies are not included, so practical performance under data issues like regime changes, reporting delays, or nonstationary zero inflation is unvalidated.",None stated.,"Develop analytic or approximate methods for unconditional ARL/SDRL (or confidence bounds) to reduce reliance on heavy Monte Carlo simulation for each design setting. Extend the framework to autocorrelated zero-inflated counts (e.g., ZIP/ZIB time series) and to robust or misspecification-resistant designs when the zero-inflated model is only approximate. Provide and validate software (e.g., an R package) to compute L* and related performance measures, and demonstrate the approach on real manufacturing/health/network count datasets.",2401.10605v1,local_papers/arxiv/2401.10605v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:50:24Z
TRUE,Univariate|Other,EWMA|Other,Both,Healthcare/medical|Theoretical/simulation only|Other,FALSE,TRUE,NA,Simulation study|Markov chain|Case study (real dataset),ARL (Average Run Length),"Phase I example uses 16 time series (days) to estimate the in-control model (Poisson INAR(1) with 2.1, 0.78). No general Phase I sample size guidance is provided.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes Stein identity-based EWMA (""Stein EWMA"") control charts for monitoring count processes with Poisson, negative binomial, or binomial in-control marginal distributions, including both i.i.d. and AR(1)-type autocorrelated count time series with these marginals. The method uses Stein identities to construct a ratio-type EWMA statistic expected to fluctuate around 1 in control, and it leverages a user-chosen bounded weight function f to target specific distributional changes beyond mean shifts (notably zero inflation, overdispersion, and underdispersion). Control limits are selected (typically symmetrically around 1) to achieve a target in-control ARL (about 370), and performance is assessed via extensive Monte Carlo simulation (R=10^4) under multiple out-of-control scenarios and dependence levels. Results show the proposed Stein EWMA charts can greatly outperform ordinary count EWMA/Shewhart charts when changes involve dispersion or zero inflation, and they provide diagnostic capability when multiple f-specific charts are run in parallel. A health-surveillance case study (childrens hospital emergency registrations) illustrates earlier signaling and scenario diagnosis (over/zero-inflation vs underdispersion) compared with standard charts.","Ordinary count EWMA: $Z_0=\mu_0$, $Z_t=\lambda X_t+(1-\lambda)Z_{t-1}$. Stein EWMA uses recursive EWMAs of Stein-moment components: $A_t=\lambda X_t f(X_t)+(1-\lambda)A_{t-1}$ and $C_t=\lambda X_t+(1-\lambda)C_{t-1}$, with a model-specific $B_t$ (e.g., Poisson: $B_t=\lambda f(X_t+1)+(1-\lambda)B_{t-1}$; NB: $B_t=\lambda(\nu+X_t)f(X_t+1)+(1-\lambda)B_{t-1}$; Bin: $B_t=\lambda(n-X_t)f(X_t+1)+(1-\lambda)B_{t-1}$). The plotted statistic is a ratio expected to be 1 in control, e.g., Poisson: $Z_t^S=A_t/(B_t C_t)$; NB: $Z_t^S=((\nu+C_t)A_t)/(B_t C_t)$; Bin: $Z_t^S=((n-C_t)A_t)/(B_t C_t)$, compared to limits $1\pm L$.","Simulations (typically with $\lambda=0.10$, $R=10^4$ replications, target $ARL_0\approx370$) show that when out-of-control behavior is driven by overdispersion, Stein EWMA with linear weights $f(x)=|x-1|$ yields smaller ARLs than ordinary EWMA; when driven by zero inflation, $f(x)=|x-1|^{1/4}$ is best. Underdispersion is difficult for ordinary EWMA (ARLs can become extremely large), while Stein EWMA with inverse weights $f(x)=1/(x+1)$ can work for low means and a shifted in-control PMF weight (e.g., $f(x)=p_P(x+2)$ or $p_N(x+2)$) improves underdispersion detection broadly. In the emergency-department example (Poisson INAR(1) in-control model with $\mu_0=2.1$, $\rho_0=0.78$), Stein EWMA charts for overdispersion/zero inflation signaled very early (e.g., at $t=6$ on March 28, 2009) and underdispersion charts signaled on a day where c-chart and ordinary EWMA did not (July 16, 2009).","The paper notes that monitoring underdispersion is particularly demanding and requires additional research activity. It also indicates that asymmetric control limits would only be reasonable if a specific out-of-control scenario were fixed, so the study restricts to symmetric limits given the broad range of scenarios considered.","The approach presumes a correctly specified in-control marginal family (Poisson/NB/Bin) and relies heavily on choosing an effective weight function $f$; in practice, selecting/tuning $f$ may be nontrivial and could require domain knowledge or trial-and-error. The study focuses primarily on zero-state ARL and argues (by analogy to prior work) that change-point location has limited impact; more systematic steady-state/conditional-delay evaluation would strengthen conclusions, especially under autocorrelation. Parallel use of multiple Stein EWMAs for diagnosis raises multiplicity/overall false-alarm control issues, but the paper mostly calibrates each chart to $ARL_0\approx370$ individually rather than controlling joint false-alarm behavior.","The authors propose developing Stein CUSUM charts for count processes, motivated by CUSUMs potential for better out-of-control performance than EWMA. They also suggest a residuals-based Stein approach (analogous to prior residual CUSUM work) to handle models with Poisson/NB/Bin conditional distributions even when marginals are not in the same family. Finally, they propose extending Stein EWMA/CUSUM ideas to continuous variables using continuous Stein identities.","Provide practical, data-driven guidance for choosing or adapting the weight function $f$ (e.g., adaptive or multi-resolution weights) and for calibrating limits when multiple Stein charts are run simultaneously (controlling the joint in-control false-alarm rate). Extend evaluation beyond zero-state ARL to steady-state ARL/conditional expected delay across a range of change-point times under serial dependence, and study robustness to marginal misspecification and parameter estimation error from Phase I. Release reproducible software (e.g., an R/Python package) implementing design/calibration for autocorrelated count models, including fast computation of ARLs and diagnostic plots.",2401.11789v1,local_papers/arxiv/2401.11789v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:50:58Z
TRUE,Multivariate|Profile monitoring|Functional data analysis,EWMA|Hotelling T-squared|Other,Both,Transportation/logistics|Manufacturing (general),NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|ANOS (Average Number of Observations to Signal)|Other,"Simulation study: Phase I sample size 2500 (training 1000, tuning 1500); Phase II: 200 sequences with shift at observation 100; bootstrap for limit setting uses nseq=500 sequences of length nobs=300. Case study: Phase I sample 568 split equally into training/tuning; Phase II consists of three datasets of sizes 220, 368, 184; bootstrap generates 200 sequences of length 200.",TRUE,R,Supplementary material (Journal/Publisher),https://doi.org/10.1080/00224065.2024.2383674,"The paper proposes an Adaptive Multivariate Functional EWMA (AMFEWMA) control chart for monitoring multivariate functional data (multichannel profiles) to detect process mean shifts. The method generalizes adaptive EWMA ideas to the functional setting by letting the EWMA weighting vary over time and across channels via score functions applied to the functional prediction errors, yielding Shewhart-like behavior for large deviations and EWMA-like behavior for small persistent shifts. Monitoring is performed via a functional Hotelling-type statistic built using an MFPCA-based representation and an estimated inverse-covariance kernel, with the upper control limit selected by a bootstrap procedure to meet a target in-control ARL. Extensive Monte Carlo simulations on DRC-inspired shift scenarios (splash/expulsion and peak-time phase shift) show that the optimally tuned AMFEWMA* achieves near-best ARL across a wide range of shift severities, bridging performance between standard multivariate functional EWMA and a Shewhart chart. A real case study on resistance spot welding in automotive body-in-white manufacturing (dynamic resistance curves from multiple spot welds per car body) demonstrates practical applicability and confirms robust performance when shift magnitude (electrode wear level) is unknown.","The adaptive statistic is defined by $Y_n(t)=(I-\Lambda_n(t))Y_{n-1}(t)+\Lambda_n(t)X_n(t)=Y_{n-1}(t)+\Lambda_n(t)E_n(t)$, where $E_n(t)=X_n(t)-Y_{n-1}(t)$ and $\Lambda_n(t)=\mathrm{diag}(w(E_{n1}(t)),\ldots,w(E_{np}(t)))$ with $w(E)=\eta(E)/E$ from adaptive score functions (piecewise-linear $\eta_1$ or smooth $\eta_2$) using parameters $(\lambda,k)$ and thresholds $C_j(t)=k\sigma_j(t)$. The chart uses a functional Hotelling-type statistic $V_n^2=\sum_{i=1}^p\sum_{j=1}^p\iint Y_{ni}(s)K_{ij}^*(s,t)Y_{nj}(t)\,ds\,dt$, with $K_{ij}^*(s,t)=\sum_{l=1}^L \rho_l^{-1}\psi_{li}(s)\psi_{lj}(t)$ from MFPCA eigenpairs, and signals when $V_n^2>h$ where $h$ is bootstrapped to achieve a target $\mathrm{ARL}_0$.","Simulations target an in-control $\mathrm{ARL}_0=20$; the proposed AMFEWMA* achieves in-control ARL close to 20 (reported around 21.92) while maintaining near-minimum out-of-control ARL across severity levels in both splash/expulsion and peak-time phase-shift scenarios. The AMFEWMA* behaves comparably to MFEWMA with small $\lambda$ for small shifts and comparably to a Shewhart/Hotelling $T^2$ chart for large shifts, yielding the lowest overall Relative Mean Index (RMI) among compared methods (Scenario 1: 0.27; Scenario 2: 0.49). In the resistance spot welding case study, AMFEWMA* with $(\lambda^*,k^*)=(0.5,4)$ produces ARLs 2.92 (wear level 1), 1.30 (wear level 2), and 1.085 (wear level 3), staying very close to the best competitor in each wear regime.",None stated.,"The method relies on Phase I data splitting into training/tuning sets and bootstrap calibration, which can be computationally heavy and may be sensitive to how representative Phase I is of true in-control operation. The monitoring statistic depends on an MFPCA truncation choice (e.g., 90% variance explained), and performance may degrade if the chosen basis/smoothing and truncation are misspecified or if shifts occur in directions not well captured by retained components. The paper largely treats observations as i.i.d.; if profiles are autocorrelated over time (common in many industrial streams), the bootstrap/ARL calibration and detection performance may be inaccurate without additional dependence modeling.",None stated.,"Develop versions robust to serial dependence (e.g., functional time-series modeling or block bootstrap calibration) and to non-Gaussian/heavy-tailed noise in functional errors. Extend the approach to monitor covariance/shape changes (not only mean shifts) and to provide diagnostic attribution to channels/time regions driving the signal. Provide open-source, ready-to-use software (e.g., an R package) and computational shortcuts for the bootstrap-based limit setting to facilitate industrial deployment in high-frequency settings.",2403.03837v2,local_papers/arxiv/2403.03837v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:51:39Z
FALSE,Other,Shewhart|Other,Phase I,Service industry|Other,NA,FALSE,NA,Case study (real dataset),Other,Uses 5 repeated observations per activity (stopwatch time study). Also mentions data were collected six times in three weeks and that each product sample was 20 units per observation; no general Phase I sample-size recommendation beyond verifying N' ≤ N via a data sufficiency test.,FALSE,None / Not applicable,Not applicable (No code used),NA,"This article is primarily a time-and-motion/work measurement study to standardize working time for product display tasks in a retail store (PT XYZ branch) for two products (X Milk and Y Bread). It maps the product display processes, collects stopwatch time observations, applies a data sufficiency test, and uses a basic control chart (control diagram with UPL/LCL) to check consistency and identify time deviations across repeated observations. The study then computes performance rating (Westinghouse method), allowance (break-time-based), normal time, and standard time for each activity and totals for each product’s display process. The resulting standard times are 15.83 minutes for X Milk and 9.18 minutes for Y Bread per 20 units. Control charts are used as a supporting tool for checking time study data stability rather than developing or evaluating SPC chart methodology.","The paper uses a data sufficiency test formula $N' = \left[\frac{k}{s}\,\frac{\sqrt{N\sum x_i^2-(\sum x_i)^2}}{\sum x_i}\right]$ (with example $k=2$, $s=0.05$). Normal time is computed as $N_t = \text{Cycle Time} \times \text{Rating Factor}$ (rating factor assumed 1.14 from Westinghouse factors). Standard time is computed as $S_t = N_t\times \frac{100\%}{100\%-\text{allowance}\%}$ with allowance computed as $\frac{\text{break time}}{\text{working time}}\times 100\%$ (12.5%).","Using 5 observations per activity, the data sufficiency test produced $N'$ values below 5 for all activities (e.g., 4.49 for carrying X Milk from warehouse to shelf), so the authors deem the samples sufficient. A control diagram for one activity shows points outside UPL/LCL on the 2nd and 4th observations, interpreted as time deviations due to lack of standard time. With rating factor 1.14 and allowance 12.50%, the total standard time is reported as 15.83 minutes for X Milk display (6 activities) and 9.18 minutes for Y Bread display (4 activities), each per 20 units.",None stated.,"The “control diagram” use is informal relative to SPC practice: subgrouping rationale, control-limit derivation, and distributional assumptions are not clearly justified, and the very small number of observations (5) limits the reliability of UPL/LCL and stability conclusions. The study is a single-site case with limited replication and no assessment of measurement system error (stopwatch/operator effects), day-to-day demand/context variation, or sustained monitoring performance (e.g., false alarm properties). The performance rating is assumed identical for all processes/products, which may bias normal/standard time estimates.",None stated.,"Future work could validate the proposed standard times over longer periods and multiple branches, and quantify between-employee and between-day variability using designed studies and measurement system analysis. If control charts are to be used for ongoing monitoring, a proper Phase I/Phase II SPC design with justified chart selection and run-length performance targets would strengthen the approach. Developing role-specific rating factors and allowances (or using predetermined motion-time systems) could improve fairness and accuracy of labor standards.",2403.09138v1,local_papers/arxiv/2403.09138v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:52:05Z
TRUE,Univariate|Other,Shewhart|Other,Both,Healthcare/medical|Pharmaceutical|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),Detection probability|False alarm rate|Other,"Model fitting for overdispersion assessment was restricted to settings with at least 5 historical clusters/controls (used because dispersion estimates can be inaccurate with small samples). Simulations consider H = {5, 10, 20, 100} historical clusters/patients.",TRUE,R,Package registry (CRAN/PyPI),https://ntp.niehs.nih.gov/data/controls,"The paper proposes using prediction intervals as historical control limits (HCL) for overdispersed Poisson count data in pre-clinical and medical quality control, accounting for clustered designs and variable baseline exposure via offsets. It develops asymptotic prediction intervals under quasi-Poisson and negative-binomial models and introduces a parametric bootstrap calibration procedure that calibrates lower and upper limits separately to achieve equal tail probabilities under skewness. Through Monte Carlo simulations comparing eight HCL methods (including Shewhart c/u charts, Laney’s overdispersion-adjusted u-chart, mean ± kSD, and calibrated/uncalibrated prediction intervals), the bootstrap-calibrated intervals best control nominal coverage/type-I error, especially under overdispersion and small historical sample sizes. Real-data demonstrations include Ames assay historical controls (revertant colonies) and multiple sclerosis relapse counts, showing how calibrated limits can flag unusual controls/patients or center-level deviations. The methods are implemented in an R package (predint) with functions for quasi-Poisson and negative-binomial prediction intervals and bootstrap calibration.","Historical control limits target a future count $y^*$ over offset $n^*$ with $P(l\le y^*\le u)=1-\alpha$ and (for two-sided limits) equal tails $P(l\le y^*)=P(y^*\le u)=1-\alpha/2$. Uncalibrated overdispersion-aware intervals take the form $[l,u]=n^*\hat\lambda \pm z_{1-\alpha/2}\sqrt{\widehat{\mathrm{Var}}(n^*\hat\lambda)+\widehat{\mathrm{Var}}(Y^*)}$ with quasi-Poisson variance (Eq. 13) and negative-binomial variance (Eq. 14). Bootstrap-calibrated limits replace $z_{1-\alpha/2}$ by separately calibrated coefficients $q_l^{\mathrm{calib}}, q_u^{\mathrm{calib}}$ to form asymmetric limits (Eqs. 15–16).","In simulations at nominal 95% coverage, classical Shewhart c- and u-chart limits severely undercover under overdispersion (reported as dropping as low as ~58% in some settings), and symmetric heuristics (mean ± 2 SD; Laney u-chart) fail to achieve equal tail probabilities under skewness. The bootstrap-calibrated quasi-Poisson and negative-binomial prediction intervals achieve coverage close to nominal even with as few as H=5 historical clusters, and their calibrated bounds better approximate the intended tail coverages. For one-sided medical-style monitoring, calibrated upper prediction limits are near 95% and appear robust to model misspecification, but negative-binomial GLM fitting often fails to converge for sparse/zero-heavy data (e.g., with $\lambda=0.1$ and $H\le10$, converged fits can fall below 50%, with a minimum reported 24.5%). In the MS registry example, applying limits from a baseline center flagged markedly elevated exceedance rates in another center (e.g., 21.45% above a 95% UPL vs 5% expected).","The authors note that overdispersion implies heterogeneity between historical clusters and may reflect a mix of controllable and uncontrollable sources; thus no clear, generally tolerable magnitude of overdispersion can be recommended. They also state that methods to compute simultaneous prediction intervals for evaluating an entire current trial (adjusting for multiple testing and within-study correlation across groups) are, to their knowledge, not available. They report practical limitations in fitting negative-binomial GLMs, with frequent non-convergence for small samples and many zeros.","The approach focuses on pointwise prediction limits (HCL) rather than sequential run-length properties central to SPC (e.g., ARL/ATS), so performance for ongoing monitoring and false-alarm behavior over time is not characterized. The bootstrap calibration is parametric (model-based); misspecification beyond NB/QP (e.g., zero-inflation, hurdle models, temporal dependence) could degrade calibration despite some robustness checks. Implementation may be computationally heavy in routine use (e.g., B=10,000 bootstraps per limit) and may require careful choices for convergence handling and dispersion constraints (e.g., enforcing $\hat\phi\ge1$).","The authors explicitly point out an open methodological need for simultaneous prediction intervals that could evaluate outcomes of a whole current trial while accounting for multiple testing and within-study correlation, noting such methodology is not available to their knowledge.","Extend the calibrated HCL framework to zero-inflated/hurdle count models and to autocorrelated or longitudinal patient-level data where observations are not exchangeable. Develop sequential-monitoring designs that connect calibrated prediction limits to run-length metrics (ARL/ATS) and provide guidance for repeated use over time. Provide faster approximations or adaptive bootstrap schemes (e.g., importance sampling, precomputed calibration curves) and expand software to handle non-convergence and model diagnostics more systematically.",2404.05282v1,local_papers/arxiv/2404.05282v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:52:43Z
TRUE,Univariate|Other,CUSUM,Phase II,Energy/utilities|Environmental monitoring|Other,TRUE,FALSE,TRUE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,Not discussed (training uses multiple years of 10-minute SCADA data; authors note a model should be trainable with “at least a few years” of SCADA data and use 80–20 chronological split in experiments).,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a wind-farm condition monitoring system based on normal-behaviour modelling with a probabilistic multi-layer perceptron (PMLP) that outputs a predictive Normal density for turbine power, allowing heteroscedastic (input-dependent) variance. A transfer-learning variant (LPMLP) pre-trains a large model using SCADA data from all turbines and then fine-tunes to a specific turbine, enabling “borrowing strength” and practical use when individual turbines have limited or missing historical data. For monitoring, the authors standardize residuals using the predicted mean and standard deviation and apply a two-sided tabular CUSUM chart to detect sustained deviations from the in-control model. Performance is demonstrated on real 10-minute SCADA data from six turbines (Kelmarsh wind farm, UK), showing improved predictive accuracy and calibration versus a sparse Gaussian process and a Bayesian neural network, and providing an early warning prior to a recorded forced outage. The work advances SPC-style monitoring in renewable-energy condition monitoring by coupling probabilistic ML prediction (with uncertainty) to a classical control-chart signaling rule tailored via a real-data classification exercise.","The probabilistic model assumes $y\mid x \sim \mathcal N(\mu(x),\sigma^2(x))$, where $\mu(x)$ and $\sigma(x)$ are produced by a branched neural network and $\sigma(x)$ uses a Softplus output to ensure positivity. Training minimizes the negative log-likelihood $L_1=-\sum_{i=1}^n \log \mathcal N(y_i\mid \mu(x_i),\sigma^2(x_i))$ (or the pooled multi-turbine version $L_2$). For monitoring, standardized residuals are $v_t=(y_t-\mu(x_t))/\sigma(x_t)$ and a two-sided tabular CUSUM is used: $S_H(t)=\max\{0, v_t-k+S_H(t-1)\}$ and $S_L(t)=\max\{0, -k-v_t+S_L(t-1)\}$, signaling when either exceeds decision interval $I$.","On out-of-sample prediction for one turbine (32,713 test intervals), LPMLP achieved the best errors and calibration: LPMLP-A1 RMSE 27.38 kW and MAE 14.81 kW (NRMSE 1.34%, NMAE 0.72%), outperforming sparse GP (RMSE 49.57 kW, MAE 33.87 kW) and Bayesian NN (RMSE 40.28 kW, MAE 22.54 kW). Calibration improved substantially for LPMLP-A1 (MCE 0.93%) compared with alternatives (e.g., sparse GP MCE 6.94%). In a fault example, the CUSUM first signaled out-of-control at 2020-06-30 04:50, ahead of a logged forced outage alarm on 2020-07-02 13:35 (converter error). In a decision-interval selection exercise with 22 fault and 22 normal 72-hour windows, choosing $I=15$ yielded precision 0.68, recall 0.77, and mean lead notice time 15.49 hours (SD 21.76).","Authors note that a principled selection of the CUSUM decision interval $I$ via historical analysis is difficult for their data because detailed operational anomalies are unavailable: among ~163K recordings, only 22 faults are reported. They also indicate that, after a signal, engineers want component-level attribution; their current approach flags anomalies but does not directly identify which turbine component/variable caused the deviation.","The CUSUM design assumes standardized residuals $v_t$ are i.i.d. $\mathcal N(0,1)$ under no-fault; with 10-minute SCADA data, serial dependence and regime changes (weather/curtailment) can violate independence and inflate false alarms unless explicitly modeled. The monitoring rule relies on a parametric Normal predictive distribution; model misspecification (heavy tails/asymmetry) could affect calibration and signaling thresholds. The chart is effectively univariate on power residuals; multistream monitoring (multiple outputs) and diagnostic isolation are not addressed beyond discussion. Code/software details are not provided, which may limit reproducibility of the ML training and monitoring pipeline.","They propose extending the approach to identify which wind-turbine component/operational characteristic is responsible after the chart signals, e.g., by monitoring additional SCADA variables (temperatures/pressures) with separate models and CUSUM charts. They also suggest replacing fine-tuning via parameter initialization with a Bayesian transfer-learning approach that uses pre-trained estimates to form informative priors, yielding more informative posteriors for turbine-specific models.","A valuable extension would be to explicitly handle autocorrelation/seasonality in $v_t$ (e.g., time-series residual modeling, prewhitening, or CUSUM for dependent data) and to calibrate $k$ and $I$ to target in-control ARL/false-alarm rates. Developing multivariate or high-dimensional monitoring across multiple SCADA-derived residual streams with built-in fault isolation could improve practical diagnostics. Robust/nonparametric monitoring based on predictive quantiles or conformal prediction could relax Normality assumptions while preserving coverage. Providing an open-source implementation and guidance on deployment (update schedules, drift detection, retraining triggers) would improve adoption in operational wind farms.",2404.16496v2,local_papers/arxiv/2404.16496v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:53:27Z
TRUE,Univariate|Other,Shewhart|CUSUM|Change-point,Phase II,Environmental monitoring|Other,NA,FALSE,FALSE,Simulation study,Detection probability|False alarm rate|Other,Not discussed,TRUE,Python,Not provided,https://doi.org/10.2166/97817890623800215|https://doi.org/10.18434/M32189|https://doi.org/10.1080/07408170701592499,"The paper proposes a real-time pipe burst localization approach for water distribution networks (WDNs) that relies on change point detection (CPD) applied to streaming nodal pressure measurements from pressure meters. It evaluates two classical SPC/CPD methods—CUSUM and Shewhart control charts—for detecting abrupt pressure changes during hydraulic transients, then localizes the burst by selecting the node with the earliest/largest pressure-change amplitude and using a directed acyclic graph of the network to infer the burst link. Pipe-burst scenarios are generated via transient simulation using EPANET-built network models executed in TSNet (Python), with pressure readings released at fixed capture intervals to emulate real-time availability. Performance is assessed primarily by burst-link localization accuracy across capture intervals (0.2s, 2s, 5s, 10s), showing higher average accuracy for CUSUM (70%) than Shewhart (65%), while both perform best at 0.2s (CUSUM 92%, Shewhart 96%). The study positions SPC-style change detection as a practical alternative when labeled historical burst data are scarce, but notes the need for further experiments on additional and real-world WDN models.","CUSUM is implemented via separate positive/negative cumulative sums computed on successive differences: $C_i^+=\max\{0, C_{i-1}^+ + (x_i-x_{i-1})-\text{drift}\}$ and $C_i^-=\max\{0, C_{i-1}^- - (x_i-x_{i-1})-\text{drift}\}$, signaling when either cumulative sum exceeds a threshold. The Shewhart chart uses control limits $\text{UCL}=\mu+s\sigma$ and $\text{LCL}=\mu-s\sigma$ (with window mean $\mu$ and standard deviation $\sigma$), and in their implementation flags changes when $|x_i-\text{mean}|>\text{threshold}\cdot\text{std}$ and also exceeds an absolute threshold; localization accuracy is computed as $\text{Acc}=P_c/P_t\times 100\%$.","Across four capture intervals, reported localization accuracies (CUSUM vs Shewhart) are: 0.2s: 92% vs 96%; 2s: 72% vs 72%; 5s: 52% vs 32%; 10s: 64% vs 60%, with averages 70% (CUSUM) and 65% (Shewhart). In the underlying burst-link identification tables, the count of correctly localized bursts (out of 25 pipes) for CUSUM is 23, 18, 13, 16 for scenarios S_C1–S_C4, and for Shewhart is 24, 18, 8, 15 for scenarios S_S1–S_S4. Best performance for both methods occurs with the finest sampling (0.2s capture interval) and a localization interval of 5. The authors attribute Shewhart’s degradation at larger capture intervals to widening control limits from increased variance during transients, reducing sensitivity to smaller pressure changes.",The authors state that the approach requires further experiments on different WDNs to assess performance and accuracy on more complex networks and on real-world WDN data/models. They also note that parameter optimization for the approach should be explored to improve accuracy and overall performance.,"The study evaluates performance mainly via localization accuracy on simulated data and does not report standard SPC metrics such as in-control/out-of-control ARL, false alarm rates over long monitoring horizons, or detection delay distributions, limiting comparability to SPC literature. Thresholds and localization intervals are tuned by trial-and-error on the studied setup, which risks overfitting to the simulated network and makes reproducibility/transfer to other WDNs uncertain without a formal design procedure. The CPD formulations appear to operate on successive differences and global mean/std without an explicit in-control model, and the impact of measurement noise, sensor faults, drift/seasonality, and serial correlation typical of SCADA streams is not rigorously analyzed. No handling of missing/irregularly sampled sensor data is described, which is a common practical issue in real deployments.","The authors propose running further experiments on different WDNs, including more complex models and real-world WDN data, to better assess performance and accuracy. They also suggest exploring parameter-optimization approaches to improve the model’s accuracy and performance.","Develop a principled chart-design/calibration procedure (e.g., targeting a specified false alarm rate or ARL under nominal conditions) and validate robustness across multiple network topologies and burst magnitudes. Extend the method to explicitly accommodate autocorrelation and nonstationarity in pressure time series (e.g., prewhitening/model-based residual charts or adaptive baselines). Evaluate detection timeliness and false alarms separately from localization, including expected detection delay and steady-state false alarm behavior under normal operational transients (valve/pump actions). Provide an open-source reference implementation (or a TSNet/WNTR add-on) and test under missing data, asynchronous sensors, and realistic noise to support field deployment.",2407.09074v1,local_papers/arxiv/2407.09074v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:54:03Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Both,Manufacturing (general)|Energy/utilities|Transportation/logistics|Other,NA,FALSE,FALSE,Simulation study|Case study (real dataset),False alarm rate|Detection probability|Other,"Not discussed (batch/window length is optimized; examples include k=5180 timestamps, and optimized values such as 5818 or 6411 observations per batch).",TRUE,None / Not applicable,Not provided,https://csegroups.case.edu/bearingdatacenter/pages/welcome-case-western-reserve-university-bearing-data-center-website,"The paper proposes a batch-based multivariate statistical process control (MSPC) method for rolling-element bearing fault detection using features extracted from fixed-length time batches of vibration signals. Each batch is decomposed via discrete Fourier transform into dominant Fourier components plus a residual, and summary features (magnitude, dominant frequency, and standard deviation per component, plus residual standard deviation) form a multivariate feature vector monitored by a PCA model. Faults are detected using PCA-based monitoring statistics: Hotelling’s $T^2$ for variation captured by the PCA model and Squared Prediction Error (SPE/SPEx) for residual (unmodeled) variation; warning/alarm limits are set at 95% and 99.8% via mean±2/3 standard deviations. Model hyperparameters (batch length, number of Fourier components, selected variables, and number of PCs) are optimized using a genetic algorithm and backward variable selection based on chart contribution diagnostics. Experiments on the CWRU bearing dataset across fault types, fault sizes, sensor locations (DE/FE), and motor loads show strong separation of healthy vs faulty batches; individual-load models reportedly achieve 100% detection and 100% normal identification, while combined-load models maintain 100% detection with ~0.001% false alarms.","Batched DFT is defined by $FT_k=\sum_{n=0}^{N-1} x_n e^{-i2\pi kn/N}$ with inverse $x_n=\sum_{k=0}^{N-1} FT_k e^{i2\pi kn/N}$. Each batch signal is decomposed as $x_t=\sum_{i=1}^{I} FT_{i,t}+r_t$, and the feature matrix uses per-component summaries $\max(FT_{i,t})$, $\mathrm{freq}(FT_{i,t})$, $\sigma(FT_{i,t})$ plus residual $\sigma(r_t)$. PCA uses standardized data $Z=(X-\bar X)/\sigma_X$ and $Z=T P^\top+E$; monitoring statistics are $T^2=\sum_{j=1}^{a} t_j^2/\lambda_j$ and $SPEx=\sum_i (z_i-\hat z_i)^2$ with limits set via $\bar T^2\pm 2/3\,\sigma_{T^2}$ (analogously for SPEx).","For the DE, an example optimized batch length is 5180 timestamps; for 0 hp, an optimized setting reported includes 4 FT components, batch length 5818 (~0.5 s at 12 kHz), 13 initial features, and 5 PCs before variable reduction. Backward variable selection yields a 6-variable subset (FT1 frequency, FT2 frequency, FT3 magnitude/variance/frequency, and residual variance), reducing the PCA model to 4 PCs while still clearly detecting faults in both $T^2$ and SPEx charts. A combined multi-load (0–3 hp) model is reported with an optimum of 4 PCs, 3 FT components, and batch length 6411 observations, detecting faults across loads but producing a small false-alarm rate (<0.001% of observations). The conclusion reports 100% failure detection and 100% normal-operation identification for individual-load models; combined-load models retain 100% detection with ~0.001% false alarms.",The study is limited to evaluation on the CWRU bearing dataset. The authors note that further research should examine suitability under installation effects during normal operation and under multiple simultaneous faults.,"Control limits are set using heuristic mean±2/3 SD thresholds rather than distribution-based limits (e.g., $F$-distribution for $T^2$ under PCA assumptions or Jackson/Mudholkar-type approximations for SPE), which may affect nominal false-alarm control and transferability across datasets. The approach treats batches as independent and does not explicitly model serial dependence within/among batches, which is common in vibration monitoring and can inflate false alarms or mask small changes. Validation is mainly on a well-known benchmark with induced faults and may not reflect realistic nonstationarity, sensor drift, or covariate shifts encountered in field condition monitoring.","The authors suggest extending evaluation beyond the CWRU dataset, and exploring robustness to installation effects in normal operation and to scenarios with multiple simultaneous faults.","A useful extension would be deriving or calibrating statistically justified control limits for $T^2$ and SPE (including Phase I estimation effects) to guarantee target in-control false-alarm rates. Incorporating autocorrelation-aware MSPC (e.g., dynamic PCA/state-space residual charts) or overlap/batch-to-batch dependence corrections could improve reliability for continuous monitoring. Providing an implementation package (e.g., Python/R) and testing on additional real industrial datasets (different machines/speeds/sensors) would strengthen reproducibility and generalizability.",2407.17236v2,local_papers/arxiv/2407.17236v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:54:46Z
TRUE,Univariate|Nonparametric|Self-starting|Image-based monitoring|Other,Shewhart|EWMA|Other,Phase II|Phase I,Manufacturing (general)|Environmental monitoring|Other,FALSE,NA,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate|Expected detection delay,"No explicit Phase I sample-size requirement; charts are designed to be usable without prior Phase-I analysis under the IC-iid assumption. The paper notes performance is poor when only one 2×2 SOP per image is available (m=n=1) and focuses simulations on grids such as (10,10), (15,15), (25,25), (40,25).",TRUE,Julia|Other,Public repository (GitHub/GitLab),https://github.com/AdaemmerP/OrdinalPatterns.jl,"The paper proposes distribution-free control charts for monitoring spatial dependence in streams of regular rectangular (lattice) data, filling a gap where existing nonparametric spatial monitoring mainly targets changes in the mean. The method uses 2×2 spatial ordinal patterns (SOPs) and their three “types” to form statistics (τb, κb, τe, κe) whose in-control distribution is uniform under spatial i.i.d., enabling self-starting (no Phase I) charting. Both Shewhart-type (memoryless) and EWMA-type (smoothed) charts are developed, along with higher-order dependence extensions using delayed SOPs and a Box–Pierce-style aggregation over multiple lags. Extensive Monte Carlo studies evaluate zero-state ARL and show SOP charts—especially the τe EWMA chart—are robust and often outperform spatial autocorrelation-function (ACF) charts under nonlinear/bilateral dependence and in the presence of outliers, while ACF charts dominate for clean linear unilateral SAR dependence. The approach is demonstrated on real data to detect heavy rainfall patterns, war-related fires, and manufacturing defects in textile images, and the implementation is released as a Julia package.","For each time t, compute the SOP-type frequency vector \(\hat p_t\) (or its EWMA-smoothed version \(\hat p_t^{(\lambda)}=\lambda\hat p_t+(1-\lambda)\hat p_{t-1}^{(\lambda)}\)). Control statistics are built from deviations of type frequencies from the i.i.d. benchmark \((1/3,1/3,1/3)^\top\), e.g., \(\tau_{e,t}^{(\lambda)}=\hat p_{t,3}^{(\lambda)}-1/3\), signaling when \(|\tau_{e,t}^{(\lambda)}|>l\). For higher-order dependence, delayed SOPs use lag \(d=(d_1,d_2)\) and a Box–Pierce-style aggregation \(\tau^{BP(\lambda,w)}_{e,t}=\sum_{d_1,d_2=1}^w (\tau^{(\lambda,d)}_{e,t}-\tau^{(d)}_{e,0})^2\) (with \(\tau^{(d)}_{e,0}=0\) under IC-iid).","IC control limits are calibrated by simulation to target zero-state \(ARL_0\approx 370\) (e.g., 10^6 replications for design), and OOC ARLs are estimated with 10^5 replications across many spatial DGPs. For unilateral linear SAR(1,1) without contamination, the ACF-based competitor typically has the smallest OOC ARL (often near 1–10 for moderate dependence), while SOP charts still reduce ARL substantially (τe usually best among SOP charts). Under additive outliers, nonlinear (quadratic) dependence, and bilateral dependence, SOP charts—especially τe—often outperform the ACF chart by large margins (reported examples include speed-ups up to about a factor 22 for certain nonlinear cases). Real-data examples show τe EWMA signaling at the onset of heavy rainfall events (hour 138) and sustained war-related fire activity; for textile images, a bootstrap-calibrated τe chart detects defective patches in Phase II.","The authors note that for discrete data, ties can be frequent; their proposed “jittering” (adding uniform noise) breaks ties but makes results depend on the random noise realization. They also caution that very small spatial grids (e.g., m=n=1, only one 2×2 SOP per rectangle) provide low information, yielding slow detection (large OOC ARLs) and thus limited practical usefulness. They additionally highlight that parametric ACF-based charts require correct in-control distribution specification and can be misleading under non-normality.","The main “distribution-free/no Phase I” guarantee is tied to the IC-iid assumption across pixels within each lattice (after jittering for discrete data); many real spatial processes are in-control but dependent, requiring ad hoc calibration (as in the textile bootstrap) and weakening the self-starting claim. The paper focuses on 2×2 SOPs/types (a strong dimensionality reduction), which may miss certain dependence structures or be less sensitive when dependence is subtle but not well reflected in type-3 frequency. Performance reporting centers on zero-state ARL; steady-state/conditional delay and diagnostic capability after a signal are not systematically developed, which are often important for deployment.","The authors propose developing “refined types” (finer partitions of the 24 SOPs) to reduce information loss and potentially improve detection for some OOC scenarios. They also suggest alternatives to jittering for discrete data, such as extending generalized ordinal patterns to the spatial setting (“generalized SOPs”) to use tie information rather than adding noise. Finally, they recommend exploring Box–Pierce-style (multi-lag) ordinal-pattern aggregations in other ordinal-pattern inference tasks beyond the present monitoring setting.","A valuable extension would be to build explicit in-control models with structured spatial dependence (beyond IC-iid) and derive principled, distribution-free or robust calibration methods (e.g., block/bootstrap schemes with theoretical guarantees). Another direction is multiscale or adaptive lag selection (choosing d or w online) to improve sensitivity across unknown dependence ranges while controlling false alarms. Providing guidance for practitioner-facing design choices (grid size effects, λ selection, computational cost) and expanding software examples/benchmarks across standard spatial datasets would also strengthen real-world adoption.",2408.17022v2,local_papers/arxiv/2408.17022v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:55:36Z
TRUE,Univariate|Other,CUSUM|Other,Phase II,Food/agriculture|Manufacturing (general)|Healthcare/medical|Pharmaceutical|Environmental monitoring|Network/cybersecurity|Finance/economics|Energy/utilities|Other,TRUE,FALSE,FALSE,Approximation methods|Economic design|Simulation study|Other,ARL (Average Run Length)|False alarm rate|Other,Design variables constrained as: sample size n ranges from 2 to 20; sampling interval h ranges from 0.01 to 2 hours; decision interval H ranges from 0.0001 to 5. Numerical example optimum uses n = 2.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a multi-objective economic-statistical design framework for a two-sided CUSUM chart to monitor shifts in a process mean. It optimizes CUSUM design parameters—sample size (n), sampling interval (h), and decision interval (H)—to simultaneously minimize expected cost per cycle (CE) and out-of-control average run length (ARLδ), while constraining in-control performance via a lower bound on ARL0. The Lorenzen–Vance (1986) unified economic model is used for CE, and Siegmund’s (1985) approximation is used to compute one- and two-sided ARLs. The multi-objective optimization is solved using NSGA-II to generate Pareto-optimal trade-offs, illustrated with a numerical example (based on Lee, 2011) and one-factor-at-a-time sensitivity analyses on cost/time parameters and shift magnitude. In the example, the minimum CE found is $9.50 at n=2, h=0.36 hours, H=4.19, representing about a 43.39% cost reduction versus Lee (2011), though the authors note they could not compare ARLδ due to lack of references for that comparison.","CUSUM statistics are defined as C_i^- = max(0,(\mu_0-K)-x_i + C_{i-1}^-) and C_i^+ = max(0, x_i-(\mu_0+K)+C_{i-1}^+), with C_0^-=C_0^+=0. ARL is computed using Siegmund’s approximation for one-sided CUSUM: ARL = (e^{-2\Delta b} + 2\Delta b - 1)/(2\Delta^2), with K=\delta/2 and b=H+1.166, then combined for two-sided via 1/ARL = 1/ARL^- + 1/ARL^+. The economic objective is the Lorenzen–Vance expected cost per cycle CE (Eq. 6) incorporating sampling cost, false-alarm and repair costs, and cycle time components driven by ARL0 and ARLδ.","Using the numerical example (from Lee, 2011; yogurt drink bottling), the Pareto set includes 82 non-dominated solutions trading off CE and ARLδ. The minimum expected cost per cycle reported is CE = $9.50 with ARLδ = 8.72 at n = 2, h = 0.36 hours, H = 4.19 (with constraints ARL0 ≥ 200 and ARLδ ≤ 14). Lee (2011) is reported to obtain CE = $16.78 with n = 2, h = 0.85 hours, H = 1.69 under normality, implying a 43.39% reduction in CE for the proposed approach. Sensitivity analysis shows that as shift size δ increases (1.0 to 2.5), both CE and ARLδ decrease, while n remains at 2; typical patterns reported are decreasing h and increasing H as δ increases.","The authors state that no comparison of ARLδ performance could be made due to the non-availability of references. They also frame the approach under specific modeling assumptions (normality for the quality characteristic and exponential time-to-assignable-cause), indicating the method is developed within those constraints.","The work relies on Siegmund (1985) ARL approximations rather than exact/Markov-chain/integral-equation ARL calculations, so accuracy for certain parameter ranges (e.g., small H, specific K choices, or extreme ARL constraints) is not validated. Autocorrelation/correlation and non-normality are not handled in the proposed formulation (despite citing Lee (2011) on correlated non-normal data), limiting applicability to many modern sensor/process settings. The computational implementation details (language, parameter settings for NSGA-II such as population size, mutation/crossover rates, stopping criteria) are not clearly documented as a reproducible software artifact, and no code is shared.","The conclusion indicates “scope for future work” but does not specify concrete extensions beyond stating that, in practice, if cost and time parameters are adequately estimated, the algorithm/pseudocode can be used for implementation.","Extend the design to correlated/autocorrelated observations and non-normal distributions (e.g., via robust/nonparametric CUSUMs or model-based residual charts) and quantify the impact on CE–ARL trade-offs. Provide exact or higher-fidelity ARL computation (e.g., Markov chain/integral equation) as an alternative to the approximation, and study approximation error in the economic-statistical optimization loop. Develop and release reproducible software (e.g., R/Python package) including NSGA-II settings and benchmarking on additional real industrial datasets, and consider additional objectives/constraints (e.g., ATS, expected delay, maintenance coupling, multiple assignable causes).",2409.04673v3,local_papers/arxiv/2409.04673v3.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:56:08Z
TRUE,Univariate|Self-starting|Bayesian,CUSUM,Phase II,Theoretical/simulation only,TRUE,FALSE,NA,Simulation study|Other,ARL (Average Run Length)|Conditional expected delay|False alarm rate,"Not discussed (no Phase I). Performance is examined for change-point locations τ = 11, 21, ..., 101, i.e., from 10 up to 100 in-control observations available before the shift.",TRUE,None / Not applicable,Not provided,NA,"The paper compares two parametric self-starting CUSUM-type control charts for detecting persistent location (mean) shifts in univariate Normal data with both mean and variance unknown: the frequentist Self-Starting CUSUM (SSC; Hawkins and Olwell, 1998) and the Bayesian Predictive Ratio CUSUM (PRC; Bourazas et al., 2023). Both charts are calibrated to have in-control ARL0 = 370 and are evaluated via extensive Monte Carlo simulation across mean shifts δ ∈ {0.5, 1, 1.5, 2} and change-point locations τ ∈ {11, 21, ..., 101}. Out-of-control performance is assessed using the Conditional Expected Delay (CED), appropriate for long-run monitoring with an unknown change time. A prior sensitivity analysis is conducted for PRC, comparing a non-informative reference prior to a weakly informative prior equivalent to about four in-control observations. Results show faster detection for larger shifts and improved performance when the change occurs later (more in-control data available), with the informative prior mainly improving early-run performance; overall PRC (non-informative) and SSC perform similarly with small differences depending on the design parameter k.","SSC standardizes the next observation using current estimates: $T_{n+1}=(X_{n+1}-\bar X_n)/s_n$, transforms it to an exactly $N(0,1)$ variable via $U_{n+1}=\Phi^{-1}\{F_{n-1}(\sqrt{n/(n+1)}\,T_{n+1})\}$, then applies a two-sided CUSUM recursion $C^+_{n+1}=\max(0,C^+_n+U_{n+1}-k_{SSC})$, $C^-_{n+1}=\min(0,C^-_n+U_{n+1}+k_{SSC})$ with signal when $\max(C^+_{n+1},|C^-_{n+1}|)>h_{SSC}$. PRC uses a Normal-Inverse-Gamma prior, derives the posterior predictive $f(X_{n+1}|X_n)$ and a shifted predictive $f'(X_{n+1}|X_n)$, forms the log predictive ratio $\log L_{n+1}=\log\{f'(X_{n+1}|X_n)/f(X_{n+1}|X_n)\}$ (Eq. 5), and updates $S^+_{n+1}=\max(0,S^+_n+\log L_{n+1})$, $S^-_{n+1}=\min(0,S^-_n-\log L_{n+1})$, signaling when $\max(S^+_{n+1},|S^-_{n+1}|)>h_{PRC}$.","Decision limits for both methods are tuned to achieve ARL0 = 370 using 10,000 simulated in-control sequences. Out-of-control performance is measured by CED(τ) estimated from 10,000 replications per scenario for δ ∈ {0.5, 1, 1.5, 2} and τ ∈ {11, 21, ..., 101}, with kSSC ∈ {0.25, 0.375, 0.5} and kPRC ∈ {0.5, 0.75, 1} (kSSC = kPRC/2 in the paired comparisons). Detection improves (CED decreases) as δ increases and as τ increases (more in-control history). For small shifts (δ = 0.5) with very early change (τ = 11), CEDs are very large (often > 300), indicating poor effectiveness early in the run. The weakly informative prior (PRCi: NIG(0,4,2,1.5)) reduces CED notably when τ is early, while PRC with reference prior (PRCn: NIG(0,0,-1/2,0)) and SSC have broadly similar CEDs overall, with SSC slightly better for small k and PRCn slightly better for larger k.",None stated.,"The study is limited to Normal data and focuses on persistent mean shifts; robustness to non-normality (heavy tails, skewness) and other change types (variance shifts, transient shifts) is not evaluated. The work relies on simulation without real case studies, so practical issues (data quality, sampling irregularity, implementation constraints) are not demonstrated. Autocorrelation is not modeled; performance under serial dependence (common in process data) remains unknown. Calibration of decision limits uses a specific simulation-based method (Regula Falsi with 10,000 sequences), but sensitivity of limits/performance to this calibration approach and Monte Carlo error is not discussed.",None stated.,"Extend comparisons to non-Normal and/or robust self-starting CUSUM variants to assess sensitivity to heavy tails and outliers. Study performance under autocorrelation and develop adaptations (e.g., residual-based or time-series-aware self-starting schemes). Include real-data case studies and provide reproducible software to facilitate adoption and benchmarking. Explore adaptive or data-driven choices of the design parameter k (and prior hyperparameters for PRC) to improve performance across a wider range of shift sizes and change-point locations.",2410.12736v1,local_papers/arxiv/2410.12736v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:56:43Z
TRUE,Profile monitoring|Functional data analysis|Other,GLR (Generalized Likelihood Ratio)|Other,Both,Manufacturing (general)|Transportation/logistics|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"Phase I is assumed partitioned into a training set and a tuning set; in simulations they use 400 training and 1000 tuning observations per cluster. In the RSW case study, Phase I has 1802 IC curves split into 901 training and 901 tuning.",TRUE,R,Not provided,NA,"The paper proposes the Functional Mixture Regression Control Chart (FMRCC) for multimode profile monitoring when a functional quality characteristic depends on covariates and the in-control population consists of multiple latent subpopulations, each with a distinct functional linear model (FLM). The method projects functional response/predictors to a finite basis via multivariate functional PCA, fits a Gaussian mixture regression model to the resulting score vectors using the EM algorithm (with BIC for selecting the number of components and covariance parametrization), and monitors new observations using a likelihood-ratio-test-based statistic. Control limits are set nonparametrically using a Phase I tuning set (empirical quantile of the monitoring statistic), and a studentized variant is introduced to mitigate covariate mean-shift effects with small Phase I samples. A Monte Carlo study compares FMRCC against FRCC, an FPCA-based functional control chart (Hotelling’s T²/SPE), and clustering+FCC, showing strongest gains when clusters differ in regression structure. A resistance spot welding case study demonstrates practical value, detecting 32 of 37 known out-of-control curves (TDR 0.864), outperforming the competing schemes.","Within component k, the (standardized) function-on-function FLM is $Y(t)=\beta_{0k}(t)+\int_S \beta_k(s,t)^\top X(s)\,ds+\varepsilon(t)$. After truncating MFPC expansions, this reduces to a finite mixture multivariate regression on scores: $\xi^Y_M = B_{LMk}^\top \xi^X_L + \varepsilon_M$, and $f(\xi^Y_M\mid \xi^X_L)=\sum_{k=1}^K \pi_k\,\phi(\xi^Y_M;B_{LMk}^\top\xi^X_L,\Sigma_k)$. The Phase II monitoring statistic uses the LRT approximation $\Lambda \approx f(\hat\xi^{Y*}_M\mid\hat\xi^{X*}_L;\hat\Psi)$ and $\hat W^*=-\log\Lambda$, with a studentized version replacing $\Sigma_k$ by a prediction-error covariance $\hat\Sigma_k^*$.","In the resistance spot welding case study, the proposed FMRCC detects 32 of 37 known OC DRCs, yielding $\widehat{\mathrm{TDR}}=0.864$ with bootstrap 95% CI $[0.756,\,0.960]$. Competing methods achieve lower detection: FRCC 0.486 (CI [0.310, 0.581]), FCC 0.486 (CI [0.323, 0.608]), and CLUST 0.621 (CI [0.472, 0.729]). In simulation (100 runs per setting), performance is evaluated via mean FAR (IC) and mean TDR (OC) across severity levels; FMRCC is especially superior when clusters differ primarily in regression coefficient functions (multimode driven by covariate-response relationships), where clustering on response alone can fail to recover multiple modes.",None stated.,"The monitoring statistic relies on an asymptotic LRT approximation ($\Lambda \to f(\cdot;\hat\Psi)$ for large Phase I sample size), so performance may degrade when Phase I is small or mixture estimation is unstable. The approach assumes conditional multivariate normality of truncated score residuals and uses BIC-based model selection; misspecification (non-Gaussian errors, wrong K/covariance structure) could affect false-alarm control and detection. Practical implementation requires MFPC truncation choices (e.g., FVE threshold) and EM initialization; sensitivity to these choices and to label-switching/identifiability issues may be nontrivial outside the studied settings.","The authors suggest extending the framework to nonlinear functional models, combining it with profile registration to handle profiles observed on different domains, and developing time-weighted monitoring strategies such as CUSUM and EWMA to improve sensitivity to small persistent shifts.","A valuable extension would be a formal small-sample calibration of control limits (e.g., bootstrap/parametric bootstrap under the fitted mixture) to reduce reliance on asymptotics and tuning-set splitting. Robust/nonparametric variants (e.g., heavy-tailed mixtures, rank-based scores) could improve resilience to outliers and non-Gaussian functional noise. Additional work on autocorrelated Phase II streams (e.g., serial dependence in profiles) and open-source implementation (e.g., an R package or funcharts extension) would improve practitioner adoption and reproducibility.",2410.20138v1,local_papers/arxiv/2410.20138v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:57:22Z
TRUE,Univariate|Other,CUSUM|GLR (Generalized Likelihood Ratio)|Change-point|Other,Phase II,Theoretical/simulation only,FALSE,FALSE,NA,Approximation methods|Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Expected detection delay|False alarm rate|Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a CUSUM sequential change-detection test with observation-adjusted (random, data-dependent) control limits, termed CUSUM-OAL, to improve sensitivity while maintaining a desired false-alarm level. The method replaces the conventional fixed CUSUM threshold with a decreasing function of a (full or sliding-window) average of the log-likelihood-ratio-type statistics, yielding adaptive thresholds that can even become negative. The authors derive large-threshold asymptotic approximations for in-control and out-of-control average run length (ARL) across regimes defined by the sign of $E_v(Z_1)$ (and equivalently Kullback–Leibler closeness to pre- vs post-change reference), giving exponential/square/linear growth forms. They also prove bounded ARL behavior in certain cases when the adaptive limit can become negative, contrasting with standard fixed-limit schemes. Monte Carlo simulations on extremely heavy-tailed Pareto-type sequences (tail index $\alpha\in(0,1)$) show substantially reduced out-of-control ARLs for detecting changes in $\alpha$ compared with a conventional CUSUM at matched ARL0, albeit with larger in-control run-length variability (SDRL).","The baseline CUSUM stopping time is $T_C(c)=\min\{n:\max_{1\le k\le n}\sum_{i=n-k+1}^n Z_i\ge c\}$ with $Z_i=\log\{p_{v1}(X_i)/p_{v0}(X_i)\}$. The proposed CUSUM-OAL replaces the constant limit by a data-dependent one: $T_C(cg)=\min\{n:\max_{1\le k\le n}\sum_{i=n-k+1}^n Z_i\ge c\,g(\hat Z_n)\}$ (and a variant using a sliding average $\hat Z_n(ac)=j^{-1}\sum_{i=n-j+1}^n Z_i$, $j=\min\{n,\lceil ac+1\rceil\}$). In the heavy-tail simulation, $X\sim f_\alpha(x)=\alpha x^{-(1+\alpha)}$ for $x\ge1$, giving $Z_k=\log(\alpha_1/\alpha_0)+(\alpha_0-\alpha_1)\log X_k$, and a linear decreasing adjustment $g_u(x)=1-u(x-\mu_0)$ for $x>\mu_0$ (else 1).","Simulations for Pareto-type data with $\alpha_0=0.90$ and matched in-control ARL0 show much smaller out-of-control ARLs for CUSUM-OAL than standard CUSUM. For example at ARL0≈300 and $\alpha_1=0.50$, conventional CUSUM has ARL≈15.28 (SDRL≈10.65) while CUSUM-OAL achieves ARL≈9.88 (SDRL≈10.99) for $u=0.5$ and ARL≈5.75 (SDRL≈9.41) for $u=5$; at $\alpha_1=0.30$, CUSUM ARL≈6.16 vs CUSUM-OAL ARL≈3.20 (u=0.5) and ≈2.07 (u=5). At ARL0≈500 and $\alpha_1=0.70$, CUSUM ARL≈65.40 vs CUSUM-OAL ≈53.05 (u=0.5) and ≈36.49 (u=5). The authors note CUSUM-OAL tends to have larger in-control run-length standard deviations than CUSUM.","The authors note that, while CUSUM-OAL yields much smaller out-of-control ARLs, “as a whole, the CUSUM-OAL tests have bigger standard deviations than the CUSUM test in the in-control state,” indicating greater in-control variability. They also emphasize that post-change distributions are usually unknown and motivate using a mixture/reference post-change distribution with a specified parameter region $V$ and prior $Q$ (implicitly limiting applicability to situations where such a set/prior is reasonable).","The main theoretical ARL results are asymptotic in a “large control limit $c$” regime; finite-sample/finite-threshold accuracy and calibration error are not fully characterized. The approach relies on specifying a reference post-change mixture distribution (choice of $V,Q$) and a tuning function $g(\cdot)$ (and parameter $u$ in simulations), but provides limited practical guidance for selecting these robustly across applications. The development appears to assume independent observations and does not address robustness to autocorrelation, model misspecification, or nonstationarity beyond the change-point. No software implementation is provided, which may hinder adoption and reproducibility.",None stated.,"Develop data-driven or optimal selection rules for the adjustment function $g(\cdot)$ (and tuning parameters like $u$ and window length $j$) under constraints on ARL0, including robust designs under misspecified $V,Q$. Extend CUSUM-OAL to dependent/autocorrelated heavy-tailed sequences (e.g., ARMA/long-memory or conditional heteroskedastic models) and study steady-state ARL and conditional delay. Provide a self-starting/Phase I-to-Phase II implementation that estimates needed quantities online for heavy-tailed contexts. Release reference software (e.g., R/Python) and benchmark against alternative robust or nonparametric change-detection methods for heavy tails.",2411.14706v1,local_papers/arxiv/2411.14706v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:57:59Z
TRUE,Multivariate|Other,Hotelling T-squared|Other,Phase II,Manufacturing (general)|Other,TRUE,TRUE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|False alarm rate|Other,"Assumes Phase I provides a “sufficiently large” dataset to estimate VAR(p) parameters (µX, Φ1…Φp, Σε), but no general minimum is given. Numerical study uses subgroup sizes n = 3, 7, 15; steel example uses n = 4; chemical example uses Phase I grouped into m = 20 subgroups of size n = 5 (from 100 observations) and Phase II also uses n = 5.",TRUE,R,Not provided,NA,"The paper develops methodology for applying the Phase II Hotelling’s T² chart directly to multivariate autocorrelated and cross-correlated observations modeled by a VAR(p) process, instead of monitoring fitted residuals. It derives closed-form expressions for the variance–covariance matrix of the sample mean under VAR(1) and extends to general VAR(p) via a companion-form VAR(1) representation, enabling the T² statistic to retain correlation information in its control limits. Performance is evaluated using in-control and out-of-control average run length (ARL) and an additional head-to-head “first-to-signal” (firstness) criterion against a residual-based T² chart. Simulations show that ignoring autocorrelation/cross-correlation inflates false alarms, and that the proposed direct-observation T² chart yields consistently lower ARL1 (faster detection), especially for small shifts, and wins more often under first-to-signal comparisons. Two illustrations are provided: a steel sheet rolling thickness monitoring example (VAR(1)) and a chemical process monitoring viscosity/temperature example (VAR(3)) with Phase I model fitting and Phase II signalling.","The proposed Phase II monitoring statistic is Hotelling’s $T_t^2=(\bar{\mathbf X}_t-\boldsymbol\mu_0)^\top\,\Sigma_{\bar X}^{-1}(\bar{\mathbf X}_t-\boldsymbol\mu_0)$, where $\Sigma_{\bar X}$ is the variance–covariance matrix of the sample mean under the assumed VAR(p) dependence (derived in closed form for VAR(1) and via companion-form for VAR(p)). Under in-control conditions, $T_t^2\sim\chi^2_v$ and the UCL is set as $\chi^2_v(\alpha)$; under a mean shift, $T_t^2$ follows a noncentral $\chi^2$ with noncentrality $d=\boldsymbol\delta^\top\Sigma_{\bar X}^{-1}\boldsymbol\delta$. For comparison, the residual-based approach charts averaged residuals with $T_t^2=\bar{\mathbf e}_t^\top\Sigma_{\bar e}^{-1}\bar{\mathbf e}_t$ and $\Sigma_{\bar e}=\Sigma_\varepsilon/n$.","A simulation study (target ARL0 = 370, i.e., α = 0.0027) shows that designing the classical i.i.d. T² chart while monitoring VAR(1) data can drastically reduce the achieved ARL0 (increased false alarms) as autocorrelation/cross-correlation parameters increase. In the ARL1 comparisons, the direct-observation T² chart is uniformly faster than the residual-based T² chart across multiple dependence settings and subgroup sizes; e.g., in one highlighted setting (Case IV, v=2, δ=1.0), ARL1 for the proposed vs residual-based chart is 95.4 vs 227.9 (n=3), 64.7 vs 121.2 (n=7), and 33.8 vs 49.4 (n=15). First-to-signal simulations (10,000 replications) show the proposed chart signals before the residual-based chart more often across scenarios; e.g., in the same Case IV with δ=1.0 and n=3, the proposed chart signals first 67.7% of the time vs 27.4% for the residual-based chart (4.9% simultaneous). Case studies demonstrate Phase II signalling on steel rolling data (detected at the 2nd and 17th OOC samples in the illustration) and on chemical process data (Phase II signals at inspections #7 and #10 after an artificial mean shift).","The method is presented for Phase II monitoring assuming a perfect model fit and accurate parameter estimation from Phase I, with all VAR(p) parameters treated as known during Phase II. The authors note that developing Phase I monitoring for VAR(p) processes and studying the impact of estimation error and model misspecification on Phase II performance are not addressed. They also state that their design is statistical (ARL-based) and does not incorporate economic considerations such as sampling and inspection costs.","The approach relies on multivariate normality and correct VAR(p) specification; robustness to heavy tails, outliers, or nonlinearity is not studied. The key assumption that consecutive subgroups (inspections) are approximately independent may be difficult to guarantee in high-frequency sensing contexts and is not stress-tested. Comparisons are mainly against a residual-based T² chart under “perfect model fit,” but broader benchmarking versus alternative multivariate time-series monitoring methods (e.g., MEWMA/MCUSUM/GLR for VAR, robust/dynamic PCA-based charts) is limited. Practical guidance on selecting subgroup size n and inspection spacing to ensure independence of T² statistics is illustrated but not fully developed into general recommendations.","They propose extending the work to Phase I monitoring for VAR(p) processes. They also suggest evaluating how Phase I estimation error and model misspecification affect Phase II performance for both direct-observation and residual-based approaches. Finally, they recommend exploring economical-statistical design of the T² chart for VAR(p) processes to incorporate sampling/inspection costs.","Develop robust/nonparametric versions of the proposed VAR(p)-based T² chart for non-normal or outlier-contaminated data and assess sensitivity to violations of stationarity. Extend the method to handle serial dependence between consecutive inspection samples (remove/relax the “independent subgroups” assumption) and to irregular sampling/missingness common in sensor networks. Provide data-driven procedures for choosing subgroup size n and sampling interval to optimize detection under dependence (possibly adaptive schemes). Release software (e.g., an R package) implementing covariance calculations for VAR(p) sample means and automated chart design/diagnostics to facilitate practitioner uptake.",2501.11649v1,local_papers/arxiv/2501.11649v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:58:40Z
TRUE,Univariate|Bayesian|Other,Shewhart|Other,Both,Transportation/logistics|Environmental monitoring|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|False alarm rate,"Case study uses 2,689 observations for Phase I parameter estimation and the last 100 observations for Phase II monitoring. No general Phase I sample-size guidance beyond this split is provided.",TRUE,R,Supplementary material (Journal/Publisher),https://www.kaggle.com/datasets/usdot/pipeline-accidents,"The paper proposes a risk-adjusted time-between-events monitoring scheme for repairable systems using a non-homogeneous Poisson process (NHPP) with time-varying intensity and covariates (via a Cox proportional-hazards-type multiplicative model). The monitored quantity is the average cost per unit time, defined as the ratio of total cost to time-between-events (AC = TC/TBE), with a two-sided probability-limits chart whose control limits are recomputed at each event (stepwise limits) conditional on the previous event time and current covariates. Dependence between TBE and cost is modeled using a copula (focused on the Gumbel copula in the main development), enabling simulation and numerical inversion to obtain control limits for the ratio statistic. Performance is evaluated primarily by in-control and out-of-control average run length (ARL), showing ARL-unbiasedness (ARL0 > ARL1) across studied shifts, and sensitivity improvements when dependence is stronger and when risk factors are properly accounted for. A real-data case study monitors U.S. oil pipeline accidents (PHMSA/Kaggle), estimating parameters in Phase I and applying the chart in Phase II to flag shifts in average cost per time unit, with additional separate charts used diagnostically for TBE and cost.","Failures follow an NHPP with intensity \(\lambda(t)\) (power-law \(\lambda(t)=\gamma\eta t^{\eta-1}\) or log-linear \(\lambda(t)=\exp(\gamma+\eta t)\)), and risk adjustment uses \(\lambda(t\mid z_t)=\lambda(t)\exp(\beta' z_t)\) with mean function \(\Lambda(t\mid z_t)=\Lambda(t)\exp(\beta' z_t)\). The monitoring statistic is \(W_i=Y_i/X_i\) (cost per time unit), with conditional CDF \(F_{W_i}(w\mid t_{i-1},z_i)=\int_0^\infty\int_0^{xw} f_{X_i,Y_i\mid t_{i-1},z_i}(x,y)\,dy\,dx\) where \(f_{X,Y}\) is built from marginals and copula density. Two-sided probability limits are event-specific: \(\mathrm{LCL}_i=F_{W_i}^{-1}(\alpha/2\mid t_{i-1},z_i,\theta_0)\), \(\mathrm{UCL}_i=F_{W_i}^{-1}(1-\alpha/2\mid t_{i-1},z_i,\theta_0)\), solved numerically/simulation.","The numerical study targets ARL0 = 200 (\(\alpha=0.005\)) and reports ARL1 via Monte Carlo under shifts in NHPP parameters (\(\gamma,\eta\)) and cost parameter (exponential rate \(\mu\)); across studied scenarios the chart is reported as ARL-unbiased (ARL0 > ARL1). Sensitivity depends on the sign/magnitude of the covariate coefficient \(\beta\) and the dependence level: ARL1 decreases (faster detection) as Kendall’s \(\tau\) increases (stronger positive dependence between TBE and cost). In a comparison against an NHPP TBE chart that ignores risk factors (Ali, 2021), the alternative can show smaller ARL1 in some cases but suffers inflated false alarms (e.g., reported IC ARL about 187.56 for \(\beta=0.50\) and 28.65 for \(\beta=2.00\) vs nominal 200). In the pipeline case study, Phase II monitoring of the last 100 events flags multiple out-of-control points indicating a potential decrease (improvement) in AC, and diagnostic separate charts suggest the shift is driven mainly by decreased total cost rather than TBE changes.","The paper notes a known drawback of monitoring a ratio/combined characteristic: when the AC chart signals, it does not directly identify whether the shift arose from TBE, cost, or both; the authors recommend supplementary individual charts for diagnosis. In simulations they state they focus on shifts in \(\gamma,\eta\), and \(\mu\) while keeping the risk-model coefficients \(\beta\) and copula dependence parameter fixed, implying performance under shifts in \(\beta\) or dependence is not studied.","Control limits require numerical inversion of the ratio CDF and are implemented via simulation/quantiles at each event, which may be computationally heavy in real-time settings or for many covariates without efficient software. The modeling relies on NHPP independent increments and a specified copula family (mainly Gumbel/positive dependence), so robustness to serial dependence beyond NHPP, model misspecification, negative dependence, or changing dependence over time is unclear. Phase I estimation uses a large historical sample in the case study; performance with smaller Phase I samples, estimation error, and parameter drift (re-estimation frequency) is not systematically evaluated.",None stated.,"Extend the framework to allow time-varying or adaptive dependence (copula parameters changing over time) and to handle negative dependence structures by broadening copula families. Develop self-starting/adaptive versions that update parameters online and quantify the impact of Phase I estimation error on ARL and false alarm rates. Provide computationally efficient implementations (e.g., analytic approximations for \(F_{W_i}\) or faster simulation) and broader real-world validations across other repairable-system domains (e.g., healthcare, manufacturing equipment).",2501.11809v1,local_papers/arxiv/2501.11809v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-26T23:59:25Z
TRUE,Univariate|Other,Shewhart,Both,Manufacturing (general)|Environmental monitoring|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Detection probability|False alarm rate|Other,"Simulation suggests large Phase I samples are needed to reduce estimation effects: variability in conditional IC ARL remains substantial unless m is very large (e.g., m>500; SDARL ~5–10% of nominal ARL0 only around m≥5000). Authors recommend avoiding very small Phase I samples (e.g., m<100) due to high variability; note conclusions mention at least ~2000 for significantly reduced variability.",TRUE,R,Upon request,https://www.R-project.org/|https://CRAN.R-project.org/package=VGAM,"The paper studies a two-sided Shewhart-type chart for individual observations (the SHK-chart) when the monitored quality characteristic is a continuous proportion in (0,1) modeled by a Kumaraswamy distribution. It focuses on the impact of estimating the in-control distribution parameters from a Phase I sample, evaluating conditional in-control performance via the conditional false alarm probability and the resulting conditional in-control ARL (CARL0) distribution. Using Monte Carlo simulation, it quantifies practitioner-to-practitioner variability across Phase I samples and shows that plug-in probability limits can yield substantial dispersion in CARL0 unless Phase I is very large. Two control-limit adjustment strategies are proposed: (A) tuning the nominal false alarm rate to match the unconditional mean ARL (AARL) to the target, and (B) tuning the false alarm rate to satisfy an exceedance-probability criterion limiting the fraction of practitioners with CARL0 below a threshold. The adjusted designs are compared for out-of-control performance under separate shifts in each Kumaraswamy shape parameter, and the methods are illustrated with a simulated manufacturing example and a real relative-humidity dataset.","The SHK-chart uses equal-tail probability limits based on the Kumaraswamy inverse CDF: $\mathrm{LCL}=F_K^{-1}(\alpha/2;\theta_{01},\theta_{02})$, $\mathrm{UCL}=F_K^{-1}(1-\alpha/2;\theta_{01},\theta_{02})$; with unknown parameters, plug-in limits substitute MLEs $\hat\theta_1,\hat\theta_2$. Conditional false alarm probability under estimated limits is $\alpha_{\hat\theta\mid\theta_0}=1-F_K(\widehat{\mathrm{UCL}};\theta_{01},\theta_{02})+F_K(\widehat{\mathrm{LCL}};\theta_{01},\theta_{02})$, yielding conditional IC ARL $\mathrm{CARL}_0=(\alpha_{\hat\theta\mid\theta_0})^{-1}$. Out-of-control ARL in the known-parameter case is $\mathrm{ARL}_1=\{1-F_K(\mathrm{UCL};\theta_{11},\theta_{12})+F_K(\mathrm{LCL};\theta_{11},\theta_{12})\}^{-1}$.","Monte Carlo results (N=25,000) show plug-in limits can produce highly variable CARL0 for moderate Phase I sizes; e.g., for nominal $\alpha=0.0027$ (target ARL0=370.4) and m=100, AARL is about 415–421 with SDARL about 331–345 across several IC parameter scenarios, and ~58% of practitioners have CARL0 below 370.4. Even at m=500, about ~50–54% of charts still have CARL0 below the nominal target, though dispersion decreases; at m=5000, the 25th percentile of CARL0 is ~346 and AARL ~371–372. Adjustment A (matching AARL within ~5%) requires a higher FAR than nominal (e.g., for m=100, $\alpha'\approx0.00291$), but does not reduce the fraction of practitioners with CARL0 below target (often still >50%). Adjustment B (exceedance-probability control) requires much smaller FAR values (e.g., for m=100, $\alpha''\approx0.00049$–0.00055 for p=0.05, eps=0), which greatly increases in-control protection but widens limits and substantially worsens detection (larger OOC AARL) for small-to-moderate shifts.","The paper notes that there is no universally optimal way to set limits with estimated parameters because different adjustments trade off guaranteed in-control performance against out-of-control detection, so practitioners must choose based on needs. It also reports that the SHK-chart with equal-tail probability limits struggles to detect increasing shifts in one parameter (notably for $\delta_2\in(1,2]$), suggesting the design may be inadequate for some shift directions. The authors indicate that addressing this (e.g., via ARL-unbiased design) is left for future research.","The study is restricted to a specific distributional model (Kumaraswamy) and a Shewhart probability-limits chart for individual observations; robustness to model misspecification (e.g., beta-like alternatives, mixtures, boundary inflation near 0/1) is not assessed. Dependence/serial correlation and non-iid sampling—common in environmental and industrial time series—are not modeled, which can materially alter false alarm behavior. The adjustment methods rely on extensive simulation to calibrate FAR for each m and IC parameter setting; practicality for routine deployment without provided software (and without a general calibration formula) may be limited. The exceedance-based adjustment can yield extremely small FARs and huge AARLs for small m, which could be impractical in real monitoring due to very slow detection and unclear guidance on choosing p and tolerance eps.","The authors suggest extending the investigation to settings with rational subgroups of size $n\ge2$ rather than individual observations, both with known and unknown parameters. They also propose studying simultaneous shifts in parameters and developing follow-up procedures to identify which parameter(s) changed. Additionally, they mention that an ARL-unbiased design may address poor detection for certain increasing shifts, but leave this as future work.","Developing analytic/approximate calibration methods (e.g., asymptotic or bootstrap-based corrections) for FAR adjustments could reduce reliance on large Monte Carlo runs for each scenario. Robust or nonparametric variants (or model-checking diagnostics) could improve performance under distributional misspecification and boundary effects typical of proportion data. Extending to autocorrelated/seasonal processes (especially for environmental RH data) and comparing against time-series residual charts or state-space approaches would improve real-world applicability. Providing an open-source R implementation (functions to fit, set limits, and calibrate adj-A/adj-B) and a standardized benchmarking study against beta-regression charts and other bounded-data SPC methods would strengthen adoption and comparative evidence.",2502.02296v1,local_papers/arxiv/2502.02296v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:00:14Z
TRUE,Univariate|Nonparametric|Other,Shewhart|Machine learning-based|Other,Phase II,Manufacturing (general)|Theoretical/simulation only,FALSE,FALSE,NA,Simulation study|Other,ARL (Average Run Length),"Design/evaluation uses subgroup sizes n = 20 and n = 50; control limits C are computed separately for these n values (UO: C=162 for n=20, C=623 for n=50; TO: C=64 for n=20, C=231 for n=50). No general Phase I sample-size guidance is provided.",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Shewhart Signed-Rank Control Chart (SS-RCC) based on the Wilcoxon signed-rank statistic to monitor processes when the data distribution is unknown and when measurement rounding creates tied observations. For untied observations, the signed-rank statistic’s distribution is approximated with a normal distribution and symmetric control limits (−C, C) are selected to achieve a desired false-alarm level (e.g., 99.73% in-control coverage). For tied observations, zeros are induced in the sign statistic due to device resolution; ties are removed and the resulting statistic with random effective sample size N is modeled using an adjusted “Scaled-Normal Distribution” (SND) that modifies height, location, and width via parameters a, b, c. A deep learning regression model (multi-layer fully connected network) is trained to estimate (a,b,c) from moments/skewness/kurtosis-related inputs for n=20 and n=50 under out-of-control settings. Performance is illustrated by computing ARL0/ARL1 under multiple symmetric Johnson-type distribution benchmarks and varying standardized resolution (ties) and mean shifts, showing increased sensitivity (lower ARL1) when ties are present in their modeled scenarios.","For untied observations, define $S_{t,k}=\mathrm{sign}(X_{t,k}-\nu_0)\in\{-1,+1\}$ and the signed-rank statistic $SR_t=\sum_{k=1}^n k\,S_{t,k}$ (support step size 2). Approximate $F_{SR_t}(s)\approx \Phi\big((s+0.5-m_1)/\sqrt{\mu_2}\big)$ and compute $f_{SR_t}(s)=F(s)-F(s-2)$; control limits are symmetric $(\mathrm{LCL},\mathrm{UCL})=(-C,C)$ with $C$ chosen numerically to meet a target $\alpha$. With ties, define $S'_{t,k}\in\{-1,0,+1\}$ with probabilities $(p_{-1},p_0,p_{+1})$, remove zeros so $SR_{t,N}=\sum_{k=1}^N k\,S'_{t,k}$ where $N\sim\mathrm{Binomial}(n,1-p_0)$, and approximate its discrete distribution by an adjusted scaled-normal form (SND) parameterized by $(a,b,c)$ fitted/estimated via deep learning.","Using a 99.73% in-control confidence level, the paper reports control-limit constants for the signed-rank chart: for untied observations, $C=162$ (n=20) and $C=623$ (n=50); for tied observations, $C=64$ (n=20) and $C=231$ (n=50). Simulation density comparisons for UOs use 1,000,000 repetitions, showing the signed-rank statistic is well approximated by a normal distribution when there are no ties. For TO out-of-control cases, the DL model is trained on 288 samples (11 inputs → 3 outputs) and achieves RMSE=0.0761 with test-set $R^2$ values of 0.9997 (a), 0.9965 (b), and 0.9963 (c), supporting accurate reconstruction of the SND used to compute ARLs. Reported ARL tables (for n=20 and n=50) indicate much smaller ARL1 for small shifts (e.g., δ=±0.1) in tied-observation scenarios than in untied scenarios, implying faster detection when ties are present in their modeled setting.","The deep learning model is constructed only for tied-observation out-of-control scenarios and only for sample sizes n = 20 and n = 50; extending it requires adding data for additional scenarios to the training dataset. The paper also notes that selecting/optimizing a suitable DL model can be challenging and may require additional tuning methods (e.g., Bayesian optimization, regularization).","The chart’s design and ARL evaluation appear to rely on approximations (normal approximation for UOs and the proposed SND for TOs) and may be sensitive to how well these approximations hold outside the simulated Johnson-family benchmarks. The DL-based estimation of (a,b,c) is trained on a relatively small synthetic dataset (288 points) with manually chosen targets for (a,b,c), which may limit reproducibility and generalization to other n values, tie mechanisms, or real processes. No real industrial case study is provided to validate performance under practical complications (autocorrelation, drift, nonstationarity, estimation of \nu_0/\sigma). Comparisons against established nonparametric charts for ties/rounding (or robust alternatives) are not clearly benchmarked beyond the paper’s own approximations.","The authors state that to expand the deep learning model beyond the presented conditions (n=20,50 and TOs under OOC), one must incorporate the relevant values associated with additional scenarios into the main dataset so the model can learn to estimate the SND parameters under those expanded conditions.","Provide an open-source implementation and a fully specified procedure for generating training labels (a,b,c) to ensure the DL component is reproducible. Extend the method to additional subgroup sizes and to self-starting/estimated-parameter settings (Phase I–II) where \nu_0 and \sigma are unknown. Evaluate robustness under autocorrelated observations and under alternative rounding/measurement-error models. Benchmark against other nonparametric and robust SPC charts (e.g., rank-based or sign-based alternatives) using standardized simulation suites and at least one real manufacturing dataset.",2503.20131v1,local_papers/arxiv/2503.20131v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:01:01Z
TRUE,Multivariate|Profile monitoring|Functional data analysis|Nonparametric,Hotelling T-squared|Other,Phase II,Transportation/logistics,NA,FALSE,FALSE,Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,"Phase I/reference sample used for training+tuning; simulation uses 2000 IC observations split equally (1000/1000) and 500 OC observations. Case study uses 708 Phase I observations split equally into training and tuning sets, and 552 Phase II observations.",TRUE,R,Package registry (CRAN/PyPI)|Supplementary material (Journal/Publisher),https://doi.org/10.1080/00401706.2025.2491369,"The paper proposes the Adaptive Multivariate Functional Control Chart (AMFCC) for Phase II monitoring and diagnosis of multivariate functional quality characteristics (multichannel profiles). It smooths noisy discrete measurements via roughness-penalized spline fitting, extracts features through multivariate functional PCA, and constructs Hotelling $T^2$-type statistics for multiple parameter settings (smoothing level and number of retained components). AMFCC adapts to unknown out-of-control conditions by converting each partial statistic to a p-value (estimated from the in-control reference sample) and combining p-values across parameter combinations using nonparametric combination rules (e.g., Fisher omnibus or Tippett). A post-signal diagnostic method combines component-wise contribution measures across parameter settings to identify which profile components drive an out-of-control signal. Extensive Monte Carlo studies and a resistance spot welding case study show higher true detection rates than competing multivariate functional control charts using fixed parameter choices.","Observed data are $Y_{ik}(t_{ikj})=X_{ik}(t_{ikj})+\varepsilon_{ikj}$ and profiles are estimated by minimizing a multivariate penalized weighted least squares criterion with roughness penalty (Eq. 2), using spline/basis expansion (Eqs. 3–5). For monitoring, a Hotelling $T^2$-type statistic $T^2_{i;\lambda,L}$ is computed using standardized smoothed profiles and an inverse-covariance operator approximated by the first $L$ multivariate functional PCs: $\hat K^*_{k_1k_2;\lambda,L}(s,t)=\sum_{l=1}^L \hat\eta_{l,\lambda}^{-1}\hat\psi_{lk_1,\lambda}(s)\hat\psi_{lk_2,\lambda}(t)$ (Eqs. 10–11). Each partial test is mapped to a p-value (estimated empirically from IC data, Eq. 17) and combined into an overall statistic $T^2_i=\Theta(p_1,\ldots,p_T)$ (Eq. 14), with examples Fisher $-2\sum\log p_t$ (Eq. 15) and Tippett $-2\log(\min p_t)$ (Eq. 16).","In simulations (two covariance scenarios; $p=5$; four mean-shift shapes and four severities), AMFCC variants (Fisher/Tippett combinations) achieve higher true detection rate while maintaining false alarm rate near the nominal $\alpha=0.05$ compared with MFCC methods that select $L$ by explained variance (70/80/90%), and non-functional Hotelling charts on averages or raw discretizations. In the automotive resistance spot welding case study (10 dynamic resistance curves per item), AMFCC attains substantially higher detection performance than competitors: TDR 0.788 (AMFCCF) and 0.779 (AMFCCT) vs 0.712 (best MFCC07), 0.520 (MCC), and 0.275 (DCC). For diagnostics, AMFCC shows higher component-wise detection: cTDR 0.392 (AMFCCF) and 0.445 (AMFCCT) vs 0.318 (MFCC07) and lower for MFCC08/09. Bootstrap confidence intervals reported in the paper place AMFCC’s TDR/cTDR strictly above competing approaches in the case study.",None stated.,"The method’s in-control calibration relies on an empirical p-value estimator and control limits learned from a Phase I reference sample (often split into training/tuning), so performance may degrade with smaller or contaminated Phase I data. The approach is computationally heavier than fixed-parameter charts because it fits smoothing/MFPCA and computes partial statistics over a grid of $(\lambda,L)$ combinations. The framework is developed and evaluated mainly for mean-shift alternatives; performance for covariance/variance shifts or other distributional changes is not the central focus.",The authors suggest extending the adaptive-combination idea to functional real-time monitoring and to settings with additional covariate information (functional regression monitoring). They also propose incorporating prior knowledge about likely out-of-control conditions to further improve AMFCC.,"Develop self-starting or streaming variants that update the in-control model and p-value calibration online without a large Phase I sample. Study robustness to autocorrelation/serial dependence and irregular sampling/missingness common in sensor streams. Provide broader benchmarks (e.g., steady-state performance, detection delay/ATS) and software defaults for selecting the $(\lambda,\delta)$ grids to balance power and computation.",2504.09684v1,local_papers/arxiv/2504.09684v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:01:41Z
TRUE,Univariate|Nonparametric|Other,EWMA|Change-point|Other,Both,Environmental monitoring,NA,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Other,Phase I uses 517 SPEI-12 observations (47 drought events) for SPEI-12 and 623 SPEI-24 observations (40 drought events) for SPEI-24 to estimate in-control medians/means and set charts to ARL0=370 (α0=0.0027).,TRUE,R,Not provided,https://spei.csic.es/database.html,"The paper proposes using distribution-free Time Between Events and Amplitude (TBEA) control-charting methods to monitor changes in drought characteristics (frequency/time between drought events and their severity/amplitude) derived from SPEI drought index data. It studies two nonparametric sequential change-point control charts—based on Mann–Whitney (location change) and Kolmogorov–Smirnov (arbitrary distribution change) statistics—applied to a TBEA ratio statistic based on normalized amplitude and inter-event time, and compares them to a distribution-free upper-sided EWMA TBEA chart (Wu et al., 2021) based on sign/“continuousified” statistics. A case study on Bologna SPEI-12 (and an additional SPEI-24 analysis) estimates in-control parameters from a Phase I period and then performs Phase II monitoring, with all charts tuned to ARL0=370 (α0=0.0027). Simulation is used to assess in-control false-alarm rates of the change-point charts (since they are not originally designed for TBEA) and to compare out-of-control performance (ARL1/SDRL and missed-alarm rates) under mean shifts in the underlying SPEI process. Results show broadly consistent evidence of worsening drought conditions over time, with change points around early/mid-2000s onward and comparable detection performance across EWMA and CP approaches.","TBEA EWMA sign-based chart: define sign statistics $ST_i=\mathrm{sign}(\theta_T-T_i)$ and $SX_i=\mathrm{sign}(X_i-\theta_X)$, then $S_i=(SX_i-ST_i)/2\in\{-1,0,1\}$. “Continuousify” $S_i$ to $S_i^*$ via a mixture of $N(-1,\sigma)$, $N(0,\sigma)$, $N(1,\sigma)$ depending on $S_i$, and compute upper-sided EWMA $Z_i^*=\max\{0,\lambda S_i^*+(1-\lambda)Z_{i-1}^*\}$ with $UCL=K\sqrt{\lambda(\sigma^2+0.5)/(2-\lambda)}$. Change-point monitoring uses normalized variables $T'=T/\mu_{T0}$ and $X'=X/\mu_{X0}$ and monitors the ratio $Z_R=X'/T'$ with sequential nonparametric CP tests: MW uses the Mann–Whitney U-statistic standardized to $T_{k,n}$ and signals when $T_{\max,n}=\max_{1\le k\le n-1}T_{k,n}$ exceeds a simulated threshold; KS uses $D_{k,t}=\sup_z|\hat F_{S_1}(z)-\hat F_{S_2}(z)|$ (standardized) compared to Monte Carlo thresholds for a target ARL0.","Charts are tuned to ARL0=370 (α0=0.0027); Phase I SPEI-12 window (517 observations) yields 47 drought events and estimates $\hat\theta_{T0}=1$, $\hat\theta_{X0}=1.2263$, $\hat\mu_{T0}=10.9149$, $\hat\mu_{X0}=1.2511$; EWMA design uses $\lambda=0.07$, $K=2.515$, $\sigma=0.125$. In a simulation mimicking the in-control case, empirical false-alarm rates for CP charts were $\hat\alpha_0=0.00222$ (KS) and $0.00182$ (MW), not exceeding the theoretical 0.0027. Out-of-control simulation (δ = −0.5, −1, −1.5; Gaussian mean shift) shows comparable ARL1 across methods: for δ=−0.5, ARL1≈23.38 (EWMA), 24.53 (KS), 26.19 (MW) with missed-alarm rates 0.31%, 1.64%, 1.45%; for δ=−1, ARL1≈9.85 (EWMA), 9.19 (KS), 9.05 (MW); for δ=−1.5, ARL1≈7.51 (EWMA), 6.64 (KS), 6.43 (MW (with KS/MW having larger SDRL). Case-study results suggest worsening drought conditions with key increases/change points around Aug 2003 and Oct 2011 (SPEI-12; KS/MW) and around Oct 2007/Oct 2016/Jun 2019 for SPEI-24 (KS), with substantial agreement between KS change points and EWMA lower turning points (event coincidence analysis yields significant p-values, e.g., ΔT=0 gives p-value 0.000).","The author notes the change-point control charts (MW/KS) are not specifically designed for TBEA monitoring, so their false-alarm rates may deviate from theoretical values; this motivates an in-control simulation study to validate empirical false-alarm rates. The study is explicitly described as non-exhaustive in terms of methods considered. The analysis uses data from a single geographic location (Bologna), limiting generalizability.","The CP approach monitors a derived ratio $Z_R=X'/T'$; changes in $X$ and $T$ can offset each other, potentially masking practically important changes in one component (severity vs. frequency) and complicating interpretation/diagnosis. Independence between successive events/ratios is effectively assumed for CP modeling; drought/event sequences may exhibit temporal dependence and seasonality, which could affect thresholds and run-length properties. Comparisons focus on a small set of charts; other modern distribution-free/robust TBEA or joint monitoring approaches (e.g., multivariate nonparametric schemes or bootstrap-calibrated limits) are not benchmarked.","The paper suggests extending the analysis to other geographical areas beyond Bologna. It also proposes investigating change-point control charts based on other statistics, such as Lepage and Cramér–von Mises control charts (as available in Ross’s cpm framework).","Develop explicit TBEA-tailored calibration for nonparametric CP charts (e.g., event-based ARL/ATS definitions, steady-state performance, and thresholds that account for discreteness and varying event rates) rather than relying on generic CP thresholds. Extend to dependence-aware methods (seasonality/autocorrelation in SPEI, clustering of drought events) using block resampling or time-series CP/SPC frameworks. Provide open-source, reproducible code/workflows (including SPEI retrieval, event definition, and chart calibration) to facilitate adoption by climate and water-resource practitioners.",2506.21970v1,local_papers/arxiv/2506.21970v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:02:26Z
TRUE,Multivariate|Other,Hotelling T-squared|Shewhart,Phase II,Energy/utilities|Theoretical/simulation only,FALSE,NA,FALSE,Simulation study|Other,Detection probability|Other,"Monitoring/identification uses batches of data: in the case study they collect one-week input–output datasets (sampling time τ = 5 minutes) for each new operating condition; monitoring datasets are collected periodically (e.g., daily tests and a two-day test). No general Phase I SPC sample-size guidance is provided.",TRUE,Python|MATLAB,Not provided,https://readingraphics.com/book-summary-thinking-fast-and-slow|https://vbn.aau.dk/en/publications/heat-roadmap-europe-4-quantifying-the-impact-of-low-carbon-heatin,"The paper proposes a two-fold data-based model adaptation architecture: a slow, event-triggered ensemble learner to handle out-of-domain uncertainty and a fast online Gaussian-process (GP) corrector to handle in-domain uncertainty. The slow component combines multiple offline-trained models using input-dependent weights based on statistical proximity (Mahalanobis distance) between the current input and each model’s training inputs, and it uses a Hotelling T² control-chart monitoring scheme to detect when the ensemble becomes unreliable and a new operating condition has appeared. Control limits are set empirically via percentiles (typical choice 99.73%), acknowledging that normality is rarely satisfied in practice. The fast component trains GP NARX models online on a sliding window of recent samples to estimate and compensate the slow model’s prediction error in real time, limiting data to control GP complexity. The approach is demonstrated in simulation on a district heating system digital twin; results show improved predictive fit when combining slow+fast learning compared with single models, arithmetic averaging, or a standalone online GP.","The monitored statistic is the Mahalanobis distance (used as Hotelling T²): $T^2(\mathbf z_e,\mathbf z)=\{(z_e(k)-\mu_z)^\top\Sigma_z^{-1}(z_e(k)-\mu_z)\}_{k\in I_e}$ with $\mu_z=\frac{1}{|I|}\sum_{k\in I}z(k)$ and $\Sigma_z=\frac{1}{|I|-1}\sum_{k\in I}(z(k)-\mu_z)(z(k)-\mu_z)^\top$. Empirical control limits are $\mathrm{LCL}=0$ and $\mathrm{UCL}=p_j$ where $P_e(T^2\le p_j)=j/100$ (typical $j=99.73$). The ensemble weighting uses $\lambda^{[i]}(u)=\frac{w^{[i]}(u)}{\sum_{i=1}^n w^{[i]}(u)}$ with $w^{[i]}(u)=1/T^2(u,u^{[i]})$, and the final prediction is $y= y_s+\hat e_s$.","In the district-heating simulation, the benchmark Hotelling T² control limits reported include $\mathrm{UCL}_e=187.5$ for the initial single-model error chart, later updated to $\mathrm{UCL}_e=1027.2$ for the ensemble, and input-chart limits $\mathrm{UCL}^{[1]}_u=59.1$ and $\mathrm{UCL}^{[2]}_u=37.3$. Model-fit (FIT) over a two-day test improves from 69.5% for the slow ensemble $M_s$ to 94.2% for the combined slow+fast model $M$; single models achieve 48.6% and 42.0%, arithmetic averaging 54.1%, and a standalone online GP 61.4%. The fast-learning GP is run at τ=5 minutes with regression horizons $n_{r,e}=n_{r,y}=4$ and sliding-window training limits $k_{\min}=25$, $k_{\max}=300$; average online correction computation time is reported as about 1.4 seconds.","The authors note that the normality assumption is rarely satisfied in real-world processes, motivating empirical (percentile-based) control limits rather than theoretical χ²/F limits. They also state that GP regression complexity grows with dataset size, so they cap the number of training samples (discarding oldest data) and mention that more advanced approximations (e.g., Nyström, finite-dimensional kernel representations) are out of scope.","The control-chart monitoring relies on empirical percentile thresholds without a detailed false-alarm calibration (e.g., in-control ARL) or formal run-length analysis, so signaling properties may be hard to compare across settings. The monitoring scheme uses batches and assumes operating conditions remain within a range for periods; performance under rapidly switching regimes or strong serial dependence is not analyzed. The case study is simulation/digital-twin based; evidence on noisy real sensor data, missing data, and model drift in practice is not provided.","Future work will establish theoretical properties when the architecture is integrated into a model predictive control (MPC) framework, including addressing the exploration–exploitation trade-off with safety guarantees.","Developing formal performance guarantees for the control-chart monitor (e.g., ARL/ATS under dependence and non-Gaussianity) and robust threshold-selection procedures would strengthen the SPC component. Extensions to handle autocorrelated residuals explicitly (e.g., residual modeling or charting filtered innovations) and missing/irregular data would improve practical deployability. Providing an open-source reference implementation (Python/MATLAB) and additional real-world case studies would aid adoption and reproducibility.",2507.12187v1,local_papers/arxiv/2507.12187v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:03:03Z
FALSE,Other,CUSUM|Machine learning-based|Other,Phase II,Transportation/logistics|Other|Theoretical/simulation only,NA,NA,FALSE,Simulation study|Other,False alarm rate|Detection probability|Expected detection delay|Other,Not discussed.,TRUE,Python|Other,Not provided,https://arxiv.org/abs/2507.13022|https://github.com/tsfresh/tsfresh|https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/|https://www.sciencedirect.com/science/article/pii/S0196890410005273|https://www.sciencedirect.com/science/article/pii/S1568494621006724|http://jmlr.org/papers/v9/vandermaaten08a.html,"The paper proposes a data-driven fault detection and diagnosis (FDD) pipeline for the electrical valve actuation system of a reusable space launcher engine, emphasizing operational requirements such as confidence estimation, out-of-distribution (OOD) detection, and false-alarm control. A temporal convolutional autoencoder (TCAE) is trained on nominal time-series windows to learn latent features and reconstruction residuals; these representations feed histogram-based gradient-boosting tree (HGBT) classifiers for binary fault detection and multiclass diagnosis, with post-hoc probability calibration (Platt/isotonic) to interpret outputs as confidence levels. To reduce false alarms, a CUSUM post-processing stage is applied to the calibrated fault probability, and threshold moving is used to address class imbalance in fault detection. OOD detection is implemented via inductive conformal anomaly detection on the autoencoder reconstruction error, with both window-level and trajectory-level decision rules (including a window-count threshold/CUSUM-like aggregation). Evaluation is performed on simulated valve-actuator trajectories; results show good diagnosis performance and strong OOD separation on synthetic OOD data, while fault detection is sensitive to prevalence shift and can suffer increased false positives and miscalibration under changing class ratios.","Fault-alarm suppression uses a CUSUM on the calibrated fault probability $x_i$: $C_i=\max\{0, C_{i-1}+(x_i-T_{fp}-k)\}$ with $C_0=0$, and a fault is triggered when $C_i>T_{cs}$ (or directly when $x_i>T_{fp}$ if CUSUM is disabled). OOD detection thresholds the autoencoder reconstruction error $e$ using inductive conformal anomaly detection: $\text{thr}_{ood}=\text{quantile}_{\lceil (n+1)(1-\alpha)\rceil/n}(e)$, and flags a window as OOD if $e>\text{thr}_{ood}$. TCAE depth selection references $L=\lceil \log_b(\frac{(T-1)(b-1)}{2(k-1)}+1)\rceil$ for dilation base $b$ and kernel size $k$.","Using the final configuration, trajectory-level fault detection achieved FPR 0.03 (test) and 0.06 (test2), with FNR 0.31 (test) and 0.34 (test2), accuracy 0.83/0.80 and precision 0.95/0.91 (test/test2), reflecting a precision–recall tradeoff. Diagnosis performance at trajectory level was high: accuracy 0.92 (test) and 0.94 (test2), with recall 0.93/0.94 and F1 0.92/0.93. OOD detection met the window-level target with FPR about 0.01 on test and 0.00 on test2, and recall 1.00 on both (for the generated synthetic OOD classes). The authors report that increasing the fault-probability threshold (e.g., $T_{fp}$ from 0.75 to 0.92) reduced false positives dramatically (21 to 1) but increased missed faults (FNR rising from ~30% to ~40%), illustrating a tuning tradeoff.","The authors note the evaluation is conducted on simulated data from a physical model, and that testing with real data is necessary to reach operational maturity and validate robustness. They highlight substantial class overlap among simulated fault classes, leading them to restrict experiments to a subset of separable classes (0, 16, 128, 511). They also acknowledge that prevalence shift between development and validation data causes performance degradation and strong miscalibration for the binary detector, and that mitigating it requires estimating deployment prevalence, which is challenging in their setting. For OOD, they caution that perfect sensitivity may be overestimated because synthetic OOD cases are clearly separated from in-distribution data and real OOD may be more similar.","The use of overlapping sliding windows creates strong temporal dependence among samples; this can invalidate assumptions behind calibration splits and conformal guarantees (exchangeability/independence) and may lead to optimistic uncertainty estimates. Applying a CUSUM to a classifier’s probability is heuristic; without SPC-style in-control modeling of the probability stream, formal false-alarm guarantees (ARL/ATS) are not established, and behavior under nonstationarity or drift is unclear. The pipeline is tuned on a limited subset of fault modes; scalability to many similar/overlapping faults and to changing operating regimes (domain shift beyond prevalence) may require additional representation learning or adaptive thresholds. Practical deployment details (computational footprint on an actual ECU, quantization, real-time constraints, and robustness to sensor dropouts/latency) are not fully demonstrated beyond reported GPU training/inference measurements.",They plan to transition from simulated to real data collected on an integrated test bench with electric motors driving real valves to validate the framework under realistic conditions. They propose exploring combined use of simulated and real data to improve training and evaluation. They also suggest investigating conformal prediction for confidence estimation in FDD because it may be more resilient to distribution shifts than traditional calibration techniques.,"Develop online/adaptive thresholding and calibration methods to handle prevalence and broader covariate shifts without needing explicit deployment prevalence estimates (e.g., test-time calibration, unsupervised drift detection, or Bayesian updating). Provide formal run-length/false-alarm analyses for the probability-CUSUM stage (e.g., ARL/ATS under in-control probability distributions) and compare to alternative sequential detectors (EWMA, Shiryaev–Roberts, GLR) on the same probability stream. Extend the approach to multivariate/heterogeneous operating regimes with explicit handling of autocorrelation and varying mission phases (phase-dependent models or hierarchical monitoring). Release reproducible code and benchmark datasets (or standardized synthetic generators) and evaluate against SPC/anomaly baselines beyond HGBT+AE (e.g., classical residual charts, likelihood-based change-point detection, and modern deep OOD methods) on real data.",2507.13022v1,local_papers/arxiv/2507.13022v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:03:45Z
FALSE,Other,Shewhart,Phase II,Healthcare/medical|Theoretical/simulation only,NA,FALSE,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate,Not discussed,TRUE,R,Public repository (GitHub/GitLab),NA,"The paper proposes deep learning survival prediction models (LSTM and CNN-LSTM) that incorporate learnable copula-based activation functions (Clayton, Gumbel, hybrid, and Clayton-ReLU) to better model nonlinear dependence in correlated, right-censored multivariate survival responses. Copula parameters are trained end-to-end (via a constrained/softplus parameterization) rather than fixed, aiming to capture lower- and upper-tail dependence. Performance is evaluated via standard prediction metrics (MSE/MAE, log-likelihood) and by monitoring residuals with Shewhart control charts using 2σ control limits, summarizing stability with Average Run Length (ARL). Evidence is provided through a Monte Carlo simulation study (Weibull survival times with exponential right-censoring, plus derived binary/ordinal responses) and a breast cancer METABRIC case study. Control charts are used as an auxiliary monitoring/diagnostic tool for model drift/stability rather than as the paper’s primary methodological contribution in SPC.","Residuals are defined as $R=Y-\hat{Y}$. Shewhart chart limits are set to $\text{UCL}=\bar{R}+2\sigma_R$ and $\text{LCL}=\bar{R}-2\sigma_R$, where $\bar{R}$ and $\sigma_R$ are the mean and empirical SD of residuals. The Average Run Length is computed as $\text{ARL}=1/P(\text{signal})$, where a signal occurs when a residual falls outside the control limits.","In simulation (Table 1), mean ARL values varied widely by model/response; for example CNN-LSTM Clayton-ReLU achieved mean ARL ≈ 38.72 (Response 1), 50.05 (Response 2), and 61.06 (Response 3), while CNN-LSTM ReLU reported mean ARL 95.10 for Response 2 but was described as unstable for binary/categorical responses in the Shewhart plots. In the METABRIC study (Table 3), CNN-LSTM Clayton reported mean ARL ≈ 29.11 (Response 1) and 34.09 (Response 2), whereas CNN-LSTM ReLU and CNN-LSTM Gumbel reported mean ARL ≈ 102.33 and 101.08 for Response 2, respectively (with noted instability concerns). The authors conclude that Clayton-ReLU (especially in CNN-LSTM) yields more stable residual behavior for binary/categorical responses compared with plain ReLU or Sigmoid activations. Overall, copula-based activations are presented as improving residual stability and reducing outliers in control charts relative to non-copula activations in several settings.","The authors note that the copula-based activation approach is currently “a functional surrogate for a more formal theory,” lacking theoretical guarantees (e.g., universal approximation, Lipschitz continuity, training stability). They also state that the framework is restricted to the bivariate case (two survival responses) due to computational constraints, and that extension to multivariate outputs would require constructions such as vine copulas.","The Shewhart chart/ARL component is used as a model-diagnostic overlay (residual monitoring) rather than developing or validating a new SPC charting method; thus SPC conclusions may not generalize to industrial process monitoring contexts. Using 2σ limits departs from conventional 3σ design without calibrating in-control ARL to a standard target, making ARL comparisons less interpretable as false-alarm control. Independence assumptions for residuals/signals (implicit in simple ARL interpretation) are not examined, and deep learning residuals may be autocorrelated or heteroskedastic. The METABRIC summary indicates a potential event-status coding anomaly (all zeros in a described subset), which could materially affect survival modeling and residual-based monitoring validity.","They propose extending the approach to competing-risks modeling for clustered survival data. They also suggest extending beyond the bivariate case using pair-copula constructions (e.g., C-vines/D-vines) to model higher-dimensional dependence structures and developing theoretical guarantees for copula-based activations (approximation properties, continuity, and training stability).","Provide calibrated SPC designs for residual monitoring (e.g., set limits to achieve a specified in-control ARL, study steady-state ARL) and assess robustness under residual autocorrelation and nonstationarity. Compare Shewhart-based monitoring with alternative charting methods (EWMA/CUSUM) for detecting gradual model drift in prediction errors. Add ablation studies isolating the benefit of copula activations from architecture/regularization choices and report uncertainty for ARL estimates (e.g., confidence intervals). Release an implementation with reproducible pipelines and clearer guidance on when residual charts are appropriate for censored outcomes and mixed response types.",2507.14641v1,local_papers/arxiv/2507.14641v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:04:27Z
TRUE,Multivariate|Profile monitoring|Other,EWMA|Hotelling T-squared|MEWMA|Machine learning-based|Other,Both,Healthcare/medical|Finance/economics|Theoretical/simulation only|Other,NA,FALSE,FALSE,Simulation study|Other,False alarm rate|Detection probability|Other,"Emphasizes that the two-sample CL approach requires a very large holdout set, especially for small false-alarm rates (e.g., for FAR α=0.001, the holdout size should be much larger than 1,000 and the paper notes it may need at least ~10,000). Proposed method uses the full training sample n for fitting and nested bootstrap for CL calibration (examples use n=2,000 and n=3,000).",TRUE,Python|Other,Not provided,NA,"The paper studies score-vector–based concept drift monitoring, where detecting changes in a supervised learning model’s predictive relationship is reduced to detecting changes in the mean of the Fisher score vector and monitored via a MEWMA chart with a Hotelling T^2 statistic. It keeps the monitoring statistic of Zhang et al. (2023) but proposes a new, data-efficient control-limit calibration method: a nested bootstrap that uses the full initial dataset for model fitting while still accounting for both Phase I model-estimation variability (outer bootstrap) and Phase II future-data variability (inner bootstrap). The authors show that a naïve nested bootstrap mis-calibrates the variability of the MEWMA statistic and derive a 0.632-like variance inflation correction, yielding time-varying control limits CL_i that control pointwise false-alarm probability at a desired level α. Through simulations on a mixed-linear regression example and a nonlinear oscillator modeled with a neural network, the method achieves much more accurate false-alarm control (notably at α=0.001) than the two-sample/holdout quantile approach, while maintaining strong post-change detectability. The approach is positioned for modern ML settings (e.g., neural networks with autodiff-computed scores) and avoids needing large holdout sets for extreme-quantile estimation.","Score vector for observation i is s(\hat\theta;x_i,y_i)=\nabla_\theta \log p(y_i\mid x_i;\theta)\rvert_{\theta=\hat\theta}, and the MEWMA recursion is z_i=\lambda s_i+(1-\lambda)z_{i-1} with z_0=0. Monitoring uses a Hotelling-type statistic T_{n+i}=(z_{n+i}-\bar s)^\top \hat\Sigma^{-1}(z_{n+i}-\bar s). Control limits are obtained from a nested bootstrap with a variance correction factor k(\lambda,i,n) (Eq. 3.3) and corrected bootstrap MEWMA \tilde z^{(b,j)}_i=z^{(b,j)}_i/\sqrt{k(\lambda,i,n)}, forming T^{(b,j)}_i=(\tilde z^{(b,j)}_i-\bar s_b)^\top \hat\Sigma_b^{-1}(\tilde z^{(b,j)}_i-\bar s_b); CL_i is the empirical (1-\alpha) quantile of \{T^{(b,j)}_i\}.","In the mixed-linear example (training n=2,000; monitoring 1,000 points; shift at 201; \lambda=0.01; \alpha=0.001; B_O=100; B_I=200), a typical run signals at observation 258, and the proposed bootstrap CL keeps empirical pointwise FAR close to 0.001, whereas the Zhang et al. (2023) two-sample CL shows highly inflated FAR (reported as exceeding 0.06). The paper notes that for \alpha=0.001 the two-sample method may require a CL-calibration subset of at least ~10,000 to estimate the 0.001 upper quantile reliably. In the nonlinear oscillator + neural network example (n=3,000; \alpha=0.001; \lambda=0.01; B_O=100; B_I=200), typical detections occur shortly after the shift (e.g., at i=215 low-noise and i=221 high-noise), and empirical PFAR curves remain close to 0.001 pre-change even under high noise (poor predictive R^2). Additional validation uses R=8000 Monte Carlo streams and reports PFAR control with B_O=50, B_I=200 (CPU-only environment).","The authors note that their work does not propose a new detection statistic; it uses the same score-based MEWMA statistic as Zhang et al. (2023) and focuses on improved control-limit calibration. They also state that computing ARLs via Monte Carlo is computationally prohibitive for their approach because the nested bootstrap would be required at each time step, so they target pointwise false-alarm rate control instead. They further mention practical numerical issues when covariance matrices are poorly conditioned and recommend adding \epsilon I before inversion.","The method controls pointwise false-alarm probability, not the overall false-alarm probability over a monitoring horizon or in-control ARL; translating pointwise control into ARL guarantees is nontrivial, especially with time-varying limits and dependence through MEWMA initialization. The approach assumes i.i.d. observations for both training and monitoring; performance and calibration under autocorrelation, seasonality, or nonstationary covariate distributions are not developed. Computational cost can be high because the outer bootstrap requires refitting potentially expensive ML models many times (B_O), and the paper does not provide ready-to-use software to lower adoption barriers. The framework is restricted to models with differentiable likelihood/score; common non-differentiable learners (tree ensembles) are excluded unless approximated.",None stated.,"Develop methods that provide in-control ARL/ATS targets (or familywise false-alarm control over time) while retaining finite-sample Phase I uncertainty accounting, possibly via calibrated steady-state limits or renewal/Markov approximations. Extend the bootstrap CL calibration to autocorrelated or seasonally varying streams (e.g., block bootstrap or model-based bootstrap for scores) and to batch/irregular sampling beyond random ordering heuristics. Provide open-source implementations (e.g., a Python package integrating PyTorch autodiff score extraction with parallel nested bootstrap) and practical guidance for selecting B_O/B_I and handling high-dimensional covariance estimation (regularized or robust precision matrices). Explore robustness to heavy tails and outliers in score vectors (robust covariance / rank-based multivariate monitoring) and broaden applicability to non-differentiable ML models via surrogate likelihoods or influence-function approximations.",2507.16749v2,local_papers/arxiv/2507.16749v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:05:11Z
TRUE,Univariate|Other,Other,Both,Environmental monitoring|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Other,"Phase I uses k in-control reference subgroups of size n each (examples: simulations use k=20, n=10; RH application uses k=3, n=10; flood application uses k=4, n=5). Phase II monitoring uses sequential test subgroups of size n.",TRUE,R,Not provided,https://github.com/RfastOfficial/Rfast|http://www.met.wau.nl/,"The paper proposes a Studentized Bootstrap Truncated-Beta Control Chart (SBTBC) to monitor process percentiles (quantiles) when the underlying proportion-type data follow a truncated beta (Tbeta) distribution and in-control parameters are unknown. Phase I estimates Tbeta parameters via MLE (numerical optimization) and constructs percentile control limits using a studentized parametric bootstrap; Phase II signals when the estimated percentile from each new subgroup falls outside the bootstrap-based LCL/UCL. Performance is evaluated primarily through Monte Carlo estimates of in-control and out-of-control run-length behavior across percentiles p∈{0.1,0.25,0.5,0.75,0.9}, FAR ν∈{0.005,0.0027,0.002}, and truncation intervals, with ARL0 values close to nominal 1/ν and ARL1 decreasing under parameter shifts. The chart is compared against a conventional beta-based bootstrap percentile chart and is reported to detect shifts faster across a range of shift magnitudes. Two real-data illustrations are given for environmental relative humidity proportions and for hydrological maximum flood levels, demonstrating practical implementation and out-of-control signaling.","The truncated beta pdf is $f(x\mid\theta_1,\theta_2)=\frac{x^{\theta_1-1}(1-x)^{\theta_2-1}}{I_b(\theta_1,\theta_2)-I_a(\theta_1,\theta_2)}$ for $a\le x\le b$, with $I_c(\theta_1,\theta_2)=\int_0^c u^{\theta_1-1}(1-u)^{\theta_2-1}\,du$, and cdf $F(x\mid\theta_1,\theta_2)=\frac{\int_0^x u^{\theta_1-1}(1-u)^{\theta_2-1}du}{I_b-I_a}$ on $[a,b]$. The monitored percentile is $\xi_p=F^{-1}(p;\theta_1,\theta_2)$ with estimate $\hat\xi_p=F^{-1}(p;\hat\theta_1,\hat\theta_2)$. Bootstrap percentiles $\hat\xi_{p}^{*}=F^{-1}(p;\hat\theta_1^{*},\hat\theta_2^{*})$ are studentized via $t_{ip}^{*}=(\hat\xi_{ip}^{*}-\hat\xi_p)/\mathrm{SE}(\hat\xi_p^{*})$, and control limits are obtained by inverting the empirical quantiles of $t_{ip}^{*}$: $\mathrm{LCL}=\bar\xi_p^{*}+t_{Lp}^{*}\,\mathrm{SE}(\hat\xi_p^{*})$, $\mathrm{UCL}=\bar\xi_p^{*}+t_{Up}^{*}\,\mathrm{SE}(\hat\xi_p^{*})$ for FAR $\nu$.","Simulations use $B=5000$ bootstrap resamples and 5000 Monte Carlo replications, with a main in-control setting $\theta_1=2,\theta_2=15$, n=10, k=20 and truncations such as (a,b)=(0,0.5),(0,0.6),(0.1,0.6). In-control ARL0 aligns with nominal targets (e.g., for ν=0.0027 nominal ARL≈370; an example reported ARL0 for p=0.5, a=0,b=0.5 is 365.066). Out-of-control ARL1 drops sharply under parameter shifts; e.g., for a=0,b=0.5 and ν=0.0027, a 4% decrease in θ1 (or θ2, holding the other IC) reduces the median-percentile ARL1 by about 22.85% (22.98%), whereas a 4% increase reduces it by about 10.05% (5.17%). In the RH case (May 2007 as IC, support (0.3,1), n=10, k=3, FAR=0.0027), the 90th-percentile limits are UCL=0.976, CL=0.926, LCL=0.805 and 3/20 simulated OOC subgroup percentiles fell below LCL under a 20% decrease in θ1; using May 2008 data against May 2007 limits, about 11/20 points fall outside limits. In the flood-level case (support (0.25,1), n=5, k=4, FAR=0.0027), 90th-percentile limits are UCL=0.789, CL=0.597, LCL=0.357 and 3/20 points exceed UCL under a 20% increase in θ1; comparisons indicate SBTBC signals earlier/more often than the bootstrap beta chart.",None stated.,"The procedure relies on correct specification of a truncated beta family and fixed truncation bounds (a,b); misspecification or uncertain/estimated truncation points could materially affect limits and signaling. The method assumes independent subgroups/observations and does not address autocorrelation common in environmental time series, which can inflate false alarms if unmodeled. Implementation depends on numerical MLE for Tbeta and intensive bootstrap/Monte Carlo computation (e.g., B=5000), which may be burdensome for routine deployment without optimized software.",None stated.,"Develop versions that handle serial dependence (e.g., model-based residual charts for ARMA/GLMM, block bootstrap, or dependent bootstrap) and assess robustness under autocorrelation. Extend to adaptive/self-starting monitoring (sequentially updating parameter estimates) and provide guidance on choosing/estimating truncation bounds (a,b). Release a reproducible software implementation (e.g., an R package) and study robustness to model misspecification and small Phase I sample sizes.",2507.23732v1,local_papers/arxiv/2507.23732v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:05:56Z
TRUE,Univariate|Other,Shewhart,Both,Manufacturing (general)|Semiconductor/electronics|Theoretical/simulation only,FALSE,FALSE,NA,Simulation study|Integral equation,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|Detection probability|False alarm rate|Other,"Phase I reference sample size is studied via simulations with m ∈ {30, 50, 100, 200, 500, 800, 1000, 2000, 5000, 8000}. The paper notes that for m > 5000 the adjusted/conditional performance is close to the nominal in-control ARL (370.4) and variability is reduced.",TRUE,None / Not applicable,Not provided,NA,"The paper develops and studies a two-sided Shewhart-type Weibull time-between-events (TBE) control chart aimed at detecting increases or decreases in the Weibull scale parameter β while assuming the shape parameter η is fixed/known. It treats both the ideal known-parameter case and the practical case where β is unknown and estimated from Phase I data via MLE, yielding plug-in (conditional) probability limits. To address degraded in-control performance under estimation, the work adjusts/assesses limits using two conditional criteria: the average of the conditional ARL (AARL/ACARL) and the standard deviation of the conditional ARL (SDARL/SDCARL). An analytical (integral) evaluation for the expected conditional ARL is presented using the gamma/chi-square distribution of the scale estimator, and a simulation study quantifies how Phase I sample size affects in-control variability and false-alarm behavior. Results show convergence toward nominal in-control ARL as Phase I sample size increases, with substantial extra variability and more frequent false alarms for small m.","For known parameters, probability (quantile) control limits are based on Weibull percentiles: $\mathrm{LCL}=\beta_0\,A_1$, $\mathrm{UCL}=\beta_0\,A_2$, where $A_1=[-\ln(1-\alpha_0/2)]^{1/\eta_0}$ and $A_2=[-\ln(\alpha_0/2)]^{1/\eta_0}$, and $\mathrm{CL}=\beta_0\Gamma(1+1/\eta_0)$. With shifts $\delta_1=\beta_1/\beta_0$ and $\delta_2=\eta_1/\eta_0$, the signal probability is $PS=P(X<\mathrm{LCL}\ \text{or}\ X>\mathrm{UCL})=1-\exp\{-(A_1/\delta_1)^{\eta_0\delta_2}\}+\exp\{-(A_2/\delta_1)^{\eta_0\delta_2}\}$ and $ARL=1/PS$. For unknown $\beta$, the MLE is $\hat\beta=\left(\frac{1}{m}\sum_{i=1}^m x_i^{\eta_0}\right)^{1/\eta_0}$ and plug-in limits use $\hat\beta$; conditional run length is geometric with parameter $CPS$ (computed by replacing $\beta_0$ with $\hat\beta$), and $ECARL$ is obtained by integrating $CARL=1/CPS$ over the induced gamma/chi-square distribution of $\hat\beta$.","Using $\alpha_0=0.0027$ (nominal in-control ARL = 370.4), simulations show ACARL increases toward 370.4 as Phase I size m grows and SDCARL decreases markedly. For example (η0,β0)=(1,1): ACARL/SDCARL are about 338.9/135.8 at m=30, 367.2/44.6 at m=500, 370.06/14.66 at m=5000, and 370.31/11.66 at m=8000. The exceedance probability EPC = P(CARL < 370.4) is around ~50% across many m values (e.g., 52.9% at m=30 and ~50.6% at m=2000), indicating that with estimated limits there is substantial conditional variability and frequent underperformance (more false alarms) for smaller m. The paper concludes that very large Phase I samples (on the order of several thousand; e.g., m>5000 in their study) are needed for conditional performance to closely match nominal IC ARL for the examined settings.",None stated.,"The method assumes the Weibull shape parameter η is fixed/known in the main estimated-parameter case, which can be unrealistic in TBE reliability applications and may materially affect false-alarm and detection properties if η is misspecified. The study focuses on Shewhart (memoryless) monitoring, which is typically less sensitive to small or moderate sustained shifts than CUSUM/EWMA/likelihood-based TBE charts; comparisons to these alternatives are not provided. Software/code details are not given, which limits reproducibility, and practical guidance is constrained because the Phase I sizes needed to stabilize performance (e.g., >5000) may be infeasible in many settings.",None stated.,"Extend the approach to jointly estimate (or robustly handle misspecification of) both η and β, including studying how estimation error in η propagates to conditional ARL properties. Develop analogous adjusted/estimated-parameter versions for more sensitive TBE monitoring schemes (e.g., Weibull CUSUM/EWMA/GLR) and compare against existing TBE charts under common benchmark shifts. Provide an implementable algorithm and open-source code to compute adjusted limits and conditional performance measures for practitioners, including guidance for realistic Phase I sizing under constraints.",2508.05790v1,local_papers/arxiv/2508.05790v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:06:19Z
FALSE,Univariate|Other,CUSUM|Change-point|Machine learning-based,Phase II,Other,NA,FALSE,NA,Simulation study|Case study (real dataset),Detection probability|Other,Not discussed,TRUE,None / Not applicable,Not provided,https://www.mathmodels.org/Problems/2024/MCM-C/index.html,"The paper proposes a sports-analytics framework to quantify and predict “momentum” in tennis singles using point-by-point match data. After testing for momentum using chi-squared independence tests on winning/losing streak contingency tables, it constructs a momentum index via an entropy weight method over selected performance features. It then applies a univariate CUSUM control chart to the momentum series to detect change points and defines a relative-distance measure to quantify the intensity/direction of momentum shifts. Finally, it builds a BP neural network optimized with PSO and uses SHAP values to assess feature importance; empirically, adding momentum-related features improves prediction metrics (e.g., AUC) on 2023 Wimbledon men’s singles matches. The CUSUM element is used as a change-point tool within a broader ML pipeline rather than as the central contribution to SPC methodology.","Momentum index is defined as $M_t=\sum_{i=1}^m w_i z_{it}$ where $z_{it}$ are standardized features and weights come from entropy weighting: $e_i=-\frac{1}{\ln T}\sum_{t=1}^T p_{it}\ln(p_{it}+\epsilon)$ and $w_i=\frac{1-e_i}{\sum_{i=1}^m(1-e_i)}$. Change-point detection uses a CUSUM recursion $C_t=M_t-\mu+C_{t-1}-d$ with thresholds $\pm h$; a change point is flagged when $C_t>h$ or $C_t<-h$ (encoded as $CP_t\in\{+1,0,-1\}$).","Using 31 Wimbledon men’s singles matches, the chi-squared test for streak-length independence reports $\chi^2=111.497$ with p-value $9.51\times 10^{-18}$, supporting momentum existence. In the final-match example, the entropy-weighted momentum metric is $M_t=0.3959z_{3t}+0.1122z_{4t}+0.0987z_{6t}+0.1162z_{7t}+0.1136z_{9t}+0.1633z_{10t}$. For that match’s momentum series, 40 change points are detected (20 positive, 20 negative) using a dynamically tuned CUSUM threshold. Adding momentum features improves predictive performance of BP+PSO from AUC 0.7125 (Base) to 0.7443 (Base+M+CP+V), and BP+PSO outperforms RF/SVM/logistic regression in AUC on the same feature set.",None stated.,"CUSUM is used as an off-the-shelf change-point detector without SPC-style design to a specified in-control ARL/false-alarm rate, making signaling properties hard to interpret in monitoring terms. Parameter/threshold selection is tuned to hit a target number of change points (rather than probabilistic control of false alarms), which can bias comparisons and limits operational deployability. The work is a sports analytics application; assumptions about independence/autocorrelation in the momentum series and the impact on change-point detection are not deeply analyzed.","The authors propose improving momentum measurement via better feature selection across psychological, technical, and physical dimensions combined with statistical methods, and improving prediction robustness by combining multiple machine learning algorithms (ensemble approaches).","For the change-point component, future work could calibrate CUSUM thresholds to explicit false-alarm constraints (e.g., target in-control ARL) and assess robustness under autocorrelation/seasonality in point sequences. A self-starting or adaptive CUSUM (estimating $\mu$ online) and uncertainty quantification for detected change points (e.g., confidence/credibility measures) would strengthen practical interpretability and deployment.",2509.01243v1,local_papers/arxiv/2509.01243v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:06:52Z
TRUE,Univariate,Shewhart|EWMA,Phase II,Healthcare/medical|Theoretical/simulation only|Other,FALSE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),"Not discussed (they study various subgroup sizes in examples, e.g., n=1 and n=10; for the real data they use first 150 observations as in-control/Phase I to estimate parameters).",TRUE,R,Not provided,https://EconPapers.repec.org/RePEc:ste:nystbu:94-10|https://books.google.com.pk/books?id=vQUNprFZKHsC|https://rdocumentation.org/packages/AER/versions/1.2-10|https://cran.r-project.org/web/packages/pscl/index.html,"The paper proposes Phase II univariate Shewhart-type and EWMA control charts for monitoring count data generated by a zero-inflated negative binomial (ZINB) distribution, targeting situations with both excess zeros and over-dispersion. It defines a ZINB-EWMA statistic based on the (subgroup) sample mean and derives in-control mean/variance expressions used to set control limits; the Shewhart-ZINB chart is obtained as the special case \(\lambda=1\). Performance is evaluated primarily via extensive Monte Carlo simulation, comparing in-control and out-of-control average run length (ARL) and standard deviation of run length (SDRL) across multiple parameter settings and smoothing parameters. Results show the EWMA chart generally attains the targeted in-control ARL more reliably and detects small-to-moderate shifts (notably in the success probability parameter p) faster than the ZINB-Shewhart chart; smaller \(\lambda\) tends to improve detection performance. A real-data illustration using the Owls dataset (via R) fits candidate count models using BIC (favoring ZINB) and shows the EWMA chart flags more out-of-control points than the Shewhart chart in Phase II. The authors also include a corrigendum correcting Table 13 values and conclude with suggested extensions to CUSUM/adaptive charts and other design enhancements.","ZINB pmf: \(P(Y=0)=\theta+(1-\theta)p^k\), and for \(y\ge1\), \(P(Y=y)=(1-\theta)\binom{y+k-1}{y}p^k(1-p)^y\). The EWMA statistic for subgroup mean \(\bar Y_i\) is \(Z_i=\lambda \bar Y_i+(1-\lambda)Z_{i-1}\), with IC mean \(E(Z_i)=k(1-\theta)(1-p)/p\) and IC variance \(\mathrm{Var}(Z_i)=\frac{\lambda}{n(2-\lambda)}\,\mathrm{Var}(Y)\), where \(\mathrm{Var}(Y)=\frac{k(1-\theta)(1-p)[1+(1-p)\theta k]}{p^2}\). Control limits: \(\mathrm{UCL}=\mu+L\sqrt{\frac{\lambda}{n(2-\lambda)}\mathrm{Var}(Y)}\), \(\mathrm{CL}=\mu\), \(\mathrm{LCL}=\max\{0,\mu-L\sqrt{\frac{\lambda}{n(2-\lambda)}\mathrm{Var}(Y)}\}\); Shewhart is obtained by setting \(\lambda=1\).","Across many simulated settings (often targeting \(\mathrm{ARL}_0\approx 500\)), the ZINB-EWMA chart achieves the desired in-control ARL much more consistently than the ZINB-Shewhart chart, which in some cases misses the target substantially (e.g., with \(n=1,k=1,p_0=0.40,\theta=0.85\), Shewhart yields \(\mathrm{ARL}_0=396.01\) vs. EWMA \(\approx 500\)). For out-of-control shifts in \(p\), EWMA often has smaller \(\mathrm{ARL}_1\) than Shewhart once matched on \(\mathrm{ARL}_0\), and performance generally improves with smaller smoothing \(\lambda\). In the real-data example (Owls dataset), ZINB is selected by BIC among Poisson, NB, ZIP, and ZINB, and Phase II monitoring flags one point (212) out-of-control under Shewhart versus seven points (1959 and 212) under EWMA. The paper reports that as \(\lambda\) increases, required L increases and EWMA detection performance tends to deteriorate, especially for small shifts; it also notes L generally decreases as k increases. A corrigendum corrects the originally published Table 13 values.","The authors note that the ZINB-Shewhart chart often does not achieve the nominal in-control \(\mathrm{ARL}_0\) (e.g., 500), and that small changes in the control-limit multiplier L can cause large changes in \(\mathrm{ARL}_0\), making \(\mathrm{ARL}_1\) comparisons less trustworthy for Shewhart in those cases. They also emphasize their focus is on Phase II charting (with parameters treated as fixed once estimated) rather than developing Phase I estimation methodology.","The approach uses normal-approximation-style limits (mean  L SD) for a discrete, highly skewed/zero-inflated distribution; this can lead to poor calibration of false-alarm rates in small-sample settings (notably n=1) unless L is tuned by simulation, limiting generalizability. The paper largely assumes independent observations and does not study robustness to autocorrelation or other temporal dependence common in count processes. Parameter estimation uncertainty from Phase I (and model selection uncertainty among Poisson/NB/ZIP/ZINB) is not propagated into Phase II performance, which can materially affect achieved ARL in practice. No reusable implementation/code is provided despite reliance on Monte Carlo tuning of L, which is critical for practitioners.","They propose extending the work to CUSUM and adaptive control charts, and developing variable sample size (VSS) and variable sampling interval (VSI) charts. They also suggest extensions to bivariate ZINB charting and proposing ARL-unbiased designs.","Developing exact/Markov-chain or integral-equation approximations for the ZINB-EWMA run-length distribution could reduce dependence on heavy Monte Carlo tuning and enable principled design. Incorporating Phase I parameter estimation effects (and self-starting or Bayesian updating) would improve practical deployment when in-control parameters are uncertain or drifting. Robust/nonparametric variants or likelihood-based GLR/CUSUM designs tailored to shifts in \(\theta\), k, and p (separately and jointly) could provide better diagnostic capability than a single mean-based EWMA. Providing an open-source R package (or code) for fitting ZINB and computing L/UCL for target ARL0 would materially increase adoption.",2509.03304v1,local_papers/arxiv/2509.03304v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:07:36Z
TRUE,Multivariate|Image-based monitoring|High-dimensional|Other,Hotelling T-squared|Machine learning-based|Other,Phase II,Energy/utilities|Theoretical/simulation only|Other,FALSE,FALSE,FALSE,Simulation study|Case study (real dataset),Detection probability|False alarm rate|Other,"Not discussed (no general Phase I reference-sample size guidance; specific window sizes used for correlation estimation include n=25 in simulation and n=8 in the case study, with thresholds tuned on validation data at specified Type-I error levels).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes an in-situ process monitoring framework that learns nonlinear, quality-oriented representations from paired high-dimensional process signatures (e.g., optical emission spectra) and offline quality measurements (e.g., CT images) using Deep Canonical Correlation Analysis (DCCA). The method is trained using only in-control (normal-condition) paired data to maximize canonical correlation between learned features, then performs online monitoring by pairing incoming process-signal windows with nearest-neighbor reference quality samples from the in-control training set and computing a correlation score as the monitoring statistic. A threshold on this correlation score is set by controlling the Type-I error rate on validation data, aiming to avoid the normality assumptions that can invalidate traditional Hotelling’s T^2 charting of extracted features. The authors provide stability/convergence and learning-theory generalization bounds for the empirical DCCA correlation objective and for finite-sample canonical correlation estimation error. Simulations and a Direct Metal Deposition additive manufacturing case study show clearer separation between normal/abnormal conditions and improved practical false-alarm/misdetection tradeoffs compared with PCA/PLS feature extraction followed by T^2 monitoring and a classifier baseline.","Offline training minimizes the negative estimated canonical correlation: $L(f,g)=-(1/N_0)\sum_{(X,Y)\in\mathcal D_0} H_{n,p}(f(X;\theta_1),g(Y;\theta_2))$, where $H_{n,p}(U,V)=\|\hat\Sigma_{11}^{-1/2}\hat\Sigma_{12}\hat\Sigma_{22}^{-1/2}\|_*$. Online monitoring forms a surrogate quality set via nearest-neighbor matching $Y^*:=\{y': x'=\arg\min_{x\in X_0} d(x^*,x), (x',y')\in(X_0,Y_0)\}$ and computes $\rho^*=H_{n,p}(f(X^*),g(Y^*))$; it signals when $\rho^*(X^*)<\tau$ with $\tau$ chosen to control Type-I error $\alpha$.","Simulation: with window size n=25 and Type-I error controlled at 0.05 on validation data, the proposed DCCA+nearest-neighbor correlation monitoring becomes increasingly better than PCA/PLS+T^2 and a classifier baseline as noise increases, particularly reducing Type-II error and improving F1 score (reported in plots across noise levels). Case study (Direct Metal Deposition): using feature dimension p=6 and window size n=8, with a validation-selected threshold targeting FPR=0.1, DCCA+NN achieves test FPR 9.5%, FNR 14.0%, and F1 87.98; with thresholds adjusted to enforce FPR=10% across methods, DCCA+NN attains FNR 14.0% and F1 87.76, while PCA+T^2 and PLS+T^2 have much higher FNR (83.5% and 77.5%) and much lower F1 (26.09 and 33.97). The authors attribute baseline instability to violation of normality assumptions underlying T^2 statistics for extracted features.","The paper notes that exact one-to-one alignment between online process signals and offline quality measurements is often infeasible in practice due to differing sampling rates, requiring alignment/aggregation heuristics (e.g., grouping spectra with CT frames and averaging/thresholding CT windows). It also states that the real case-study dataset cannot be made publicly available due to licensing restrictions (available upon request).","The online procedure relies on nearest-neighbor matching of incoming signals to reference in-control quality samples; performance may degrade if the in-control reference set is not representative, if the distance metric is poorly chosen, or under concept drift. The monitoring rule is not framed in standard run-length terms (e.g., ARL/ATS), making it harder to compare directly with classical SPC designs or to tune for desired in-control/out-of-control signaling properties over time. The i.i.d. assumption for paired samples may be unrealistic for streaming manufacturing signals with serial dependence, and no explicit method is provided to handle autocorrelation or nonstationarity beyond windowing.","The authors suggest extending the framework to leverage the learned correlated features to directly reconstruct quality-related images from process signature signals, potentially via joint development of a generative model to enhance interpretability and visualization of emerging defects.","Developing run-length/ATS-based design and calibration for the correlation-score detector (including steady-state behavior) would connect the method more directly to SPC practice. Robust/self-starting variants that adapt reference sets and thresholds under drift, and extensions that explicitly model serial dependence in process signatures, would improve deployability for real streaming settings. Providing open-source implementations and benchmarking against additional modern multistream/image monitoring approaches (e.g., deep autoencoder control charts or likelihood-based change-point detectors) would strengthen reproducibility and comparative evidence.",2509.19652v1,local_papers/arxiv/2509.19652v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:08:14Z
TRUE,Univariate|High-dimensional|Nonparametric|Image-based monitoring|Other,EWMA|Other,Phase II,Semiconductor/electronics|Theoretical/simulation only|Other,FALSE,TRUE,FALSE,Simulation study|Case study (real dataset)|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length)|False alarm rate|Detection probability,Manifold fitting theory cited requires Phase I size on the order of $m=O(\sigma^{-(d+3)})$ observations to fit $\hat M$ with Hausdorff error $O(\sigma^2\log(1/\sigma))$. For chart setup they recommend at least 10 in-control deviations and a window size $w\ge 5$; examples use splits such as 700 (fit manifold) + 400 (fit AR models) + 100 (set chart) in Phase I.,TRUE,Python|MATLAB,Public repository (GitHub/GitLab),https://github.com/iburaktas/Manifold-SPC,"The paper proposes two Phase II SPC frameworks for high-dimensional, dynamic processes assuming observations lie near a nonlinear low-dimensional manifold. The first (manifold fitting) fits an in-control manifold directly in the ambient space using a state-of-the-art manifold fitting algorithm and monitors the scalar deviation/distances $\|Y_t-\hat\pi(Y_t)\|$ with a new distribution-free univariate EWMA chart (UDFM) designed to control Type I error (hence in-control ARL). The second (manifold learning) learns an explicit out-of-sample embedding (e.g., LPP/NPE), filters serial dependence in the embedded coordinates via AR models, and then applies the distribution-free multivariate DFEWMA chart to obtain controllable ARL in the presence of autocorrelation. Extensive Monte Carlo experiments on a synthetic sphere process and replicated Tennessee Eastman simulations show in-control ARL close to nominal (e.g., 20) and competitive or better out-of-control ARL for manifold fitting versus manifold learning; PCA can fail under nonlinear structure. A real image case study (Kolektor commutator surfaces) demonstrates anomaly detection by fitting a manifold to non-defective images and signaling defects with the UDFM chart.","Observations follow $Y_t=X_t+E_t$ (in control) and $Y_t=X_t+\Delta+E_t$ (out of control), with monitoring based on deviations from the fitted manifold: $d_t=\|Y_t-\hat\pi(Y_t)\|_2$. The proposed UDFM uses a one-sided rank-based statistic $Z_{j,n}=\frac{1}{m+n}\max\{0, R_{j,n}-\frac{m+n+1}{2}\}$ combined in a rolling-window EWMA: $T_{n,w,\lambda}=\frac{\sum_{j=n-w+1}^n (1-\lambda)^{n-j}(Z_{j,n}-\mathbb{E}[Z_{j,n}])}{\sqrt{\mathrm{Var}(\sum_{j=n-w+1}^n (1-\lambda)^{n-j}Z_{j,n})}}$, with control limits set to satisfy a conditional false-alarm probability $\alpha$, yielding geometric run length under $H_0$. For autocorrelation, AR models are fit to in-control deviations (or embedded coordinates) and charting is performed on forecast residuals.","Across 10,000 Monte Carlo replications, reported ARL-in values are close to nominal targets (e.g., \~20) for all frameworks after calibration. On a synthetic 2-sphere example (D=6), manifold fitting detects shifts both on- and off-manifold subspaces; for a shift of 10$\sigma$ in a dimension missed by manifold learning, manifold fitting achieved ARL\_out \~2 (SDRL \<1) while manifold learning methods stayed near ARL\_in \~20. On replicated Tennessee Eastman faults (D=300) with amplitudes 0.05–0.1, PCA monitoring largely failed (ARL\_out \~20) while nonlinear methods detected faults; manifold fitting was best for larger shifts (e.g., faults 3/4 at 0.1 with ARL\_out \~2.09) and NPE/LPP were best for smaller shifts (e.g., faults 3/4 at 0.05 with ARL\_out \~3.3 vs MF \~6.5). In the extreme high-dimensional case m=280<D=300, MF retained ARL\_in \~21.3 but with reduced detection power (e.g., faults 3/4 at 0.05 ARL\_out \~9.9). On the Kolektor image dataset with nominal ARL\_in=200, the MF-UDFM chart signaled by the 5th defective image.","The paper notes UDFM is formulated under an i.i.d. assumption for the (estimated) deviation sequence, acknowledging that estimated deviations may exhibit mild temporal dependence and suggesting prewhitening via AR modeling when needed. It also notes the practical need to choose manifold-fitting tuning constants (e.g., radii constants $C_0,C_1,C_2$) and estimate $\sigma$, since these are not provided by the original manifold-fitting reference; their proposed estimator is pragmatic rather than unbiased. For manifold learning, it states methods like LPP/NPE become ill-posed when $m<D$, limiting applicability in extreme high-dimensional settings.","The controllable Type I error relies on permutation-based calibration conditional on pooled samples and on the quality/stability of the fitted manifold and prewhitening; performance may degrade under nonstationarity, drift, or changing autocorrelation structure. The manifold fitting step can be computationally heavy in very large D and large m (nearest-neighbor searches/weighted averages per point), and the paper does not quantify runtime/memory or provide complexity guidance for industrial streaming constraints. Robustness to heavy-tailed noise, missingness/irregular sampling, and non-mean-shift faults (beyond brief discussion) is not fully developed; the method also depends on choosing window size w and EWMA parameters without an economic/design study.",The authors suggest extending the framework beyond mean shifts to detect changes in the shape of the underlying manifold (analogous to covariance changes). They also propose using the projected points on the manifold (not just deviations) to help diagnose which variables caused a signal.,"Develop a principled, data-driven method to tune manifold-fitting radii/constants and EWMA parameters to meet desired ARL tradeoffs with minimal practitioner intervention. Extend the distribution-free monitoring to handle irregular sampling/missing data and stronger serial dependence (e.g., state-space/VAR prewhitening) while retaining Type I error guarantees. Provide computational scaling studies and optimized implementations (approximate neighbors, batching) for very large D (images/video) and real-time deployment, plus broader real-world benchmarks beyond TE and Kolektor.",2509.19820v1,local_papers/arxiv/2509.19820v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:09:00Z
TRUE,Univariate|Other,Shewhart|Other,Phase I,Manufacturing (general)|Healthcare/medical|Pharmaceutical|Energy/utilities|Theoretical/simulation only|Other,TRUE,FALSE,NA,Simulation study|Case study (real dataset),ARL (Average Run Length)|Other,"Simulations and illustrative examples use 50 subgroups with subgroup sizes n = 5, 10, and 15; initial 30 subgroups are in-control (δ = 0) and remaining 20 are out-of-control (δ = 1). No general Phase I minimum sample-size guidance is given beyond these study settings.",TRUE,R,Not provided,NA,"The paper proposes improved Shewhart-type control charting structures for monitoring a process mean using repetitive sampling and one auxiliary variable, under a bivariate normality assumption for (Y, X). Building on the existing Mr chart (a regression-estimator-based chart), it introduces two new charting statistics: Mrep based on a ratio–product exponential estimator and Mrwp based on a weighted difference-cum-exponential estimator. Control limits are discussed via probability limits and the usual 3-sigma limits, and performance is evaluated primarily through Monte Carlo simulation under repetitive sampling. The proposed Mrep and especially Mrwp are reported to yield smaller out-of-control ARLs (faster detection) than the baseline charts across a range of subgroup sizes (n = 5, 10, 15), correlations, and mean shifts. An illustrative worked example with generated subgroup data is included to show earlier and more frequent signaling after a mean shift compared with existing estimators.","Existing regression-based charting statistic: $M_r = \bar{y} + b(\mu_x-\bar{x})$, where $b=r_{xy}(s_y/s_x)$. Proposed ratio–product exponential estimator used as a charting statistic: $\widehat{\bar{Y}}_{S,RP}=\bar{y}\left[\alpha\exp\left(\frac{\bar{X}-\bar{x}}{\bar{X}+\bar{x}}\right)+(1-\alpha)\exp\left(\frac{\bar{x}-\bar{X}}{\bar{x}+\bar{X}}\right)\right]$, with $\alpha_{opt}=\tfrac12+\rho_{xy}C_y/C_x$. Proposed weighted difference-cum-exponential statistic: $\widehat{\bar{Y}}^{\,*}_{P}=\left(\widehat{\bar{Y}}_{S,RP}+w_1(\bar{X}-\bar{x})+w_2\bar{y}\right)\exp\left(\frac{\bar{X}-\bar{x}}{\bar{X}+\bar{x}}\right)$ with optimal $w_1,w_2$ given in the paper. 3-sigma limits are expressed generically as $\text{LCL}=\bar{M}_r-3\sigma_{M_r}$, $\text{CL}=\bar{M}_r$, $\text{UCL}=\bar{M}_r+3\sigma_{M_r}$ (with additional scaling forms provided).","Across simulated scenarios (reported via ARL curves) with target in-control ARLs of 200, 371, and 500, and subgroup sizes n = 5, 10, 15, the proposed T2 (Mrep) and T3 (Mrwp) generally have lower out-of-control ARL than the baseline T0 ($\bar{y}$) and T1 (Mr), implying faster detection of mean shifts. In an illustrative 50-subgroup example with a shift introduced after subgroup 30 (δ = 1), T2 and T3 signal immediately at subgroup 31, whereas T0/T1’s first signal is reported much later (e.g., subgroup 43 in the cited case). The example also reports more total out-of-control signals for T2/T3 than for T0/T1 after the shift, indicating higher sensitivity. The paper further claims T3 is typically best among the considered charts when comparing additional metrics (EQL, RARL, PCI) computed via simulation.","The authors frame the development largely under an assumption of bivariate normality for (Y, X) and focus on Phase I monitoring; they also note the work is mainly supported by simulation/illustrative examples and suggest using practical datasets as an extension. They imply performance depends on the availability and usefulness (correlation) of auxiliary information.","Despite discussion of “repetitive sampling,” the paper does not clearly formalize the operational sampling/decision protocol (e.g., how many repeats, stopping/acceptance rules) in a way that would allow straightforward implementation and fair cost/time comparison to standard fixed-sample charts. Results are primarily presented as qualitative ARL-curve comparisons without a transparent tabulation of ARL values across shift sizes and designs, limiting reproducibility and practical parameter tuning. The approach relies on known or effectively known auxiliary mean $\mu_x$ and strong correlation; robustness to misspecification, non-normality, and estimation of $\mu_x$ in practice is not demonstrated. No explicit treatment of autocorrelation, which is common in industrial data streams, is provided even though it is mentioned in the background.","The paper suggests extending the work to other distributions/settings (explicitly mentioning Bayesian and multivariate distributions), incorporating more auxiliary variables, implementing runs rules, investigating varying sampling strategies, and adding memory to the proposed structures via EWMA/CUSUM-type frameworks.","Provide a precise, implementable repetitive-sampling algorithm (including average sample number or inspection effort) and evaluate economic/ASN trade-offs versus standard Shewhart/EWMA/CUSUM charts. Study robustness when auxiliary parameters (e.g., $\mu_x$) are unknown and must be estimated, including Phase I/Phase II transitions and self-starting variants. Extend the design to autocorrelated processes (e.g., residual charts/ARMA modeling) and to non-normal/robust versions (rank-based or bootstrap-calibrated limits). Release an R implementation to enable reproducible ARL studies and practitioner adoption.",2510.05086v1,local_papers/arxiv/2510.05086v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:09:43Z
TRUE,Nonparametric|Image-based monitoring|Other,Change-point|Other,Phase II,Manufacturing (general)|Other,NA,FALSE,NA,Simulation study|Other,ARL (Average Run Length)|SDRL (Standard Deviation of Run Length),They recommend using at least m0 = 200 Phase I (reference) in-control parts because smaller m0 (≤150) yields inflated in-control ARL/SDRL due to an insufficiently large permutation null distribution.,TRUE,R|Python,Supplementary material (Journal/Publisher),https://drive.google.com/drive/folders/1yz9N3H_M7iiEAk2uMXwYWD4qmdg7qk-1,"The paper proposes a new statistical process control (SPC) framework for monitoring 3D lattice structures in additive manufacturing using Topological Data Analysis (TDA), specifically persistent homology (PH) in dimensions 0 and 1. Parts are represented by point clouds/meshes, converted to persistence diagrams, and compared using distances between diagrams; a nonparametric permutation test is used to decide whether a new part differs from a Phase I in-control reference set. The resulting Phase II monitoring scheme applies the permutation test sequentially, with an adaptive critical value (from the permutation distribution) and performance assessed via extensive Monte Carlo run-length simulations. The study finds strong sensitivity for certain geometric/topological defects (e.g., layer shift, collapsed edges, missing strut), especially when using 1-dimensional loop features, while joint monitoring of 0- and 1-dimensional features approximately halves the in-control ARL due to multiple testing. The method is compared with intrinsic spectral SPC based on Laplace–Beltrami spectra, showing faster detection for moderate defects/noise in many scenarios but weaker performance for small-severity “collapse” defects in some settings.","Distances between persistence diagrams are discussed, including the p-Wasserstein distance $d_p(X,Y)=\left(\inf_{\varphi:X\to Y}\sum_{x\in X}\|x-\varphi(x)\|_p^p\right)^{1/p}$ and the bottleneck distance $d_\infty(X,Y)=\inf_{\varphi:X\to Y}\sup_{x\in X}\|x-\varphi(x)\|_\infty$. For computational speed they use a TDAstats duration-based distance: compute durations (death−birth), pad the shorter vector with zeros, sort, then $d(X,Y)=\sum_{i=1}^{n_Y}|x_{(i)}-y_{(i)}|$. The permutation-test statistic for a reference set $\{X_1,\dots,X_{m_0}\}$ is the within-group sum $d_{\text{in-group}}=\sum_{1\le i<j\le m_0} d(X_i,X_j)$; the null distribution is formed by all $m_0+1$ label permutations when testing a single new part against the reference set.","In-control run-length simulations (10,000 reps, $\alpha=0.05$) show ARL/SDRL approach the nominal geometric values (ARL≈20, SDRL≈19.49) when the Phase I reference size is about $m_0\ge 200$; for smaller $m_0$ (≤150) the ARL/SDRL are inflated (e.g., around 25 and >30 in several tables). Joint monitoring of 0D and 1D features roughly halves the in-control ARL (≈10–12), consistent with near-independence of the two tests. Out-of-control results show very fast detection for some defects using 1D features (e.g., for cube/lattice defects with $\delta=0.05$, 1D ARLs ≈1.00–1.25 vs 0D ARLs ≈14.7–16.0). For the “egg” part, collapse defects parameterized by $k\sigma$ show decreasing ARL with larger severity (e.g., at $2\sigma$ ARL=1; at $1/5\sigma$ ARLs ~16.5 (1D) and ~18.1 (0D) with $m_0=200$). In comparisons without skeletonization (5,000 reps), the TDA-based method detects large/moderate defects essentially immediately in many settings (ARL≈1–1.35 for some severities), while the spectral method can outperform TDA for very small collapse severities (e.g., at $1/5\sigma$ spectral ARL ~3.41 vs TDA 1D ARL ~5.64).","They note a potential bug/limitation in the TDA package’s alphaComplexDiag: it may compute all $n-1$ lower-dimensional features regardless of the specified maxdimension parameter (e.g., returns 0D–2D features even if a smaller maxdimension is requested), though they state this is not a serious concern given the function’s speed. They also acknowledge that their chosen fast diagram distance (TDAstats phom.dist based on durations) can underestimate differences because it ignores birth/death locations and can yield zero distance in some extreme cases. Additionally, they explicitly restrict their monitoring to 0- and 1-dimensional features and do not address 2-dimensional features in this study.","The proposed Phase II scheme assumes (or relies on) approximate independence across sequential tests/parts; in practice, additive manufacturing processes and measurement pipelines may induce serial dependence that could distort the geometric ARL approximation. The adaptive limit is recomputed via permutations involving the new part, but the statistical properties under parameter estimation/contamination in the Phase I reference set (e.g., undetected outliers, drift, mixed in-control states) are not fully characterized. The duration-only distance may lose power for defects that primarily shift feature birth/death times without changing persistence, potentially causing missed detections relative to Wasserstein/bottleneck metrics. Finally, the approach can be computationally heavy for real-time deployment on high-resolution scans unless aggressive downsampling/skeletonization is acceptable, which may itself remove diagnostically relevant features.","They propose validating the Phase II scheme on sequences of real additively manufactured lattice parts to demonstrate practical robustness. They suggest extending monitoring to 2-dimensional topological features to broaden defect sensitivity. They also propose extending the method from mesh/point-cloud representations to voxel data from industrial tomography, paralleling prior LB-spectrum work on voxel data.","Developing a faster yet geometry-faithful persistence-diagram distance (or kernel/embedding) with better accuracy–compute tradeoffs than duration-only sorting could improve power without sacrificing throughput. Robust Phase I procedures (e.g., reference-set cleaning, outlier-resistant diagram summaries, or iterative trimming) would help when the in-control baseline is contaminated. Incorporating methods for autocorrelated/temporally evolving topology (e.g., state-space modeling of diagram embeddings or change-point models) could improve monitoring for gradual drift. Providing an open-source R/Python package with optimized implementations and reproducible benchmarks across standard 3D SPC datasets would accelerate adoption and fair comparison to competing intrinsic geometric SPC methods.",2510.11740v1,local_papers/arxiv/2510.11740v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:10:34Z
TRUE,Univariate|Nonparametric|Other,Change-point|Other,Phase I,Manufacturing (general)|Theoretical/simulation only,FALSE,FALSE,NA,Exact distribution theory|Simulation study|Case study (real dataset)|Other,Detection probability|False alarm rate|Other,Not discussed.,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Phase I distribution-free control charts for individual observations to monitor an unknown target (location) parameter for both continuous and discrete data. Observations are thresholded into a Bernoulli sequence and then monitored using runs-and-patterns statistics, specifically the number of success runs $R_n$ (R-1 chart) and the scan statistic $S_n(r)$ (R-2 chart). Exact conditional in-control signal probabilities and control limits are obtained via the finite Markov chain imbedding (FMCI) technique combined with random permutation/conditioning on the number of successes, making the procedures distribution-free with respect to the underlying process distribution. Simulations under change-point mean-shift scenarios show the charts achieve the nominal in-control signal probability (e.g., 0.005 in experiments) and have competitive out-of-control detection probabilities versus existing Phase I nonparametric charts (MW, KS, ELR), with particularly good performance for heavy-tailed distributions. A piston-ring case study demonstrates both charts can signal and localize an instability region near the end of the sequence using window/run localization outputs and conditional p-values.","Data are transformed by thresholding: $X_i = \mathbf{1}(Y_i \ge c)$, with baseline proportion $p_0 = \Pr(Y\ge c)$ and $c$ often set as an empirical quantile $\hat q_{1-p_0}$. For the runs chart, the exact conditional pmf is computed by FMCI: $\Pr(R_n=r\mid N_1=n_1)=\xi_0\left(\prod_{t=1}^{n_2} M_t^{(1)}\right)e_r$, and the chart signals if $R_n \le R_n(\alpha)$ (lower $\alpha$-percentile). For the scan chart, the statistic is $S_n(r)=\max_{1\le t\le n-r+1}\sum_{i=t}^{t+r-1}X_i$ and the FMCI-derived conditional CDF is $\Pr(S_n(r)<s\mid N_1=n_1)=\xi_0\left(\prod_{t=1}^{n} N_t^{(2)}\right)\mathbf{1}$; the chart signals if $S_n(r)\ge S_n(r,\alpha)$ (upper percentile), using randomization when discreteness prevents exact sizing.","Simulated in-control signal probabilities for nominal 0.005 are close to target across Normal(0,1), Exp(1), and t(3) settings (Table 2), e.g., for $n=50$: $R_n$ 0.0045/0.0057/0.0055 and for scan charts: $S_n(10)$ 0.0051/0.0042/0.0048, $S_n(25)$ 0.0043/0.0039/0.0049, $S_n(40)$ 0.0053/0.0054/0.0055 (Normal/Exp/t respectively). Out-of-control performance (signal probabilities/power) varies with change-point location and window size, with scan charts strongest when the window size matches the cluster length and with comparatively better performance for heavy-tailed t(3) than for Exp(1) relative to MW/KS/ELR benchmarks. Recommended baseline proportions $p_0$ depend on scenario and window size (Table 1), generally avoiding extreme sparsity/density (typical ranges about 0.1–0.8, with smaller $p_0$ suggested for right-skewed data). In the piston-ring example (40 sample means), at $\alpha=0.05$ the charts signal with observed values $R_{40}=4$, $S_{40}(6)=5$, $S_{40}(10)=7$ and localize late-sample windows/runs (e.g., indices 31–40 and 34–40) with conditional p-values around 0.0123–0.0525 and a run p-value 0.0253.","The authors note that runs/patterns statistics are discrete, so achieving an exact in-control signal probability at a prescribed level may not always be possible; they therefore use a randomized test to attain the desired in-control signal probability. They also discuss that extreme choices of the threshold/baseline proportion $p_0$ can make the Bernoulli sequence too sparse or too dense, reducing chart resolution and detection power, motivating restricting $p_0$ to a moderate range.","Although distribution-free conditional on $N_1$, the method relies on i.i.d. observations; performance under autocorrelation or other dependence (common in Phase I data) is not addressed. The approach introduces a tuning choice ($c$ or $p_0$, and scan window size $r$) that can materially affect power, but there is no fully data-driven, theoretically justified selection rule beyond heuristic recommendations. Computational complexity and scalability of the FMCI state space for more complex patterns (or larger $n$, multiple windows/rules) is not quantified, which may affect practical adoption.","The authors state that the framework is flexible and can use other run/pattern statistics beyond the number of success runs and scan statistic. They suggest practitioners (and future work) can tailor statistics to detect other shift types (e.g., variance changes) and construct corresponding distribution-free control charts using the same conditional-distribution/FMCI approach.","Extend the charts to handle autocorrelated or non-i.i.d. Phase I data (e.g., via permutation blocks, model-based residuals, or robust dependence-adjusted conditioning). Develop principled, automated selection of $p_0$ and scan window size $r$ (e.g., maximizing expected power under a class of alternatives while controlling conditional size). Provide open-source software and computational benchmarks for FMCI implementations, and study robustness when the threshold $c$ is estimated from the same Phase I data used for testing (including small-sample effects and multiple-testing adjustments across candidate $r$).",2511.13672v1,local_papers/arxiv/2511.13672v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:11:15Z
FALSE,Other,Other,NA,Other,NA,NA,NA,Simulation study|Other,Other,Not discussed,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/immortal5655/ChartAnchor,"This paper introduces ChartAnchor, a benchmark for “chart grounding,” defined as bidirectional alignment between a chart’s visual appearance and its structured semantics (tabular data and plotting specification). The benchmark contains 8,068 validated chart–table–code triples spanning 30 chart types and multiple plotting libraries (notably matplotlib and plotly), sourced from real-world web charts and augmented versions of existing chart datasets. It defines two tasks: (1) chart-to-code generation (produce executable Python plotting code to replicate a chart) and (2) controlled chart-to-table reconstruction (recover exact tabular values given predefined column headers), enabling cross-checking of visual and numeric fidelity. The evaluation framework combines functional validity (execution/parsing), visual-structure consistency (text, color, type, layout), semantic data fidelity via tuple-based matching with numerical/string tolerances, and perceptual similarity (CLIPScore). Experiments across 14 multimodal LLMs show strong structural/layout performance for top proprietary models but persistent weaknesses in precise numeric recovery and color fidelity, motivating future work on more reliable structured reasoning.","Key evaluation defines tuple matching under tolerance for string fields via edit distance $J(p_i,q_i)$ and numeric fields via relative error $e(p_i,q_i)$. A predicted tuple is correct only if all fields meet a chosen tolerance level (strict: $J\le 0$ or $e\le 0$; slight: $J\le 3$ or $e\le 0.05$; high: $J\le 5$ or $e\le 0.10$). Under a tolerance level, precision, recall, F1, and IoU are computed as $P=|M|/|Pred|$, $R=|M|/|GT|$, $F1=2PR/(P+R)$, and $IoU=|M|/(|Pred|+|GT|-|M|)$, where $|M|$ is the number of matched tuples.","ChartAnchor includes 8,068 chart–table–code triples across 30 chart types; average image resolution is 3,346×2,266 and mean code length is about 628 tokens (mean 1,469 characters). On chart-to-code, GPT-4o reports the best overall score (63.02) with high pass rate (91.88), while Claude-3-7-Sonnet achieves the highest CLIPScore (94.11) despite lower overall structural/data fidelity than GPT-4o. On controlled chart-to-table, Claude-3-7-Sonnet leads strict and slight tolerance F1 (10.86 strict; 30.52 slight), while Qwen2.5-VL-32B achieves the best high-tolerance F1 (41.04). Across models, color and exact numeric recovery are consistently weaker than layout/type recovery, and strict-tolerance scores are much lower than slight/high, indicating poor fine-grained numerical precision.","The authors note that the current benchmark focuses on static chart understanding and does not cover interactive or dynamic visualizations such as drill-down interactions, animated transitions, or multi-view dashboards. They state that such charts add complexity from temporal structure and interaction states, which are not evaluated in the current version.","Because ChartAnchor is a benchmarking dataset for chart-to-code/table tasks, its reported model comparisons can be sensitive to prompt design, rendering/parsing toolchains, and the completeness of type-specific data extractors used for fidelity scoring; these dependencies may limit reproducibility across environments. The benchmark’s strong reliance on executable Python plotting code may underrepresent real-world chart pipelines that use other ecosystems (e.g., Vega/Vega-Lite beyond limited inclusion, D3, Tableau exports) or non-Python workflows. The tuple-matching tolerances (e.g., 5% relative error) may not reflect domain-critical precision needs (finance/science), and robustness to OCR errors or ambiguous axis scaling may vary substantially by chart type.","The authors propose extending ChartAnchor to incorporate interactive and time-varying visualizations, including simulating interaction flows and temporal data changes. They suggest evaluating model capabilities for dynamic semantics and multi-state rendering to better reflect real-world analytical environments.","Future work could add explicit robustness evaluations for noisy renders (compression, blur), occlusions, and diverse typography to better approximate charts encountered in documents and dashboards. Expanding chart-to-code targets beyond Python (e.g., Vega-Lite, D3) and providing standardized parsers/extractors for all supported types would improve generalizability and comparability across toolchains. Providing an official evaluation package (with pinned dependencies and containerized execution) and more extensive real-world case studies would strengthen reproducibility and practical adoption.",2512.01017v2,local_papers/arxiv/2512.01017v2.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:11:59Z
TRUE,Univariate|Multivariate|Nonparametric|High-dimensional|Other,Shewhart|Machine learning-based|Other,Both,Manufacturing (general)|Theoretical/simulation only,FALSE,NA,NA,Simulation study|Other,False alarm rate|Detection probability|Other,Not discussed.,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/christopherburger/ConformalSPC,"The paper proposes a hybrid Statistical Process Control (SPC) framework that integrates conformal prediction to create distribution-free, model-agnostic monitoring tools with controlled long-run false alarm rates under an exchangeability (in-control) assumption. It introduces Conformal-Enhanced Control Charts that replace parametric 3-sigma limits with a conformal threshold based on calibration nonconformity scores, and also advocates plotting conformal prediction intervals over time to visualize uncertainty and detect “uncertainty spikes” as leading indicators of instability. It further proposes Conformal-Enhanced Process Monitoring for multivariate/high-dimensional settings by reframing MSPC as online anomaly detection and charting conformal p-values against a single significance threshold α. The methods are illustrated via simulated examples (including normal and exponential data) and conceptual comparisons to traditional Shewhart and Hotelling T²-style monitoring, emphasizing robustness to non-normality and interpretability for operators. Overall, it advances SPC by providing a practical pathway to distribution-free limits and unified monitoring metrics while retaining familiar chart-based workflows.","A nonconformity score is defined from in-control calibration data, e.g. for individuals $s(X_i)=|X_i-\mathrm{median}(X_1,\ldots,X_n)|$; for subgroups $s(\bar X_i)=|\bar X_i-\mathrm{median}(\bar X_1,\ldots,\bar X_n)|$; for variability $s(R_i)=|R_i-\mathrm{median}(R_1,\ldots,R_n)|$. The conformal control limit is the upper threshold $q$ given by the $\lceil(1-\alpha)(n+1)\rceil$-th smallest calibration score, and a new point signals if $s(X_{new})>q$. With a predictive model $\hat y(x)$, the score can be residual-based $s(x,y)=|y-\hat y(x)|$ (or normalized $|y-\hat y(x)|/\hat\sigma(x)$), yielding conformal intervals $[\hat y(x_{new})-q,\hat y(x_{new})+q]$; multivariate monitoring is summarized by a conformal p-value time series charted against a horizontal limit at $\alpha$.","The paper presents simulated chart illustrations comparing traditional Shewhart charts to conformal-enhanced charts for both normally distributed and exponentially distributed data, including an induced mean shift; it reports qualitatively that the conformal-enhanced chart shows greater sensitivity to true shifts while maintaining a user-chosen false alarm level $\alpha$ (e.g., $\alpha=0.0027$ to emulate 3-sigma behavior). It also demonstrates a conformal p-value chart concept for multivariate process snapshots, where points below $\alpha$ are flagged as anomalies and smaller p-values indicate stronger evidence. No numerical ARL/ATS tables or specific quantitative improvement percentages are provided in the excerpt; performance is conveyed primarily through plots and controlled-error-rate arguments. The framework highlights an additional “uncertainty spike” signal when using adaptive/normalized scores that widen conformal intervals under unfamiliar/noisy conditions.","The author notes that conformal guarantees depend on the quality and representativeness of the initial in-control calibration dataset; if calibration data are not truly in-control, the guarantees are compromised. The paper also states that the proposed nonconformity scores are simple and robust but that producing tight, informative prediction intervals is heuristic, and optimizing nonconformity scores for specific industrial applications remains an open area.","The approach relies on exchangeability (often violated by autocorrelation, drift, seasonality, and feedback control loops in industrial time series), but the paper does not spell out how to handle dependence (e.g., block/conformal methods or weighted conformal). For multivariate monitoring, the p-value chart’s performance will hinge strongly on the chosen anomaly detection model and score calibration; guidance on model selection, stability, and diagnostic decomposition to identify contributing variables is limited. There is little formal SPC performance characterization (e.g., in-control/out-of-control ARL, ATS, steady-state behavior) or head-to-head benchmarking against established nonparametric SPC charts and MSPC methods under standard shift scenarios. Practical issues such as recalibration, concept drift handling, and computational/latency considerations for online deployment are not fully developed.","Future work is suggested on developing more sophisticated, adaptive nonconformity scores that learn from process data to improve efficiency and interval tightness. The author also calls for applying and validating the proposed ideas in real manufacturing settings to demonstrate practical utility and robustness beyond simulations.","Develop time-series-aware conformal SPC variants that relax exchangeability, such as block/online conformal methods, conformalized residuals with ARMA/state-space modeling, or weighted conformal under gradual drift. Provide SPC-standard design guidance and theory (e.g., ARL/ATS approximations, steady-state vs. zero-state properties, and run rules) and compare systematically to EWMA/CUSUM, rank-based charts, and modern MSPC methods across common shift types (mean, variance, covariance, intermittent faults). Extend the multivariate p-value approach with interpretable diagnostics (variable attribution, contribution plots) and automated root-cause assistance. Package the methodology into reusable software (e.g., an R/Python library) with reproducible benchmarks and practitioner-facing calibration/recalibration protocols.",2512.23602v1,local_papers/arxiv/2512.23602v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:12:34Z
TRUE,Univariate|Nonparametric|Other,EWMA,Phase II,Manufacturing (general)|Healthcare/medical|Network/cybersecurity|Theoretical/simulation only,FALSE,FALSE,NA,Exact distribution theory|Simulation study,Other,Not discussed,TRUE,R,In text/Appendix,NA,"The paper derives exact, finite-sample expressions for the mean and time-varying variance of an EWMA statistic tailored to binomial monitoring in multiple independent data streams. It constructs a Cumulative Standardized Binomial EWMA (CSB-EWMA) by transforming raw stream observations into Bernoulli indicators based on exceedance of a known in-control median (p0 = 0.5), then aggregating to cumulative counts and standardizing. Using the exact variance (rather than early-phase asymptotic approximations), it defines adaptive, time-varying control limits that are statistically valid from the first observation and converge to constant ±L limits asymptotically. The theoretical derivations are validated via Monte Carlo simulation (10,000 replications) showing close agreement between theoretical and empirical mean/variance trajectories, with variance converging quickly to 1. The work provides a methodological foundation for distribution-free (median-based) binary-outcome monitoring across parallel streams in SPC contexts.","Binary recoding: $x_{ij}=I(y_{ij}>\mu_{0i})$ so that under in-control conditions $x_{ij}\sim\text{Bernoulli}(p_0=0.5)$ and per-time counts $C_t=\sum_{i=1}^k x_{it}\sim\text{Bin}(k,p_0)$. Cumulative count $Q_t=\sum_{j=1}^t C_j$ with $E[Q_t]=t\mu$ and $\operatorname{Var}(Q_t)=t\sigma^2$, where $\mu=kp_0$ and $\sigma^2=kp_0(1-p_0)$; standardized statistic $W_t=(Q_t-\mu t)/\sqrt{t\sigma^2}$. EWMA recursion $r_t=\lambda W_t+(1-\lambda)r_{t-1}$ gives $E[r_t]=(1-\lambda)^t r_0$ and an exact finite-$t$ variance based on $\operatorname{Cov}(W_i,W_j)=\sqrt{\min(i,j)/\max(i,j)}$; adaptive limits: $\text{UCL}_t=(1-\lambda)^t r_0+L\sqrt{\operatorname{Var}(r_t)}$, $\text{LCL}_t=(1-\lambda)^t r_0-L\sqrt{\operatorname{Var}(r_t)}$, approaching $\pm L$ as $t\to\infty$.","Monte Carlo validation with $N_{sim}=10{,}000$, $k=10$, $p_0=0.5$, $\lambda=0.2$, $r_0=0$, and $t_{max}=1000$ shows theoretical mean essentially 0 and simulated means near 0 across time (max absolute bias reported as $9.00\times 10^{-3}$; RMS bias $2.89\times 10^{-3}$). Theoretical vs simulated variances match closely: at $t=10$ (0.6369 vs 0.6385, 0.26% relative bias), $t=50$ (0.9512 vs 0.9359, -1.61%), $t=100$ (0.9768 vs 0.9758, -0.10%), $t=500$ (0.9955 vs 0.9967, 0.12%), and $t=1000$ (0.9978 vs 0.9996, 0.19%). The variance converges rapidly to its asymptotic value 1, reaching at least 0.99 by $t=227$ as reported. These results support the correctness of the exact finite-sample variance implementation and the resulting adaptive control limits.",None stated,"The development assumes independence across streams and (implicitly) across time; performance under autocorrelation, cross-stream dependence, or nonstationary in-control behavior is not assessed. The approach also assumes a known in-control median (and uses $p_0=0.5$ from median exceedance), leaving open how to estimate/maintain the median in practice (Phase I) and how estimation error affects false-alarm properties. The paper validates mean/variance moments but does not report run-length metrics (e.g., in-control/out-of-control ARL) or detection performance under specific shifts, which are central for chart design comparisons. Real-data case studies are not included, so practical robustness to ties, discretization, and data-quality issues is unclear.","The paper notes that the validation ensures a foundation for subsequent performance comparisons against asymptotic methods, implying future work on comparative performance evaluation of the exact-variance CSB-EWMA versus asymptotic approaches.","Evaluate chart performance using ARL/ATS/false-alarm rate under a range of out-of-control shifts (including small/large changes and transient shifts) and compare to alternative binomial/proportion EWMAs and CUSUMs. Extend the method to handle cross-stream dependence and temporal autocorrelation (e.g., via modeling, robust covariance adjustments, or block/bootstrap calibration). Develop a Phase I procedure to estimate the in-control median (or target exceedance probability) with uncertainty propagation into control limits, potentially yielding a self-starting version. Provide packaged software (e.g., an R package) and demonstrate the approach on real multi-stream applications (healthcare surveillance, manufacturing defect streams, cybersecurity event logs).",2601.09968v1,local_papers/arxiv/2601.09968v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:13:09Z
TRUE,Functional data analysis|Nonparametric|Other,Hotelling T-squared|Other,Phase II,Transportation/logistics|Other,FALSE,NA,FALSE,Simulation study|Case study (real dataset),ARL (Average Run Length)|Detection probability|False alarm rate|Other,"Uses subgrouping (e.g., daily segments in the case study) and recommends/uses $N_0=30$ distributions as training data; for rank-based charting uses short tuning data with default $m=30$ (historical/tuning length).",TRUE,None / Not applicable,Not provided,NA,"The paper proposes a warping function-based control chart for detecting distributional changes (including complex shape deformations) in damage-sensitive features (DSFs) from structural health monitoring data. DSF observations are first subgrouped and converted into a sequence of estimated PDFs; each PDF is then mapped to a warping function relative to a reference distribution, so both location shifts and shape changes are represented as changes in warping functions. Warping functions are transformed via the SRSF/tangent-space mapping, reduced by FPCA, and monitored using two features: a functional Hotelling’s $T^2$ statistic for variation in the principal subspace and an SPE statistic for residual variation. To improve robustness and avoid long Phase-I tuning, the charting statistic is a rank-based, nonparametric Y-max statistic built from standardized Mann–Whitney change-point statistics (Zhou et al., 2009), yielding the Warp-RANK-$T^2$ and Warp-RANK-SPE charts. Simulation studies show higher detection power than PDF-FPCA control charts and a Bayes-space change-point detector, and a real bridge case study (cable tension ratio distributions) demonstrates practical detection and change-point localization with robustness to outliers/transient anomalies.","Key steps: (i) Warping transformation between two PDFs $f,g$ on $[0,1]$ via a unique warping function $\gamma\in\Gamma$ such that $g(x)=f(\gamma(x))\dot\gamma(x)$ and equivalently $G(x)=F(\gamma(x))$, giving $\gamma(x)=F^{-1}(G(x))$. (ii) Warping functions to tangent space using SRSF $q(x)=\sqrt{\dot\gamma(x)}$ and mapping $v(x)=\frac{\theta}{\sin\theta}\big(q(x)-q_{id}(x)\cos\theta\big)$ with $\theta=\cos^{-1}\left(\int q_{id}(x)q(x)dx\right)$. (iii) FPCA scores yield monitoring features $T_k^2=\sum_{j=1}^K \xi_{k,j}^2/\rho_j$ and $SPE_k=\int (v_k(x)-\hat v_k(x))^2dx$; charting uses rank-based SMW/WMA and $Y_{\max}(m,n)=\max_j |Y(j,n)|$ compared to a control limit from Zhou et al. (2009).","In Simulation Study I (100 replications per setting), empirical detection power for small changes is substantially higher for the proposed method. For Scenario I (length 130, change at 100), detection power at $\delta=0.10$ is 0.60 (proposed) vs 0.02 (PDF-FPCA-CC) vs 0.04 (Bayes-CPD); at $\delta=0.15$ it is 0.94 vs 0.09 vs 0.04. For Scenario II (length 200), at $\delta=0.07$ it is 0.83 vs 0.11 vs 0.09; at $\delta=0.10$ it is 0.99 vs 0.16 vs 0.10. In Simulation Study II with injected outlying PDFs, the proposed rank-based chart correctly estimates the change point at $\hat\tau=200$ while the direct charting approach triggers false alarms at outlier locations; increasing smoothing parameter $\lambda$ (e.g., toward 0.30) reduces robustness and can induce false alarms. In the bridge case study (170 daily PDFs per cable pair, 30-day training), detected change points closely match visually assessed ones (SCP-a 53, SCP-b 61, SCP-c 101, SCP-d 119) with robustness to sporadic anomalies.",None stated.,"The method relies on PDF estimation from subgrouped data (kernel density estimation and boundary-corrected estimators), so performance may depend strongly on subgroup size/segmentation choice and bandwidth selection, but systematic sensitivity analysis is limited. It also assumes distributions can be supported on a common finite interval and uses preprocessing/mixing (e.g., $\epsilon=0.1$ with a uniform density) to avoid numerical instability; this may bias warping functions when true densities have near-zero regions. Control limits are taken from Zhou et al. (2009) tables rather than derived for the full two-chart procedure (Warp-RANK-$T^2$ and Warp-RANK-SPE used jointly), so the combined false-alarm behavior may differ from nominal unless adjusted.",None stated.,"Develop principled guidance for selecting subgroup size/segmentation and KDE bandwidth to optimize detection delay vs robustness, and study the combined (two-chart) in-control ARL when Warp-RANK-$T^2$ and Warp-RANK-SPE are used together. Extend the approach to explicitly handle serial dependence in DSF streams and irregular/missing sampling common in SHM. Provide open-source software to facilitate adoption and benchmarking against additional distributional monitoring methods (e.g., Wasserstein-based and energy-distance charts).",2601.12221v1,local_papers/arxiv/2601.12221v1.pdf,openai,gpt-5.2-2025-12-11,1,2026-01-27T00:13:53Z
